These experiments use a simplified setup: one map (Kairos Junction), one race match-up (Protoss versus Protoss), reinforcement learning and league experiments limited to 1010 steps, only main agents, and a 50%–50% mix of self-play and PFSP, unless stated otherwise (see Methods). The first column shows Elo ratings24 against ablation test agents (each rating was estimated with 11,000 full games of StarCraft II). a, b, Comparing different league compositions using Elo of the main agents (a) and relative population performance of the whole leagues (b), which measures exploitability. c, d, Comparing different multi-agent learning algorithms using Elo (c) and a proxy for forgetting: the minimum win rate against all past versions, averaged over time (d). Naive self-play has a high Elo, but is more forgetful. See Extended Data Fig. 5 for more in-depth comparison. e, Ablation study of the different mechanisms to use human data. Human init, supervised learning initialization of parameters of the neural network. g, APM limits relative to those used in AlphaStar. Reducing APM substantially reduces performance. Unexpectedly, increasing APM also reduces performance, possibly because the agent spends more effort on refining micro-tactics than on learning diverse strategies. f, h, Comparison of architectures using the win rate of supervised agents (trained in Protoss versus all) against the built-in elite bot. j, Elo scores of StarCraft II built-in bots. Ratings are anchored by a bot that never acts. i, k, Reinforcement learning ablations, measured by training a best response against fixed opponents to avoid multi-agent dynamics.