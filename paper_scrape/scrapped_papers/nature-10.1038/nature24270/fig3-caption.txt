a, Performance of self-play reinforcement learning. The plot shows the performance of each MCTS player  from each iteration i of reinforcement learning in AlphaGo Zero. Elo ratings were computed from evaluation games between different players, using 0.4 s of thinking time per move (see Methods). For comparison, a similar player trained by supervised learning from human data, using the KGS dataset, is also shown. b, Prediction accuracy on human professional moves. The plot shows the accuracy of the neural network, at each iteration of self-play i, in predicting human professional moves from the GoKifu dataset. The accuracy measures the percentage of positions in which the neural network assigns the highest probability to the human move. The accuracy of a neural network trained by supervised learning is also shown. c, Mean-squared error (MSE) of human professional game outcomes. The plot shows the MSE of the neural network, at each iteration of self-play i, in predicting the outcome of human professional games from the GoKifu dataset. The MSE is between the actual outcome z ∈ {−1, +1} and the neural network value v, scaled by a factor of  to the range of 0–1. The MSE of a neural network trained by supervised learning is also shown.