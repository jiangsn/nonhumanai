<html class="js" lang="en">
 <head>
  <link as="font" crossorigin="" href="/static/fonts/HardingText-Regular-Web-cecd90984f.woff2" rel="preload" type="font/woff2"/>
  <title>
   Clinically applicable deep learning for diagnosis and referral in retinal disease | Nature Medicine
  </title>
  <script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_SVG.js">
  </script>
  <script id="save-data-connection-testing">
   function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
  </script>
  <script data-consent="www-nature-com.proxy.lib.ohio-state.edu" src="/static/js/cookie-consent-es5-bundle-2b0f06c1e4.js">
  </script>
  <link crossorigin="" href="https://push-content.springernature.io" rel="preconnect"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="pc,mobile" name="applicable-device"/>
  <meta content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes" name="viewport"/>
  <meta content="5a2dc4ab3fcb9b0393241ffbbb490480" name="360-site-verification"/>
  <script data-test="dataLayer">
   window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"diagnosis;eye-manifestations;machine-learning;three-dimensional-imaging","webtrendsContentCategory":null,"webtrendsContentCollection":"Digital Medicine","webtrendsContentGroup":"Nature Medicine","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/s41591-018-0107-6"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Jeffrey De  Fauw","Joseph R. Ledsam","Bernardino Romera-Paredes","Stanislav Nikolov","Nenad Tomasev","Sam Blackwell","Harry Askham","Xavier Glorot","Brendan O’Donoghue","Daniel Visentin","George van den  Driessche","Balaji Lakshminarayanan","Clemens Meyer","Faith Mackinder","Simon Bouton","Kareem Ayoub","Reena Chopra","Dominic King","Alan Karthikesalingam","Cían O. Hughes","Rosalind Raine","Julian Hughes","Dawn A. Sim","Catherine Egan","Adnan Tufail","Hugh Montgomery","Demis Hassabis","Geraint Rees","Trevor Back","Peng T. Khaw","Mustafa Suleyman","Julien Cornebise","Pearse A. Keane","Olaf Ronneberger"],"publishedAt":1534118400,"publishedAtString":"2018-08-13","title":"Clinically applicable deep learning for diagnosis and referral in retinal disease","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"nm","title":"nature medicine","volume":"24","issue":"9"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":{"id":"egjifhdcih"}},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false},{"name":"getftr-entitled","active":false}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        ga4ServerUrl: 'https://collect-nature-com.proxy.lib.ohio-state.edu',
        imprint: 'nature'
    });
  </script>
  <script>
   (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
  </script>
  <style>
   @media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) { .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-editorial-summary__container .c-article-editorial-summary__button:focus{outline:3px solid #fece3e;will-change:transform}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:16px}.c-recommendations-title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.24;margin:0;padding-bottom:16px}.c-recommendations-close{background-color:transparent;border:0;cursor:pointer;height:2em;margin-right:-10px;margin-top:-5px;width:2em}.c-recommendations-authors{line-height:1.24;margin:0}.c-recommendations-list-container{position:relative}.c-recommendations-list{display:flex;flex-wrap:nowrap;justify-content:space-between;margin:0 auto;overflow-x:hidden;padding:0 0 16px;scroll-behavior:smooth;scroll-snap-type:x mandatory;width:calc(100% - 128px)}@media only screen and (max-width:539px){.c-recommendations-list{display:block;height:40vh;overflow-y:auto;width:100%}}.c-recommendations-list__item{display:flex;flex:0 0 calc(33.3333% - 24px);margin:0 24px 0 0;scroll-snap-align:center}@media only screen and (max-width:539px){.c-recommendations-list__item{margin:0;padding:0 0 16px}}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 16px 0 0;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #d5d5d5;height:auto;min-height:0;position:relative;transform:translateY(0)}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:#069;text-decoration:none}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}@media only screen and (max-width:539px){.c-recommendations-column-switch{display:flex;flex-direction:column-reverse}}.js-greyout-page-background{background-color:rgba(34,34,34,.75);bottom:0;left:0;position:fixed;right:0;top:0}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a{color:inherit}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px;padding:0 16px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px;padding:0}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:540px){.u-hide-at-sm{display:none;visibility:hidden}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-display-flex{display:flex;width:100%}.u-flex-direction-column{flex-direction:column}.u-justify-content-space-between{justify-content:space-between}.u-flex-static{flex:0 0 auto}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px} }
  </style>
  <link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/static/css/enhanced-article-nature-branded-30b9d5ba44.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
  <noscript>
   <link href="/static/css/enhanced-article-nature-branded-30b9d5ba44.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" rel="stylesheet" type="text/css"/>
  </noscript>
  <link href="/static/css/article-print-122346e276.css" media="print" rel="stylesheet" type="text/css"/>
  <link href="/static/images/favicons/nature/apple-touch-icon-f39cb19454.png" rel="apple-touch-icon" sizes="180x180"/>
  <link href="/static/images/favicons/nature/favicon-32x32-3fe59ece92.png" rel="icon" sizes="32x32" type="image/png"/>
  <link href="/static/images/favicons/nature/favicon-16x16-951651ab72.png" rel="icon" sizes="16x16" type="image/png"/>
  <link crossorigin="use-credentials" href="/static/manifest.json" rel="manifest"/>
  <link color="#000000" href="/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg" rel="mask-icon"/>
  <link href="/static/images/favicons/nature/favicon.ico" rel="shortcut icon"/>
  <meta content="#000000" name="msapplication-TileColor"/>
  <meta content="/static/browserconfig.xml" name="msapplication-config"/>
  <meta content="#000000" name="theme-color"/>
  <meta content="Nature" name="application-name"/>
  <script>
   (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
  </script>
  <!-- Google Tag Manager -->
  <script data-test="gtm-head">
   window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
  </script>
  <!-- End Google Tag Manager -->
  <script>
   (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp-static-nature-com.proxy.lib.ohio-state.edu/production_live/consent-bundle-8-23.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp-static-nature-com.proxy.lib.ohio-state.edu/production_live/consent-bundle-8-23.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-2b0f06c1e4.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
  </script>
  <script id="js-position0">
   (function(w, d) {
        w.idpVerifyPrefix = 'https://verify-nature-com.proxy.lib.ohio-state.edu';
        w.ra21Host = 'https://wayf-springernature-com.proxy.lib.ohio-state.edu';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        var polyfillsUrl = function() {
            var features = {
                'IntersectionObserver': window.IntersectionObserver,
                'Promise': window.Promise,
                'URLSearchParams': window.URLSearchParams,
                'Symbol.iterator': window.Symbol && Symbol.iterator,
                'Array.from': Array.from,
                'Array.prototype.includes': Array.prototype.includes,
                'Array.prototype.find': Array.prototype.find,
                'Array.prototype.forEach': Array.prototype.forEach,
                'NodeList.prototype.forEach': NodeList.prototype.forEach,
                'Element.prototype.closest': Element.prototype.closest,
                'Element.prototype.prepend': Element.prototype.prepend,
                'Element.prototype.remove': Element.prototype.remove,
                'Object.assign': Object.assign
            };
            var req = [];
            for (var feature in features) {
                if (Object.prototype.hasOwnProperty.call(features, feature) && !features[feature]) {
                    req.push(feature);
                }
            }
            if (req.length) {
                return 'https://polyfill.io/v3/polyfill.min.js?features=' + req.join('%2C') + '&flags=always';
            }
            return null;
        };

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    {src: polyfillsUrl(), test: 'polyfills-js', noinit: true},
                    
                        {src: '/static/js/global-article-es6-bundle-62a1428781.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-ca6a20c074.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-7850fbb459.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-bd334e676d.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-c634a291c7.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        
                            var conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-2626f1bdf8.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-bd14bd0747.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-2d5c465efd.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
  </script>
  <script data-test="global-article-js" defer="" id="js-position1" src="/static/js/global-article-es6-bundle-62a1428781.js">
  </script>
  <script data-test="shared-js" defer="" id="js-position2" src="/static/js/shared-es6-bundle-7850fbb459.js">
  </script>
  <script data-test="header-150-js" defer="" id="js-position3" src="/static/js/header-150-es6-bundle-5bb959eaa1.js">
  </script>
  <script async="" data-test="math-js" id="js-position4" src="/static/js/math-es6-bundle-bd14bd0747.js">
  </script>
  <meta content="noarchive" name="robots"/>
  <meta content="Yes" name="access"/>
  <link href="https://www-nature-com.proxy.lib.ohio-state.edu/search" rel="search"/>
  <link href="https://www-nature-com.proxy.lib.ohio-state.edu/opensearch/opensearch.xml" rel="search" title="nature.com" type="application/opensearchdescription+xml"/>
  <link href="https://www-nature-com.proxy.lib.ohio-state.edu/opensearch/request" rel="search" title="nature.com" type="application/sru+xml"/>
  <script type="application/ld+json">
   {"mainEntity":{"headline":"Clinically applicable deep learning for diagnosis and referral in retinal disease","description":"The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting. A novel deep learning architecture performs device-independent tissue segmentation of clinical 3D retinal images followed by separate diagnostic classification that meets or exceeds human expert clinical diagnoses of retinal disease.","datePublished":"2018-08-13","dateModified":"2018-08-13","pageStart":"1342","pageEnd":"1350","sameAs":"https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41591-018-0107-6","keywords":"Diagnosis,Eye manifestations,Machine learning,Three-dimensional imaging,Biomedicine,general,Cancer Research,Metabolic Diseases,Infectious Diseases,Molecular Medicine,Neurosciences","image":"https://static-content.springer.com/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig1_HTML.jpg","isPartOf":{"name":"Nature Medicine","issn":["1546-170X","1078-8956"],"volumeNumber":"24","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www-springernature-com.proxy.lib.ohio-state.edu/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Jeffrey De  Fauw","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Joseph R. Ledsam","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Bernardino Romera-Paredes","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Stanislav Nikolov","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Nenad Tomasev","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Sam Blackwell","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Harry Askham","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Xavier Glorot","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Brendan O’Donoghue","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Daniel Visentin","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"George van den  Driessche","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Balaji Lakshminarayanan","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Clemens Meyer","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Faith Mackinder","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Simon Bouton","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Kareem Ayoub","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Reena Chopra","url":"http://orcid.org/0000-0002-4264-8329","affiliation":[{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology","address":{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Dominic King","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Alan Karthikesalingam","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Cían O. Hughes","url":"http://orcid.org/0000-0001-6901-0985","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"},{"name":"University College London","address":{"name":"University College London, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Rosalind Raine","affiliation":[{"name":"University College London","address":{"name":"University College London, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Julian Hughes","affiliation":[{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology","address":{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Dawn A. Sim","affiliation":[{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology","address":{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Catherine Egan","affiliation":[{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology","address":{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Adnan Tufail","affiliation":[{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology","address":{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Hugh Montgomery","url":"http://orcid.org/0000-0001-8797-5019","affiliation":[{"name":"University College London","address":{"name":"University College London, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Demis Hassabis","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Geraint Rees","url":"http://orcid.org/0000-0002-9623-7007","affiliation":[{"name":"University College London","address":{"name":"University College London, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Trevor Back","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Peng T. Khaw","affiliation":[{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology","address":{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Mustafa Suleyman","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Julien Cornebise","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"},{"name":"University College London","address":{"name":"University College London, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Pearse A. Keane","url":"http://orcid.org/0000-0002-9239-745X","affiliation":[{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology","address":{"name":"NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"email":"pearse.keane@moorfields.nhs.uk","@type":"Person"},{"name":"Olaf Ronneberger","url":"http://orcid.org/0000-0002-4266-1515","affiliation":[{"name":"DeepMind","address":{"name":"DeepMind, London, UK","@type":"PostalAddress"},"@type":"Organization"}],"email":"olafr@deepmind.com","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}
  </script>
  <link href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6" rel="canonical"/>
  <meta content="41591" name="journal_id"/>
  <meta content="Clinically applicable deep learning for diagnosis and referral in retinal disease" name="dc.title"/>
  <meta content="Nature Medicine 2018 24:9" name="dc.source"/>
  <meta content="text/html" name="dc.format"/>
  <meta content="Nature Publishing Group" name="dc.publisher"/>
  <meta content="2018-08-13" name="dc.date"/>
  <meta content="OriginalPaper" name="dc.type"/>
  <meta content="En" name="dc.language"/>
  <meta content="2018 The Author(s)" name="dc.copyright"/>
  <meta content="2018 The Author(s)" name="dc.rights"/>
  <meta content="journalpermissions@springernature.com" name="dc.rightsAgent"/>
  <meta content="The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting. A novel deep learning architecture performs device-independent tissue segmentation of clinical 3D retinal images followed by separate diagnostic classification that meets or exceeds human expert clinical diagnoses of retinal disease." name="dc.description"/>
  <meta content="1546-170X" name="prism.issn"/>
  <meta content="Nature Medicine" name="prism.publicationName"/>
  <meta content="2018-08-13" name="prism.publicationDate"/>
  <meta content="24" name="prism.volume"/>
  <meta content="9" name="prism.number"/>
  <meta content="OriginalPaper" name="prism.section"/>
  <meta content="1342" name="prism.startingPage"/>
  <meta content="1350" name="prism.endingPage"/>
  <meta content="2018 The Author(s)" name="prism.copyright"/>
  <meta content="journalpermissions@springernature.com" name="prism.rightsAgent"/>
  <meta content="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6" name="prism.url"/>
  <meta content="doi:10.1038/s41591-018-0107-6" name="prism.doi"/>
  <meta content="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6.pdf" name="citation_pdf_url"/>
  <meta content="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6" name="citation_fulltext_html_url"/>
  <meta content="Nature Medicine" name="citation_journal_title"/>
  <meta content="Nat Med" name="citation_journal_abbrev"/>
  <meta content="Nature Publishing Group" name="citation_publisher"/>
  <meta content="1546-170X" name="citation_issn"/>
  <meta content="Clinically applicable deep learning for diagnosis and referral in retinal disease" name="citation_title"/>
  <meta content="24" name="citation_volume"/>
  <meta content="9" name="citation_issue"/>
  <meta content="2018/09" name="citation_publication_date"/>
  <meta content="2018/08/13" name="citation_online_date"/>
  <meta content="1342" name="citation_firstpage"/>
  <meta content="1350" name="citation_lastpage"/>
  <meta content="Article" name="citation_article_type"/>
  <meta content="en" name="citation_language"/>
  <meta content="doi:10.1038/s41591-018-0107-6" name="dc.identifier"/>
  <meta content="10.1038/s41591-018-0107-6" name="DOI"/>
  <meta content="162601" name="size"/>
  <meta content="10.1038/s41591-018-0107-6" name="citation_doi"/>
  <meta content="http://api.springer-com.proxy.lib.ohio-state.edu/xmldata/jats?q=doi:10.1038/s41591-018-0107-6&amp;api_key=" name="citation_springer_api_url"/>
  <meta content="The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting. A novel deep learning architecture performs device-independent tissue segmentation of clinical 3D retinal images followed by separate diagnostic classification that meets or exceeds human expert clinical diagnoses of retinal disease." name="description"/>
  <meta content="De  Fauw, Jeffrey" name="dc.creator"/>
  <meta content="Ledsam, Joseph R." name="dc.creator"/>
  <meta content="Romera-Paredes, Bernardino" name="dc.creator"/>
  <meta content="Nikolov, Stanislav" name="dc.creator"/>
  <meta content="Tomasev, Nenad" name="dc.creator"/>
  <meta content="Blackwell, Sam" name="dc.creator"/>
  <meta content="Askham, Harry" name="dc.creator"/>
  <meta content="Glorot, Xavier" name="dc.creator"/>
  <meta content="O’Donoghue, Brendan" name="dc.creator"/>
  <meta content="Visentin, Daniel" name="dc.creator"/>
  <meta content="van den  Driessche, George" name="dc.creator"/>
  <meta content="Lakshminarayanan, Balaji" name="dc.creator"/>
  <meta content="Meyer, Clemens" name="dc.creator"/>
  <meta content="Mackinder, Faith" name="dc.creator"/>
  <meta content="Bouton, Simon" name="dc.creator"/>
  <meta content="Ayoub, Kareem" name="dc.creator"/>
  <meta content="Chopra, Reena" name="dc.creator"/>
  <meta content="King, Dominic" name="dc.creator"/>
  <meta content="Karthikesalingam, Alan" name="dc.creator"/>
  <meta content="Hughes, Cían O." name="dc.creator"/>
  <meta content="Raine, Rosalind" name="dc.creator"/>
  <meta content="Hughes, Julian" name="dc.creator"/>
  <meta content="Sim, Dawn A." name="dc.creator"/>
  <meta content="Egan, Catherine" name="dc.creator"/>
  <meta content="Tufail, Adnan" name="dc.creator"/>
  <meta content="Montgomery, Hugh" name="dc.creator"/>
  <meta content="Hassabis, Demis" name="dc.creator"/>
  <meta content="Rees, Geraint" name="dc.creator"/>
  <meta content="Back, Trevor" name="dc.creator"/>
  <meta content="Khaw, Peng T." name="dc.creator"/>
  <meta content="Suleyman, Mustafa" name="dc.creator"/>
  <meta content="Cornebise, Julien" name="dc.creator"/>
  <meta content="Keane, Pearse A." name="dc.creator"/>
  <meta content="Ronneberger, Olaf" name="dc.creator"/>
  <meta content="Diagnosis" name="dc.subject"/>
  <meta content="Eye manifestations" name="dc.subject"/>
  <meta content="Machine learning" name="dc.subject"/>
  <meta content="Three-dimensional imaging" name="dc.subject"/>
  <meta content="OECD. Computed tomography (CT) exams (indicator). (2017); 
https://doi-org.proxy.lib.ohio-state.edu/10.1787/3c994537-en


" name="citation_reference"/>
  <meta content="OECD. Magnetic resonance imaging (MRI) exams (indicator). (2017). 
https://doi-org.proxy.lib.ohio-state.edu/10.1787/1d89353f-en


" name="citation_reference"/>
  <meta content="citation_journal_title=Eye; citation_title=Surveillance of sight loss due to delay in ophthalmic treatment or review: frequency, cause and outcome; citation_author=B Foot, C MacEwen; citation_volume=31; citation_publication_date=2017; citation_pages=771-775; citation_doi=10.1038/eye.2017.1; citation_id=CR3" name="citation_reference"/>
  <meta content="citation_journal_title=Br. J. Ophthalmol.; citation_title=The estimated prevalence and incidence of late stage age related macular degeneration in the UK; citation_author=CG Owen; citation_volume=96; citation_publication_date=2012; citation_pages=752-756; citation_doi=10.1136/bjophthalmol-2011-301109; citation_id=CR4" name="citation_reference"/>
  <meta content="citation_journal_title=Am. J. Ophthalmol.; citation_title=Incidence of late-stage age-related macular degeneration in American whites: systematic review and meta-analysis; citation_author=AR Rudnicka; citation_volume=160; citation_publication_date=2015; citation_pages=85-93; citation_doi=10.1016/j.ajo.2015.04.003; citation_id=CR5" name="citation_reference"/>
  <meta content="citation_journal_title=Lancet Glob. Health; citation_title=Magnitude, temporal trends, and projections of the global prevalence of blindness and distance and near vision impairment: a systematic review and meta-analysis; citation_author=RRA Bourne; citation_volume=5; citation_publication_date=2017; citation_pages=e888-e897; citation_doi=10.1016/S2214-109X(17)30293-0; citation_id=CR6" name="citation_reference"/>
  <meta content="citation_journal_title=Eye; citation_title=A view of the current and future role of optical coherence tomography in the management of age-related macular degeneration; citation_author=U Schmidt-Erfurth, S Klimscha, SM Waldstein, H Bogunović; citation_volume=31; citation_publication_date=2017; citation_pages=26-44; citation_doi=10.1038/eye.2016.227; citation_id=CR7" name="citation_reference"/>
  <meta content="citation_journal_title=J. Am. Med. Assoc.; citation_title=Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs; citation_author=V Gulshan; citation_volume=316; citation_publication_date=2016; citation_pages=2402-2410; citation_doi=10.1001/jama.2016.17216; citation_id=CR8" name="citation_reference"/>
  <meta content="citation_journal_title=Nature; citation_title=Dermatologist-level classification of skin cancer with deep neural networks; citation_author=A Esteva; citation_volume=542; citation_publication_date=2017; citation_pages=115–-118; citation_doi=10.1038/nature21056; citation_id=CR9" name="citation_reference"/>
  <meta content="citation_journal_title=Science; citation_title=Optical coherence tomography; citation_author=D Huang; citation_volume=254; citation_publication_date=1991; citation_pages=1178-1181; citation_doi=10.1126/science.1957169; citation_id=CR10" name="citation_reference"/>
  <meta content="citation_journal_title=Eye; citation_title=How to defuse a demographic time bomb: the way forward?; citation_author=JC Buchan; citation_volume=31; citation_publication_date=2017; citation_pages=1519-1522; citation_doi=10.1038/eye.2017.114; citation_id=CR11" name="citation_reference"/>
  <meta content="citation_journal_title=Telemed. J. E Health; citation_title=A modeled economic analysis of a digital teleophthalmology system as used by three federal healthcare agencies for detecting proliferative diabetic retinopathy; citation_author=JD Whited; citation_volume=11; citation_publication_date=2005; citation_pages=641-651; citation_doi=10.1089/tmj.2005.11.641; citation_id=CR12" name="citation_reference"/>
  <meta content="Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: convolutional networks for biomedical image segmentation. in Navab N., Hornegger J., Wells W., Frangi A. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol. 9351 (Springer, Cham, Switzerland, 2015)." name="citation_reference"/>
  <meta content="Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. in Ourselin, S., Joskowicz, L., Sabuncu, M., Unal, G., Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science, vol. 9901 (Springer, Cham, Switzerland; 2016)." name="citation_reference"/>
  <meta content="citation_journal_title=Graefes Arch. Clin. Exp. Ophthalmol.; citation_title=Delay between medical indication to anti-VEGF treatment in age-related macular degeneration can result in a loss of visual acuity; citation_author=PS Muether, MM Hermann, K Koch, S Fauser; citation_volume=249; citation_publication_date=2011; citation_pages=633-637; citation_doi=10.1007/s00417-010-1520-9; citation_id=CR15" name="citation_reference"/>
  <meta content="citation_journal_title=Eye; citation_title=Delay in treating age-related macular degeneration in Spain is associated with progressive vision loss; citation_author=L Arias; citation_volume=23; citation_publication_date=2009; citation_pages=326-333; citation_doi=10.1038/sj.eye.6703053; citation_id=CR16" name="citation_reference"/>
  <meta content="citation_journal_title=Biomed. Opt. Express; citation_title=Transfer learning based classification of optical coherence tomography images with diabetic macular edema and dry age-related macular degeneration; citation_author=SPK Karri, D Chakraborty, J Chatterjee; citation_volume=8; citation_publication_date=2017; citation_pages=579-592; citation_doi=10.1364/BOE.8.000579; citation_id=CR17" name="citation_reference"/>
  <meta content="Apostolopoulos, S., Ciller, C., De Zanet, S. I., Wolf, S. &amp; Sznitman, R. RetiNet: automatic AMD identification in OCT volumetric data. Preprint at 
http://arxiv-org.proxy.lib.ohio-state.edu/abs/1610.03628v1

 (2016)." name="citation_reference"/>
  <meta content="citation_journal_title=Ophthalmology; citation_title=Quantitative classification of eyes with and without intermediate age-related macular degeneration using optical coherence tomography; citation_author=S Farsiu; citation_volume=121; citation_publication_date=2014; citation_pages=162-172; citation_doi=10.1016/j.ophtha.2013.07.013; citation_id=CR19" name="citation_reference"/>
  <meta content="citation_journal_title=Biomed. Opt. Express; citation_title=Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images; citation_author=PP Srinivasan; citation_volume=5; citation_publication_date=2014; citation_pages=3568-3577; citation_doi=10.1364/BOE.5.003568; citation_id=CR20" name="citation_reference"/>
  <meta content="citation_journal_title=Ophthalmol. Retin.; citation_title=Deep learning is effective for classifying normal versus age-related macular degeneration OCT images; citation_author=CS Lee, DM Baughman, AY Lee; citation_volume=1; citation_publication_date=2017; citation_pages=322-327; citation_doi=10.1016/j.oret.2016.12.009; citation_id=CR21" name="citation_reference"/>
  <meta content="citation_journal_title=Biomed. Opt. Express; citation_title=Automatic segmentation of nine retinal layer boundaries in OCT images of non-exudative AMD patients using deep learning and graph search; citation_author=L Fang; citation_volume=8; citation_publication_date=2017; citation_pages=2732-2744; citation_doi=10.1364/BOE.8.002732; citation_id=CR22" name="citation_reference"/>
  <meta content="citation_journal_title=Biomed. Opt. Express; citation_title=Deep-learning based, automated segmentation of macular edema in optical coherence tomography; citation_author=CS Lee; citation_volume=8; citation_publication_date=2017; citation_pages=3440-3448; citation_doi=10.1364/BOE.8.003440; citation_id=CR23" name="citation_reference"/>
  <meta content="Lu, D. et al. Retinal fluid segmentation and detection in optical coherence tomography images using fully convolutional neural network. Preprint at 
http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.04778v1

 (2017)." name="citation_reference"/>
  <meta content="citation_journal_title=Biomed. Opt. Express; citation_title=ReLayNet: retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional network; citation_author=AG Roy; citation_volume=8; citation_publication_date=2017; citation_pages=3627-3642; citation_doi=10.1364/BOE.8.003627; citation_id=CR25" name="citation_reference"/>
  <meta content="citation_journal_title=Nature; citation_title=Can we open the black box of AI?; citation_author=D Castelvecchi; citation_volume=538; citation_publication_date=2016; citation_pages=20-23; citation_doi=10.1038/538020a; citation_id=CR26" name="citation_reference"/>
  <meta content="citation_journal_title=Ophthalmol. Retin.; citation_title=Machine learning to analyze the prognostic value of current imaging biomarkers in neovascular age-related macular degeneration; citation_author=U Schmidt-Erfurth; citation_volume=2; citation_publication_date=2018; citation_pages=24-30; citation_doi=10.1016/j.oret.2017.03.015; citation_id=CR27" name="citation_reference"/>
  <meta content="citation_journal_title=Ophthalmology; citation_title=Fully automated detection and quantification of macular fluid in OCT using deep learning; citation_author=T Schlegl; citation_volume=125; citation_publication_date=2018; citation_pages=549-558; citation_doi=10.1016/j.ophtha.2017.10.031; citation_id=CR28" name="citation_reference"/>
  <meta content="citation_journal_title=Saudi J. Ophthalmol.; citation_title=Predicting visual outcomes for macular disease using optical coherence tomography; citation_author=PA Keane, SR Sadda; citation_volume=25; citation_publication_date=2011; citation_pages=145-158; citation_doi=10.1016/j.sjopt.2011.01.003; citation_id=CR29" name="citation_reference"/>
  <meta content="citation_journal_title=Ophthalmology; citation_title=Anatomic clinical trial endpoints for nonexudative age-related macular degeneration; citation_author=KB Schaal, PJ Rosenfeld, G Gregori, Z Yehoshua, WJ Feuer; citation_volume=123; citation_publication_date=2016; citation_pages=1060-1079; citation_doi=10.1016/j.ophtha.2016.01.034; citation_id=CR30" name="citation_reference"/>
  <meta content="citation_journal_title=Prog. Retin. Eye Res.; citation_title=A paradigm shift in imaging biomarkers in neovascular age-related macular degeneration; citation_author=U Schmidt-Erfurth, SM Waldstein; citation_volume=50; citation_publication_date=2016; citation_pages=1-24; citation_doi=10.1016/j.preteyeres.2015.07.007; citation_id=CR31" name="citation_reference"/>
  <meta content="citation_journal_title=Invest. Ophthalmol. Vis. Sci.; citation_title=Decade-long profile of imaging biomarker use in ophthalmic clinical trials; citation_author=E Villani; citation_volume=58; citation_publication_date=2017; citation_pages=BIO76-BIO81; citation_doi=10.1167/iovs.17-21790; citation_id=CR32" name="citation_reference"/>
  <meta content="citation_journal_title=Transl. Vis. Sci. Technol.; citation_title=Human factor and usability testing of a binocular optical coherence tomography system; citation_author=R Chopra, PJ Mulholland, AM Dubis, RS Anderson, PA Keane; citation_volume=6; citation_publication_date=2017; citation_pages=16; citation_doi=10.1167/tvst.6.4.16; citation_id=CR33" name="citation_reference"/>
  <meta content="citation_journal_title=Nat. Methods; citation_title=Fiji: an open-source platform for biological-image analysis; citation_author=J Schindelin; citation_volume=9; citation_publication_date=2012; citation_pages=676-682; citation_doi=10.1038/nmeth.2019; citation_id=CR34" name="citation_reference"/>
  <meta content="citation_journal_title=Surv. Ophthalmol.; citation_title=Evaluation of age-related macular degeneration with optical coherence tomography; citation_author=PA Keane; citation_volume=57; citation_publication_date=2012; citation_pages=389-414; citation_doi=10.1016/j.survophthal.2012.01.006; citation_id=CR35" name="citation_reference"/>
  <meta content="citation_journal_title=Ophthalmology; citation_title=Comparison of optical coherence tomography assessments in the comparison of age-related macular degeneration treatments trials; citation_author=FA Folgar; citation_volume=121; citation_publication_date=2014; citation_pages=1956-1965; citation_doi=10.1016/j.ophtha.2014.04.020; citation_id=CR36" name="citation_reference"/>
  <meta content="Duker, J. S., Waheed, N. K. &amp; Goldman, D. Handbook of Retinal OCT: Optical Coherence Tomography E-Book (Elsevier Health Sciences, Oxford, UK; 2013)." name="citation_reference"/>
  <meta content="Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &amp; Wojna, Z. Rethinking the inception architecture for computer vision. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2818–2826 (2016)." name="citation_reference"/>
  <meta content="Abadi, M. et al. TensorFlow: large-scale machine learning on heterogeneous systems. Preprint at 
https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467

 (2016)." name="citation_reference"/>
  <meta content="Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. in Proceedings of the 3rd International Conference on Learning Representations (ICLR). Preprint at 
http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980

 (2015)." name="citation_reference"/>
  <meta content="Huang, G., Liu, Z., Weinberger, K. Q. &amp; van der Maaten, L. Densely connected convolutional networks. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2261–2269 (2017)." name="citation_reference"/>
  <meta content="Lakshminarayanan, B., Pritzel, A. &amp; Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. Adv. Neural Inf. Process. Syst. 6405–6416 (2017)." name="citation_reference"/>
  <meta content="citation_journal_title=F1000Res; citation_title=Automated analysis of retinal imaging using machine learning techniques for computer vision; citation_author=J Fauw; citation_volume=5; citation_publication_date=2016; citation_pages=1573; citation_doi=10.12688/f1000research.8996.1; citation_id=CR43" name="citation_reference"/>
  <meta content="De  Fauw, Jeffrey" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Ledsam, Joseph R." name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Romera-Paredes, Bernardino" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Nikolov, Stanislav" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Tomasev, Nenad" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Blackwell, Sam" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Askham, Harry" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Glorot, Xavier" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="O’Donoghue, Brendan" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Visentin, Daniel" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="van den  Driessche, George" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Lakshminarayanan, Balaji" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Meyer, Clemens" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Mackinder, Faith" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Bouton, Simon" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Ayoub, Kareem" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Chopra, Reena" name="citation_author"/>
  <meta content="NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK" name="citation_author_institution"/>
  <meta content="King, Dominic" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Karthikesalingam, Alan" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Hughes, Cían O." name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="University College London, London, UK" name="citation_author_institution"/>
  <meta content="Raine, Rosalind" name="citation_author"/>
  <meta content="University College London, London, UK" name="citation_author_institution"/>
  <meta content="Hughes, Julian" name="citation_author"/>
  <meta content="NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK" name="citation_author_institution"/>
  <meta content="Sim, Dawn A." name="citation_author"/>
  <meta content="NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK" name="citation_author_institution"/>
  <meta content="Egan, Catherine" name="citation_author"/>
  <meta content="NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK" name="citation_author_institution"/>
  <meta content="Tufail, Adnan" name="citation_author"/>
  <meta content="NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK" name="citation_author_institution"/>
  <meta content="Montgomery, Hugh" name="citation_author"/>
  <meta content="University College London, London, UK" name="citation_author_institution"/>
  <meta content="Hassabis, Demis" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Rees, Geraint" name="citation_author"/>
  <meta content="University College London, London, UK" name="citation_author_institution"/>
  <meta content="Back, Trevor" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Khaw, Peng T." name="citation_author"/>
  <meta content="NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK" name="citation_author_institution"/>
  <meta content="Suleyman, Mustafa" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="Cornebise, Julien" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="University College London, London, UK" name="citation_author_institution"/>
  <meta content="Keane, Pearse A." name="citation_author"/>
  <meta content="NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK" name="citation_author_institution"/>
  <meta content="Ronneberger, Olaf" name="citation_author"/>
  <meta content="DeepMind, London, UK" name="citation_author_institution"/>
  <meta content="https://www-nature-com.proxy.lib.ohio-state.edu/platform/readcube-access" name="access_endpoint"/>
  <meta content="@naturemedicine" name="twitter:site"/>
  <meta content="summary_large_image" name="twitter:card"/>
  <meta content="Content cover image" name="twitter:image:alt"/>
  <meta content="Clinically applicable deep learning for diagnosis and referral in retinal disease" name="twitter:title"/>
  <meta content="Nature Medicine - A novel deep learning architecture performs device-independent tissue segmentation of clinical 3D retinal images followed by separate diagnostic classification that meets or..." name="twitter:description"/>
  <meta content="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig1_HTML.jpg" name="twitter:image"/>
  <meta content="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6" property="og:url"/>
  <meta content="article" property="og:type"/>
  <meta content="Nature" property="og:site_name"/>
  <meta content="Clinically applicable deep learning for diagnosis and referral in retinal disease - Nature Medicine" property="og:title"/>
  <meta content="A novel deep learning architecture performs device-independent tissue segmentation of clinical 3D retinal images followed by separate diagnostic classification that meets or exceeds human expert clinical diagnoses of retinal disease." property="og:description"/>
  <meta content="https://media-springernature-com.proxy.lib.ohio-state.edu/m685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig1_HTML.jpg" property="og:image"/>
  <script>
   window.eligibleForRa21 = 'true';
  </script>
  <script async="" src="https://injections.readcube.com/nature/inject.ef8e25d3.js" type="text/javascript">
  </script>
  <style type="text/css">
   .c-cookie-banner {
			background-color: #01324b;
			color: white;
			font-size: 1rem;
			position: fixed;
			bottom: 0;
			left: 0;
			right: 0;
			padding: 16px 0;
			font-family: sans-serif;
			z-index: 100002;
			text-align: center;
		}
		.c-cookie-banner__container {
			margin: 0 auto;
			max-width: 1280px;
			padding: 0 16px;
		}
		.c-cookie-banner p {
			margin-bottom: 8px;
		}
		.c-cookie-banner p:last-child {
			margin-bottom: 0;
		}	
		.c-cookie-banner__dismiss {
			background-color: transparent;
			border: 0;
			padding: 0;
			margin-left: 4px;
			color: inherit;
			text-decoration: underline;
			font-size: inherit;
		}
		.c-cookie-banner__dismiss:hover {
			text-decoration: none;
		}
  </style>
  <link href="https://injections.readcube.com/styles/nature_checkout.7bec98da.css" rel="stylesheet" type="text/css"/>
  <style type="text/css">
   .MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
  </style>
  <style type="text/css">
   #MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
  </style>
  <style type="text/css">
   .MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
  </style>
  <style type="text/css">
   #MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
  </style>
  <style type="text/css">
   .MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
  </style>
  <style type="text/css">
   .MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <script>
   window.dataLayer = window.dataLayer || [];
            window.dataLayer.push({
                recommendations: {
                    recommender: 'semantic',
                    model: 'specter',
                    policy_id: 'speedy-BootstrappedUCB',
                    timestamp: 1698033328,
                    embedded_user: 'null'
                }
            });
  </script>
 </head>
 <body class="article-page">
  <div id="MathJax_Message" style="">
   Loading [MathJax]/jax/output/HTML-CSS/config.js
  </div>
  <noscript>
   <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0">
   </iframe>
  </noscript>
  <div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
   <a class="c-skip-link" href="#content">
    Skip to main content
   </a>
   <div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
     <p>
      Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.
     </p>
    </div>
   </div>
   <div class="u-lazy-ad-wrapper u-mbs-0">
    <div class="deferred-placeholder" data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container" data-replace="true">
    </div>
    <aside class="c-ad c-ad--728x90">
     <div class="c-ad__inner" data-container-type="banner-advert">
      <p class="c-ad__label">
       Advertisement
      </p>
      <div class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide" data-ad-type="top" data-gpt="" data-gpt-sizes="728x90" data-gpt-targeting="type=article;pos=top;artid=s41591-018-0107-6;doi=10.1038/s41591-018-0107-6;subjmeta=114,1305,139,1421,1482,1807,2025,631,692,700;kwrd=Diagnosis,Eye+manifestations,Machine+learning,Three-dimensional+imaging" data-gpt-unitpath="/285/medicine.nature.com/article" data-pa11y-ignore="" data-test="top-ad" id="div-gpt-ad-top-1">
       <noscript>
        <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/medicine.nature.com/article&amp;sz=728x90&amp;c=-692299853&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41591-018-0107-6%26doi%3D10.1038/s41591-018-0107-6%26subjmeta%3D114,1305,139,1421,1482,1807,2025,631,692,700%26kwrd%3DDiagnosis,Eye+manifestations,Machine+learning,Three-dimensional+imaging">
         <img alt="Advertisement" data-test="gpt-advert-fallback-img" height="90" src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/medicine.nature.com/article&amp;sz=728x90&amp;c=-692299853&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41591-018-0107-6%26doi%3D10.1038/s41591-018-0107-6%26subjmeta%3D114,1305,139,1421,1482,1807,2025,631,692,700%26kwrd%3DDiagnosis,Eye+manifestations,Machine+learning,Three-dimensional+imaging" width="728"/>
        </a>
       </noscript>
      </div>
     </div>
    </aside>
   </div>
   <header class="c-header" data-header="" data-track-component="nature-150-split-header" id="header" style="border-color:#e40428">
    <div class="c-header__row">
     <div class="c-header__container">
      <div class="c-header__split">
       <div class="c-header__logo-container">
        <a data-track="click" data-track-action="home" data-track-label="image" href="/nm">
         <picture class="c-header__logo">
          <source media="(min-width: 875px)" srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/full/nature-cms/uploads/product/nm/header-95e59e63930e5d6009bad2c23a42ab2d.svg"/>
          <img alt="Nature Medicine" height="32" src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/nature-cms/uploads/product/nm/header-95e59e63930e5d6009bad2c23a42ab2d.svg"/>
         </picture>
        </a>
       </div>
       <ul class="c-header__menu c-header__menu--global">
        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
         <a class="c-header__link" data-test="siteindex-link" data-track="click" data-track-action="open nature research index" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/siteindex">
          <span>
           View all journals
          </span>
         </a>
        </li>
        <li class="c-header__item c-header__item--padding c-header__item--pipe">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button" href="javascript:;" role="button">
          <span>
           Search
          </span>
          <svg aria-hidden="true" focusable="false" height="22" role="img" viewbox="0 0 18 18" width="22" xmlns="http://www.w3.org/2000/svg">
           <path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z">
           </path>
          </svg>
         </a>
         <div class="c-header__dropdown c-header__dropdown--full-width has-tethered u-js-hide" data-track-component="nature-150-split-header" hidden="" id="search-menu">
          <div class="c-header__container">
           <h2 class="c-header__visually-hidden">
            Search
           </h2>
           <form action="/search" autocomplete="off" class="c-header__search-form" data-test="inline-search" method="get" role="search">
            <label class="c-header__heading" for="keywords">
             Search articles by subject, keyword or author
            </label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
             <div>
              <input class="c-header__input" id="keywords" name="q" required="" type="text" value=""/>
             </div>
             <div class="c-header__search-layout">
              <div>
               <label class="c-header__visually-hidden" for="results-from">
                Show results from
               </label>
               <select class="c-header__select" id="results-from" name="journal">
                <option selected="" value="">
                 All journals
                </option>
                <option value="nm">
                 This journal
                </option>
               </select>
              </div>
              <div>
               <button class="c-header__search-button" type="submit">
                Search
               </button>
              </div>
             </div>
            </div>
           </form>
           <div class="c-header__flush">
            <a class="c-header__link" data-track="click" data-track-action="advanced search" data-track-label="link" href="/search/advanced">
             Advanced search
            </a>
           </div>
           <h3 class="c-header__heading c-header__heading--keyline">
            Quick links
           </h3>
           <ul class="c-header__list">
            <li>
             <a class="c-header__link" data-track="click" data-track-action="explore articles by subject" data-track-label="link" href="/subjects">
              Explore articles by subject
             </a>
            </li>
            <li>
             <a class="c-header__link" data-track="click" data-track-action="find a job" data-track-label="link" href="/naturecareers">
              Find a job
             </a>
            </li>
            <li>
             <a class="c-header__link" data-track="click" data-track-action="guide to authors" data-track-label="link" href="/authors/index.html">
              Guide to authors
             </a>
            </li>
            <li>
             <a class="c-header__link" data-track="click" data-track-action="editorial policies" data-track-label="link" href="/authors/editorial_policies/">
              Editorial policies
             </a>
            </li>
           </ul>
          </div>
         </div>
        </li>
        <li class="c-header__item c-header__item--padding">
         <a class="c-header__link eds-c-header__link" href="https://idp-nature-com.proxy.lib.ohio-state.edu/auth/personal/springernature?redirect_uri=https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6" id="identity-account-widget">
          Log in
         </a>
        </li>
       </ul>
      </div>
     </div>
    </div>
    <div class="c-header__row">
     <div class="c-header__container" data-test="navigation-row">
      <div class="c-header__split">
       <ul class="c-header__menu c-header__menu--journal">
        <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" data-test="menu-button--explore" data-track="click" data-track-action="open explore expander" data-track-label="button" href="javascript:;" role="button">
          <span>
           <span class="c-header__show-text">
            Explore
           </span>
           content
          </span>
          <svg aria-hidden="true" focusable="false" height="16" role="img" viewbox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg">
           <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)">
           </path>
          </svg>
         </a>
         <nav aria-labelledby="Explore-content" class="c-header__dropdown has-tethered u-js-hide" data-test="Explore-content" data-track-component="nature-150-split-header" hidden="" id="explore">
          <div class="c-header__container">
           <h2 class="c-header__heading c-header__heading--js-hide" id="Explore-content">
            Explore content
           </h2>
           <ul class="c-header__list c-header__list--js-stack">
            <li class="c-header__item">
             <a class="c-header__link" data-test="explore-nav-item" data-track="click" data-track-action="research articles" data-track-label="link" href="/nm/research-articles">
              Research articles
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-test="explore-nav-item" data-track="click" data-track-action="reviews &amp; analysis" data-track-label="link" href="/nm/reviews-and-analysis">
              Reviews &amp; Analysis
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-test="explore-nav-item" data-track="click" data-track-action="news &amp; comment" data-track-label="link" href="/nm/news-and-comment">
              News &amp; Comment
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-test="explore-nav-item" data-track="click" data-track-action="podcasts" data-track-label="link" href="/nm/podcast">
              Podcasts
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-test="explore-nav-item" data-track="click" data-track-action="current issue" data-track-label="link" href="/nm/current-issue">
              Current issue
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-test="explore-nav-item" data-track="click" data-track-action="collections" data-track-label="link" href="/nm/collections">
              Collections
             </a>
            </li>
           </ul>
           <ul class="c-header__list c-header__list--js-stack">
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="facebook" data-track-label="link" href="https://www.facebook.com/Nature-Medicine-193691346949/">
              Follow us on Facebook
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="twitter" data-track-label="link" href="https://twitter.com/naturemedicine">
              Follow us on Twitter
             </a>
            </li>
            <li class="c-header__item c-header__item--hide-lg">
             <a class="c-header__link" data-track="click" data-track-action="Sign up for alerts" data-track-external="" data-track-label="link (mobile dropdown)" href="https://www-nature-com.proxy.lib.ohio-state.edu/my-account/alerts/subscribe-journal?list-id=5" rel="nofollow">
              Sign up for alerts
              <svg aria-hidden="true" focusable="false" height="18" role="img" viewbox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg">
               <path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff">
               </path>
              </svg>
             </a>
            </li>
            <li class="c-header__item c-header__item--hide-lg">
             <a class="c-header__link" data-track="click" data-track-action="rss feed" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/nm.rss">
              <span>
               RSS feed
              </span>
             </a>
            </li>
           </ul>
          </div>
         </nav>
        </li>
        <li class="c-header__item c-header__item--dropdown-menu">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" data-test="menu-button--about-the-journal" data-track="click" data-track-action="open about the journal expander" data-track-label="button" href="javascript:;" role="button">
          <span>
           About
           <span class="c-header__show-text">
            the journal
           </span>
          </span>
          <svg aria-hidden="true" focusable="false" height="16" role="img" viewbox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg">
           <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)">
           </path>
          </svg>
         </a>
         <nav aria-labelledby="About-the-journal" class="c-header__dropdown has-tethered u-js-hide" data-test="about-the-journal" data-track-component="nature-150-split-header" hidden="" id="about-the-journal">
          <div class="c-header__container">
           <h2 class="c-header__heading c-header__heading--js-hide" id="About-the-journal">
            About the journal
           </h2>
           <ul class="c-header__list c-header__list--js-stack">
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="aims &amp; scope" data-track-label="link" href="/nm/aims">
              Aims &amp; Scope
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="journal information" data-track-label="link" href="/nm/journal-information">
              Journal Information
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="journal metrics" data-track-label="link" href="/nm/journal-impact">
              Journal Metrics
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="about the editors" data-track-label="link" href="/nm/editors">
              About the Editors
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="our publishing models" data-track-label="link" href="/nm/our-publishing-models">
              Our publishing models
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="editorial values statement" data-track-label="link" href="/nm/editorial-values-statement">
              Editorial Values Statement
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="editorial policies" data-track-label="link" href="/nm/editorial-policies">
              Editorial Policies
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="content types" data-track-label="link" href="/nm/content">
              Content Types
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="nature medicine classic collection" data-track-label="link" href="/nm/classics">
              Nature Medicine Classic Collection
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="web feeds" data-track-label="link" href="/nm/web-feeds">
              Web Feeds
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="posters" data-track-label="link" href="/nm/posters">
              Posters
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="contact" data-track-label="link" href="/nm/contact">
              Contact
             </a>
            </li>
           </ul>
          </div>
         </nav>
        </li>
        <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link c-header__link--dropdown-menu" data-header-expander="" data-test="menu-button--publish" data-track="click" data-track-action="open publish with us expander" data-track-label="button" href="javascript:;" role="button">
          <span>
           Publish
           <span class="c-header__show-text">
            with us
           </span>
          </span>
          <svg aria-hidden="true" focusable="false" height="16" role="img" viewbox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg">
           <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)">
           </path>
          </svg>
         </a>
         <nav aria-labelledby="Publish-with-us-label" class="c-header__dropdown has-tethered u-js-hide" data-test="publish-with-us" data-track-component="nature-150-split-header" hidden="" id="publish-with-us">
          <div class="c-header__container">
           <h2 class="c-header__heading c-header__heading--js-hide" id="Publish-with-us-label">
            Publish with us
           </h2>
           <ul class="c-header__list c-header__list--js-stack">
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="submission guidelines" data-track-label="link" href="/nm/submission-guidelines">
              Submission Guidelines
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-track="click" data-track-action="for reviewers" data-track-label="link" href="/nm/for-reviewers">
              For Reviewers
             </a>
            </li>
            <li class="c-header__item">
             <a class="c-header__link" data-test="nature-author-services" data-track="click" data-track-action="manuscript author services" data-track-label="link manuscript author services" href="https://authorservices-springernature-com.proxy.lib.ohio-state.edu/go/sn/?utm_source=For+Authors&amp;utm_medium=Website_Nature&amp;utm_campaign=Platform+Experimentation+2022&amp;utm_id=PE2022">
              Language editing services
             </a>
            </li>
            <li class="c-header__item c-header__item--keyline">
             <a class="c-header__link" data-track="click" data-track-action="submit manuscript" data-track-external="" data-track-label="link (publish with us dropdown menu)" href="https://mts-nmed-nature-com.proxy.lib.ohio-state.edu/cgi-bin/main.plex">
              Submit manuscript
              <svg aria-hidden="true" focusable="false" height="18" role="img" viewbox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg">
               <path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff">
               </path>
              </svg>
             </a>
            </li>
           </ul>
          </div>
         </nav>
        </li>
       </ul>
       <ul class="c-header__menu c-header__menu--hide-lg-max">
        <li class="c-header__item">
         <a class="c-header__link" data-track="click" data-track-action="Sign up for alerts" data-track-external="" data-track-label="link (desktop site header)" href="https://www-nature-com.proxy.lib.ohio-state.edu/my-account/alerts/subscribe-journal?list-id=5" rel="nofollow">
          <span>
           Sign up for alerts
          </span>
          <svg aria-hidden="true" focusable="false" height="18" role="img" viewbox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg">
           <path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222">
           </path>
          </svg>
         </a>
        </li>
        <li class="c-header__item c-header__item--pipe">
         <a class="c-header__link" data-track="click" data-track-action="rss feed" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/nm.rss">
          <span>
           RSS feed
          </span>
         </a>
        </li>
       </ul>
      </div>
     </div>
    </div>
   </header>
   <nav aria-label="breadcrumbs" class="u-mb-16">
    <div class="u-container">
     <ol class="c-breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList">
      <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature" href="/" itemprop="item">
        <span itemprop="name">
         nature
        </span>
       </a>
       <meta content="1" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature medicine" href="/nm" itemprop="item">
        <span itemprop="name">
         nature medicine
        </span>
       </a>
       <meta content="2" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles" href="/nm/articles?type=article" itemprop="item">
        <span itemprop="name">
         articles
        </span>
       </a>
       <meta content="3" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <span itemprop="name">
        article
       </span>
       <meta content="4" itemprop="position"/>
      </li>
     </ol>
    </div>
   </nav>
  </div>
  <div class="u-container u-mt-32 u-mb-32 u-clearfix" data-component="article-container" data-container-type="article" id="content">
   <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
    <div aria-hidden="true" class="c-context-bar u-hide" data-context-bar="" data-context-bar-with-recommendations="" data-test="context-bar">
     <div class="c-context-bar__container u-container">
      <div class="c-context-bar__title">
       Clinically applicable deep learning for diagnosis and referral in retinal disease
      </div>
      <div class="c-pdf-download u-clear-both js-pdf-download">
       <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="download-pdf" data-track="click" data-track-action="download pdf" data-track-external="" data-track-label="link" download="" href="/articles/s41591-018-0107-6.pdf">
        <span class="c-pdf-download__text">
         Download PDF
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
     </div>
     <div id="recommendations">
      <div class="c-recommendations__container u-container u-display-none" data-component-recommendations="">
       <aside class="c-status-message c-status-message--success u-display-none" data-component-status-msg="">
        <svg aria-label="success:" class="c-status-message__icon" focusable="false" height="24" role="img" width="24">
         <use xlink:href="#icon-success">
         </use>
        </svg>
        <div class="c-status-message__message" id="success-message" tabindex="-1">
         Your content has downloaded
        </div>
       </aside>
       <div class="c-recommendations-header u-display-flex u-justify-content-space-between">
        <h2 class="c-recommendations-title" id="recommendation-heading">
         Similar content being viewed by others
        </h2>
        <button aria-label="Close" class="c-recommendations-close u-flex-static" data-track="click" data-track-action="close recommendations" type="button">
         <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
          <use xlink:href="#icon-close">
          </use>
         </svg>
        </button>
       </div>
       <section aria-labelledby="recommendation-heading" aria-roledescription="carousel">
        <p class="u-visually-hidden">
         Slider with three content items shown per slide. Use the Previous and Next buttons to navigate the slides or the slide controller buttons at the end to navigate through each slide.
        </p>
        <div class="c-recommendations-list-container">
         <div class="c-recommendations-list">
          <div aria-label="Recommendation 1 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs41746-021-00411-w/MediaObjects/41746_2021_411_Fig1_HTML.png"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 1" data-track-label="10.1038/s41746-021-00411-w" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41746-021-00411-w" itemprop="url">
                 Automated identification of clinical features from sparsely annotated 3-dimensional medical imaging
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 08 March 2021
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               Nadav Rakocz, Jeffrey N. Chiang, … Eran Halperin
              </p>
             </div>
            </div>
           </article>
          </div>
          <div aria-label="Recommendation 2 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs41598-019-46294-6/MediaObjects/41598_2019_46294_Fig1_HTML.png"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 2" data-track-label="10.1038/s41598-019-46294-6" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41598-019-46294-6" itemprop="url">
                 Detecting Retinal Nerve Fibre Layer Segmentation Errors on Spectral Domain-Optical Coherence Tomography with a Deep Learning Algorithm
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 08 July 2019
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               Alessandro A. Jammal, Atalie C. Thompson, … Felipe A. Medeiros
              </p>
             </div>
            </div>
           </article>
          </div>
          <div aria-label="Recommendation 3 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs41598-020-69814-1/MediaObjects/41598_2020_69814_Fig1_HTML.png"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 3" data-track-label="10.1038/s41598-020-69814-1" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41598-020-69814-1" itemprop="url">
                 Unbiased identification of novel subclinical imaging biomarkers using unsupervised deep learning
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 31 July 2020
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               Sebastian M. Waldstein, Philipp Seeböck, … Ursula Schmidt-Erfurth
              </p>
             </div>
            </div>
           </article>
          </div>
          <div aria-label="Recommendation 4 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs41598-022-17709-8/MediaObjects/41598_2022_17709_Fig1_HTML.jpg"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 4" data-track-label="10.1038/s41598-022-17709-8" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41598-022-17709-8" itemprop="url">
                 Deep learning for quality assessment of optical coherence tomography angiography images
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 12 August 2022
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               Rahul M. Dhodapkar, Emily Li, … Jay C. Wang
              </p>
             </div>
            </div>
           </article>
          </div>
          <div aria-label="Recommendation 5 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs41433-020-01191-5/MediaObjects/41433_2020_1191_Fig1_HTML.png"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 5" data-track-label="10.1038/s41433-020-01191-5" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41433-020-01191-5" itemprop="url">
                 Deep learning in glaucoma with optical coherence tomography: a review
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 07 October 2020
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               An Ran Ran, Clement C. Tham, … Carol Y. Cheung
              </p>
             </div>
            </div>
           </article>
          </div>
          <div aria-label="Recommendation 6 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs41467-019-13922-8/MediaObjects/41467_2019_13922_Fig1_HTML.png"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 6" data-track-label="10.1038/s41467-019-13922-8" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41467-019-13922-8" itemprop="url">
                 Predicting optical coherence tomography-derived diabetic macular edema grades from fundus photographs using deep learning
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 08 January 2020
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               Avinash V. Varadarajan, Pinal Bavishi, … Dale R. Webster
              </p>
             </div>
            </div>
           </article>
          </div>
          <div aria-label="Recommendation 7 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs41746-020-0247-1/MediaObjects/41746_2020_247_Fig1_HTML.png"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 7" data-track-label="10.1038/s41746-020-0247-1" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41746-020-0247-1" itemprop="url">
                 Technical and imaging factors influencing performance of deep learning systems for diabetic retinopathy
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 23 March 2020
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               Michelle Y. T. Yip, Gilbert Lim, … Daniel S. W. Ting
              </p>
             </div>
            </div>
           </article>
          </div>
          <div aria-label="Recommendation 8 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs41746-021-00438-z/MediaObjects/41746_2021_438_Fig1_HTML.png"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 8" data-track-label="10.1038/s41746-021-00438-z" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41746-021-00438-z" itemprop="url">
                 Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 07 April 2021
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               Ravi Aggarwal, Viknesh Sounderajah, … Ara Darzi
              </p>
             </div>
            </div>
           </article>
          </div>
          <div aria-label="Recommendation 9 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
           <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
            <div class="c-card__layout u-full-height">
             <div class="c-card__image">
              <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1038%2Fs42256-020-00247-1/MediaObjects/42256_2020_247_Fig1_HTML.png"/>
             </div>
             <div class="c-card__body u-display-flex u-flex-direction-column">
              <div class="c-recommendations-column-switch">
               <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                <a class="c-card__link" data-track="click" data-track-action="click recommendations - 9" data-track-label="10.1038/s42256-020-00247-1" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s42256-020-00247-1" itemprop="url">
                 Self-supervised retinal thickness prediction enables deep learning from unlabelled data to boost classification of diabetic retinopathy
                </a>
               </h3>
               <div class="c-card__section c-meta">
                <span class="c-meta__item u-sans-serif">
                 Article
                </span>
                <span class="c-meta__item u-sans-serif">
                 09 November 2020
                </span>
               </div>
              </div>
              <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
               Olle G. Holmberg, Niklas D. Köhler, … Fabian J. Theis
              </p>
             </div>
            </div>
           </article>
          </div>
         </div>
        </div>
       </section>
      </div>
      <div class="js-greyout-page-background" data-component-grey-background="" style="display:none">
      </div>
     </div>
    </div>
    <article lang="en">
     <div class="c-article-header">
      <header>
       <ul class="c-article-identifiers" data-test="article-identifier">
        <li class="c-article-identifiers__item" data-test="article-category">
         Article
        </li>
        <li class="c-article-identifiers__item">
         <a data-track="click" data-track-action="publication date" data-track-label="link" href="#article-info">
          Published:
          <time datetime="2018-08-13">
           13 August 2018
          </time>
         </a>
        </li>
       </ul>
       <h1 class="c-article-title" data-article-title="" data-test="article-title">
        Clinically applicable deep learning for diagnosis and referral in retinal disease
       </h1>
       <ul class="c-article-author-list c-article-author-list--long js-no-scroll" data-component-authors-activator="authors-list" data-test="authors-list">
        <li class="c-article-author-list__item">
         <a data-author-popup="auth-Jeffrey-Fauw-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jeffrey-Fauw-Aff1">
          Jeffrey De  Fauw
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item">
         <a data-author-popup="auth-Joseph_R_-Ledsam-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Joseph_R_-Ledsam-Aff1">
          Joseph R. Ledsam
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Bernardino-Romera_Paredes-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Bernardino-Romera_Paredes-Aff1">
          Bernardino Romera-Paredes
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Stanislav-Nikolov-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stanislav-Nikolov-Aff1">
          Stanislav Nikolov
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Nenad-Tomasev-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Nenad-Tomasev-Aff1">
          Nenad Tomasev
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Sam-Blackwell-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Sam-Blackwell-Aff1">
          Sam Blackwell
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Harry-Askham-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Harry-Askham-Aff1">
          Harry Askham
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Xavier-Glorot-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Xavier-Glorot-Aff1">
          Xavier Glorot
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Brendan-O_Donoghue-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Brendan-O_Donoghue-Aff1">
          Brendan O’Donoghue
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Daniel-Visentin-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daniel-Visentin-Aff1">
          Daniel Visentin
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-George-_Driessche-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-George-_Driessche-Aff1">
          George van den  Driessche
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Balaji-Lakshminarayanan-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Balaji-Lakshminarayanan-Aff1">
          Balaji Lakshminarayanan
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Clemens-Meyer-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Clemens-Meyer-Aff1">
          Clemens Meyer
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Faith-Mackinder-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Faith-Mackinder-Aff1">
          Faith Mackinder
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Simon-Bouton-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Simon-Bouton-Aff1">
          Simon Bouton
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Kareem-Ayoub-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kareem-Ayoub-Aff1">
          Kareem Ayoub
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Reena-Chopra-Aff2" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Reena-Chopra-Aff2">
          Reena Chopra
         </a>
         <span class="u-js-hide">
          <a class="js-orcid" href="http://orcid.org/0000-0002-4264-8329">
           <span class="u-visually-hidden">
            ORCID:
           </span>
           orcid.org/0000-0002-4264-8329
          </a>
         </span>
         <sup class="u-js-hide">
          <a href="#Aff2" tabindex="-1">
           2
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Dominic-King-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Dominic-King-Aff1">
          Dominic King
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Alan-Karthikesalingam-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Alan-Karthikesalingam-Aff1">
          Alan Karthikesalingam
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-C_an_O_-Hughes-Aff1-Aff3" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-C_an_O_-Hughes-Aff1-Aff3">
          Cían O. Hughes
         </a>
         <span class="u-js-hide">
          <a class="js-orcid" href="http://orcid.org/0000-0001-6901-0985">
           <span class="u-visually-hidden">
            ORCID:
           </span>
           orcid.org/0000-0001-6901-0985
          </a>
         </span>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
          ,
          <a href="#Aff3" tabindex="-1">
           3
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Rosalind-Raine-Aff3" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Rosalind-Raine-Aff3">
          Rosalind Raine
         </a>
         <sup class="u-js-hide">
          <a href="#Aff3" tabindex="-1">
           3
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Julian-Hughes-Aff2" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Julian-Hughes-Aff2">
          Julian Hughes
         </a>
         <sup class="u-js-hide">
          <a href="#Aff2" tabindex="-1">
           2
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Dawn_A_-Sim-Aff2" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Dawn_A_-Sim-Aff2">
          Dawn A. Sim
         </a>
         <sup class="u-js-hide">
          <a href="#Aff2" tabindex="-1">
           2
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Catherine-Egan-Aff2" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Catherine-Egan-Aff2">
          Catherine Egan
         </a>
         <sup class="u-js-hide">
          <a href="#Aff2" tabindex="-1">
           2
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Adnan-Tufail-Aff2" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Adnan-Tufail-Aff2">
          Adnan Tufail
         </a>
         <sup class="u-js-hide">
          <a href="#Aff2" tabindex="-1">
           2
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Hugh-Montgomery-Aff3" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hugh-Montgomery-Aff3">
          Hugh Montgomery
         </a>
         <span class="u-js-hide">
          <a class="js-orcid" href="http://orcid.org/0000-0001-8797-5019">
           <span class="u-visually-hidden">
            ORCID:
           </span>
           orcid.org/0000-0001-8797-5019
          </a>
         </span>
         <sup class="u-js-hide">
          <a href="#Aff3" tabindex="-1">
           3
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Demis-Hassabis-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Demis-Hassabis-Aff1">
          Demis Hassabis
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Geraint-Rees-Aff3" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Geraint-Rees-Aff3">
          Geraint Rees
         </a>
         <span class="u-js-hide">
          <a class="js-orcid" href="http://orcid.org/0000-0002-9623-7007">
           <span class="u-visually-hidden">
            ORCID:
           </span>
           orcid.org/0000-0002-9623-7007
          </a>
         </span>
         <sup class="u-js-hide">
          <a href="#Aff3" tabindex="-1">
           3
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Trevor-Back-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Trevor-Back-Aff1">
          Trevor Back
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Peng_T_-Khaw-Aff2" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Peng_T_-Khaw-Aff2">
          Peng T. Khaw
         </a>
         <sup class="u-js-hide">
          <a href="#Aff2" tabindex="-1">
           2
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Mustafa-Suleyman-Aff1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mustafa-Suleyman-Aff1">
          Mustafa Suleyman
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Julien-Cornebise-Aff1-Aff3" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Julien-Cornebise-Aff1-Aff3">
          Julien Cornebise
         </a>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         <sup class="u-js-hide">
          <a href="#na1" tabindex="-1">
           na1
          </a>
         </sup>
         <sup class="u-js-hide">
          <a href="#nAff3" tabindex="-1">
           nAff3
          </a>
         </sup>
         ,
        </li>
        <li class="c-article-author-list__item c-article-author-list__item--hide c-article-author-list__item--hide-small-screen">
         <a data-author-popup="auth-Pearse_A_-Keane-Aff2" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Pearse_A_-Keane-Aff2">
          Pearse A. Keane
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
           <use xlink:href="#icon-email-new" xmlns:xlink="http://www.w3.org/1999/xlink">
           </use>
          </svg>
         </a>
         <span class="u-js-hide">
          <a class="js-orcid" href="http://orcid.org/0000-0002-9239-745X">
           <span class="u-visually-hidden">
            ORCID:
           </span>
           orcid.org/0000-0002-9239-745X
          </a>
         </span>
         <sup class="u-js-hide">
          <a href="#Aff2" tabindex="-1">
           2
          </a>
         </sup>
         <sup class="u-js-hide">
          <a href="#na1" tabindex="-1">
           na1
          </a>
         </sup>
         &amp;
        </li>
        <li aria-label="Show all 34 authors for this article" class="c-article-author-list__show-more" title="Show all 34 authors for this article">
         …
        </li>
        <li class="c-article-author-list__item">
         <a data-author-popup="auth-Olaf-Ronneberger-Aff1" data-corresp-id="c2" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Olaf-Ronneberger-Aff1">
          Olaf Ronneberger
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
           <use xlink:href="#icon-email-new" xmlns:xlink="http://www.w3.org/1999/xlink">
           </use>
          </svg>
         </a>
         <span class="u-js-hide">
          <a class="js-orcid" href="http://orcid.org/0000-0002-4266-1515">
           <span class="u-visually-hidden">
            ORCID:
           </span>
           orcid.org/0000-0002-4266-1515
          </a>
         </span>
         <sup class="u-js-hide">
          <a href="#Aff1" tabindex="-1">
           1
          </a>
         </sup>
         <sup class="u-js-hide">
          <a href="#na1" tabindex="-1">
           na1
          </a>
         </sup>
        </li>
       </ul>
       <button aria-expanded="false" class="c-article-author-list__button">
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
         <use xlink:href="#icon-plus" xmlns:xlink="http://www.w3.org/1999/xlink">
         </use>
        </svg>
        <span>
         Show authors
        </span>
       </button>
       <p class="c-article-info-details" data-container-section="info">
        <a data-test="journal-link" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link" href="/nm">
         <i data-test="journal-title">
          Nature Medicine
         </i>
        </a>
        <b data-test="journal-volume">
         <span class="u-visually-hidden">
          volume
         </span>
         24
        </b>
        ,
        <span class="u-visually-hidden">
         pages
        </span>
        1342–1350 (
        <span data-test="article-publication-year">
         2018
        </span>
        )
        <a class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link" href="#citeas">
         Cite this article
        </a>
       </p>
       <div class="c-article-metrics-bar__wrapper u-clear-both">
        <ul class="c-article-metrics-bar u-list-reset">
         <li class="c-article-metrics-bar__item">
          <p class="c-article-metrics-bar__count">
           61k
           <span class="c-article-metrics-bar__label">
            Accesses
           </span>
          </p>
         </li>
         <li class="c-article-metrics-bar__item">
          <p class="c-article-metrics-bar__count">
           1279
           <span class="c-article-metrics-bar__label">
            Citations
           </span>
          </p>
         </li>
         <li class="c-article-metrics-bar__item">
          <p class="c-article-metrics-bar__count">
           2095
           <span class="c-article-metrics-bar__label">
            Altmetric
           </span>
          </p>
         </li>
         <li class="c-article-metrics-bar__item">
          <p class="c-article-metrics-bar__details">
           <a data-track="click" data-track-action="view metrics" data-track-label="link" href="/articles/s41591-018-0107-6/metrics" rel="nofollow">
            Metrics
            <span class="u-visually-hidden">
             details
            </span>
           </a>
          </p>
         </li>
        </ul>
       </div>
      </header>
     </div>
     <div class="c-article-body">
      <section aria-labelledby="Abs1" data-title="Abstract" lang="en">
       <div class="c-article-section" id="Abs1-section">
        <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">
         Abstract
        </h2>
        <div class="c-article-section__content" id="Abs1-content">
         <p>
          The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.
         </p>
        </div>
       </div>
      </section>
      <noscript>
       <div class="c-nature-box c-nature-box--side" data-component="entitlement-box">
        <p class="c-nature-box__text js-text">
         You have full access to this article via your institution.
        </p>
        <div class="c-pdf-download u-clear-both js-pdf-download">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="download-pdf" data-track="click" data-track-action="download pdf" data-track-external="" data-track-label="link" download="" href="/articles/s41591-018-0107-6.pdf">
          <span class="c-pdf-download__text">
           Download PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </noscript>
      <div class="js-context-bar-sticky-point-mobile">
       <div aria-hidden="true" class="c-nature-box c-nature-box--side u-display-none u-hide-print" data-component="entitlement-box" id="entitlement-box-entitled-mobile">
        <p aria-hidden="true" class="c-nature-box__text js-text u-display-none">
         You have full access to this article via
         <strong>
          Ohio State University Libraries
         </strong>
        </p>
        <div class="c-pdf-download u-clear-both js-pdf-download">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="download-pdf" data-track="click" data-track-action="download pdf" data-track-external="" data-track-label="link" download="" href="/articles/s41591-018-0107-6.pdf">
          <span class="c-pdf-download__text">
           Download PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div class="main-content">
       <section data-title="Main">
        <div class="c-article-section" id="Sec1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">
          Main
         </h2>
         <div class="c-article-section__content" id="Sec1-content">
          <p>
           Medical imaging is expanding globally at an unprecedented rate
           <sup>
            <a aria-label="Reference 1" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR1" id="ref-link-section-d261796601e731" title="OECD. Computed tomography (CT) exams (indicator). (2017); 
https://doi-org.proxy.lib.ohio-state.edu/10.1787/3c994537-en


">
             1
            </a>
            ,
            <a aria-label="Reference 2" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR2" id="ref-link-section-d261796601e734" title="OECD. Magnetic resonance imaging (MRI) exams (indicator). (2017). 
https://doi-org.proxy.lib.ohio-state.edu/10.1787/1d89353f-en


">
             2
            </a>
           </sup>
           , leading to an ever-expanding quantity of data that requires human expertise and judgement to interpret and triage. In many clinical specialities there is a relative shortage of this expertise to provide timely diagnosis and referral. For example, in ophthalmology, the widespread availability of optical coherence tomography (OCT) has not been matched by the availability of expert humans to interpret scans and refer patients to the appropriate clinical care
           <sup>
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR3" id="ref-link-section-d261796601e738" title="Foot, B. &amp; MacEwen, C. Surveillance of sight loss due to delay in ophthalmic treatment or review: frequency, cause and outcome. Eye 31, 771–775 (2017).">
             3
            </a>
           </sup>
           . This problem is exacerbated by the marked increase in prevalence of sight-threatening diseases for which OCT is the gold standard of initial assessment
           <sup>
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d261796601e742" title="Owen, C. G. et al. The estimated prevalence and incidence of late stage age related macular degeneration in the UK. Br. J. Ophthalmol. 96, 752–756 (2012).">
             4
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d261796601e742_1" title="Rudnicka, A. R. et al. Incidence of late-stage age-related macular degeneration in American whites: systematic review and meta-analysis. Am. J. Ophthalmol. 160, 85–93 (2015).">
             5
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d261796601e742_2" title="Bourne, R. R. A. et al. Magnitude, temporal trends, and projections of the global prevalence of blindness and distance and near vision impairment: a systematic review and meta-analysis. Lancet Glob. Health 5, e888–e897 (2017).">
             6
            </a>
            ,
            <a aria-label="Reference 7" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR7" id="ref-link-section-d261796601e745" title="Schmidt-Erfurth, U., Klimscha, S., Waldstein, S. M. &amp; Bogunović, H. A view of the current and future role of optical coherence tomography in the management of age-related macular degeneration. Eye 31, 26–44 (2017).">
             7
            </a>
           </sup>
           .
          </p>
          <p>
           Artificial intelligence (AI) provides a promising solution for such medical image interpretation and triage, but despite recent breakthrough studies in which expert-level performance on two-dimensional photographs in preclinical settings has been demonstrated
           <sup>
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR8" id="ref-link-section-d261796601e752" title="Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. J. Am. Med. Assoc. 316, 2402–2410 (2016).">
             8
            </a>
            ,
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR9" id="ref-link-section-d261796601e755" title="Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115––118 (2017).">
             9
            </a>
           </sup>
           , prospective clinical application of this technology remains stymied by three key challenges. First, AI (typically trained on hundreds of thousands of examples from one canonical dataset) must generalize to new populations and devices without a substantial loss of performance, and without prohibitive data requirements for retraining. Second, AI tools must be applicable to real-world scans, problems and pathways, and designed for clinical evaluation and deployment. Finally, AI tools must match or exceed the performance of human experts in such real-world situations. Recent work applying AI to OCT has shown promise in resolving some of these criteria in isolation, but has not yet shown clinical applicability by resolving all three.
          </p>
         </div>
        </div>
       </section>
       <section data-title="Results">
        <div class="c-article-section" id="Sec2-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">
          Results
         </h2>
         <div class="c-article-section__content" id="Sec2-content">
          <h3 class="c-article__sub-heading" id="Sec3">
           Clinical application and AI architecture
          </h3>
          <p>
           We developed our architecture in the challenging context of OCT imaging for ophthalmology. We tested this approach for patient triage in a typical ophthalmology clinical referral pathway, comprising more than 50 common diagnoses for which OCT provides the definitive imaging modality (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           ). OCT is a three-dimensional volumetric medical imaging technique analogous to three-dimensional ultrasonography but measuring the reflection of near-infrared light rather than sound waves at a resolution for living human tissue of ~5 µm
           <sup>
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR10" id="ref-link-section-d261796601e774" title="Huang, D. et al. Optical coherence tomography. Science 254, 1178–1181 (1991).">
             10
            </a>
           </sup>
           . OCT is now one of the most common imaging procedures with 5.35 million OCT scans performed in the US Medicare population in 2014 alone (see
           <a href="https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html">
            https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html
           </a>
           ). It has been widely adopted across the UK National Health Service (NHS) for comprehensive initial assessment and triage of patients requiring rapid non-elective assessment of acute and chronic sight loss. Rapid access ‘virtual’ OCT clinics have become the standard of care
           <sup>
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR11" id="ref-link-section-d261796601e782" title="Buchan, J. C. et al. How to defuse a demographic time bomb: the way forward? Eye 31, 1519–1522 (2017).">
             11
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR12" id="ref-link-section-d261796601e785" title="Whited, J. D. et al. A modeled economic analysis of a digital teleophthalmology system as used by three federal healthcare agencies for detecting proliferative diabetic retinopathy. Telemed. J. E Health 11, 641–651 (2005).">
             12
            </a>
           </sup>
           . In such clinics, expert clinicians interpret the OCT and clinical history to diagnose and triage patients with pathology affecting the macula, the central part of the retina that is required for high-resolution, color vision.
          </p>
          <p>
           Automated diagnosis of a medical image, even for a single disease, faces two main challenges: technical variations in the imaging process (different devices, noise, ageing of the components and so on), and patient-to-patient variability in pathological manifestations of disease. Existing deep learning approaches
           <sup>
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR8" id="ref-link-section-d261796601e792" title="Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. J. Am. Med. Assoc. 316, 2402–2410 (2016).">
             8
            </a>
            ,
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR9" id="ref-link-section-d261796601e795" title="Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115––118 (2017).">
             9
            </a>
           </sup>
           tried to deal with all combinations of these variations using a single end-to-end black-box network, thus typically requiring millions of labeled scans. By contrast, our framework decouples the two problems (technical variations in the imaging process and pathology variants) and solves them independently (see Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig1">
            1
           </a>
           ). A deep segmentation network (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig1">
            1b
           </a>
           ) creates a detailed device-independent tissue-segmentation map. Subsequently, a deep classification network (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig1">
            1d
           </a>
           ) analyses this segmentation map and provides diagnoses and referral suggestions.
          </p>
          <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Our proposed AI framework." id="figure-1">
           <figure>
            <figcaption>
             <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">
              Fig. 1: Our proposed AI framework.
             </b>
            </figcaption>
            <div class="c-article-section__figure-content">
             <div class="c-article-section__figure-item">
              <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/articles/s41591-018-0107-6/figures/1" rel="nofollow">
               <picture>
                <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig1_HTML.jpg?as=webp" type="image/webp"/>
                <img alt="figure 1" aria-describedby="Fig1" height="379" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig1_HTML.jpg" width="685"/>
               </picture>
              </a>
             </div>
             <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc">
              <p>
               <b>
                a
               </b>
               , Raw retinal OCT scan (6 × 6 × 2.3 mm³ around the macula).
               <b>
                b
               </b>
               , Deep segmentation network, trained with manually segmented OCT scans.
               <b>
                c
               </b>
               , Resulting tissue segmentation map.
               <b>
                d
               </b>
               , Deep classification network, trained with tissue maps with confirmed diagnoses and optimal referral decisions.
               <b>
                e
               </b>
               , Predicted diagnosis probabilities and referral suggestions.
              </p>
             </div>
            </div>
            <div class="u-text-right u-hide-print">
             <a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/articles/s41591-018-0107-6/figures/1" rel="nofollow">
              <span>
               Full size image
              </span>
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </div>
           </figure>
          </div>
          <p>
           The segmentation network (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig1">
            1b
           </a>
           ) uses a three-dimensional U-Net architecture
           <sup>
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR13" id="ref-link-section-d261796601e837" title="Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: convolutional networks for biomedical image segmentation. in Navab N., Hornegger J., Wells W., Frangi A. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol. 9351 (Springer, Cham, Switzerland, 2015).">
             13
            </a>
            ,
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR14" id="ref-link-section-d261796601e840" title="Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. in Ourselin, S., Joskowicz, L., Sabuncu, M., Unal, G., Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science, vol. 9901 (Springer, Cham, Switzerland; 2016).">
             14
            </a>
           </sup>
           to translate the raw OCT scan into a tissue map (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig1">
            1c
           </a>
           ) with 15 classes including anatomy, pathology and image artefacts (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            2
           </a>
           ). It was trained with 877 clinical OCT scans (Topcon 3D OCT, Topcon) with sparse manual segmentations (dataset 1 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           , see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘Manual segmentation’ and ‘Datasets’ for full breakdown of scan dataset). Only approximately three representative slices out of the 128 slices of each scan were manually segmented (see Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            4
           </a>
           for image sizes). This sparse annotation procedure
           <sup>
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR14" id="ref-link-section-d261796601e860" title="Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. in Ourselin, S., Joskowicz, L., Sabuncu, M., Unal, G., Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science, vol. 9901 (Springer, Cham, Switzerland; 2016).">
             14
            </a>
           </sup>
           allowed us to cover a large variety of scans and pathologies with the same workload as approximately 21 dense manual segmentations. Examples of the output of our segmentation network for illustrative pathologies are shown in Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig2">
            2
           </a>
           .
          </p>
          <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Results of the segmentation network." id="figure-2">
           <figure>
            <figcaption>
             <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">
              Fig. 2: Results of the segmentation network.
             </b>
            </figcaption>
            <div class="c-article-section__figure-content">
             <div class="c-article-section__figure-item">
              <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/articles/s41591-018-0107-6/figures/2" rel="nofollow">
               <picture>
                <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig2_HTML.jpg?as=webp" type="image/webp"/>
                <img alt="figure 2" aria-describedby="Fig2" height="476" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig2_HTML.jpg" width="685"/>
               </picture>
              </a>
             </div>
             <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc">
              <p>
               Three selected two-dimensional slices from the
               <i>
                n
               </i>
               = 224 OCT scans in the segmentation test set (left) with manual segmentation (middle) and automated segmentation (right; detailed color legend in Supplementary Table
               <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
                2
               </a>
               ).
               <b>
                a
               </b>
               , A patient with diabetic macular edema.
               <b>
                b
               </b>
               , A patient with choroidal neovascularization resulting from age-related macular degeneration (AMD), demonstrating extensive fibrovascular pigment epithelium detachment and associated subretinal fluid.
               <b>
                c
               </b>
               , A patient with neovascular AMD with extensive subretinal hyperreflective material. Further examples of the variation of pathology with model segmentation and diagnostic performance can be found in Supplementary Videos
               <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM3">
                1
               </a>
               –
               <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM11">
                9
               </a>
               . In all examples the classification network predicted the correct diagnosis. Scale bars, 0.5 mm.
              </p>
             </div>
            </div>
            <div class="u-text-right u-hide-print">
             <a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/articles/s41591-018-0107-6/figures/2" rel="nofollow">
              <span>
               Full size image
              </span>
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </div>
           </figure>
          </div>
          <p>
           The classification network (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig1">
            1d
           </a>
           ) analyses the tissue-segmentation map (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig1">
            1c
           </a>
           ) and as the primary outcome provides one of four referral suggestions currently used in clinical practice at Moorfields Eye Hospital (please see Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           for a list of retinal conditions associated with these referral suggestions). Additionally, it reports the presence or absence of multiple, concomitant retinal pathologies (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            5
           </a>
           ). To construct the training set for this network, we assembled 14,884 OCT scan volumes obtained from 7,621 patients who were referred to the hospital with symptoms suggestive of macular pathology (see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘Clinical labeling’). These OCT scans were automatically segmented using our segmentation network. The resulting segmentation maps with the clinical labels built the training set for the classification network (dataset 3 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           , illustrated in Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig1">
            1d
           </a>
           ).
          </p>
          <p>
           A central challenge in OCT-image segmentation is the presence of ambiguous regions, where the true tissue type cannot be deduced from the image, and thus multiple equally plausible interpretations exist. To address this issue, we trained not one but multiple instances of the segmentation network. Each network instance creates a full segmentation map for the given scan, resulting in multiple hypotheses (see Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           ). Analogous to multiple human experts, these segmentation maps agree in areas with clear image structures but may contain different (but plausible) interpretations in ambiguous low-quality regions. These multiple segmentation hypotheses from our network can be displayed as a video, in which the ambiguous regions and the proposed interpretations are clearly visible (see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘Visualization of results in clinical practice’; use of this viewer across a range of challenging macular diseases is illustrated in Supplementary Videos
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM3">
            1
           </a>
           –
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM11">
            9
           </a>
           ).
          </p>
          <h3 class="c-article__sub-heading" id="Sec4">
           Achieving expert performance on referral decisions
          </h3>
          <p>
           To evaluate our framework, we first defined a gold standard. This used information that is not available at the first patient visit and OCT scan, by examining the patient clinical records to determine the final diagnosis and optimal referral pathway in the light of the (subsequently obtained) information. Such a gold standard can only be obtained retrospectively. Gold standard labels were acquired for 997 patients that were not included in the training dataset (dataset 5 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            5
           </a>
           ). We then tested our framework on this dataset. For each patient, we obtained the referral suggestion of our framework plus an independent referral suggestion from eight clinical experts, four of whom were retina specialists and four optometrists trained in medical retina (see Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            6
           </a>
           for more information). Each expert provided two separate decisions, one (like our framework) from the OCT scan alone (dataset 7 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            5
           </a>
           ); and one from the OCT plus fundus image and clinical notes (dataset 8 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            5
           </a>
           , see Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            2
           </a>
           ), in two separate sessions spaced at least two weeks apart. We compared each of these performances (framework and two expert decisions) against the gold standard.
          </p>
          <p>
           Our framework achieved and in some cases exceeded expert performance (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig3">
            3
           </a>
           ). To illustrate this, Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig3">
            3a
           </a>
           displays performance on ‘urgent referrals’, the most important clinical referral decision (mainly for pathologies that cause choroidal neovascularization; see Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           ) versus all other referral decisions as a receiver operating characteristic (ROC) plot (plots for the other decisions are shown in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). Performance of our framework matched our two best retina specialists and had a significantly higher performance than the other two retinal specialists and all four optometrists when they used only the OCT scans to make their referral suggestion (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig3">
            3a
           </a>
           , filled markers). When experts had access to the fundus image and patient summary notes to make their decision, their performance improved (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig3">
            3a
           </a>
           , empty markers) but our framework remained as good as the five best experts and continued to significantly outperform the other three (see
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            Supplementary Information
           </a>
           ).
          </p>
          <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Results on the patient referral decision." id="figure-3">
           <figure>
            <figcaption>
             <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">
              Fig. 3: Results on the patient referral decision.
             </b>
            </figcaption>
            <div class="c-article-section__figure-content">
             <div class="c-article-section__figure-item">
              <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/articles/s41591-018-0107-6/figures/3" rel="nofollow">
               <picture>
                <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig3_HTML.jpg?as=webp" type="image/webp"/>
                <img alt="figure 3" aria-describedby="Fig3" height="918" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig3_HTML.jpg" width="685"/>
               </picture>
              </a>
             </div>
             <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc">
              <p>
               Performance on an independent test set of
               <i>
                n
               </i>
               = 997 patients (252 urgent, 230 semi-urgent, 266 routine, 249 observation only).
               <b>
                a
               </b>
               , ROC diagram for urgent referral (for choroidal neovascularization (CNV)) versus all other referrals. The blue ROC curve is created by sweeping a threshold over the predicted probability of a particular clinical diagnosis. Points outside the light blue area correspond to a significantly different performance (95% confidence level, using a two-sided exact binomial test). The asterisk denotes the performance of our model in the ‘balanced performance’ setting. Filled markers denote experts’ performance using OCT only; empty markers denote their performance using OCT, fundus image and summary notes. Dashed lines connect the two performance points of each expert.
               <b>
                b
               </b>
               , Confusion matrices with patient numbers for referral decision for our framework and the two best retina specialists. These show the number of patients for each combination of gold standard decision and predicted decision. The numbers of correct decisions are found on the diagonal. Wrong decisions due to overdiagnosis are in the bottom-left triangle, and wrong decisions due to underdiagnosis are in the top-right triangle.
               <b>
                c
               </b>
               , Total error rate (1 − accuracy) on referral decision. Values outside the light-blue area (3.9–7.3%) are significantly different (95% confidence interval, using a two-sided exact binomial test) to the framework performance (5.5%). AUC, area under curve.
              </p>
             </div>
            </div>
            <div class="u-text-right u-hide-print">
             <a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/articles/s41591-018-0107-6/figures/3" rel="nofollow">
              <span>
               Full size image
              </span>
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </div>
           </figure>
          </div>
          <p>
           To provide a more complete picture, the overall performance of our framework on all four clinical referral suggestions (urgent, semi-urgent, routine and observation only) compared to the two highest performing retina specialists is displayed in Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig3">
            3b
           </a>
           . The framework performed comparably to the two best-performing retina specialists, and made no clinically-serious wrong decisions (top right element of each matrix; that is, referring a patient who needs an urgent referral to observation only). Confusion matrices for the assessments of the other human experts are shown in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            4
           </a>
           . The aggregated number of wrong referral decisions is displayed as error rate (1 − accuracy) for our framework and all experts in Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig3">
            3c
           </a>
           . Our framework (5.5% error rate) performed comparably to the two best retina specialists (6.7% and 6.8% error rate) and significantly outperformed the other six experts in the ‘OCT only’ setting. Significance thresholds (3.9% for higher performance and 7.3% for lower performance) were derived by a two-sided exact binomial test, incorporating uncertainty from both the experts and the algorithm (see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘Statistical analysis’). When experts additionally used the fundus image and the summary notes of the patient, five approached the performance of our framework (three retina specialists and two optometrists), which continued to significantly outperform the remaining three (one retina specialist and two optometrists).
          </p>
          <p>
           Our framework uses an ensemble of five segmentation and five classification model instances (see Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           ) to achieve these results. Beside the benefits of an uncertainty measure, ensembling also significantly improves overall performance compared to a single model instance. Error rates for different ensemble sizes are shown in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            5
           </a>
           . With more segmentation model instances and more classification model instances, performance increases. The bottom right cells in that table illustrate that performance differences between 4 × 4 model instances and 5 × 5 model instances are only marginal, so we do not expect significant changes by adding more instances. The accumulated number of diagnostic errors does not fully reflect the clinical consequences that an incorrect referral decision might have for patients, which depends also on the specific diagnosis that was missed. For example, failing to diagnose sight-threatening conditions could result in rapid visual loss
           <sup>
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR3" id="ref-link-section-d261796601e1032" title="Foot, B. &amp; MacEwen, C. Surveillance of sight loss due to delay in ophthalmic treatment or review: frequency, cause and outcome. Eye 31, 771–775 (2017).">
             3
            </a>
            ,
            <a aria-label="Reference 15" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR15" id="ref-link-section-d261796601e1035" title="Muether, P. S., Hermann, M. M., Koch, K. &amp; Fauser, S. Delay between medical indication to anti-VEGF treatment in age-related macular degeneration can result in a loss of visual acuity. Graefes Arch. Clin. Exp. Ophthalmol. 249, 633–637 (2011).">
             15
            </a>
            ,
            <a aria-label="Reference 16" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR16" id="ref-link-section-d261796601e1038" title="Arias, L. et al. Delay in treating age-related macular degeneration in Spain is associated with progressive vision loss. Eye 23, 326–333 (2009).">
             16
            </a>
           </sup>
           , which is not the case for many other diagnoses. For an initial quantitative estimation of these consequences, we weighted different types of diagnostic errors according to the judgement of our clinical experts of the clinical impact of erroneous classification (expressed as penalty points; see Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            6a
           </a>
           ). We derived a score for our framework and each expert as a weighted average of all wrong diagnoses. This revealed that our framework achieved a lower average penalty score than any of our experts (Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            6b
           </a>
           ). We further optimized the decisions of our framework to minimize this specific score (see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘Optimizing the ensemble output for sensitivity, specificity and penalty scores’) which further improved performance (Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            6b
           </a>
           ). Therefore, expert performance of our framework is not achieved at the cost of missing clinically important sight-threatening diagnoses.
          </p>
          <p>
           To examine how our proposed two-stage architecture compared to a traditional single-stage architecture, we trained an end-to-end classification network with the same architecture as our second stage to directly map from a raw OCT scan to a referral decision (see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘End-to-end classification network’). The error rate achieved with an ensemble of five network instances was 5.5%, which was not significantly different from the performance of the two-stage architecture. This validates our choice of the two-stage architecture that offers several clinical advantages (see Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            7
           </a>
           ).
          </p>
          <h3 class="c-article__sub-heading" id="Sec5">
           Achieving expert performance on retinal morphology
          </h3>
          <p>
           The referral decision recommended by our framework is determined by the most urgent diagnosis detected on each scan (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           ). Patients may also have multiple concomitant retinal pathologies. These additional pathologies do not change the referral decision, but may have implications for further investigations and treatment. Our framework was therefore also trained to predict the probability of a patient having one or more of several pathologies (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            5
           </a>
           ).
          </p>
          <p>
           To evaluate performance on diagnosing multiple pathologies, a ‘silver standard’ for each scan was established by majority vote from the eight experts who evaluated the OCT scan, fundus image and patient summary notes (dataset 6 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). This majority vote biases the assessment against our framework. Nevertheless, our framework demonstrated an area under the ROC curve that was over 99% for most of the pathologies (and over 96% for all of them; Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            7
           </a>
           ), on par with the performance of the experts on OCT only. As with earlier evaluations, performance of the experts improved when they were provided also with the fundus image and patient summary notes. This improvement was most marked in pathologies classed as ‘routine referral’, for example geographic atrophy and central serous retinopathy. Many of these pathologies are conditions for which the fundus photograph or demographic information would be expected to provide important information, indicating that there is scope for future work to improve the model. However even in the worst case our framework still performed on par with at least one retinal specialist and one optometrist (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            6
           </a>
           and Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            8
           </a>
           ).
          </p>
          <h3 class="c-article__sub-heading" id="Sec6">
           Generalization to a new scanning device type
          </h3>
          <p>
           A key benefit of our two-stage framework is the device independence of the second stage. Using our framework on a new device generation thus only requires retraining of the segmentation stage to learn how each tissue type appears in the new scan, whereas the knowledge about patient-to-patient variability in pathological manifestation of different diseases, which it had learned from the approximately 15,000 training cases, can be reused. To demonstrate this generalization, we collected an independent test set of clinical scans from 116 patients (plus confirmed clinical outcomes) recorded with a different OCT scanner type from a different vendor (Spectralis, Heidelberg Engineering,; hereafter ‘device type 2’). This dataset is listed as dataset 11 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           (see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘Datasets’). We selected this device type for several reasons. It is the second most used device type at Moorfields Eye hospital for these examinations, giving rise to a sufficient number of scans. It has a similar worldwide market share as device type 1. But most importantly, this device type provides a large difference in scan characteristics compared to the original device type (see Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            9
           </a>
           ).
          </p>
          <p>
           To evaluate the effect of a different scanning device type, we initially fed the OCT scans from device type 2 into our framework, which was trained only on scans from device type 1 (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig4">
            4a
           </a>
           ). The segmentation network was clearly ‘confused’ by the changed appearance of these structures and attempted to explain them as additional retinal layers (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig4">
            4a
           </a>
           , middle). Consequently, performance was poor with a total error rate for referral suggestions of 46.6% (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig4">
            4a
           </a>
           , right). Uncertainty of the segmentation network on these (never seen) types of images resulted in five strongly different segmentation hypotheses (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig4">
            4b
           </a>
           ).
          </p>
          <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Generalization to a new scanning device type." id="figure-4">
           <figure>
            <figcaption>
             <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">
              Fig. 4: Generalization to a new scanning device type.
             </b>
            </figcaption>
            <div class="c-article-section__figure-content">
             <div class="c-article-section__figure-item">
              <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/articles/s41591-018-0107-6/figures/4" rel="nofollow">
               <picture>
                <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig4_HTML.jpg?as=webp" type="image/webp"/>
                <img alt="figure 4" aria-describedby="Fig4" height="457" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig4_HTML.jpg" width="685"/>
               </picture>
              </a>
             </div>
             <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc">
              <p>
               <b>
                a
               </b>
               , Low performance of original network on OCT scans from the new device type 2. Left, the selected slice shows the different appearance of structures in device type 2. Middle, a poor quality segmentation map created with our original segmentation network (color legend in Supplementary Table
               <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
                2
               </a>
               ). Right, resulting performance on a new test set of
               <i>
                n
               </i>
               = 116 patients. The confusion matrix shows patient numbers for the referral suggestion.
               <b>
                b
               </b>
               , All five segmentation hypotheses from our original network. The strong variations show the large uncertainty.
               <b>
                c
               </b>
               , High performance was attained on the device type 2 test set (
               <i>
                n
               </i>
               = 116) after retraining the segmentation network with OCT scans from device type 1 and device type 2. The classification network is unchanged.
               <b>
                d
               </b>
               , All five segmentation hypotheses from the retrained segmentation network. The network is confident in the interpretation of most structures, and just highlights the ambiguities in the sub-retinal pigment epithelium (RPE) space. Scale bars: 0.5 mm.
              </p>
             </div>
            </div>
            <div class="u-text-right u-hide-print">
             <a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/articles/s41591-018-0107-6/figures/4" rel="nofollow">
              <span>
               Full size image
              </span>
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </div>
           </figure>
          </div>
          <p>
           We next collected an additional segmentation training set with 152 scans (527 manually segmented slices in total) from this device (dataset 9 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ), and retrained the segmentation network with both the training scans from the original device type 1 and the new device type 2 (see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘Segmentation network’). The classification network was not modified.
          </p>
          <p>
           Our retrained system (adapted segmentation network and unchanged classification network) now achieved a similarly high level of performance on device type 2 as on the original device (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig4">
            4c
           </a>
           ). It suggested incorrect referral decisions for 4 out of the 116 cases, a total error rate of 3.4%. Owing to the small number of cases in the new test set, this is not significantly different from the error rate of 5.5% on device type 1 (
           <i>
            P
           </i>
           (4 out of 116 &lt; 55 out of 997) = 0.774; see
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           ‘Statistical analysis’). For continuity with our previous evaluation, we also measured performance against retina specialists accessing OCT scans plus fundus images and clinical notes (dataset 12 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). Our experts achieved the following error rates (all with access to imaging and clinical notes): retinal specialist one: 2 errors = 1.7% error rate; retinal specialist two: 2 errors = 1.7% error rate; retinal specialist three: 4 errors = 3.4% error rate; retinal specialist four: 3 errors = 2.6% error rate; retinal specialist five: 3 errors = 2.6% error rate. These differences in performance between our framework and the best human retina specialists did not reach statistical significance (
           <i>
            P
           </i>
           (4 out of 116 &gt; 2 out of 116) = 0.776).
          </p>
          <p>
           To verify that device type 2 provides the greatest difference in scan characteristics, we performed a feasibility study on the small number of OCT scans from Cirrus HD-OCT 5000 with AngioPlex (Carl Zeiss Meditec) devices available in Moorfields Eye Hospital (dataset of 61 scans; not included here). Applying our original network to these images, we already obtained an error rate of 16.4%. This rate was much lower than that originally obtained with device type 2 (46.6%), consistent with the claim that device type 2 provides a larger difference in scan characteristics from device type 1. Retraining of the segmentation network with 6 manually segmented scans reduced the error rate to 9.8%.
          </p>
          <p>
           Table
           <a data-track="click" data-track-action="table anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Tab1">
            1
           </a>
           summarizes our results. For device type 1, our architecture required 877 training scans with manual segmentations and 14,884 training scans with gold standard referral decisions to achieve expert performance on referral decisions (5.5% error rate). For device type 2, we only required 152 additional training scans with manual segmentations and not a single additional training scan with gold standard referral decisions to achieve the same performance on referral decisions on this device type (3.4% error rate).
          </p>
          <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1">
           <figure>
            <figcaption class="c-article-table__figcaption">
             <b data-test="table-caption" id="Tab1">
              Table 1 Number of training scans and achieved performance on the two device types
             </b>
            </figcaption>
            <div class="u-text-right u-hide-print">
             <a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/articles/s41591-018-0107-6/tables/1" rel="nofollow">
              <span>
               Full size table
              </span>
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </div>
           </figure>
          </div>
         </div>
        </div>
       </section>
       <section data-title="Discussion">
        <div class="c-article-section" id="Sec7-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">
          Discussion
         </h2>
         <div class="c-article-section__content" id="Sec7-content">
          <p>
           Recent work in which AI is used for the automated diagnosis of OCT scans shows encouraging results; however, until now such studies have relied on selective and clinically unrepresentative OCT datasets. For example, several authors
           <sup>
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d261796601e1252" title="Karri, S. P. K., Chakraborty, D. &amp; Chatterjee, J. Transfer learning based classification of optical coherence tomography images with diabetic macular edema and dry age-related macular degeneration. Biomed. Opt. Express 8, 579–592 (2017).">
             17
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d261796601e1252_1" title="Apostolopoulos, S., Ciller, C., De Zanet, S. I., Wolf, S. &amp; Sznitman, R. RetiNet: automatic AMD identification in OCT volumetric data. Preprint at 
http://arxiv-org.proxy.lib.ohio-state.edu/abs/1610.03628v1

 (2016).">
             18
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR19" id="ref-link-section-d261796601e1252_2" title="Farsiu, S. et al. Quantitative classification of eyes with and without intermediate age-related macular degeneration using optical coherence tomography. Ophthalmology 121, 162–172 (2014).">
             19
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d261796601e1252_3" title="Srinivasan, P. P. et al. Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images. Biomed. Opt. Express 5, 3568–3577 (2014).">
             20
            </a>
            ,
            <a aria-label="Reference 21" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR21" id="ref-link-section-d261796601e1255" title="Lee, C. S., Baughman, D. M. &amp; Lee, A. Y. Deep learning is effective for classifying normal versus age-related macular degeneration OCT images. Ophthalmol. Retin. 1, 322–327 (2017).">
             21
            </a>
           </sup>
           report high performance on automated classification of age-related macular degeneration (AMD) from OCT scans. However, they tested their algorithms on smaller datasets that exclude other pathologies. By contrast, here we demonstrate expert performance on multiple clinical referral suggestions for two independent test datasets of 997 and 116 clinical OCT scans that include a wide range of retinal pathologies.
          </p>
          <p>
           Several recent studies used deep learning-based architectures to deliver successful segmentation of OCT scans
           <sup>
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d261796601e1262" title="Fang, L. et al. Automatic segmentation of nine retinal layer boundaries in OCT images of non-exudative AMD patients using deep learning and graph search. Biomed. Opt. Express 8, 2732–2744 (2017).">
             22
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d261796601e1262_1" title="Lee, C. S. et al. Deep-learning based, automated segmentation of macular edema in optical coherence tomography. Biomed. Opt. Express 8, 3440–3448 (2017).">
             23
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d261796601e1262_2" title="Lu, D. et al. Retinal fluid segmentation and detection in optical coherence tomography images using fully convolutional neural network. Preprint at 
http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.04778v1

 (2017).">
             24
            </a>
            ,
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR25" id="ref-link-section-d261796601e1265" title="Roy, A. G. et al. ReLayNet: retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional network. Biomed. Opt. Express 8, 3627–3642 (2017).">
             25
            </a>
           </sup>
           . This earlier work focused on a subset of diagnostically relevant tissues types (for example, intraretinal fluid) and applied two-dimensional models in samples of between 10 and 42 patients. In the present work, we go beyond these earlier studies by applying three-dimensional models, segmenting a much larger range of diagnostically relevant tissue types, and connect such segmentation to clinically relevant real-world referral recommendations.
          </p>
          <p>
           We evaluated our framework on a broad range of real-world images from routine clinical practice at 32 different Moorfields Eye Hospital sites, which cover diverse populations within London and surrounding areas, using 37 individual OCT devices (28 device type 1 and 9 device type 2). The two device types that we tested are both used widely in routine clinical practice at Moorfields Eye Hospital, the largest eye hospital in Europe and North America, and provided a large difference in scan characteristics.
          </p>
          <p>
           Our framework has a number of potential benefits. The derivation of device-independent segmentation of the OCT scan creates an intermediate representation that is readily viewable by a clinical expert and integrates into clinical workflows (see Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig5">
            5
           </a>
           for the clinical results viewer). Moreover, the use of an ensemble of five segmentation network instances allows us to present ambiguities arising from the imaging process to the decision network (and could potentially be used for automated quality control).
          </p>
          <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Visualization of the segmentation results as thickness maps." id="figure-5">
           <figure>
            <figcaption>
             <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig5">
              Fig. 5: Visualization of the segmentation results as thickness maps.
             </b>
            </figcaption>
            <div class="c-article-section__figure-content">
             <div class="c-article-section__figure-item">
              <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/articles/s41591-018-0107-6/figures/5" rel="nofollow">
               <picture>
                <source srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig5_HTML.jpg?as=webp" type="image/webp"/>
                <img alt="figure 5" aria-describedby="Fig5" height="521" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig5_HTML.jpg" width="685"/>
               </picture>
              </a>
             </div>
             <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc">
              <p>
               <b>
                a
               </b>
               , The average intensity projection of the OCT scan along A-scan direction (frontal view of the eye) is overlaid with a thickness map of the fibrovascular pigment epithelium detachment (PED, red segment).
               <b>
                b
               </b>
               , Screenshot from our OCT viewer. First row (left), referral suggestion, tissue volumes and diagnosis probabilities. The highlighted bars correspond to the selected segmentation model. First–third rows, thickness maps of the 10 relevant tissue types from segmentation model instance 2. The two healthy tissue types (high level retina and RPE) are displayed in a black–blue–green–brown–white color map, the pathological tissues (all others) are displayed as overlay on a projection of the raw OCT scan. The thin white line indicates the position of slice 80. Fourth row, slice 80 from the OCT scan and the segmentation map from segmentation model instance 2. Detailed tissue legend in Supplementary Table
               <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
                2
               </a>
               . The slice and model instance can be interactively selected (see Supplementary Video
               <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM3">
                1
               </a>
               ).
              </p>
             </div>
            </div>
            <div class="u-text-right u-hide-print">
             <a aria-label="Full size image figure 5" class="c-article__pill-button" data-test="article-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure5 Full size image" data-track-label="button" href="/articles/s41591-018-0107-6/figures/5" rel="nofollow">
              <span>
               Full size image
              </span>
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </div>
           </figure>
          </div>
          <p>
           The ‘black box’ problem has been identified as an impediment to the application of deep learning in healthcare
           <sup>
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR26" id="ref-link-section-d261796601e1302" title="Castelvecchi, D. Can we open the black box of AI? Nature 538, 20–23 (2016).">
             26
            </a>
           </sup>
           . Here we created a framework with a structure that closely matches the clinical decision-making process, separating judgements about the scan itself from the subsequent referral decision. This allows a clinician to inspect and visualize an interpretable segmentation, rather than simply being presented with a diagnosis and referral suggestion. Such an approach to medical imaging AI offers potential insights into the decision process, in a fashion more typical of clinical practice. For example, an interpretable representation is particularly useful in difficult and ambiguous cases. Such cases are common in medicine and even expert medical practitioners can find it difficult to reach consensus (for example, our eight experts only agreed on 63.5% of cases even when accessing all information).
          </p>
          <p>
           Our segmentation map assigns only one label per pixel, and it may not be possible to use the framework directly in other clinical pathways for which the tissue-segmentation map does not contain all required information for a diagnosis (for example, in certain radiomics applications). To keep the advantages of the intermediate device-independent representation in such applications, future work can potentially augment the tissue-segmentation map with multiple labels per pixel to encode local tissue features, or with additional channels that encode continuous features such as an inflammatory reaction. This may be of particular value for other components of the retina, such as the nerve fibre layer, and may be of importance for multiple ocular and brain disorders, such as glaucoma and dementia.
          </p>
          <p>
           Although we have demonstrated the performance of our framework in the domain of a clinical treatment pathway, the approach has potential utility in clinical training in which the medical professionals must learn to read medical images. In addition, a wide variety of non-medically qualified health professionals have an interest in appropriately reading and understanding medical images. Our framework produces a visualizable segmentation and achieves expert performance on diagnosis and referral decisions for a large number of scans and pathologies. This therefore raises the intriguing possibility that such a framework could be evaluated as a tool for effectively training healthcare professionals to expert levels.
          </p>
          <p>
           The segmentation output itself can also be used to quantify retinal morphology and derive measurements of particular pathologies (for example, the location and volume of fibrovascular pigment epithelium detachment and macular edema). Some of these measurements (such as retinal thickness and intraretinal fluid) can currently be derived automatically
           <sup>
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR27" id="ref-link-section-d261796601e1315" title="Schmidt-Erfurth, U. et al. Machine learning to analyze the prognostic value of current imaging biomarkers in neovascular age-related macular degeneration. Ophthalmol. Retin. 2, 24–30 (2018).">
             27
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR28" id="ref-link-section-d261796601e1318" title="Schlegl, T. et al. Fully automated detection and quantification of macular fluid in OCT using deep learning. Ophthalmology 125, 549–558 (2018).">
             28
            </a>
           </sup>
           , used to investigate correlations with visual outcomes
           <sup>
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR27" id="ref-link-section-d261796601e1322" title="Schmidt-Erfurth, U. et al. Machine learning to analyze the prognostic value of current imaging biomarkers in neovascular age-related macular degeneration. Ophthalmol. Retin. 2, 24–30 (2018).">
             27
            </a>
           </sup>
           and as an end point in clinical trials of therapies for retinal disease
           <sup>
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR29" id="ref-link-section-d261796601e1326" title="Keane, P. A. &amp; Sadda, S. R. Predicting visual outcomes for macular disease using optical coherence tomography. Saudi J. Ophthalmol. 25, 145–158 (2011).">
             29
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d261796601e1326_1" title="Schaal, K. B., Rosenfeld, P. J., Gregori, G., Yehoshua, Z. &amp; Feuer, W. J. Anatomic clinical trial endpoints for nonexudative age-related macular degeneration. Ophthalmology 123, 1060–1079 (2016).">
             30
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR31" id="ref-link-section-d261796601e1326_2" title="Schmidt-Erfurth, U. &amp; Waldstein, S. M. A paradigm shift in imaging biomarkers in neovascular age-related macular degeneration. Prog. Retin. Eye Res. 50, 1–24 (2016).">
             31
            </a>
            ,
            <a aria-label="Reference 32" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR32" id="ref-link-section-d261796601e1329" title="Villani, E. et al. Decade-long profile of imaging biomarker use in ophthalmic clinical trials. Invest. Ophthalmol. Vis. Sci. 58, BIO76–BIO81 (2017).">
             32
            </a>
           </sup>
           . Our framework can be used to define and validate a broader range of automatically derived quantitative measurements.
          </p>
          <p>
           Our framework can triage scans at first presentation of a patient into a small number of pathways used in routine clinical practice with a performance matching or exceeding both the expert retina specialists and optometrists who staff virtual clinics in a UK NHS setting. Future work can now directly seek evidence for the efficacy of such a framework in a randomized controlled trial. The output of our framework can be optimized to penalize different diagnostic errors, and thus for other clinically important metrics. For example, the potential improvement to patient quality of life of different diagnostic decisions, or avoiding the harm of unnecessary investigation that might come from a false-positive diagnosis, could all be incorporated into future work.
          </p>
          <p>
           Globally, ophthalmology clinical referral pathways vary, and the range of diseases that can potentially be diagnosed by OCT includes pathologies additional to the macular diseases that were studied here. We studied a major clinical referral pathway in a global center of clinical excellence focusing on 53 key diagnoses relevant to the national (NHS) referral pathways. Our work opens up the possibility of testing the clinical applicability of this approach in other global settings and clinical pathways, such as emergency macular assessment clinics in the UK NHS, triage and assessment in community eye care centers and the monitoring of disease during treatment regimes. Furthermore, devices such as binocular OCT
           <sup>
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR33" id="ref-link-section-d261796601e1339" title="Chopra, R., Mulholland, P. J., Dubis, A. M., Anderson, R. S. &amp; Keane, P. A. Human factor and usability testing of a binocular optical coherence tomography system. Transl. Vis. Sci. Technol. 6, 16 (2017).">
             33
            </a>
           </sup>
           have the potential to increase accessibility in emerging economies. Images produced by such devices will differ in resolution, contrast and image quality from the state-of-the-art devices studied here, and existing AI models trained on current state-of-the-art devices may perform poorly on such new devices. Our proposed two-stage model offers exciting possibilities that enable the use of models more efficiently in countries where state-of-the-art OCT devices are too costly for widespread adoption.
          </p>
          <p>
           In conclusion, we present a novel framework that analyses clinical OCT scans and makes referral suggestions to a standard that is comparable to clinical experts. Although we focussed on one common type of medical imaging, future work can address a much wider range of medical imaging techniques, and incorporate clinical diagnoses and tissue types well outside the immediate application that was demonstrated here.
          </p>
         </div>
        </div>
       </section>
       <section data-title="Methods">
        <div class="c-article-section" id="Sec8-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">
          Methods
         </h2>
         <div class="c-article-section__content" id="Sec8-content">
          <h3 class="c-article__sub-heading" id="Sec9">
           Ethics and information governance
          </h3>
          <p>
           This work and the collection of data on implied consent received national Research Ethics Committee (REC) approval from the Cambridge East REC and Health Research Authority approval (reference 16/EE/0253); it complies with all relevant ethical regulations. Deidentification was performed in line with the Information Commissioner’s Anonymization: managing data protection risk code of practice (
           <a href="https://ico-org-uk.proxy.lib.ohio-state.edu/media/1061/anonymisation-code.pdf">
            https://ico-org-uk.proxy.lib.ohio-state.edu/media/1061/anonymisation-code.pdf
           </a>
           ), and validated by the Moorfields Eye Hospital Information Technology and Information Governance departments, respectively. Only deidentified retrospective data were used for research, without the active involvement of patients.
          </p>
          <h3 class="c-article__sub-heading" id="Sec10">
           Visualization of results in clinical practice
          </h3>
          <p>
           To facilitate viewing of the results in routine clinical practice, we display the obtained three-dimensional segmentation maps as two-dimensional thickness maps overlaid on a projection of the raw OCT scan (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig5">
            5a
           </a>
           ). The thickness maps for all tissue types are displayed side-by-side in our interactive OCT viewer (Fig.
           <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Fig5">
            5b
           </a>
           and Supplementary Video
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM3">
            1
           </a>
           ). Our system also provides measures for its degree of certainty on both overall referral decision, and each specific retinal disease feature. In most common clinical scenarios, the algorithm will both provide the diagnosis with a high degree of certainty and highlight classical disease features (for example, ‘wet’ AMD; Supplementary Video
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM4">
            2
           </a>
           ). This visualization may be particularly useful for difficult and ambiguous cases, such as the diagnosis of choroidal neovascularization formation in cases of chronic central serous retinopathy (Supplementary Videos
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM7">
            5
           </a>
           ,
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM9">
            7
           </a>
           ) or in advanced geographic atrophy due to AMD (Supplementary Video
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM8">
            6
           </a>
           ). Such visualization may also allow clinicians to discard an automated diagnosis or referral suggestion in obvious failure cases, such as when poor image quality leads to erroneous segmentation results (Supplementary Video
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM10">
            8
           </a>
           ). Furthermore, in a screening context the tissue segmentation map can facilitate quality assurance procedures, whether in normal cases (Supplementary Video
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM5">
            3
           </a>
           ) or in disease cases (for example, diabetic macular edema in the context of diabetic retinopathy screening, Supplementary Video
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM6">
            4
           </a>
           ).
          </p>
          <h3 class="c-article__sub-heading" id="Sec11">
           Datasets and clinical taxonomy
          </h3>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec12">
           Datasets
          </h4>
          <p>
           Data were selected from a retrospective cohort of all patients who attended Moorfields Eye Hospital NHS Foundation Trust, a world renowned tertiary referral center with 32 clinic sites serving an urban, mixed socioeconomic and ethnicity population centered around London, United Kingdom, between 1 June 2012 and 31 January 2017, who received OCT imaging (Topcon 3D OCT, Topcon; Spectralis, Heidelberg Engineering) as part of their routine clinical care. Conditions with fewer than ten cases, and data from patients who had manually requested that their data should not be shared, were excluded before research began. OCT scan sets containing severe artefacts or marked reductions in signal strength to the point at which retinal interfaces could not be identified were also excluded from the study (Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            10
           </a>
           ), as such scans are non-diagnostic and in practice would usually be retaken. Scans to which no diagnostic label could be attached (as described below) were excluded from the present study. For OCT examinations that were labeled as urgent or semi-urgent in the Moorfields OpenEyes electronic health record only scans taken prior to treatment beginning were included; during treatment, resolution of pathology invalidates the database labels. The dataset selection and stratification process is displayed in a CONSORT flow diagram in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            11
           </a>
           .
          </p>
          <p>
           Two OCT device types were selected for investigation. 3D OCT-2000 (Topcon, Japan) was selected as device type 1, because of its routine use in the clinical pathway that we studied. For device type 1, a total of 15,877 OCT scans from 7,981 individual patients (mean age 69.5; 3,686 male, 4,294 female, 1 gender unknown) were eligible for inclusion in the work (datasets 3 and 4 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). To create a test set representative of the real-world clinical application, 997 additional patients (mean age 63.1; 443 male, 551 female, 3 gender unknown) presenting to Moorfields with visual disturbance during the retrospective period were selected and only their referral OCT examination was selected for inclusion in the test set (dataset 5 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ); a sample size requirement of 553 to detect sensitivity and specificity at 0.05 marginal error and 95% confidence was used to inform the number included. To demonstrate the generalizability of our approach, Spectralis OCT (Heidelberg Engineering) was chosen as ‘device type 2’. For generalizability experiments, a second test set of clinical OCT scans from 116 patients (mean age 58.2; 59 male, 57 female) presenting in the same manner were selected using the same methodology and selection criteria (dataset 11 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). Examples of differences between the two devices types are shown in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            9
           </a>
           . Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            8
           </a>
           shows a breakdown of patients and triage categories in the datasets.
          </p>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec13">
           Clinical taxonomy
          </h4>
          <p>
           OCT examinations were mapped from individual diagnoses and treatment information to specific triage decisions (urgent referral, semi-urgent referral, routine referral and observation only) to a medical retina clinic setting (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           ). Where possible, the presence or absence of additional pathologies was added as a label (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            5
           </a>
           ). The dataset represents the full variety of medical retina patients presenting and receiving treatment at Moorfields Eye Hospital. Although the exact mapping was chosen to be relevant to the triage decisions at Moorfields Eye Hospital where the research work took place, the framework is generalizable to other systems at centers with different triage requirements (for example, optometrists working in a high-street clinic setting or ophthalmologists without subspecialty retinal expertise). Scans meeting the exclusion criteria were removed from the database before splitting the data into training, validation and test sets. Supplementary Figure
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            12
           </a>
           provides an example of variation within the ‘urgent referral’ label class.
          </p>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec14">
           Clinical labeling
          </h4>
          <p>
           Clinical labels for the 14,884 scans in dataset 3 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           were assigned through an automated notes search with trained ophthalmologist and optometrist review of the OCT scans. The presence or absence of choroidal neovascularization, referable macular edema, normal and other pathologies visible on the OCT scan were recorded. In addition, patients with choroidal neovascularization or macular edema confirmed through treatment were labeled directly from the Moorfields OpenEyes electronic health record. A validation subset of 993 scans (993 patients) was graded separately by three junior graders (ophthalmologists specializing in medical retina) with disagreement in clinical labels arbitrated by a senior retinal specialist with over 10 years of experience and image reading center certification for OCT segmentation (dataset 4 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). The test set was further verified by full review of the notes with access to follow up data with both junior and senior grader review. Junior and senior graders were separate to those participating in the evaluation of expert performance.
          </p>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec15">
           Manual segmentation
          </h4>
          <p>
           A subset of 1,101 scans from device type 1 and a set of 264 scans from device type 2 were manually segmented using the segmentation editor plugin for ImageJ (Fiji)
           <sup>
            <a aria-label="Reference 34" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR34" id="ref-link-section-d261796601e1477" title="Schindelin, J. et al. Fiji: an open-source platform for biological-image analysis. Nat. Methods 9, 676–682 (2012).">
             34
            </a>
           </sup>
           (datasets 1, 2, 9 and 10 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). The segmentation labels were chosen to distinguish all relevant diagnoses for the referral decision, as well as potential artefacts that may affect the diagnostic quality of the whole or part of the scan. In particular, the current state of art does not differentiate between the three different types of pigment epithelial detachment, or segment out areas of fibrosis scarring or blood as hyperreflective material
           <sup>
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR27" id="ref-link-section-d261796601e1484" title="Schmidt-Erfurth, U. et al. Machine learning to analyze the prognostic value of current imaging biomarkers in neovascular age-related macular degeneration. Ophthalmol. Retin. 2, 24–30 (2018).">
             27
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR28" id="ref-link-section-d261796601e1487" title="Schlegl, T. et al. Fully automated detection and quantification of macular fluid in OCT using deep learning. Ophthalmology 125, 549–558 (2018).">
             28
            </a>
           </sup>
           . Anatomical delineations and nomenclature are consistent with standard grading criteria for the evaluation of OCT
           <sup>
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR35" id="ref-link-section-d261796601e1491" title="Keane, P. A. et al. Evaluation of age-related macular degeneration with optical coherence tomography. Surv. Ophthalmol. 57, 389–414 (2012).">
             35
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d261796601e1491_1" title="Folgar, F. A. et al. Comparison of optical coherence tomography assessments in the comparison of age-related macular degeneration treatments trials. Ophthalmology 121, 1956–1965 (2014).">
             36
            </a>
            ,
            <a aria-label="Reference 37" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR37" id="ref-link-section-d261796601e1494" title="Duker, J. S., Waheed, N. K. &amp; Goldman, D. Handbook of Retinal OCT: Optical Coherence Tomography E-Book (Elsevier Health Sciences, Oxford, UK; 2013).">
             37
            </a>
           </sup>
           . The segmentation examples were selected and segmented by ophthalmologists specializing in medical retina as representative cases for pathological features. These were reviewed and edited by a senior ophthalmologist with over 10 years of experience and image reading center certification for OCT segmentation. Per OCT, 3–5 slices were chosen for segmentation, which best represented the pathological features (Supplementary Tables
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            2
           </a>
           ,
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            9
           </a>
           and Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            13
           </a>
           ).
          </p>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec16">
           Evaluating the expert performance
          </h4>
          <p>
           To evaluate expert performance on the test set, eight clinical experts were recruited for an evaluation study. Participants included four consultant ophthalmologists at Moorfields Eye Hospital with fellowship-level subspecialty training in medical retinal disease and extensive clinical experience (21, 21, 12.5 and 11.5 years of experience) and four optometrists at Moorfields Eye Hospital with specialist training in OCT interpretation and retinal diseases (15, 9, 6 and 2.5 years of experience). These are referred to as retinal specialists 1–4 and optometrists 1–4 in the rest of the paper (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            10
           </a>
           ). Each expert was instructed to provide a triage decision (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           ) and to record the presence or absence of defined pathological features (Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            5
           </a>
           ).
          </p>
          <p>
           To assess the performance in a realistic clinical environment, all scans were read in a random order twice with at least a week between readings. During the initial review, only the OCT scan was presented (dataset 7 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). During the second review, participants were presented with all the information available at the time of triage: OCT and fundus scans, age, gender, ethnicity and where available information on visual acuity and a short clinical vignette (dataset 8 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). The model only received the OCT scan.
          </p>
          <p>
           To assess the difference between the test set for device type 1 and device type 2, five clinical experts were recruited for a further evaluation study (dataset 12 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). Participants were five consultant ophthalmologists at Moorfields Eye Hospital with fellowship-level subspecialty training in medical retinal disease (21, 21, 12.5, 11.5 and 11 years of experience). Four were participants in the device type 1 evaluation study, while the other was a new participant for this study and is referred to as retina specialist five.
          </p>
          <h3 class="c-article__sub-heading" id="Sec17">
           Network architectures and training protocol
          </h3>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec18">
           Segmentation network
          </h4>
          <p>
           The first stage of our framework consists of a segmentation network that takes as input part of the OCT scan, and outputs a part of a segmentation map. That is, it predicts for each voxel one tissue type out of the 15 classes described in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            2
           </a>
           . At training time, the input of the network consists of 9 contiguous slices of an OCT, and the goal of the network is to segment the central slice. The input is therefore a 448 × 512 × 9 voxels image, and the output is an estimated probability over the 15 classes, for each of the 448 × 512 × 1 output voxels. None of the convolutions made across the slices (
           <i>
            z
           </i>
           dimension) adds padding to its input. As a result, we can exploit shared computations at inference time to predict any number of contiguous slices in parallel, which was only limited by the memory capacity of the system.
          </p>
          <p>
           The structure of the segmentation convolutional neural network model is shown in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            14
           </a>
           . It uses a three-dimensional U-Net architecture
           <sup>
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR14" id="ref-link-section-d261796601e1566" title="Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. in Ourselin, S., Joskowicz, L., Sabuncu, M., Unal, G., Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science, vol. 9901 (Springer, Cham, Switzerland; 2016).">
             14
            </a>
           </sup>
           , consisting of an analysis (downwards) path, a synthesis (upwards) path, and shortcut connections between blocks of the same level and different paths. We applied four variations over it. First, we used 3 × 3 × 1 convolutions with padding and 1 × 1 × 3 convolutions without padding instead of 3 × 3 × 3 convolutions without padding. Second, downsampling and upsampling operations were carried out through parameter-free bilinear interpolation, replacing max-pooling and up-convolution. Third, we introduced one extra residual connection within each block of layers, so that the output of each block consists of the sum of the features of the last layer, and the first layer of the block in which the features dimensions match. Finally, the middle block of layers between the analysis and synthesis paths is composed of a sequence of fully connected layers. The first variation allows us to control the receptive field for
           <i>
            z
           </i>
           separately and is furthermore less computationally intensive. The second and third variation aimed at improving the gradients flow throughout the network, which makes the training process easier. The last variation extends the receptive field such that each pixel in the output effectively has the whole input contained within its receptive field.
          </p>
          <p>
           We used per-voxel cross entropy as the loss function, with 0.1 label-smoothing regularization
           <sup>
            <a aria-label="Reference 38" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR38" id="ref-link-section-d261796601e1576" title="Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &amp; Wojna, Z. Rethinking the inception architecture for computer vision. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2818–2826 (2016).">
             38
            </a>
           </sup>
           . We have neither used dropout nor weight decay as regularization means, as preliminary experiments showed that this did not improve the performance. We trained the model in TensorFlow
           <sup>
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR39" id="ref-link-section-d261796601e1580" title="Abadi, M. et al. TensorFlow: large-scale machine learning on heterogeneous systems. Preprint at 
https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467

 (2016).">
             39
            </a>
           </sup>
           with the Adam optimizer
           <sup>
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR40" id="ref-link-section-d261796601e1584" title="Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. in Proceedings of the 3rd International Conference on Learning Representations (ICLR). Preprint at 
http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980

 (2015).">
             40
            </a>
           </sup>
           for 160,000 iterations on 8 graphics processing units (GPUs) with dataset 1 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           . The initial learning rate was 0.0001 and set to 0.0001/2 after 10% of the total iterations, 0.0001/4 after 20%, 0.0001/8 after 50%, 0.0001/64 after 70%, 0.0001/256 after 90% and finally 0.0001/512 for the final 5% of training. All decisions and hyperparameters above were selected on the basis of their performance on a validation set (dataset 2 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ).
          </p>
          <p>
           To improve the generalization abilities of our model, we augmented the data by applying affine and elastic transformations jointly over the inputs and ground-truth segmentations
           <sup>
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR13" id="ref-link-section-d261796601e1597" title="Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: convolutional networks for biomedical image segmentation. in Navab N., Hornegger J., Wells W., Frangi A. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol. 9351 (Springer, Cham, Switzerland, 2015).">
             13
            </a>
            ,
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR14" id="ref-link-section-d261796601e1600" title="Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. in Ourselin, S., Joskowicz, L., Sabuncu, M., Unal, G., Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science, vol. 9901 (Springer, Cham, Switzerland; 2016).">
             14
            </a>
           </sup>
           . Intensity transformations over the inputs were also applied.
          </p>
          <p>
           Our segmentation network for device type 2, which is shown in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            15
           </a>
           , is trained on scans from both devices (datasets 1 and 9 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ) with the aim of leveraging the large number of labeled instances for device type 1. It has three changes compared to the architecture for device type 1. First, we subsample the input from device type 1 (128 slices) to match the resolution of device type 2 (49 slices) and apply slight padding in height to the scans of device type 2 to give them of the same shape in height and width as the scans of device type 1. Second, the input first goes through one of two ‘device adaptation branches’, depending on the device type of the input scan. The architecture of this branch consists of three convolutions with padding, with one residual connection as in the other blocks, and is identical for both device types (see Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            15
           </a>
           ). The network can then simply learn to compensate for the changes between device types early on and map them to a common representation. Lastly, the number of feature maps on the first level of the analysis path is halved from 32 to 16 such that the overall architecture still has fewer parameters than the architecture for device type 1. During training, the network was presented with a ratio of 2.5: 1 for training samples from device type 2:device type 1. All decisions and hyperparameters above were selected on the basis of their performance on a validation set (datasets 2 and 10 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ).
          </p>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec19">
           Classification network
          </h4>
          <p>
           The classification network learned to map a segmentation map to the four referral decisions and the ten additional diagnoses (see Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            16
           </a>
           ). For device type 1, it takes as input a 300 × 350 × 43 subsampling of the original 448 × 512 × 128 segmentation map created by the segmentation network described above. The output is a 14-component vector. For device type 2, for which the scans originally were 448 × 512 × 49, we first upscaled the segmentation map to the same resolution as for device type 1 and then proceed identically as for device type 1. The architecture uses a three-dimensional version of the dense blocks described previously
           <sup>
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR41" id="ref-link-section-d261796601e1631" title="Huang, G., Liu, Z., Weinberger, K. Q. &amp; van der Maaten, L. Densely connected convolutional networks. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2261–2269 (2017).">
             41
            </a>
           </sup>
           using 3 × 3 × 1 and 1 × 1 × 3 convolutions. The details of its structure are shown in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            16
           </a>
           . We found using dense convolution blocks to be critical for training classification networks on large three-dimensional volumes. The inputs are one-hot encoded and augmented by random three-dimensional affine and elastic transformations
           <sup>
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR14" id="ref-link-section-d261796601e1638" title="Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. in Ourselin, S., Joskowicz, L., Sabuncu, M., Unal, G., Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science, vol. 9901 (Springer, Cham, Switzerland; 2016).">
             14
            </a>
           </sup>
           . The loss was the sum of the softmax cross entropy loss for the first four components (multi-class referral decision) and the sigmoid cross entropy losses for the remaining ten components (additional diagnoses labels). We also used a small amount (0.05) of label-smoothing regularization
           <sup>
            <a aria-label="Reference 38" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR38" id="ref-link-section-d261796601e1642" title="Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &amp; Wojna, Z. Rethinking the inception architecture for computer vision. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2818–2826 (2016).">
             38
            </a>
           </sup>
           and added some (1 × 10
           <sup>
            −5
           </sup>
           ) weight decay. We trained the model in TensorFlow
           <sup>
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR39" id="ref-link-section-d261796601e1649" title="Abadi, M. et al. TensorFlow: large-scale machine learning on heterogeneous systems. Preprint at 
https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467

 (2016).">
             39
            </a>
           </sup>
           with the Adam optimiser
           <sup>
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR40" id="ref-link-section-d261796601e1653" title="Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. in Proceedings of the 3rd International Conference on Learning Representations (ICLR). Preprint at 
http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980

 (2015).">
             40
            </a>
           </sup>
           for 160,000 iterations of batch size 8 spread across 8 GPUs with 1 sample per GPU with dataset 3 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           . The initial learning rate was 0.02 and set to 0.02/2 after 10% of the total iterations, 0.02/4 after 20%, 0.02/8 after 50%, 0.02/64 after 70%, 0.02/256 after 90% and finally 0.02/512 for the final 5% of training. All decisions and hyperparameters described above were selected on the basis of their performance on a validation set (dataset 4 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ).
          </p>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec20">
           Ensembling
          </h4>
          <p>
           For both of these networks we trained five instances. We trained the same network with a different order of the inputs and different random weight initializations
           <sup>
            <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR42" id="ref-link-section-d261796601e1671" title="Lakshminarayanan, B., Pritzel, A. &amp; Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. Adv. Neural Inf. Process. Syst. 6405–6416 (2017).">
             42
            </a>
           </sup>
           . Previously published experiments
           <sup>
            <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR42" id="ref-link-section-d261796601e1675" title="Lakshminarayanan, B., Pritzel, A. &amp; Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. Adv. Neural Inf. Process. Syst. 6405–6416 (2017).">
             42
            </a>
           </sup>
           suggest that five instances are sufficient in most settings, so we also used this number. For our experiments, we applied the five instances of our segmentation model to the input scan resulting in five segmentation maps. The five instances of our classification model were then applied to each of the segmentation maps, resulting in a total of 25 classification outputs per scan, as illustrated in Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            1
           </a>
           . The results reported are obtained after averaging the probabilities estimated by these models.
          </p>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec21">
           Optimizing the ensemble output for sensitivity, specificity and penalty scores
          </h4>
          <p>
           For different applications, the preferred compromise between a high hit rate (sensitivity) and a low false alarm rate (1 − specificity) can be different. For the binary diagnosis decisions, we computed an optimal rescaling factor
           <i>
            a
           </i>
           for the pseudo-probabilities, such that a 50% threshold achieves maximal (sensitivity + specificity)/2 on the validation set (dataset 4 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). The rescaling was done by
           <i>
            p
           </i>
           <i>
            =
           </i>
           <i>
            aq
           </i>
           /(
           <i>
            aq
           </i>
           + (1 −
           <i>
            a
           </i>
           )(1 −
           <i>
            q
           </i>
           )), where
           <i>
            q
           </i>
           denotes the ensemble output and
           <i>
            p
           </i>
           the reweighted probability. We used (sensitivity + specificity)/2 instead of the total accuracy to avoid the bias due to the low number of patients with a positive condition in the validation set (and in the test set). For a balanced set with equal numbers of positive and negative samples this term is exactly the accuracy.
          </p>
          <p>
           For the four-way referral decision (where the highest probability wins), we optimized four scaling factors using the validation set to reduce the overall cost specified by the misclassification penalty matrix (Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            6
           </a>
           ). A first set of factors was optimized for a balance between high accuracy and low penalty points (referred to as "our model (1)" in Supplementary Figure
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            6
           </a>
           ), a second set of factors was optimized for penalty cost only (referred to as "our model (2)" in Supplementary Figure
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            6
           </a>
           ). The cost matrix for the balanced performance was computed by averaging the normalized cost matrix for accuracy (a matrix with 0 in the diagonal elements and 1 in the off-diagonal elements) and the normalized penalty cost matrix. Normalization was performed by dividing the matrix by the sum of all elements. The optimization of the four factors was done with the Adam optimiser using a softmax layer and a weighted cross-entropy loss layer.
          </p>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec22">
           End-to-end classification network
          </h4>
          <p>
           The network architecture for the end-to-end classification experiments was identical to the architecture of the classification network in the two-stage approach (see ‘Classification network’ and Supplementary Fig.
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            16
           </a>
           ) with a small adaption. To roughly obtain the same number of parameters, we added a dense layer (two convolutions with seven channels output each) that translates the single-channel raw OCT to a 14-channel feature map. All selected hyperparameters and augmentation strategies were identical to the original classification network. We trained five network instances on the training set with 14,884 raw OCT scans from device type 1 (dataset 13 in Supplementary Table
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            3
           </a>
           ). Each network instance was initialized with different random weights and was presented with the training images in a different order. After training, we also computed an optimal reweighting on the validation set (as we did for the two-stage model) and tested the ensemble on the test set.
          </p>
          <h3 class="c-article__sub-heading" id="Sec23">
           Statistical analysis
          </h3>
          <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec24">
           Significant differences using a two-sided exact binomial test
          </h4>
          <p>
           The comparison of our model’s performance to the expert’s performance is based on the assumption that our model and the expert have an unknown but constant performance. That is, every inspected eye scan is correctly diagnosed by our model with the probability
           <i>
            p
           </i>
           <sub>
            mod
           </sub>
           , and correctly diagnosed by the expert with probability
           <i>
            p
           </i>
           <sub>
            exp
           </sub>
           . For
           <i>
            N
           </i>
           eye scans the number of correct diagnoses
           <i>
            k
           </i>
           is therefore binomially distributed with Pr(
           <i>
            k
           </i>
           ) = B(
           <i>
            k
           </i>
           |
           <i>
            p
           </i>
           ,
           <i>
            N
           </i>
           ). If our model achieves
           <i>
            k
           </i>
           <sub>
            mod
           </sub>
           correct diagnoses and the expert achieves
           <i>
            k
           </i>
           <sub>
            exp
           </sub>
           correct diagnoses, the probability that the true performance of our model
           <i>
            p
           </i>
           <sub>
            mod
           </sub>
           is higher than the true performance of the expert
           <i>
            p
           </i>
           <sub>
            exp
           </sub>
           is
          </p>
          <div class="c-article-equation" id="Equa">
           <div class="c-article-equation__content">
            <span class="mathjax-tex">
             $$\begin{array}{l}\Pr \left( {p_{{\mathrm{mod}}} &gt; p_{\exp }\left| {k_{\bmod },k_{\exp },N} \right.} \right) = \cr \frac{{{\int}_0^1 {\mathrm{B}} \left( {k_{\bmod }\left| {p_1,N} \right.} \right)\,{\int}_0^{p_1} {\mathrm{B}} \left( {k_{\exp }\left| {p_2,N} \right.} \right) dp_2\, dp_1}}{{{\int}_0^1 {\mathrm{B}} \left( {k_{\bmod }\left| {p,N} \right.} \right) dp\,{\int}_0^1 {\mathrm{B}} \left( {k_{\exp }\left| {p,N} \right.} \right) dp}}\end{array}$$
            </span>
           </div>
          </div>
          <p>
           The probability for a lower performance, that is Pr
           <i>
            (p
           </i>
           <sub>
            mod
           </sub>
           &lt;
           <i>
            p
           </i>
           <sub>
            exp
           </sub>
           |
           <i>
            k
           </i>
           <sub>
            mod
           </sub>
           ,
           <i>
            k
           </i>
           <sub>
            exp
           </sub>
           ,
           <i>
            N)
           </i>
           is derived analogously. For all comparisons, a confidence level of 95% was used. The formula was numerically integrated using in-house code.
          </p>
          <p>
           Further details on the methods are described in a published protocol describing the DeepMind collaboration with Moorfields Eye Hospital
           <sup>
            <a aria-label="Reference 43" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41591-018-0107-6#ref-CR43" id="ref-link-section-d261796601e1999" title="De Fauw, J. et al. Automated analysis of retinal imaging using machine learning techniques for computer vision. F1000Res 5, 1573 (2016).">
             43
            </a>
           </sup>
           .
          </p>
          <h3 class="c-article__sub-heading" id="Sec25">
           Reporting Summary
          </h3>
          <p>
           Further information on experimental design is available in the
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM2">
            Nature Research Reporting Summary
           </a>
           linked to this article.
          </p>
          <h3 class="c-article__sub-heading" id="Sec26">
           Code availability
          </h3>
          <p>
           The code base for the deep-learning framework makes use of proprietary components and we are unable to publicly release the full code base. However, all experiments and implementation details are described in sufficient detail in the
           <a data-track="click" data-track-action="section anchor" data-track-label="link" href="/articles/s41591-018-0107-6#Sec8">
            Methods
           </a>
           and in the
           <a data-track="click" data-track-action="supplementary material anchor" data-track-label="link" href="/articles/s41591-018-0107-6#MOESM1">
            Supplementary Figs.
           </a>
           to enable independent replication with non-proprietary libraries. The three-dimensional augmentation code (using the caffe framework) is available as part of the three-dimensional U-net source code at
           <a href="https://lmb.informatik.uni-freiburg.de/resources/opensource/unet.en.html">
            https://lmb.informatik.uni-freiburg.de/resources/opensource/unet.en.html
           </a>
           . Additionally, although we are unable to make all the Google proprietary components available, we are in the process of making the augmentation operations for TensorFlow available in the official TensorFlow code.
          </p>
          <h3 class="c-article__sub-heading" id="Sec27">
           Data availability
          </h3>
          <p>
           The clinical data used for the training, validation and test sets were collected at Moorfields Eye Hospital and transferred to the DeepMind data center in the UK in deidentified format. Data were used with both local and national permissions. They are not publicly available and restrictions apply to their use. The data, or a test subset, may be available from Moorfields Eye Hospital NHS Foundation Trust subject to local and national ethical approvals.
          </p>
         </div>
        </div>
       </section>
      </div>
      <div>
       <div id="MagazineFulltextArticleBodySuffix">
        <section aria-labelledby="Bib1" data-title="References">
         <div class="c-article-section" id="Bib1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">
           References
          </h2>
          <div class="c-article-section__content" id="Bib1-content">
           <div data-container-section="references">
            <ol class="c-article-references" data-track-component="outbound reference">
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1.">
              <p class="c-article-references__text" id="ref-CR1">
               OECD. Computed tomography (CT) exams (indicator). (2017);
               <a data-track="click" data-track-action="external reference" data-track-label="10.1787/3c994537-en" href="https://doi-org.proxy.lib.ohio-state.edu/10.1787/3c994537-en">
                https://doi-org.proxy.lib.ohio-state.edu/10.1787/3c994537-en
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2.">
              <p class="c-article-references__text" id="ref-CR2">
               OECD. Magnetic resonance imaging (MRI) exams (indicator). (2017).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1787/1d89353f-en" href="https://doi-org.proxy.lib.ohio-state.edu/10.1787/1d89353f-en">
                https://doi-org.proxy.lib.ohio-state.edu/10.1787/1d89353f-en
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3.">
              <p class="c-article-references__text" id="ref-CR3">
               Foot, B. &amp; MacEwen, C. Surveillance of sight loss due to delay in ophthalmic treatment or review: frequency, cause and outcome.
               <i>
                Eye
               </i>
               <b>
                31
               </b>
               , 771–775 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 3" data-doi="10.1038/eye.2017.1" data-track="click" data-track-action="article reference" data-track-label="10.1038/eye.2017.1" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Feye.2017.1" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 3" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:STN:280:DC%2BC1c7pvVOitw%3D%3D" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 3" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Surveillance%20of%20sight%20loss%20due%20to%20delay%20in%20ophthalmic%20treatment%20or%20review%3A%20frequency%2C%20cause%20and%20outcome&amp;journal=Eye&amp;doi=10.1038%2Feye.2017.1&amp;volume=31&amp;pages=771-775&amp;publication_year=2017&amp;author=Foot%2CB&amp;author=MacEwen%2CC" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4.">
              <p class="c-article-references__text" id="ref-CR4">
               Owen, C. G. et al. The estimated prevalence and incidence of late stage age related macular degeneration in the UK.
               <i>
                Br. J. Ophthalmol.
               </i>
               <b>
                96
               </b>
               , 752–756 (2012).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 4" data-doi="10.1136/bjophthalmol-2011-301109" data-track="click" data-track-action="article reference" data-track-label="10.1136/bjophthalmol-2011-301109" href="https://doi-org.proxy.lib.ohio-state.edu/10.1136%2Fbjophthalmol-2011-301109" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 4" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20estimated%20prevalence%20and%20incidence%20of%20late%20stage%20age%20related%20macular%20degeneration%20in%20the%20UK&amp;journal=Br.%20J.%20Ophthalmol.&amp;doi=10.1136%2Fbjophthalmol-2011-301109&amp;volume=96&amp;pages=752-756&amp;publication_year=2012&amp;author=Owen%2CCG" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5.">
              <p class="c-article-references__text" id="ref-CR5">
               Rudnicka, A. R. et al. Incidence of late-stage age-related macular degeneration in American whites: systematic review and meta-analysis.
               <i>
                Am. J. Ophthalmol.
               </i>
               <b>
                160
               </b>
               , 85–93 (2015).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 5" data-doi="10.1016/j.ajo.2015.04.003" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ajo.2015.04.003" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ajo.2015.04.003" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 5" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Incidence%20of%20late-stage%20age-related%20macular%20degeneration%20in%20American%20whites%3A%20systematic%20review%20and%20meta-analysis&amp;journal=Am.%20J.%20Ophthalmol.&amp;doi=10.1016%2Fj.ajo.2015.04.003&amp;volume=160&amp;pages=85-93&amp;publication_year=2015&amp;author=Rudnicka%2CAR" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6.">
              <p class="c-article-references__text" id="ref-CR6">
               Bourne, R. R. A. et al. Magnitude, temporal trends, and projections of the global prevalence of blindness and distance and near vision impairment: a systematic review and meta-analysis.
               <i>
                Lancet Glob. Health
               </i>
               <b>
                5
               </b>
               , e888–e897 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 6" data-doi="10.1016/S2214-109X(17)30293-0" data-track="click" data-track-action="article reference" data-track-label="10.1016/S2214-109X(17)30293-0" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2FS2214-109X%2817%2930293-0" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 6" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Magnitude%2C%20temporal%20trends%2C%20and%20projections%20of%20the%20global%20prevalence%20of%20blindness%20and%20distance%20and%20near%20vision%20impairment%3A%20a%20systematic%20review%20and%20meta-analysis&amp;journal=Lancet%20Glob.%20Health&amp;doi=10.1016%2FS2214-109X%2817%2930293-0&amp;volume=5&amp;pages=e888-e897&amp;publication_year=2017&amp;author=Bourne%2CRRA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7.">
              <p class="c-article-references__text" id="ref-CR7">
               Schmidt-Erfurth, U., Klimscha, S., Waldstein, S. M. &amp; Bogunović, H. A view of the current and future role of optical coherence tomography in the management of age-related macular degeneration.
               <i>
                Eye
               </i>
               <b>
                31
               </b>
               , 26–44 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 7" data-doi="10.1038/eye.2016.227" data-track="click" data-track-action="article reference" data-track-label="10.1038/eye.2016.227" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Feye.2016.227" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 7" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:STN:280:DC%2BC2sjgtlWnuw%3D%3D" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 7" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20view%20of%20the%20current%20and%20future%20role%20of%20optical%20coherence%20tomography%20in%20the%20management%20of%20age-related%20macular%20degeneration&amp;journal=Eye&amp;doi=10.1038%2Feye.2016.227&amp;volume=31&amp;pages=26-44&amp;publication_year=2017&amp;author=Schmidt-Erfurth%2CU&amp;author=Klimscha%2CS&amp;author=Waldstein%2CSM&amp;author=Bogunovi%C4%87%2CH" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8.">
              <p class="c-article-references__text" id="ref-CR8">
               Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs.
               <i>
                J. Am. Med. Assoc.
               </i>
               <b>
                316
               </b>
               , 2402–2410 (2016).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 8" data-doi="10.1001/jama.2016.17216" data-track="click" data-track-action="article reference" data-track-label="10.1001/jama.2016.17216" href="https://doi-org.proxy.lib.ohio-state.edu/10.1001%2Fjama.2016.17216" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 8" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20and%20validation%20of%20a%20deep%20learning%20algorithm%20for%20detection%20of%20diabetic%20retinopathy%20in%20retinal%20fundus%20photographs&amp;journal=J.%20Am.%20Med.%20Assoc.&amp;doi=10.1001%2Fjama.2016.17216&amp;volume=316&amp;pages=2402-2410&amp;publication_year=2016&amp;author=Gulshan%2CV" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9.">
              <p class="c-article-references__text" id="ref-CR9">
               Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks.
               <i>
                Nature
               </i>
               <b>
                542
               </b>
               , 115––118 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 9" data-doi="10.1038/nature21056" data-track="click" data-track-action="article reference" data-track-label="10.1038/nature21056" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Fnature21056" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 9" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Dermatologist-level%20classification%20of%20skin%20cancer%20with%20deep%20neural%20networks&amp;journal=Nature&amp;doi=10.1038%2Fnature21056&amp;volume=542&amp;pages=115%E2%80%93-118&amp;publication_year=2017&amp;author=Esteva%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10.">
              <p class="c-article-references__text" id="ref-CR10">
               Huang, D. et al. Optical coherence tomography.
               <i>
                Science
               </i>
               <b>
                254
               </b>
               , 1178–1181 (1991).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 10" data-doi="10.1126/science.1957169" data-track="click" data-track-action="article reference" data-track-label="10.1126/science.1957169" href="https://doi-org.proxy.lib.ohio-state.edu/10.1126%2Fscience.1957169" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 10" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:STN:280:DyaK38%2Fms12lsA%3D%3D" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 10" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Optical%20coherence%20tomography&amp;journal=Science&amp;doi=10.1126%2Fscience.1957169&amp;volume=254&amp;pages=1178-1181&amp;publication_year=1991&amp;author=Huang%2CD" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11.">
              <p class="c-article-references__text" id="ref-CR11">
               Buchan, J. C. et al. How to defuse a demographic time bomb: the way forward?
               <i>
                Eye
               </i>
               <b>
                31
               </b>
               , 1519–1522 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 11" data-doi="10.1038/eye.2017.114" data-track="click" data-track-action="article reference" data-track-label="10.1038/eye.2017.114" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Feye.2017.114" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 11" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:STN:280:DC%2BC1cnptVClug%3D%3D" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 11" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20to%20defuse%20a%20demographic%20time%20bomb%3A%20the%20way%20forward%3F&amp;journal=Eye&amp;doi=10.1038%2Feye.2017.114&amp;volume=31&amp;pages=1519-1522&amp;publication_year=2017&amp;author=Buchan%2CJC" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12.">
              <p class="c-article-references__text" id="ref-CR12">
               Whited, J. D. et al. A modeled economic analysis of a digital teleophthalmology system as used by three federal healthcare agencies for detecting proliferative diabetic retinopathy.
               <i>
                Telemed. J. E Health
               </i>
               <b>
                11
               </b>
               , 641–651 (2005).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 12" data-doi="10.1089/tmj.2005.11.641" data-track="click" data-track-action="article reference" data-track-label="10.1089/tmj.2005.11.641" href="https://doi-org.proxy.lib.ohio-state.edu/10.1089%2Ftmj.2005.11.641" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 12" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20modeled%20economic%20analysis%20of%20a%20digital%20teleophthalmology%20system%20as%20used%20by%20three%20federal%20healthcare%20agencies%20for%20detecting%20proliferative%20diabetic%20retinopathy&amp;journal=Telemed.%20J.%20E%20Health&amp;doi=10.1089%2Ftmj.2005.11.641&amp;volume=11&amp;pages=641-651&amp;publication_year=2005&amp;author=Whited%2CJD" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13.">
              <p class="c-article-references__text" id="ref-CR13">
               Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: convolutional networks for biomedical image segmentation. in Navab N., Hornegger J., Wells W., Frangi A. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol. 9351 (Springer, Cham, Switzerland, 2015).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14.">
              <p class="c-article-references__text" id="ref-CR14">
               Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. in Ourselin, S., Joskowicz, L., Sabuncu, M., Unal, G., Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science, vol. 9901 (Springer, Cham, Switzerland; 2016).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15.">
              <p class="c-article-references__text" id="ref-CR15">
               Muether, P. S., Hermann, M. M., Koch, K. &amp; Fauser, S. Delay between medical indication to anti-VEGF treatment in age-related macular degeneration can result in a loss of visual acuity.
               <i>
                Graefes Arch. Clin. Exp. Ophthalmol.
               </i>
               <b>
                249
               </b>
               , 633–637 (2011).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 15" data-doi="10.1007/s00417-010-1520-9" data-track="click" data-track-action="article reference" data-track-label="10.1007/s00417-010-1520-9" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs00417-010-1520-9" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 15" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXltlKjsbk%3D" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 15" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Delay%20between%20medical%20indication%20to%20anti-VEGF%20treatment%20in%20age-related%20macular%20degeneration%20can%20result%20in%20a%20loss%20of%20visual%20acuity&amp;journal=Graefes%20Arch.%20Clin.%20Exp.%20Ophthalmol.&amp;doi=10.1007%2Fs00417-010-1520-9&amp;volume=249&amp;pages=633-637&amp;publication_year=2011&amp;author=Muether%2CPS&amp;author=Hermann%2CMM&amp;author=Koch%2CK&amp;author=Fauser%2CS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16.">
              <p class="c-article-references__text" id="ref-CR16">
               Arias, L. et al. Delay in treating age-related macular degeneration in Spain is associated with progressive vision loss.
               <i>
                Eye
               </i>
               <b>
                23
               </b>
               , 326–333 (2009).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 16" data-doi="10.1038/sj.eye.6703053" data-track="click" data-track-action="article reference" data-track-label="10.1038/sj.eye.6703053" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Fsj.eye.6703053" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 16" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:STN:280:DC%2BD1M7js1Ojsw%3D%3D" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 16" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Delay%20in%20treating%20age-related%20macular%20degeneration%20in%20Spain%20is%20associated%20with%20progressive%20vision%20loss&amp;journal=Eye&amp;doi=10.1038%2Fsj.eye.6703053&amp;volume=23&amp;pages=326-333&amp;publication_year=2009&amp;author=Arias%2CL" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17.">
              <p class="c-article-references__text" id="ref-CR17">
               Karri, S. P. K., Chakraborty, D. &amp; Chatterjee, J. Transfer learning based classification of optical coherence tomography images with diabetic macular edema and dry age-related macular degeneration.
               <i>
                Biomed. Opt. Express
               </i>
               <b>
                8
               </b>
               , 579–592 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 17" data-doi="10.1364/BOE.8.000579" data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.8.000579" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.8.000579" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 17" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:STN:280:DC%2BC1czjvFGhtw%3D%3D" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 17" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20learning%20based%20classification%20of%20optical%20coherence%20tomography%20images%20with%20diabetic%20macular%20edema%20and%20dry%20age-related%20macular%20degeneration&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.8.000579&amp;volume=8&amp;pages=579-592&amp;publication_year=2017&amp;author=Karri%2CSPK&amp;author=Chakraborty%2CD&amp;author=Chatterjee%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18.">
              <p class="c-article-references__text" id="ref-CR18">
               Apostolopoulos, S., Ciller, C., De Zanet, S. I., Wolf, S. &amp; Sznitman, R. RetiNet: automatic AMD identification in OCT volumetric data. Preprint at
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1610.03628v1" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1610.03628v1">
                http://arxiv-org.proxy.lib.ohio-state.edu/abs/1610.03628v1
               </a>
               (2016).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19.">
              <p class="c-article-references__text" id="ref-CR19">
               Farsiu, S. et al. Quantitative classification of eyes with and without intermediate age-related macular degeneration using optical coherence tomography.
               <i>
                Ophthalmology
               </i>
               <b>
                121
               </b>
               , 162–172 (2014).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 19" data-doi="10.1016/j.ophtha.2013.07.013" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ophtha.2013.07.013" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ophtha.2013.07.013" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 19" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantitative%20classification%20of%20eyes%20with%20and%20without%20intermediate%20age-related%20macular%20degeneration%20using%20optical%20coherence%20tomography&amp;journal=Ophthalmology&amp;doi=10.1016%2Fj.ophtha.2013.07.013&amp;volume=121&amp;pages=162-172&amp;publication_year=2014&amp;author=Farsiu%2CS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20.">
              <p class="c-article-references__text" id="ref-CR20">
               Srinivasan, P. P. et al. Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images.
               <i>
                Biomed. Opt. Express
               </i>
               <b>
                5
               </b>
               , 3568–3577 (2014).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 20" data-doi="10.1364/BOE.5.003568" data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.5.003568" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.5.003568" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 20" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Fully%20automated%20detection%20of%20diabetic%20macular%20edema%20and%20dry%20age-related%20macular%20degeneration%20from%20optical%20coherence%20tomography%20images&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.5.003568&amp;volume=5&amp;pages=3568-3577&amp;publication_year=2014&amp;author=Srinivasan%2CPP" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21.">
              <p class="c-article-references__text" id="ref-CR21">
               Lee, C. S., Baughman, D. M. &amp; Lee, A. Y. Deep learning is effective for classifying normal versus age-related macular degeneration OCT images.
               <i>
                Ophthalmol. Retin.
               </i>
               <b>
                1
               </b>
               , 322–327 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 21" data-doi="10.1016/j.oret.2016.12.009" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.oret.2016.12.009" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.oret.2016.12.009" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 21" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20is%20effective%20for%20classifying%20normal%20versus%20age-related%20macular%20degeneration%20OCT%20images&amp;journal=Ophthalmol.%20Retin.&amp;doi=10.1016%2Fj.oret.2016.12.009&amp;volume=1&amp;pages=322-327&amp;publication_year=2017&amp;author=Lee%2CCS&amp;author=Baughman%2CDM&amp;author=Lee%2CAY" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22.">
              <p class="c-article-references__text" id="ref-CR22">
               Fang, L. et al. Automatic segmentation of nine retinal layer boundaries in OCT images of non-exudative AMD patients using deep learning and graph search.
               <i>
                Biomed. Opt. Express
               </i>
               <b>
                8
               </b>
               , 2732–2744 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 22" data-doi="10.1364/BOE.8.002732" data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.8.002732" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.8.002732" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 22" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20segmentation%20of%20nine%20retinal%20layer%20boundaries%20in%20OCT%20images%20of%20non-exudative%20AMD%20patients%20using%20deep%20learning%20and%20graph%20search&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.8.002732&amp;volume=8&amp;pages=2732-2744&amp;publication_year=2017&amp;author=Fang%2CL" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23.">
              <p class="c-article-references__text" id="ref-CR23">
               Lee, C. S. et al. Deep-learning based, automated segmentation of macular edema in optical coherence tomography.
               <i>
                Biomed. Opt. Express
               </i>
               <b>
                8
               </b>
               , 3440–3448 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 23" data-doi="10.1364/BOE.8.003440" data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.8.003440" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.8.003440" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 23" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep-learning%20based%2C%20automated%20segmentation%20of%20macular%20edema%20in%20optical%20coherence%20tomography&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.8.003440&amp;volume=8&amp;pages=3440-3448&amp;publication_year=2017&amp;author=Lee%2CCS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24.">
              <p class="c-article-references__text" id="ref-CR24">
               Lu, D. et al. Retinal fluid segmentation and detection in optical coherence tomography images using fully convolutional neural network. Preprint at
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.04778v1" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.04778v1">
                http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.04778v1
               </a>
               (2017).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25.">
              <p class="c-article-references__text" id="ref-CR25">
               Roy, A. G. et al. ReLayNet: retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional network.
               <i>
                Biomed. Opt. Express
               </i>
               <b>
                8
               </b>
               , 3627–3642 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 25" data-doi="10.1364/BOE.8.003627" data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.8.003627" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.8.003627" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 25" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=ReLayNet%3A%20retinal%20layer%20and%20fluid%20segmentation%20of%20macular%20optical%20coherence%20tomography%20using%20fully%20convolutional%20network&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.8.003627&amp;volume=8&amp;pages=3627-3642&amp;publication_year=2017&amp;author=Roy%2CAG" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26.">
              <p class="c-article-references__text" id="ref-CR26">
               Castelvecchi, D. Can we open the black box of AI?
               <i>
                Nature
               </i>
               <b>
                538
               </b>
               , 20–23 (2016).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 26" data-doi="10.1038/538020a" data-track="click" data-track-action="article reference" data-track-label="10.1038/538020a" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2F538020a" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 26" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:CAS:528:DC%2BC28Xhs1ehsr7F" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 26" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Can%20we%20open%20the%20black%20box%20of%20AI%3F&amp;journal=Nature&amp;doi=10.1038%2F538020a&amp;volume=538&amp;pages=20-23&amp;publication_year=2016&amp;author=Castelvecchi%2CD" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27.">
              <p class="c-article-references__text" id="ref-CR27">
               Schmidt-Erfurth, U. et al. Machine learning to analyze the prognostic value of current imaging biomarkers in neovascular age-related macular degeneration.
               <i>
                Ophthalmol. Retin.
               </i>
               <b>
                2
               </b>
               , 24–30 (2018).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 27" data-doi="10.1016/j.oret.2017.03.015" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.oret.2017.03.015" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.oret.2017.03.015" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 27" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Machine%20learning%20to%20analyze%20the%20prognostic%20value%20of%20current%20imaging%20biomarkers%20in%20neovascular%20age-related%20macular%20degeneration&amp;journal=Ophthalmol.%20Retin.&amp;doi=10.1016%2Fj.oret.2017.03.015&amp;volume=2&amp;pages=24-30&amp;publication_year=2018&amp;author=Schmidt-Erfurth%2CU" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28.">
              <p class="c-article-references__text" id="ref-CR28">
               Schlegl, T. et al. Fully automated detection and quantification of macular fluid in OCT using deep learning.
               <i>
                Ophthalmology
               </i>
               <b>
                125
               </b>
               , 549–558 (2018).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 28" data-doi="10.1016/j.ophtha.2017.10.031" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ophtha.2017.10.031" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ophtha.2017.10.031" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 28" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Fully%20automated%20detection%20and%20quantification%20of%20macular%20fluid%20in%20OCT%20using%20deep%20learning&amp;journal=Ophthalmology&amp;doi=10.1016%2Fj.ophtha.2017.10.031&amp;volume=125&amp;pages=549-558&amp;publication_year=2018&amp;author=Schlegl%2CT" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29.">
              <p class="c-article-references__text" id="ref-CR29">
               Keane, P. A. &amp; Sadda, S. R. Predicting visual outcomes for macular disease using optical coherence tomography.
               <i>
                Saudi J. Ophthalmol.
               </i>
               <b>
                25
               </b>
               , 145–158 (2011).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 29" data-doi="10.1016/j.sjopt.2011.01.003" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.sjopt.2011.01.003" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.sjopt.2011.01.003" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 29" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20visual%20outcomes%20for%20macular%20disease%20using%20optical%20coherence%20tomography&amp;journal=Saudi%20J.%20Ophthalmol.&amp;doi=10.1016%2Fj.sjopt.2011.01.003&amp;volume=25&amp;pages=145-158&amp;publication_year=2011&amp;author=Keane%2CPA&amp;author=Sadda%2CSR" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30.">
              <p class="c-article-references__text" id="ref-CR30">
               Schaal, K. B., Rosenfeld, P. J., Gregori, G., Yehoshua, Z. &amp; Feuer, W. J. Anatomic clinical trial endpoints for nonexudative age-related macular degeneration.
               <i>
                Ophthalmology
               </i>
               <b>
                123
               </b>
               , 1060–1079 (2016).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 30" data-doi="10.1016/j.ophtha.2016.01.034" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ophtha.2016.01.034" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ophtha.2016.01.034" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 30" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Anatomic%20clinical%20trial%20endpoints%20for%20nonexudative%20age-related%20macular%20degeneration&amp;journal=Ophthalmology&amp;doi=10.1016%2Fj.ophtha.2016.01.034&amp;volume=123&amp;pages=1060-1079&amp;publication_year=2016&amp;author=Schaal%2CKB&amp;author=Rosenfeld%2CPJ&amp;author=Gregori%2CG&amp;author=Yehoshua%2CZ&amp;author=Feuer%2CWJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31.">
              <p class="c-article-references__text" id="ref-CR31">
               Schmidt-Erfurth, U. &amp; Waldstein, S. M. A paradigm shift in imaging biomarkers in neovascular age-related macular degeneration.
               <i>
                Prog. Retin. Eye Res.
               </i>
               <b>
                50
               </b>
               , 1–24 (2016).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 31" data-doi="10.1016/j.preteyeres.2015.07.007" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.preteyeres.2015.07.007" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.preteyeres.2015.07.007" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 31" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXhsV2mu7nF" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 31" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20paradigm%20shift%20in%20imaging%20biomarkers%20in%20neovascular%20age-related%20macular%20degeneration&amp;journal=Prog.%20Retin.%20Eye%20Res.&amp;doi=10.1016%2Fj.preteyeres.2015.07.007&amp;volume=50&amp;pages=1-24&amp;publication_year=2016&amp;author=Schmidt-Erfurth%2CU&amp;author=Waldstein%2CSM" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32.">
              <p class="c-article-references__text" id="ref-CR32">
               Villani, E. et al. Decade-long profile of imaging biomarker use in ophthalmic clinical trials.
               <i>
                Invest. Ophthalmol. Vis. Sci.
               </i>
               <b>
                58
               </b>
               , BIO76–BIO81 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 32" data-doi="10.1167/iovs.17-21790" data-track="click" data-track-action="article reference" data-track-label="10.1167/iovs.17-21790" href="https://doi-org.proxy.lib.ohio-state.edu/10.1167%2Fiovs.17-21790" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 32" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Decade-long%20profile%20of%20imaging%20biomarker%20use%20in%20ophthalmic%20clinical%20trials&amp;journal=Invest.%20Ophthalmol.%20Vis.%20Sci.&amp;doi=10.1167%2Fiovs.17-21790&amp;volume=58&amp;pages=BIO76-BIO81&amp;publication_year=2017&amp;author=Villani%2CE" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33.">
              <p class="c-article-references__text" id="ref-CR33">
               Chopra, R., Mulholland, P. J., Dubis, A. M., Anderson, R. S. &amp; Keane, P. A. Human factor and usability testing of a binocular optical coherence tomography system.
               <i>
                Transl. Vis. Sci. Technol.
               </i>
               <b>
                6
               </b>
               , 16 (2017).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 33" data-doi="10.1167/tvst.6.4.16" data-track="click" data-track-action="article reference" data-track-label="10.1167/tvst.6.4.16" href="https://doi-org.proxy.lib.ohio-state.edu/10.1167%2Ftvst.6.4.16" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 33" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20factor%20and%20usability%20testing%20of%20a%20binocular%20optical%20coherence%20tomography%20system&amp;journal=Transl.%20Vis.%20Sci.%20Technol.&amp;doi=10.1167%2Ftvst.6.4.16&amp;volume=6&amp;publication_year=2017&amp;author=Chopra%2CR&amp;author=Mulholland%2CPJ&amp;author=Dubis%2CAM&amp;author=Anderson%2CRS&amp;author=Keane%2CPA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34.">
              <p class="c-article-references__text" id="ref-CR34">
               Schindelin, J. et al. Fiji: an open-source platform for biological-image analysis.
               <i>
                Nat. Methods
               </i>
               <b>
                9
               </b>
               , 676–682 (2012).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 34" data-doi="10.1038/nmeth.2019" data-track="click" data-track-action="article reference" data-track-label="10.1038/nmeth.2019" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Fnmeth.2019" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="CAS reference 34" data-track="click" data-track-action="cas reference" data-track-label="link" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhtVKnurbJ" rel="nofollow noopener">
                CAS
               </a>
               <a aria-label="Google Scholar reference 34" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Fiji%3A%20an%20open-source%20platform%20for%20biological-image%20analysis&amp;journal=Nat.%20Methods&amp;doi=10.1038%2Fnmeth.2019&amp;volume=9&amp;pages=676-682&amp;publication_year=2012&amp;author=Schindelin%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35.">
              <p class="c-article-references__text" id="ref-CR35">
               Keane, P. A. et al. Evaluation of age-related macular degeneration with optical coherence tomography.
               <i>
                Surv. Ophthalmol.
               </i>
               <b>
                57
               </b>
               , 389–414 (2012).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 35" data-doi="10.1016/j.survophthal.2012.01.006" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.survophthal.2012.01.006" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.survophthal.2012.01.006" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 35" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20of%20age-related%20macular%20degeneration%20with%20optical%20coherence%20tomography&amp;journal=Surv.%20Ophthalmol.&amp;doi=10.1016%2Fj.survophthal.2012.01.006&amp;volume=57&amp;pages=389-414&amp;publication_year=2012&amp;author=Keane%2CPA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36.">
              <p class="c-article-references__text" id="ref-CR36">
               Folgar, F. A. et al. Comparison of optical coherence tomography assessments in the comparison of age-related macular degeneration treatments trials.
               <i>
                Ophthalmology
               </i>
               <b>
                121
               </b>
               , 1956–1965 (2014).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 36" data-doi="10.1016/j.ophtha.2014.04.020" data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ophtha.2014.04.020" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ophtha.2014.04.020" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 36" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparison%20of%20optical%20coherence%20tomography%20assessments%20in%20the%20comparison%20of%20age-related%20macular%20degeneration%20treatments%20trials&amp;journal=Ophthalmology&amp;doi=10.1016%2Fj.ophtha.2014.04.020&amp;volume=121&amp;pages=1956-1965&amp;publication_year=2014&amp;author=Folgar%2CFA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37.">
              <p class="c-article-references__text" id="ref-CR37">
               Duker, J. S., Waheed, N. K. &amp; Goldman, D.
               <i>
                Handbook of Retinal OCT: Optical Coherence Tomography E-Book
               </i>
               (Elsevier Health Sciences, Oxford, UK; 2013).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38.">
              <p class="c-article-references__text" id="ref-CR38">
               Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &amp; Wojna, Z. Rethinking the inception architecture for computer vision.
               <i>
                Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit
               </i>
               . 2818–2826 (2016).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39.">
              <p class="c-article-references__text" id="ref-CR39">
               Abadi, M. et al. TensorFlow: large-scale machine learning on heterogeneous systems. Preprint at
               <a data-track="click" data-track-action="external reference" data-track-label="https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467" href="https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467">
                https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467
               </a>
               (2016).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40.">
              <p class="c-article-references__text" id="ref-CR40">
               Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. in Proceedings of the 3rd International Conference on Learning Representations (ICLR). Preprint at
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980">
                http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980
               </a>
               (2015).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41.">
              <p class="c-article-references__text" id="ref-CR41">
               Huang, G., Liu, Z., Weinberger, K. Q. &amp; van der Maaten, L. Densely connected convolutional networks.
               <i>
                Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.
               </i>
               2261–2269 (2017).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42.">
              <p class="c-article-references__text" id="ref-CR42">
               Lakshminarayanan, B., Pritzel, A. &amp; Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles.
               <i>
                Adv. Neural Inf. Process. Syst
               </i>
               . 6405–6416 (2017).
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43.">
              <p class="c-article-references__text" id="ref-CR43">
               De Fauw, J. et al. Automated analysis of retinal imaging using machine learning techniques for computer vision.
               <i>
                F1000Res
               </i>
               <b>
                5
               </b>
               , 1573 (2016).
              </p>
              <p class="c-article-references__links u-hide-print">
               <a aria-label="Article reference 43" data-doi="10.12688/f1000research.8996.1" data-track="click" data-track-action="article reference" data-track-label="10.12688/f1000research.8996.1" href="https://doi-org.proxy.lib.ohio-state.edu/10.12688%2Ff1000research.8996.1" rel="nofollow noopener">
                Article
               </a>
               <a aria-label="Google Scholar reference 43" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20analysis%20of%20retinal%20imaging%20using%20machine%20learning%20techniques%20for%20computer%20vision&amp;journal=F1000Res&amp;doi=10.12688%2Ff1000research.8996.1&amp;volume=5&amp;publication_year=2016&amp;author=Fauw%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
            </ol>
            <p class="c-article-references__download u-hide-print">
             <a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1038/s41591-018-0107-6?format=refman&amp;flavour=references" rel="nofollow">
              Download references
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </p>
           </div>
          </div>
         </div>
        </section>
       </div>
       <section data-title="Acknowledgements">
        <div class="c-article-section" id="Ack1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">
          Acknowledgements
         </h2>
         <div class="c-article-section__content" id="Ack1-content">
          <p>
           We thank K. Kavukcuoglu, A. Zisserman, M. Jaderberg, K. Simonyan for discussions, A. Cain and M. Cant for work on the visuals, D. Mitchell and M. Johnson for infrastructure and systems administration, J. Morgan and OpenEyes for providing the electronic health record records, T. Peto, P. Blows, A. O’Shea and the NIHR Clinical Research Facility for work on the labeling, T. Heeran, M. Lukic, K. Kortum, K. Fasler, S. Wagner and N. Pontikos for work on the labeling, E. Steele, V. Louw, S. Gill and the rest of Moorfields IT team for work on the data collection and deidentification, S. Al-Abed and N. Smith for Moorfields technical advice at project initiation, R. Wood and D. Corder at Softwire for engineering support at Moorfields, R. Ogbe and the Moorfields Information Governance team for support, M. Hassard for Moorfields research and development support, K. Bonstein and the National Institute for Health Research (NIHR) for support at the Moorfields Biomedical Research Centre (BRC), J. Besley for legal assistance, E. Manna for patient engagement and support, and the rest of the DeepMind team for their support, ideas and encouragement. P.A.K. is supported by an NIHR Clinician Scientist Award (NIHR-CS-2014-14-023). D.A.S., A.T., C.E. and P.T.K. are supported by the NIHR Biomedical Research Centre at Moorfields Eye Hospital NHS Foundation Trust and UCL Institute of Ophthalmology and the NIHR Moorfields Clinical Research Facility. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health. R.C. receives studentship support from the College of Optometrists, United Kingdom.
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="author-information" data-title="Author information">
        <div class="c-article-section" id="author-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">
          Author information
         </h2>
         <div class="c-article-section__content" id="author-information-content">
          <span class="c-article-author-information__subtitle u-visually-hidden" id="author-notes">
           Author notes
          </span>
          <ol class="c-article-author-information__list">
           <li class="c-article-author-information__item" id="nAff3">
            <p class="c-article-author-information__authors-list">
             Julien Cornebise
            </p>
            <p class="js-present-address">
             Present address: University College London, London, UK
            </p>
           </li>
           <li class="c-article-author-information__item" id="na1">
            <p>
             These authors contributed equally: Julien Cornebise, Pearse A. Keane, Olaf Ronneberger.
            </p>
           </li>
          </ol>
          <h3 class="c-article__sub-heading" id="affiliations">
           Authors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff1">
            <p class="c-article-author-affiliation__address">
             DeepMind, London, UK
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Jeffrey De  Fauw, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel Visentin, George van den  Driessche, Balaji Lakshminarayanan, Clemens Meyer, Faith Mackinder, Simon Bouton, Kareem Ayoub, Dominic King, Alan Karthikesalingam, Cían O. Hughes, Demis Hassabis, Trevor Back, Mustafa Suleyman, Julien Cornebise &amp; Olaf Ronneberger
            </p>
           </li>
           <li id="Aff2">
            <p class="c-article-author-affiliation__address">
             NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London, UK
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Reena Chopra, Julian Hughes, Dawn A. Sim, Catherine Egan, Adnan Tufail, Peng T. Khaw &amp; Pearse A. Keane
            </p>
           </li>
           <li id="Aff3">
            <p class="c-article-author-affiliation__address">
             University College London, London, UK
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Cían O. Hughes, Rosalind Raine, Hugh Montgomery &amp; Geraint Rees
            </p>
           </li>
          </ol>
          <div class="u-js-hide u-hide-print" data-test="author-info">
           <span class="c-article__sub-heading">
            Authors
           </span>
           <ol class="c-article-authors-search u-list-reset">
            <li id="auth-Jeffrey-Fauw-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Jeffrey De  Fauw
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Jeffrey%20De%20%20Fauw" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Jeffrey%20De%20%20Fauw" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jeffrey%20De%20%20Fauw%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Joseph_R_-Ledsam-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Joseph R. Ledsam
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Joseph%20R.%20Ledsam" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Joseph%20R.%20Ledsam" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Joseph%20R.%20Ledsam%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Bernardino-Romera_Paredes-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Bernardino Romera-Paredes
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Bernardino%20Romera-Paredes" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Bernardino%20Romera-Paredes" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Bernardino%20Romera-Paredes%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Stanislav-Nikolov-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Stanislav Nikolov
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Stanislav%20Nikolov" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Stanislav%20Nikolov" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stanislav%20Nikolov%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Nenad-Tomasev-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Nenad Tomasev
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Nenad%20Tomasev" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Nenad%20Tomasev" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nenad%20Tomasev%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Sam-Blackwell-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Sam Blackwell
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Sam%20Blackwell" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Sam%20Blackwell" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sam%20Blackwell%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Harry-Askham-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Harry Askham
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Harry%20Askham" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Harry%20Askham" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Harry%20Askham%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Xavier-Glorot-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Xavier Glorot
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Xavier%20Glorot" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Xavier%20Glorot" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xavier%20Glorot%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Brendan-O_Donoghue-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Brendan O’Donoghue
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Brendan%20O%E2%80%99Donoghue" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Brendan%20O%E2%80%99Donoghue" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Brendan%20O%E2%80%99Donoghue%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Daniel-Visentin-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Daniel Visentin
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Daniel%20Visentin" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Daniel%20Visentin" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daniel%20Visentin%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-George-_Driessche-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              George van den  Driessche
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=George%20van%20den%20%20Driessche" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=George%20van%20den%20%20Driessche" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22George%20van%20den%20%20Driessche%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Balaji-Lakshminarayanan-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Balaji Lakshminarayanan
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Balaji%20Lakshminarayanan" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Balaji%20Lakshminarayanan" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Balaji%20Lakshminarayanan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Clemens-Meyer-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Clemens Meyer
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Clemens%20Meyer" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Clemens%20Meyer" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Clemens%20Meyer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Faith-Mackinder-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Faith Mackinder
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Faith%20Mackinder" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Faith%20Mackinder" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Faith%20Mackinder%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Simon-Bouton-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Simon Bouton
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Simon%20Bouton" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Simon%20Bouton" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Simon%20Bouton%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Kareem-Ayoub-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Kareem Ayoub
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Kareem%20Ayoub" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Kareem%20Ayoub" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kareem%20Ayoub%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Reena-Chopra-Aff2">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Reena Chopra
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Reena%20Chopra" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Reena%20Chopra" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Reena%20Chopra%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Dominic-King-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Dominic King
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Dominic%20King" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Dominic%20King" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dominic%20King%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Alan-Karthikesalingam-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Alan Karthikesalingam
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Alan%20Karthikesalingam" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Alan%20Karthikesalingam" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Alan%20Karthikesalingam%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-C_an_O_-Hughes-Aff1-Aff3">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Cían O. Hughes
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=C%C3%ADan%20O.%20Hughes" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=C%C3%ADan%20O.%20Hughes" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22C%C3%ADan%20O.%20Hughes%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Rosalind-Raine-Aff3">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Rosalind Raine
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Rosalind%20Raine" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Rosalind%20Raine" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rosalind%20Raine%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Julian-Hughes-Aff2">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Julian Hughes
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Julian%20Hughes" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Julian%20Hughes" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Julian%20Hughes%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Dawn_A_-Sim-Aff2">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Dawn A. Sim
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Dawn%20A.%20Sim" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Dawn%20A.%20Sim" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dawn%20A.%20Sim%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Catherine-Egan-Aff2">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Catherine Egan
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Catherine%20Egan" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Catherine%20Egan" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Catherine%20Egan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Adnan-Tufail-Aff2">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Adnan Tufail
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Adnan%20Tufail" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Adnan%20Tufail" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Adnan%20Tufail%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Hugh-Montgomery-Aff3">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Hugh Montgomery
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Hugh%20Montgomery" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Hugh%20Montgomery" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hugh%20Montgomery%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Demis-Hassabis-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Demis Hassabis
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Demis%20Hassabis" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Demis%20Hassabis" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Demis%20Hassabis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Geraint-Rees-Aff3">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Geraint Rees
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Geraint%20Rees" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Geraint%20Rees" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Geraint%20Rees%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Trevor-Back-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Trevor Back
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Trevor%20Back" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Trevor%20Back" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Trevor%20Back%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Peng_T_-Khaw-Aff2">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Peng T. Khaw
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Peng%20T.%20Khaw" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Peng%20T.%20Khaw" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Peng%20T.%20Khaw%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Mustafa-Suleyman-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Mustafa Suleyman
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Mustafa%20Suleyman" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Mustafa%20Suleyman" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mustafa%20Suleyman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Julien-Cornebise-Aff1-Aff3">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Julien Cornebise
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Julien%20Cornebise" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Julien%20Cornebise" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Julien%20Cornebise%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Pearse_A_-Keane-Aff2">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Pearse A. Keane
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Pearse%20A.%20Keane" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Pearse%20A.%20Keane" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Pearse%20A.%20Keane%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Olaf-Ronneberger-Aff1">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Olaf Ronneberger
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?author=Olaf%20Ronneberger" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Olaf%20Ronneberger" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Olaf%20Ronneberger%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
           </ol>
          </div>
          <h3 class="c-article__sub-heading" id="contributions">
           Contributions
          </h3>
          <p>
           P.A.K., M.S., J.C., D.H., P.T.K., T.B. and K.A. initiated the project and the collaboration. O.R., J.D.F., B.R.-P. and S.N. developed the network architectures, training and testing setup. P.A.K., J.R.L. and R.C. designed the clinical setup. P.A.K., J.R.L., J.C., R.C., D.A.S., C.E. and A.T. created the dataset and defined clinical labels. J.D.F., B.R.-P., S.N., N.T., S.Bl., H.A., B.O., D.V., G.v.d.D., O.R. and J.C. contributed to the software engineering. J.R.L., S.Bl. and H.A. created the database. P.A.K., J.R.L., D.K., A.K., C.O.H. and R.R. contributed clinical expertise. O.R., P.A.K., J.D.F., J.R.L., B.R.-P., S.N., N.T. and X.G. analysed the data. T.B., S.Bo., J.C., J.H., F.M. and C.M. managed the project. O.R., P.A.K., J.R.L., J.D.F., B.R.-P., G.R. and H.M. wrote the paper. B.L. contributed to the uncertainty estimation.
          </p>
          <h3 class="c-article__sub-heading" id="corresponding-author">
           Corresponding authors
          </h3>
          <p id="corresponding-author-list">
           Correspondence to
           <a href="mailto:pearse.keane@moorfields.nhs.uk" id="corresp-c1">
            Pearse A. Keane
           </a>
           or
           <a href="mailto:olafr@deepmind.com" id="corresp-c2">
            Olaf Ronneberger
           </a>
           .
          </p>
         </div>
        </div>
       </section>
       <section data-title="Ethics declarations">
        <div class="c-article-section" id="ethics-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">
          Ethics declarations
         </h2>
         <div class="c-article-section__content" id="ethics-content">
          <h3 class="c-article__sub-heading" id="FPar1">
           Competing interests
          </h3>
          <p>
           P.A.K., G.R., H.M. and R.R. are paid contractors of DeepMind. P.A.K. has received speaker fees from Heidelberg Engineering, Topcon, Haag-Streit, Allergan, Novartis and Bayer. P.A.K. has served on advisory boards for Novartis and Bayer, and is an external consultant for DeepMind and Optos. A.T. has served on advisory boards for the following companies: Allergan, Bayer, Genentech, GlaxoSmithKline, Novartis, Roche. C.E. has received speaker fees from Heidelberg Engineering and Haag-Streit UK. P.T.K. has served on advisory boards for Aerie, Allergan, Alcon, Belkin Laser, Novartis and Santen. D.A.S. has received speaker fees from Novartis, Bayer, Allergan, Haag-Streit. The authors have no other competing interests to disclose.
          </p>
         </div>
        </div>
       </section>
       <section data-title="Additional information">
        <div class="c-article-section" id="additional-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">
          Additional information
         </h2>
         <div class="c-article-section__content" id="additional-information-content">
          <p>
           <b>
            Publisher’s note:
           </b>
           Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
          </p>
         </div>
        </div>
       </section>
       <section data-title="Supplementary Information">
        <div class="c-article-section" id="Sec29-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec29">
          Supplementary Information
         </h2>
         <div class="c-article-section__content" id="Sec29-content">
          <div data-test="supplementary-info">
           <div class="c-article-figshare-container" data-test="figshare-container" id="figshareContainer">
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary text and figures" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM1_ESM.pdf">
              Supplementary Text and Figures
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Supplementary Figures 1–16 and Supplementary Tables 1–10
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="reporting summary" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM2_ESM.pdf">
              Reporting Summary
             </a>
            </h3>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM3">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 1" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM3_ESM.mp4">
              Supplementary Video 1
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              OCT viewer | This video demonstrates the interaction with the OCT viewer. The OCT scan belongs to a 72 year old female presented with increasing visual distortion over a 4 month period; the OCT shows loss of RPE consistent with geographic atrophy. The view first goes through the whole volume (128 slices) for a fixed tissue map hypothesis, followed by showing the different tissue map hypotheses for a given slice. Finally, we let the collage cycle through the different hypotheses continually while scrolling through the volume, pausing on several slices briefly to show the variations. The color legend for all segmentation maps is available in Supplementary Table 2.
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM4">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 2" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM4_ESM.mp4">
              Supplementary Video 2
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Wet AMD | Choroidal neovascularization (CNV) is the pathognomonic feature of the neovascular (“wet”) form of age-related macular degeneration (AMD) and requires urgent treatment to prevent irreversible visual loss. A 72-year old man presented with a history of reduced vision in his left eye. Best corrected visual acuity in the affected eye was 38 Early Treatment Diabetic Retinopathy Study (ETDRS) letters. The model correctly selects the Most Urgent Diagnosis as “CNV”, suggesting referral to an ophthalmologist on an urgent basis. The model segmentation highlights growth of the neovascular tissue in the sub-retinal pigment epithelium (RPE) space – a so-called fibrovascular pigment epithelium detachment (PED). Subretinal fluid can be seen surrounding the inferior margins of the fibrovascular PED indicating the presence of ongoing CNV leakage.
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM5">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 3" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM5_ESM.mp4">
              Supplementary Video 3
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Normal | Scans are quick and safe to perform and are thus commonly used in the screening of patients without visual symptoms or other ophthalmic findings. A 46-year old man who was referred for retinal specialist review. Best corrected visual acuity was 6/6. The model correctly selects the referral decision as “Observation Only”, suggesting that the OCT findings in isolation do not require referral to an ophthalmologist. The model accurately delineates the neurosensory retina without the presence of any pathologic compartments. It also highlights partial separation of the posterior hyaloid of the vitreous – this is a normal finding as the vitreous gel increasingly liquefies with age.
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM6">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 4" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM6_ESM.mp4">
              Supplementary Video 4
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Diabetic macular edema | Accumulation of this fluid in the macula – diabetic macular edema (DME) – is the commonest cause of visual impairment in diabetes. A 54-year old man with diabetes was referred to Moorfields for ophthalmologist review with best corrected visual acuity in the affected eye of 45 ETDRS letters. The model correctly detects the presence of macular retinal edema (MRE) and suggests semi-urgent ophthalmology referral. The model highlights intraretinal fluid accumulation, with cystoid spaces in both the inner nuclear and outer plexiform layers, and a mixed petaloid/honeycomb appearance on the en face images. There is also an accompanying significant increase in total retinal thickness.
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM7">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 5" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM7_ESM.mp4">
              Supplementary Video 5
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Ambiguous case (chronic central serous retinopathy) | In chronic CSR, diagnosis of secondary CNV formation is often challenging due to the frequent presence of shallow irregular pigment epithelium detachments (PEDs). A 60-64 year old woman presented with a history of CSR in her left eye. The model correctly detects the presence of CSR but is far less certain about the presence of CNV. It highlights a gravitational tract of subretinal fluid with a discrete area of fibrovascular PED superior to the fovea.
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM8">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 6" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM8_ESM.mp4">
              Supplementary Video 6
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Ambiguous case (advanced geographic atrophy) | In advanced forms of AMD, geographic atrophy (GA) may sometimes coexist with CNV formation. In such cases, the CNV component may be clinically silent, and the fundus appearance may be limited to that of GA, making the diagnosis difficult. A 84-year old man was referred to Moorfields. Best corrected visual acuity in the affected eye was 1/60. The ground truth diagnosis was GA and routine referral was recommended. While the model correctly diagnoses the presence of GA and drusen, it suggests urgent referral due to the possible presence of CNV. The presence of subretinal hyperreflective on model segmentation is suggestive of previous CNV formation.
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM9">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 7" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM9_ESM.mp4">
              Supplementary Video 7
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Difficult case of choroidal neovascularization | A 30 year old male patient, with a known history of CSR, presented with acute visual loss in his left eye and was diagnosed with secondary CNV formation. At this visit, the OCT scans lack many of the prototypical features of CSR, such as subretinal fluid accumulation. The model correctly diagnoses the presence of CNV and suggests the presence of CSR, but with far less certainty.
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM10">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 8" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM10_ESM.mp4">
              Supplementary Video 8
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Failure case (partial-thickness macular hole) | Ocular media opacities may sometimes cause artefactual reductions in OCT signal strength and this can make accurate image segmentation challenging. Due to localized reduction in OCT signal strength in this case, some of the models erroneously detect the presence of a partial thickness macular hole. As a result, the models are uncertain as to whether the eye is normal or whether routine referral is required.
             </p>
            </div>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM11">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary video 9" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_MOESM11_ESM.mp4">
              Supplementary Video 9
             </a>
            </h3>
            <div class="c-article-supplementary__description" data-component="thumbnail-container">
             <p>
              Integration with other clinical information | Retinal angiomatous proliferation (RAP) is a variant of choroidal neovascularization (CNV) due to age-related macular degeneration (AMD). A 75-79 year old woman presented with reduced vision in her left eye. The model segmentation highlights the presence of a fibrovascular pigment epithelium detachment (PED) with subretinal hyperreflective material, overlying intraretinal fluid, and surrounding drusen. These findings are highly suggestive of RAP - in its early stages, this can be misdiagnosed as macular retinal edema (MRE), particularly in elderly patients with diabetes. The interpretable representation reduces the risk of misdiagnosis and allows the clinician to easily correlate these findings with other clinical information, e.g., fundus fluorescein angiography.
             </p>
            </div>
           </div>
          </div>
         </div>
        </div>
       </section>
       <section data-title="Rights and permissions">
        <div class="c-article-section" id="rightslink-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">
          Rights and permissions
         </h2>
         <div class="c-article-section__content" id="rightslink-content">
          <p class="c-article-rights">
           <a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Clinically%20applicable%20deep%20learning%20for%20diagnosis%20and%20referral%20in%20retinal%20disease&amp;author=Jeffrey%20De%20%20Fauw%20et%20al&amp;contentID=10.1038%2Fs41591-018-0107-6&amp;copyright=The%20Author%28s%29&amp;publication=1078-8956&amp;publicationDate=2018-08-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true">
            Reprints and Permissions
           </a>
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="article-info" data-title="About this article">
        <div class="c-article-section" id="article-info-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">
          About this article
         </h2>
         <div class="c-article-section__content" id="article-info-content">
          <div class="c-bibliographic-information">
           <div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border">
            <a data-crossmark="10.1038/s41591-018-0107-6" data-test="crossmark" data-track="click" data-track-action="Click Crossmark" data-track-label="link" href="https://crossmark-crossref-org.proxy.lib.ohio-state.edu/dialog/?doi=10.1038/s41591-018-0107-6" rel="noopener" target="_blank">
             <img alt="Check for updates. Verify currency and authenticity via CrossMark" height="81" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57"/>
            </a>
           </div>
           <div class="c-bibliographic-information__column">
            <h3 class="c-article__sub-heading" id="citeas">
             Cite this article
            </h3>
            <p class="c-bibliographic-information__citation">
             De  Fauw, J., Ledsam, J.R., Romera-Paredes, B.
             <i>
              et al.
             </i>
             Clinically applicable deep learning for diagnosis and referral in retinal disease.
             <i>
              Nat Med
             </i>
             <b>
              24
             </b>
             , 1342–1350 (2018). https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41591-018-0107-6
            </p>
            <p class="c-bibliographic-information__download-citation u-hide-print">
             <a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1038/s41591-018-0107-6?format=refman&amp;flavour=citation" rel="nofollow">
              Download citation
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </p>
            <ul class="c-bibliographic-information__list" data-test="publication-history">
             <li class="c-bibliographic-information__list-item">
              <p>
               Received
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2017-12-19">
                 19 December 2017
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p>
               Accepted
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2018-06-01">
                 01 June 2018
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p>
               Published
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2018-08-13">
                 13 August 2018
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p>
               Issue Date
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2018-09">
                 September 2018
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width">
              <p>
               <abbr title="Digital Object Identifier">
                DOI
               </abbr>
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41591-018-0107-6
               </span>
              </p>
             </li>
            </ul>
            <div data-component="share-box">
             <div class="c-article-share-box u-display-block">
              <h3 class="c-article__sub-heading">
               Share this article
              </h3>
              <p class="c-article-share-box__description">
               Anyone you share the following link with will be able to read this content:
              </p>
              <button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url" type="button">
               Get shareable link
              </button>
              <div class="js-no-share-url-container u-display-none" hidden="">
               <p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">
                Sorry, a shareable link is not currently available for this article.
               </p>
              </div>
              <div class="js-share-url-container u-display-none" hidden="">
               <p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url">
               </p>
               <button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url" type="button">
                Copy to clipboard
               </button>
              </div>
              <p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
               Provided by the Springer Nature SharedIt content-sharing initiative
              </p>
             </div>
            </div>
            <div data-component="article-info-list">
             <h3 class="c-article__sub-heading">
              Subjects
             </h3>
             <ul class="c-article-subject-list">
              <li class="c-article-subject-list__subject">
               <a data-track="click" data-track-action="view subject" data-track-label="link" href="/subjects/diagnosis">
                Diagnosis
               </a>
              </li>
              <li class="c-article-subject-list__subject">
               <a data-track="click" data-track-action="view subject" data-track-label="link" href="/subjects/eye-manifestations">
                Eye manifestations
               </a>
              </li>
              <li class="c-article-subject-list__subject">
               <a data-track="click" data-track-action="view subject" data-track-label="link" href="/subjects/machine-learning">
                Machine learning
               </a>
              </li>
              <li class="c-article-subject-list__subject">
               <a data-track="click" data-track-action="view subject" data-track-label="link" href="/subjects/three-dimensional-imaging">
                Three-dimensional imaging
               </a>
              </li>
             </ul>
            </div>
           </div>
          </div>
         </div>
        </div>
       </section>
      </div>
      <section>
       <div class="c-article-section js-article-section" id="further-reading-section">
        <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">
         This article is cited by
        </h2>
        <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
         <ul class="c-article-further-reading__list" id="further-reading-list">
          <li class="c-article-further-reading__item js-ref-item">
           <h3 class="c-article-further-reading__title">
            <a class="print-link" data-track="click" data-track-action="view further reading article" data-track-label="link:Image augmentation and automated measurement of endotracheal-tube-to-carina distance on chest radiographs in intensive care unit using a deep learning model with external validation" href="https://doi-org.proxy.lib.ohio-state.edu/10.1186/s13054-023-04320-0">
             Image augmentation and automated measurement of endotracheal-tube-to-carina distance on chest radiographs in intensive care unit using a deep learning model with external validation
            </a>
           </h3>
           <ul class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto" data-test="author-list">
            <li>
             Matthieu Oliver
            </li>
            <li>
             Amélie Renou
            </li>
            <li>
             Jerôme Allyn
            </li>
           </ul>
           <p class="c-article-further-reading__journal-title">
            <i>
             Critical Care
            </i>
            (2023)
           </p>
          </li>
          <li class="c-article-further-reading__item js-ref-item">
           <h3 class="c-article-further-reading__title">
            <a class="print-link" data-track="click" data-track-action="view further reading article" data-track-label="link:Ethics and governance of trustworthy medical artificial intelligence" href="https://doi-org.proxy.lib.ohio-state.edu/10.1186/s12911-023-02103-9">
             Ethics and governance of trustworthy medical artificial intelligence
            </a>
           </h3>
           <ul class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto" data-test="author-list">
            <li>
             Jie Zhang
            </li>
            <li>
             Zong-ming Zhang
            </li>
           </ul>
           <p class="c-article-further-reading__journal-title">
            <i>
             BMC Medical Informatics and Decision Making
            </i>
            (2023)
           </p>
          </li>
          <li class="c-article-further-reading__item js-ref-item">
           <h3 class="c-article-further-reading__title">
            <a class="print-link" data-track="click" data-track-action="view further reading article" data-track-label="link:High accuracy epidermal growth factor receptor mutation prediction via histopathological deep learning" href="https://doi-org.proxy.lib.ohio-state.edu/10.1186/s12890-023-02537-x">
             High accuracy epidermal growth factor receptor mutation prediction via histopathological deep learning
            </a>
           </h3>
           <ul class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto" data-test="author-list">
            <li>
             Dan Zhao
            </li>
            <li>
             Yanli Zhao
            </li>
            <li>
             Mulan Jin
            </li>
           </ul>
           <p class="c-article-further-reading__journal-title">
            <i>
             BMC Pulmonary Medicine
            </i>
            (2023)
           </p>
          </li>
          <li class="c-article-further-reading__item js-ref-item">
           <h3 class="c-article-further-reading__title">
            <a class="print-link" data-track="click" data-track-action="view further reading article" data-track-label="link:Therapeutic response in the HAWK and HARRIER trials using deep learning in retinal fluid volume and compartment analysis" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41433-022-02077-4">
             Therapeutic response in the HAWK and HARRIER trials using deep learning in retinal fluid volume and compartment analysis
            </a>
           </h3>
           <ul class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto" data-test="author-list">
            <li>
             Ursula Schmidt-Erfurth
            </li>
            <li>
             Zufar Mulyukov
            </li>
            <li>
             Hrvoje Bogunović
            </li>
           </ul>
           <p class="c-article-further-reading__journal-title">
            <i>
             Eye
            </i>
            (2023)
           </p>
          </li>
          <li class="c-article-further-reading__item js-ref-item">
           <h3 class="c-article-further-reading__title">
            <a class="print-link" data-track="click" data-track-action="view further reading article" data-track-label="link:Personalized treatment supported by automated quantitative fluid analysis in active neovascular age-related macular degeneration (nAMD)—a phase III, prospective, multicentre, randomized study: design and methods" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038/s41433-022-02154-8">
             Personalized treatment supported by automated quantitative fluid analysis in active neovascular age-related macular degeneration (nAMD)—a phase III, prospective, multicentre, randomized study: design and methods
            </a>
           </h3>
           <ul class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto" data-test="author-list">
            <li>
             Leonard M. Coulibaly
            </li>
            <li>
             Stefan Sacu
            </li>
            <li>
             Ursula Schmidt-Erfurth
            </li>
           </ul>
           <p class="c-article-further-reading__journal-title">
            <i>
             Eye
            </i>
            (2023)
           </p>
          </li>
         </ul>
        </div>
       </div>
      </section>
     </div>
    </article>
   </main>
   <aside aria-label="Article navigation" class="c-article-extras u-hide-print" data-component-reading-companion="" data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
     <noscript>
      <div class="c-nature-box c-nature-box--side" data-component="entitlement-box">
       <p class="c-nature-box__text js-text">
        You have full access to this article via your institution.
       </p>
       <div class="c-pdf-download u-clear-both js-pdf-download">
        <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="download-pdf" data-track="click" data-track-action="download pdf" data-track-external="" data-track-label="link" download="" href="/articles/s41591-018-0107-6.pdf">
         <span class="c-pdf-download__text">
          Download PDF
         </span>
         <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
          <use xlink:href="#icon-download">
          </use>
         </svg>
        </a>
       </div>
      </div>
     </noscript>
     <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
      <div class="c-nature-box c-nature-box--side u-hide-print" data-component="entitlement-box" id="entitlement-box-right-column">
       <p class="c-nature-box__text js-text">
        You have full access to this article via
        <strong>
         Ohio State University Libraries
        </strong>
       </p>
       <div class="c-pdf-download u-clear-both js-pdf-download">
        <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-draft-ignore="true" data-readcube-pdf-url="true" data-test="download-pdf" data-track="click" data-track-action="download pdf" data-track-external="" data-track-label="link" download="" href="/articles/s41591-018-0107-6.pdf">
         <span class="c-pdf-download__text">
          Download PDF
         </span>
         <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
          <use xlink:href="#icon-download">
          </use>
         </svg>
        </a>
       </div>
      </div>
     </div>
    </div>
    <div class="c-article-associated-content__container">
     <section>
      <h2 class="c-article-associated-content__title u-mb-24">
       Associated Content
      </h2>
      <div class="c-article-associated-content__collection focus u-mb-24">
       <section>
        <p class="c-article-associated-content__collection-label u-sans-serif u-text-bold u-mb-8">
         Focus
        </p>
        <h3 class="c-article-associated-content__collection-title u-h3 u-mb-8">
         <a class="u-link-inherit" data-test="collection-link" data-track="click" data-track-action="view focus" data-track-category="associated content" data-track-label="focus" href="https://www-nature-com.proxy.lib.ohio-state.edu/collections/egjifhdcih">
          Digital Medicine
         </a>
        </h3>
       </section>
      </div>
      <div class="u-full-height u-mb-24">
       <article class="u-full-height c-card c-card--flush">
        <div class="c-card__layout u-full-height">
         <div class="c-card__body">
          <h3 class="c-card__title">
           <a class="c-card__link u-link-inherit" data-track="click" data-track-action="view article" data-track-category="associated content" data-track-label="research" href="/articles/s41591-018-0147-y">
            Automated deep-neural-network surveillance of cranial images for acute neurologic events
           </a>
          </h3>
          <ul class="c-author-list c-author-list--compact c-author-list--truncated" data-test="author-list">
           <li>
            Joseph J. Titano
           </li>
           <li>
            Marcus Badgeley
           </li>
           <li>
            Eric K. Oermann
           </li>
          </ul>
          <div class="c-card__section c-meta">
           <span class="c-meta__item">
            Nature Medicine
           </span>
           <span class="c-meta__item" data-test="article.type">
            <span class="c-meta__type">
             Letter
            </span>
           </span>
           <time class="c-meta__item" datetime="2018-08-13">
            13 Aug 2018
           </time>
          </div>
         </div>
        </div>
       </article>
      </div>
      <div class="u-full-height u-mb-24">
       <article class="u-full-height c-card c-card--flush">
        <div class="c-card__layout u-full-height">
         <div class="c-card__body">
          <h3 class="c-card__title">
           <a class="c-card__link u-link-inherit" data-track="click" data-track-action="view article" data-track-category="associated content" data-track-label="news_and_views" href="/articles/s41591-018-0178-4">
            New machine-learning technologies for computer-aided diagnosis
           </a>
          </h3>
          <ul class="c-author-list c-author-list--compact" data-test="author-list">
           <li>
            Charles J. Lynch
           </li>
           <li>
            Conor Liston
           </li>
          </ul>
          <div class="c-card__section c-meta">
           <span class="c-meta__item">
            Nature Medicine
           </span>
           <span class="c-meta__item" data-test="article.type">
            <span class="c-meta__type">
             News &amp; Views
            </span>
           </span>
           <time class="c-meta__item" datetime="2018-09-03">
            03 Sept 2018
           </time>
          </div>
         </div>
        </div>
       </article>
      </div>
     </section>
    </div>
    <script>
     window.dataLayer = window.dataLayer || [];
            window.dataLayer[0] = window.dataLayer[0] || {};
            window.dataLayer[0].content = window.dataLayer[0].content || {};
            window.dataLayer[0].content.associatedContentTypes = "focus;research;news_and_views";
    </script>
    <div class="c-reading-companion">
     <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky" style="top: 40px;">
      <ul class="c-reading-companion__tabs" role="tablist">
       <li role="presentation">
        <button aria-controls="tabpanel-sections" aria-selected="true" class="c-reading-companion__tab c-reading-companion__tab--active" data-tab-target="sections" data-track="click" data-track-action="sections tab" data-track-label="tab" id="tab-sections" role="tab">
         Sections
        </button>
       </li>
       <li role="presentation">
        <button aria-controls="tabpanel-figures" aria-selected="false" class="c-reading-companion__tab" data-tab-target="figures" data-track="click" data-track-action="figures tab" data-track-label="tab" id="tab-figures" role="tab" tabindex="-1">
         Figures
        </button>
       </li>
       <li role="presentation">
        <button aria-controls="tabpanel-references" aria-selected="false" class="c-reading-companion__tab" data-tab-target="references" data-track="click" data-track-action="references tab" data-track-label="tab" id="tab-references" role="tab" tabindex="-1">
         References
        </button>
       </li>
      </ul>
      <div aria-labelledby="tab-sections" class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections" role="tabpanel">
       <div class="c-reading-companion__scroll-pane" style="max-height: none;">
        <ul class="c-reading-companion__sections-list">
         <li class="c-reading-companion__section-item" id="rc-sec-Abs1">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Abstract" href="#Abs1">
           Abstract
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-Sec1">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Main" href="#Sec1">
           Main
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-Sec2">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Results" href="#Sec2">
           Results
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-Sec7">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Discussion" href="#Sec7">
           Discussion
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-Sec8">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Methods" href="#Sec8">
           Methods
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-Bib1">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:References" href="#Bib1">
           References
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-Ack1">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Acknowledgements" href="#Ack1">
           Acknowledgements
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-author-information">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Author information" href="#author-information">
           Author information
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-ethics">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Ethics declarations" href="#ethics">
           Ethics declarations
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-additional-information">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Additional information" href="#additional-information">
           Additional information
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-Sec29">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Supplementary Information" href="#Sec29">
           Supplementary Information
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-rightslink">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:Rights and permissions" href="#rightslink">
           Rights and permissions
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-article-info">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:About this article" href="#article-info">
           About this article
          </a>
         </li>
         <li class="c-reading-companion__section-item" id="rc-sec-further-reading">
          <a data-track="click" data-track-action="section anchor" data-track-label="link:This article is cited by" href="#further-reading">
           This article is cited by
          </a>
         </li>
        </ul>
       </div>
       <div class="u-lazy-ad-wrapper u-mt-16 u-show" data-component-mpu="">
        <div class="c-ad c-ad--300x250">
         <div class="c-ad__inner">
          <p class="c-ad__label">
           Advertisement
          </p>
          <div class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide" data-ad-type="right" data-gpt="" data-gpt-sizes="300x250" data-gpt-targeting="type=article;pos=right;artid=s41591-018-0107-6;doi=10.1038/s41591-018-0107-6;subjmeta=114,1305,139,1421,1482,1807,2025,631,692,700;kwrd=Diagnosis,Eye+manifestations,Machine+learning,Three-dimensional+imaging" data-gpt-unitpath="/285/medicine.nature.com/article" data-pa11y-ignore="" data-test="right-ad" id="div-gpt-ad-right-2">
           <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/medicine.nature.com/article&amp;sz=300x250&amp;c=-488080609&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41591-018-0107-6%26doi%3D10.1038/s41591-018-0107-6%26subjmeta%3D114,1305,139,1421,1482,1807,2025,631,692,700%26kwrd%3DDiagnosis,Eye+manifestations,Machine+learning,Three-dimensional+imaging">
             <img alt="Advertisement" data-test="gpt-advert-fallback-img" height="250" src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/medicine.nature.com/article&amp;sz=300x250&amp;c=-488080609&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41591-018-0107-6%26doi%3D10.1038/s41591-018-0107-6%26subjmeta%3D114,1305,139,1421,1482,1807,2025,631,692,700%26kwrd%3DDiagnosis,Eye+manifestations,Machine+learning,Three-dimensional+imaging" width="300"/>
            </a>
           </noscript>
          </div>
         </div>
        </div>
       </div>
      </div>
      <div aria-labelledby="tab-figures" class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures" role="tabpanel">
       <div class="c-reading-companion__scroll-pane">
        <ul class="c-reading-companion__figures-list">
         <li class="c-reading-companion__figure-item">
          <figure>
           <figcaption>
            <b class="c-reading-companion__figure-title u-h4" id="rc-Fig1">
             Fig. 1: Our proposed AI framework.
            </b>
           </figcaption>
           <picture>
            <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig1_HTML.jpg?"/>
            <img alt="figure 1" aria-describedby="rc-Fig1" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig1_HTML.jpg"/>
           </picture>
           <p class="c-reading-companion__figure-links">
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             View in article
            </a>
            <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6/figures/1" rel="nofollow">
             Full size image
             <svg class="u-icon" height="16" width="16">
              <use href="#icon-chevron-right">
              </use>
             </svg>
            </a>
           </p>
          </figure>
         </li>
         <li class="c-reading-companion__figure-item">
          <figure>
           <figcaption>
            <b class="c-reading-companion__figure-title u-h4" id="rc-Fig2">
             Fig. 2: Results of the segmentation network.
            </b>
           </figcaption>
           <picture>
            <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig2_HTML.jpg?"/>
            <img alt="figure 2" aria-describedby="rc-Fig2" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig2_HTML.jpg"/>
           </picture>
           <p class="c-reading-companion__figure-links">
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             View in article
            </a>
            <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6/figures/2" rel="nofollow">
             Full size image
             <svg class="u-icon" height="16" width="16">
              <use href="#icon-chevron-right">
              </use>
             </svg>
            </a>
           </p>
          </figure>
         </li>
         <li class="c-reading-companion__figure-item">
          <figure>
           <figcaption>
            <b class="c-reading-companion__figure-title u-h4" id="rc-Fig3">
             Fig. 3: Results on the patient referral decision.
            </b>
           </figcaption>
           <picture>
            <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig3_HTML.jpg?"/>
            <img alt="figure 3" aria-describedby="rc-Fig3" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig3_HTML.jpg"/>
           </picture>
           <p class="c-reading-companion__figure-links">
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             View in article
            </a>
            <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6/figures/3" rel="nofollow">
             Full size image
             <svg class="u-icon" height="16" width="16">
              <use href="#icon-chevron-right">
              </use>
             </svg>
            </a>
           </p>
          </figure>
         </li>
         <li class="c-reading-companion__figure-item">
          <figure>
           <figcaption>
            <b class="c-reading-companion__figure-title u-h4" id="rc-Fig4">
             Fig. 4: Generalization to a new scanning device type.
            </b>
           </figcaption>
           <picture>
            <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig4_HTML.jpg?"/>
            <img alt="figure 4" aria-describedby="rc-Fig4" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig4_HTML.jpg"/>
           </picture>
           <p class="c-reading-companion__figure-links">
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
             View in article
            </a>
            <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6/figures/4" rel="nofollow">
             Full size image
             <svg class="u-icon" height="16" width="16">
              <use href="#icon-chevron-right">
              </use>
             </svg>
            </a>
           </p>
          </figure>
         </li>
         <li class="c-reading-companion__figure-item">
          <figure>
           <figcaption>
            <b class="c-reading-companion__figure-title u-h4" id="rc-Fig5">
             Fig. 5: Visualization of the segmentation results as thickness maps.
            </b>
           </figcaption>
           <picture>
            <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig5_HTML.jpg?"/>
            <img alt="figure 5" aria-describedby="rc-Fig5" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/art%3A10.1038%2Fs41591-018-0107-6/MediaObjects/41591_2018_107_Fig5_HTML.jpg"/>
           </picture>
           <p class="c-reading-companion__figure-links">
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             View in article
            </a>
            <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/s41591-018-0107-6/figures/5" rel="nofollow">
             Full size image
             <svg class="u-icon" height="16" width="16">
              <use href="#icon-chevron-right">
              </use>
             </svg>
            </a>
           </p>
          </figure>
         </li>
        </ul>
       </div>
      </div>
      <div aria-labelledby="tab-references" class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references" role="tabpanel">
       <div class="c-reading-companion__scroll-pane">
        <ol class="c-reading-companion__references-list c-reading-companion__references-list--numeric">
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR1">
           OECD. Computed tomography (CT) exams (indicator). (2017);
           <a data-track="click" data-track-action="external reference" data-track-label="10.1787/3c994537-en" href="https://doi-org.proxy.lib.ohio-state.edu/10.1787/3c994537-en">
            https://doi-org.proxy.lib.ohio-state.edu/10.1787/3c994537-en
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR2">
           OECD. Magnetic resonance imaging (MRI) exams (indicator). (2017).
           <a data-track="click" data-track-action="external reference" data-track-label="10.1787/1d89353f-en" href="https://doi-org.proxy.lib.ohio-state.edu/10.1787/1d89353f-en">
            https://doi-org.proxy.lib.ohio-state.edu/10.1787/1d89353f-en
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR3">
           Foot, B. &amp; MacEwen, C. Surveillance of sight loss due to delay in ophthalmic treatment or review: frequency, cause and outcome.
           <i>
            Eye
           </i>
           <b>
            31
           </b>
           , 771–775 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1038/eye.2017.1" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Feye.2017.1">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:STN:280:DC%2BC1c7pvVOitw%3D%3D">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Surveillance%20of%20sight%20loss%20due%20to%20delay%20in%20ophthalmic%20treatment%20or%20review%3A%20frequency%2C%20cause%20and%20outcome&amp;journal=Eye&amp;doi=10.1038%2Feye.2017.1&amp;volume=31&amp;pages=771-775&amp;publication_year=2017&amp;author=Foot%2CB&amp;author=MacEwen%2CC">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR4">
           Owen, C. G. et al. The estimated prevalence and incidence of late stage age related macular degeneration in the UK.
           <i>
            Br. J. Ophthalmol.
           </i>
           <b>
            96
           </b>
           , 752–756 (2012).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1136/bjophthalmol-2011-301109" href="https://doi-org.proxy.lib.ohio-state.edu/10.1136%2Fbjophthalmol-2011-301109">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20estimated%20prevalence%20and%20incidence%20of%20late%20stage%20age%20related%20macular%20degeneration%20in%20the%20UK&amp;journal=Br.%20J.%20Ophthalmol.&amp;doi=10.1136%2Fbjophthalmol-2011-301109&amp;volume=96&amp;pages=752-756&amp;publication_year=2012&amp;author=Owen%2CCG">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR5">
           Rudnicka, A. R. et al. Incidence of late-stage age-related macular degeneration in American whites: systematic review and meta-analysis.
           <i>
            Am. J. Ophthalmol.
           </i>
           <b>
            160
           </b>
           , 85–93 (2015).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ajo.2015.04.003" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ajo.2015.04.003">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Incidence%20of%20late-stage%20age-related%20macular%20degeneration%20in%20American%20whites%3A%20systematic%20review%20and%20meta-analysis&amp;journal=Am.%20J.%20Ophthalmol.&amp;doi=10.1016%2Fj.ajo.2015.04.003&amp;volume=160&amp;pages=85-93&amp;publication_year=2015&amp;author=Rudnicka%2CAR">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR6">
           Bourne, R. R. A. et al. Magnitude, temporal trends, and projections of the global prevalence of blindness and distance and near vision impairment: a systematic review and meta-analysis.
           <i>
            Lancet Glob. Health
           </i>
           <b>
            5
           </b>
           , e888–e897 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/S2214-109X(17)30293-0" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2FS2214-109X%2817%2930293-0">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Magnitude%2C%20temporal%20trends%2C%20and%20projections%20of%20the%20global%20prevalence%20of%20blindness%20and%20distance%20and%20near%20vision%20impairment%3A%20a%20systematic%20review%20and%20meta-analysis&amp;journal=Lancet%20Glob.%20Health&amp;doi=10.1016%2FS2214-109X%2817%2930293-0&amp;volume=5&amp;pages=e888-e897&amp;publication_year=2017&amp;author=Bourne%2CRRA">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR7">
           Schmidt-Erfurth, U., Klimscha, S., Waldstein, S. M. &amp; Bogunović, H. A view of the current and future role of optical coherence tomography in the management of age-related macular degeneration.
           <i>
            Eye
           </i>
           <b>
            31
           </b>
           , 26–44 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1038/eye.2016.227" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Feye.2016.227">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:STN:280:DC%2BC2sjgtlWnuw%3D%3D">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20view%20of%20the%20current%20and%20future%20role%20of%20optical%20coherence%20tomography%20in%20the%20management%20of%20age-related%20macular%20degeneration&amp;journal=Eye&amp;doi=10.1038%2Feye.2016.227&amp;volume=31&amp;pages=26-44&amp;publication_year=2017&amp;author=Schmidt-Erfurth%2CU&amp;author=Klimscha%2CS&amp;author=Waldstein%2CSM&amp;author=Bogunovi%C4%87%2CH">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR8">
           Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs.
           <i>
            J. Am. Med. Assoc.
           </i>
           <b>
            316
           </b>
           , 2402–2410 (2016).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1001/jama.2016.17216" href="https://doi-org.proxy.lib.ohio-state.edu/10.1001%2Fjama.2016.17216">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20and%20validation%20of%20a%20deep%20learning%20algorithm%20for%20detection%20of%20diabetic%20retinopathy%20in%20retinal%20fundus%20photographs&amp;journal=J.%20Am.%20Med.%20Assoc.&amp;doi=10.1001%2Fjama.2016.17216&amp;volume=316&amp;pages=2402-2410&amp;publication_year=2016&amp;author=Gulshan%2CV">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR9">
           Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks.
           <i>
            Nature
           </i>
           <b>
            542
           </b>
           , 115––118 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1038/nature21056" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Fnature21056">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Dermatologist-level%20classification%20of%20skin%20cancer%20with%20deep%20neural%20networks&amp;journal=Nature&amp;doi=10.1038%2Fnature21056&amp;volume=542&amp;pages=115%E2%80%93-118&amp;publication_year=2017&amp;author=Esteva%2CA">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR10">
           Huang, D. et al. Optical coherence tomography.
           <i>
            Science
           </i>
           <b>
            254
           </b>
           , 1178–1181 (1991).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1126/science.1957169" href="https://doi-org.proxy.lib.ohio-state.edu/10.1126%2Fscience.1957169">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:STN:280:DyaK38%2Fms12lsA%3D%3D">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Optical%20coherence%20tomography&amp;journal=Science&amp;doi=10.1126%2Fscience.1957169&amp;volume=254&amp;pages=1178-1181&amp;publication_year=1991&amp;author=Huang%2CD">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR11">
           Buchan, J. C. et al. How to defuse a demographic time bomb: the way forward?
           <i>
            Eye
           </i>
           <b>
            31
           </b>
           , 1519–1522 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1038/eye.2017.114" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Feye.2017.114">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:STN:280:DC%2BC1cnptVClug%3D%3D">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20to%20defuse%20a%20demographic%20time%20bomb%3A%20the%20way%20forward%3F&amp;journal=Eye&amp;doi=10.1038%2Feye.2017.114&amp;volume=31&amp;pages=1519-1522&amp;publication_year=2017&amp;author=Buchan%2CJC">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR12">
           Whited, J. D. et al. A modeled economic analysis of a digital teleophthalmology system as used by three federal healthcare agencies for detecting proliferative diabetic retinopathy.
           <i>
            Telemed. J. E Health
           </i>
           <b>
            11
           </b>
           , 641–651 (2005).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1089/tmj.2005.11.641" href="https://doi-org.proxy.lib.ohio-state.edu/10.1089%2Ftmj.2005.11.641">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20modeled%20economic%20analysis%20of%20a%20digital%20teleophthalmology%20system%20as%20used%20by%20three%20federal%20healthcare%20agencies%20for%20detecting%20proliferative%20diabetic%20retinopathy&amp;journal=Telemed.%20J.%20E%20Health&amp;doi=10.1089%2Ftmj.2005.11.641&amp;volume=11&amp;pages=641-651&amp;publication_year=2005&amp;author=Whited%2CJD">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR13">
           Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: convolutional networks for biomedical image segmentation. in Navab N., Hornegger J., Wells W., Frangi A. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol. 9351 (Springer, Cham, Switzerland, 2015).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR14">
           Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. in Ourselin, S., Joskowicz, L., Sabuncu, M., Unal, G., Wells, W. (eds.) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. MICCAI 2016. Lecture Notes in Computer Science, vol. 9901 (Springer, Cham, Switzerland; 2016).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR15">
           Muether, P. S., Hermann, M. M., Koch, K. &amp; Fauser, S. Delay between medical indication to anti-VEGF treatment in age-related macular degeneration can result in a loss of visual acuity.
           <i>
            Graefes Arch. Clin. Exp. Ophthalmol.
           </i>
           <b>
            249
           </b>
           , 633–637 (2011).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1007/s00417-010-1520-9" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs00417-010-1520-9">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:CAS:528:DC%2BC3MXltlKjsbk%3D">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Delay%20between%20medical%20indication%20to%20anti-VEGF%20treatment%20in%20age-related%20macular%20degeneration%20can%20result%20in%20a%20loss%20of%20visual%20acuity&amp;journal=Graefes%20Arch.%20Clin.%20Exp.%20Ophthalmol.&amp;doi=10.1007%2Fs00417-010-1520-9&amp;volume=249&amp;pages=633-637&amp;publication_year=2011&amp;author=Muether%2CPS&amp;author=Hermann%2CMM&amp;author=Koch%2CK&amp;author=Fauser%2CS">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR16">
           Arias, L. et al. Delay in treating age-related macular degeneration in Spain is associated with progressive vision loss.
           <i>
            Eye
           </i>
           <b>
            23
           </b>
           , 326–333 (2009).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1038/sj.eye.6703053" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Fsj.eye.6703053">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:STN:280:DC%2BD1M7js1Ojsw%3D%3D">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Delay%20in%20treating%20age-related%20macular%20degeneration%20in%20Spain%20is%20associated%20with%20progressive%20vision%20loss&amp;journal=Eye&amp;doi=10.1038%2Fsj.eye.6703053&amp;volume=23&amp;pages=326-333&amp;publication_year=2009&amp;author=Arias%2CL">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR17">
           Karri, S. P. K., Chakraborty, D. &amp; Chatterjee, J. Transfer learning based classification of optical coherence tomography images with diabetic macular edema and dry age-related macular degeneration.
           <i>
            Biomed. Opt. Express
           </i>
           <b>
            8
           </b>
           , 579–592 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.8.000579" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.8.000579">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:STN:280:DC%2BC1czjvFGhtw%3D%3D">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20learning%20based%20classification%20of%20optical%20coherence%20tomography%20images%20with%20diabetic%20macular%20edema%20and%20dry%20age-related%20macular%20degeneration&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.8.000579&amp;volume=8&amp;pages=579-592&amp;publication_year=2017&amp;author=Karri%2CSPK&amp;author=Chakraborty%2CD&amp;author=Chatterjee%2CJ">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR18">
           Apostolopoulos, S., Ciller, C., De Zanet, S. I., Wolf, S. &amp; Sznitman, R. RetiNet: automatic AMD identification in OCT volumetric data. Preprint at
           <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1610.03628v1" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1610.03628v1">
            http://arxiv-org.proxy.lib.ohio-state.edu/abs/1610.03628v1
           </a>
           (2016).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR19">
           Farsiu, S. et al. Quantitative classification of eyes with and without intermediate age-related macular degeneration using optical coherence tomography.
           <i>
            Ophthalmology
           </i>
           <b>
            121
           </b>
           , 162–172 (2014).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ophtha.2013.07.013" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ophtha.2013.07.013">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantitative%20classification%20of%20eyes%20with%20and%20without%20intermediate%20age-related%20macular%20degeneration%20using%20optical%20coherence%20tomography&amp;journal=Ophthalmology&amp;doi=10.1016%2Fj.ophtha.2013.07.013&amp;volume=121&amp;pages=162-172&amp;publication_year=2014&amp;author=Farsiu%2CS">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR20">
           Srinivasan, P. P. et al. Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images.
           <i>
            Biomed. Opt. Express
           </i>
           <b>
            5
           </b>
           , 3568–3577 (2014).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.5.003568" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.5.003568">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Fully%20automated%20detection%20of%20diabetic%20macular%20edema%20and%20dry%20age-related%20macular%20degeneration%20from%20optical%20coherence%20tomography%20images&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.5.003568&amp;volume=5&amp;pages=3568-3577&amp;publication_year=2014&amp;author=Srinivasan%2CPP">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR21">
           Lee, C. S., Baughman, D. M. &amp; Lee, A. Y. Deep learning is effective for classifying normal versus age-related macular degeneration OCT images.
           <i>
            Ophthalmol. Retin.
           </i>
           <b>
            1
           </b>
           , 322–327 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.oret.2016.12.009" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.oret.2016.12.009">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20is%20effective%20for%20classifying%20normal%20versus%20age-related%20macular%20degeneration%20OCT%20images&amp;journal=Ophthalmol.%20Retin.&amp;doi=10.1016%2Fj.oret.2016.12.009&amp;volume=1&amp;pages=322-327&amp;publication_year=2017&amp;author=Lee%2CCS&amp;author=Baughman%2CDM&amp;author=Lee%2CAY">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR22">
           Fang, L. et al. Automatic segmentation of nine retinal layer boundaries in OCT images of non-exudative AMD patients using deep learning and graph search.
           <i>
            Biomed. Opt. Express
           </i>
           <b>
            8
           </b>
           , 2732–2744 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.8.002732" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.8.002732">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20segmentation%20of%20nine%20retinal%20layer%20boundaries%20in%20OCT%20images%20of%20non-exudative%20AMD%20patients%20using%20deep%20learning%20and%20graph%20search&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.8.002732&amp;volume=8&amp;pages=2732-2744&amp;publication_year=2017&amp;author=Fang%2CL">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR23">
           Lee, C. S. et al. Deep-learning based, automated segmentation of macular edema in optical coherence tomography.
           <i>
            Biomed. Opt. Express
           </i>
           <b>
            8
           </b>
           , 3440–3448 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.8.003440" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.8.003440">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep-learning%20based%2C%20automated%20segmentation%20of%20macular%20edema%20in%20optical%20coherence%20tomography&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.8.003440&amp;volume=8&amp;pages=3440-3448&amp;publication_year=2017&amp;author=Lee%2CCS">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR24">
           Lu, D. et al. Retinal fluid segmentation and detection in optical coherence tomography images using fully convolutional neural network. Preprint at
           <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.04778v1" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.04778v1">
            http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.04778v1
           </a>
           (2017).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR25">
           Roy, A. G. et al. ReLayNet: retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional network.
           <i>
            Biomed. Opt. Express
           </i>
           <b>
            8
           </b>
           , 3627–3642 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1364/BOE.8.003627" href="https://doi-org.proxy.lib.ohio-state.edu/10.1364%2FBOE.8.003627">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=ReLayNet%3A%20retinal%20layer%20and%20fluid%20segmentation%20of%20macular%20optical%20coherence%20tomography%20using%20fully%20convolutional%20network&amp;journal=Biomed.%20Opt.%20Express&amp;doi=10.1364%2FBOE.8.003627&amp;volume=8&amp;pages=3627-3642&amp;publication_year=2017&amp;author=Roy%2CAG">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR26">
           Castelvecchi, D. Can we open the black box of AI?
           <i>
            Nature
           </i>
           <b>
            538
           </b>
           , 20–23 (2016).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1038/538020a" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2F538020a">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:CAS:528:DC%2BC28Xhs1ehsr7F">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Can%20we%20open%20the%20black%20box%20of%20AI%3F&amp;journal=Nature&amp;doi=10.1038%2F538020a&amp;volume=538&amp;pages=20-23&amp;publication_year=2016&amp;author=Castelvecchi%2CD">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR27">
           Schmidt-Erfurth, U. et al. Machine learning to analyze the prognostic value of current imaging biomarkers in neovascular age-related macular degeneration.
           <i>
            Ophthalmol. Retin.
           </i>
           <b>
            2
           </b>
           , 24–30 (2018).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.oret.2017.03.015" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.oret.2017.03.015">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Machine%20learning%20to%20analyze%20the%20prognostic%20value%20of%20current%20imaging%20biomarkers%20in%20neovascular%20age-related%20macular%20degeneration&amp;journal=Ophthalmol.%20Retin.&amp;doi=10.1016%2Fj.oret.2017.03.015&amp;volume=2&amp;pages=24-30&amp;publication_year=2018&amp;author=Schmidt-Erfurth%2CU">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR28">
           Schlegl, T. et al. Fully automated detection and quantification of macular fluid in OCT using deep learning.
           <i>
            Ophthalmology
           </i>
           <b>
            125
           </b>
           , 549–558 (2018).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ophtha.2017.10.031" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ophtha.2017.10.031">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Fully%20automated%20detection%20and%20quantification%20of%20macular%20fluid%20in%20OCT%20using%20deep%20learning&amp;journal=Ophthalmology&amp;doi=10.1016%2Fj.ophtha.2017.10.031&amp;volume=125&amp;pages=549-558&amp;publication_year=2018&amp;author=Schlegl%2CT">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR29">
           Keane, P. A. &amp; Sadda, S. R. Predicting visual outcomes for macular disease using optical coherence tomography.
           <i>
            Saudi J. Ophthalmol.
           </i>
           <b>
            25
           </b>
           , 145–158 (2011).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.sjopt.2011.01.003" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.sjopt.2011.01.003">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20visual%20outcomes%20for%20macular%20disease%20using%20optical%20coherence%20tomography&amp;journal=Saudi%20J.%20Ophthalmol.&amp;doi=10.1016%2Fj.sjopt.2011.01.003&amp;volume=25&amp;pages=145-158&amp;publication_year=2011&amp;author=Keane%2CPA&amp;author=Sadda%2CSR">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR30">
           Schaal, K. B., Rosenfeld, P. J., Gregori, G., Yehoshua, Z. &amp; Feuer, W. J. Anatomic clinical trial endpoints for nonexudative age-related macular degeneration.
           <i>
            Ophthalmology
           </i>
           <b>
            123
           </b>
           , 1060–1079 (2016).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ophtha.2016.01.034" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ophtha.2016.01.034">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Anatomic%20clinical%20trial%20endpoints%20for%20nonexudative%20age-related%20macular%20degeneration&amp;journal=Ophthalmology&amp;doi=10.1016%2Fj.ophtha.2016.01.034&amp;volume=123&amp;pages=1060-1079&amp;publication_year=2016&amp;author=Schaal%2CKB&amp;author=Rosenfeld%2CPJ&amp;author=Gregori%2CG&amp;author=Yehoshua%2CZ&amp;author=Feuer%2CWJ">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR31">
           Schmidt-Erfurth, U. &amp; Waldstein, S. M. A paradigm shift in imaging biomarkers in neovascular age-related macular degeneration.
           <i>
            Prog. Retin. Eye Res.
           </i>
           <b>
            50
           </b>
           , 1–24 (2016).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.preteyeres.2015.07.007" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.preteyeres.2015.07.007">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:CAS:528:DC%2BC2MXhsV2mu7nF">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20paradigm%20shift%20in%20imaging%20biomarkers%20in%20neovascular%20age-related%20macular%20degeneration&amp;journal=Prog.%20Retin.%20Eye%20Res.&amp;doi=10.1016%2Fj.preteyeres.2015.07.007&amp;volume=50&amp;pages=1-24&amp;publication_year=2016&amp;author=Schmidt-Erfurth%2CU&amp;author=Waldstein%2CSM">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR32">
           Villani, E. et al. Decade-long profile of imaging biomarker use in ophthalmic clinical trials.
           <i>
            Invest. Ophthalmol. Vis. Sci.
           </i>
           <b>
            58
           </b>
           , BIO76–BIO81 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1167/iovs.17-21790" href="https://doi-org.proxy.lib.ohio-state.edu/10.1167%2Fiovs.17-21790">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Decade-long%20profile%20of%20imaging%20biomarker%20use%20in%20ophthalmic%20clinical%20trials&amp;journal=Invest.%20Ophthalmol.%20Vis.%20Sci.&amp;doi=10.1167%2Fiovs.17-21790&amp;volume=58&amp;pages=BIO76-BIO81&amp;publication_year=2017&amp;author=Villani%2CE">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR33">
           Chopra, R., Mulholland, P. J., Dubis, A. M., Anderson, R. S. &amp; Keane, P. A. Human factor and usability testing of a binocular optical coherence tomography system.
           <i>
            Transl. Vis. Sci. Technol.
           </i>
           <b>
            6
           </b>
           , 16 (2017).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1167/tvst.6.4.16" href="https://doi-org.proxy.lib.ohio-state.edu/10.1167%2Ftvst.6.4.16">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20factor%20and%20usability%20testing%20of%20a%20binocular%20optical%20coherence%20tomography%20system&amp;journal=Transl.%20Vis.%20Sci.%20Technol.&amp;doi=10.1167%2Ftvst.6.4.16&amp;volume=6&amp;publication_year=2017&amp;author=Chopra%2CR&amp;author=Mulholland%2CPJ&amp;author=Dubis%2CAM&amp;author=Anderson%2CRS&amp;author=Keane%2CPA">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR34">
           Schindelin, J. et al. Fiji: an open-source platform for biological-image analysis.
           <i>
            Nat. Methods
           </i>
           <b>
            9
           </b>
           , 676–682 (2012).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1038/nmeth.2019" href="https://doi-org.proxy.lib.ohio-state.edu/10.1038%2Fnmeth.2019">
            Article
           </a>
           <a data-track="click" data-track-action="cas reference" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/articles/cas-redirect/1:CAS:528:DC%2BC38XhtVKnurbJ">
            CAS
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Fiji%3A%20an%20open-source%20platform%20for%20biological-image%20analysis&amp;journal=Nat.%20Methods&amp;doi=10.1038%2Fnmeth.2019&amp;volume=9&amp;pages=676-682&amp;publication_year=2012&amp;author=Schindelin%2CJ">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR35">
           Keane, P. A. et al. Evaluation of age-related macular degeneration with optical coherence tomography.
           <i>
            Surv. Ophthalmol.
           </i>
           <b>
            57
           </b>
           , 389–414 (2012).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.survophthal.2012.01.006" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.survophthal.2012.01.006">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20of%20age-related%20macular%20degeneration%20with%20optical%20coherence%20tomography&amp;journal=Surv.%20Ophthalmol.&amp;doi=10.1016%2Fj.survophthal.2012.01.006&amp;volume=57&amp;pages=389-414&amp;publication_year=2012&amp;author=Keane%2CPA">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR36">
           Folgar, F. A. et al. Comparison of optical coherence tomography assessments in the comparison of age-related macular degeneration treatments trials.
           <i>
            Ophthalmology
           </i>
           <b>
            121
           </b>
           , 1956–1965 (2014).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.1016/j.ophtha.2014.04.020" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.ophtha.2014.04.020">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparison%20of%20optical%20coherence%20tomography%20assessments%20in%20the%20comparison%20of%20age-related%20macular%20degeneration%20treatments%20trials&amp;journal=Ophthalmology&amp;doi=10.1016%2Fj.ophtha.2014.04.020&amp;volume=121&amp;pages=1956-1965&amp;publication_year=2014&amp;author=Folgar%2CFA">
            Google Scholar
           </a>
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR37">
           Duker, J. S., Waheed, N. K. &amp; Goldman, D.
           <i>
            Handbook of Retinal OCT: Optical Coherence Tomography E-Book
           </i>
           (Elsevier Health Sciences, Oxford, UK; 2013).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR38">
           Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &amp; Wojna, Z. Rethinking the inception architecture for computer vision.
           <i>
            Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit
           </i>
           . 2818–2826 (2016).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR39">
           Abadi, M. et al. TensorFlow: large-scale machine learning on heterogeneous systems. Preprint at
           <a data-track="click" data-track-action="external reference" data-track-label="https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467" href="https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467">
            https://arxiv-org.proxy.lib.ohio-state.edu/abs/1603.04467
           </a>
           (2016).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR40">
           Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. in Proceedings of the 3rd International Conference on Learning Representations (ICLR). Preprint at
           <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980">
            http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980
           </a>
           (2015).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR41">
           Huang, G., Liu, Z., Weinberger, K. Q. &amp; van der Maaten, L. Densely connected convolutional networks.
           <i>
            Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.
           </i>
           2261–2269 (2017).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR42">
           Lakshminarayanan, B., Pritzel, A. &amp; Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles.
           <i>
            Adv. Neural Inf. Process. Syst
           </i>
           . 6405–6416 (2017).
          </p>
         </li>
         <li class="c-reading-companion__reference-item">
          <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR43">
           De Fauw, J. et al. Automated analysis of retinal imaging using machine learning techniques for computer vision.
           <i>
            F1000Res
           </i>
           <b>
            5
           </b>
           , 1573 (2016).
          </p>
          <p class="c-reading-companion__reference-links">
           <a data-track="click" data-track-action="article reference" data-track-label="10.12688/f1000research.8996.1" href="https://doi-org.proxy.lib.ohio-state.edu/10.12688%2Ff1000research.8996.1">
            Article
           </a>
           <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20analysis%20of%20retinal%20imaging%20using%20machine%20learning%20techniques%20for%20computer%20vision&amp;journal=F1000Res&amp;doi=10.12688%2Ff1000research.8996.1&amp;volume=5&amp;publication_year=2016&amp;author=Fauw%2CJ">
            Google Scholar
           </a>
          </p>
         </li>
        </ol>
       </div>
      </div>
     </div>
    </div>
   </aside>
  </div>
  <footer class="composite-layer" itemscope="" itemtype="http://schema.org/Periodical">
   <meta content="Springer Nature" itemprop="publisher"/>
   <div class="u-mt-16 u-mb-16">
    <div class="u-container">
     <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
      <p class="c-meta u-ma-0 u-flex-shrink">
       <span class="c-meta__item">
        Nature Medicine (
        <i>
         Nat Med
        </i>
        )
       </span>
       <span class="c-meta__item">
        <abbr title="International Standard Serial Number">
         ISSN
        </abbr>
        <span itemprop="onlineIssn">
         1546-170X
        </span>
        (online)
       </span>
       <span class="c-meta__item">
        <abbr title="International Standard Serial Number">
         ISSN
        </abbr>
        <span itemprop="printIssn">
         1078-8956
        </span>
        (print)
       </span>
      </p>
     </div>
    </div>
   </div>
   <div class="c-footer">
    <div class="u-hide-print" data-track-component="footer">
     <h2 class="u-visually-hidden">
      nature.com sitemap
     </h2>
     <div class="c-footer__container">
      <div class="c-footer__grid c-footer__group--separator">
       <div class="c-footer__group">
        <h3 class="c-footer__heading u-mt-0">
         About Nature Portfolio
        </h3>
        <ul class="c-footer__list">
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="about us" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/npg_/company_info/index.html">
           About us
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="press releases" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/npg_/press_room/press_releases.html">
           Press releases
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="press office" data-track-label="link" href="https://press-nature-com.proxy.lib.ohio-state.edu/">
           Press office
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="contact us" data-track-label="link" href="https://support-nature-com.proxy.lib.ohio-state.edu/support/home">
           Contact us
          </a>
         </li>
        </ul>
       </div>
       <div class="c-footer__group">
        <h3 class="c-footer__heading u-mt-0">
         Discover content
        </h3>
        <ul class="c-footer__list">
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="journals a-z" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/siteindex">
           Journals A-Z
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="article by subject" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/subjects/">
           Articles by subject
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nano" data-track-label="link" href="https://nano-nature-com.proxy.lib.ohio-state.edu/">
           Nano
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="protocol exchange" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/protocolexchange/">
           Protocol Exchange
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature index" data-track-label="link" href="https://www.natureindex.com/">
           Nature Index
          </a>
         </li>
        </ul>
       </div>
       <div class="c-footer__group">
        <h3 class="c-footer__heading u-mt-0">
         Publishing policies
        </h3>
        <ul class="c-footer__list">
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="Nature portfolio policies" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/authors/editorial_policies/">
           Nature portfolio policies
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="open access" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/nature-research/open-access">
           Open access
          </a>
         </li>
        </ul>
       </div>
       <div class="c-footer__group">
        <h3 class="c-footer__heading u-mt-0">
         Author &amp; Researcher services
        </h3>
        <ul class="c-footer__list">
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="reprints and permissions" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/reprints/">
           Reprints &amp; permissions
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="data research service" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors/research-data">
           Research data
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="language editing" data-track-label="link" href="https://authorservices-springernature-com.proxy.lib.ohio-state.edu/language-editing/">
           Language editing
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="scientific editing" data-track-label="link" href="https://authorservices-springernature-com.proxy.lib.ohio-state.edu/scientific-editing/">
           Scientific editing
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature masterclasses" data-track-label="link" href="https://masterclasses-nature-com.proxy.lib.ohio-state.edu/">
           Nature Masterclasses
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="live expert trainer-led workshops" data-track-label="link" href="https://masterclasses-nature-com.proxy.lib.ohio-state.edu/live-expert-trainer-led/23649702">
           Live Expert Trainer-led workshops
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="research solutions" data-track-label="link" href="https://solutions-springernature-com.proxy.lib.ohio-state.edu/">
           Research Solutions
          </a>
         </li>
        </ul>
       </div>
       <div class="c-footer__group">
        <h3 class="c-footer__heading u-mt-0">
         Libraries &amp; institutions
        </h3>
        <ul class="c-footer__list">
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="librarian service and tools" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/librarians/tools-services">
           Librarian service &amp; tools
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="librarian portal" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/librarians/manage-your-account/librarianportal">
           Librarian portal
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="open research" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/openresearch/about-open-access/information-for-institutions/">
           Open research
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="Recommend to library" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/librarians/recommend-to-your-library">
           Recommend to library
          </a>
         </li>
        </ul>
       </div>
       <div class="c-footer__group">
        <h3 class="c-footer__heading u-mt-0">
         Advertising &amp; partnerships
        </h3>
        <ul class="c-footer__list">
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="advertising" data-track-label="link" href="https://partnerships-nature-com.proxy.lib.ohio-state.edu/product/digital-advertising/">
           Advertising
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="partnerships and services" data-track-label="link" href="https://partnerships-nature-com.proxy.lib.ohio-state.edu/">
           Partnerships &amp; Services
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="media kits" data-track-label="link" href="https://partnerships-nature-com.proxy.lib.ohio-state.edu/media-kits/">
           Media kits
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track-action="branded content" data-track-label="link" href="https://partnerships-nature-com.proxy.lib.ohio-state.edu/product/branded-content-native-advertising/">
           Branded
                        content
          </a>
         </li>
        </ul>
       </div>
       <div class="c-footer__group">
        <h3 class="c-footer__heading u-mt-0">
         Career development
        </h3>
        <ul class="c-footer__list">
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature careers" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/naturecareers">
           Nature Careers
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature conferences" data-track-label="link" href="https://conferences-nature-com.proxy.lib.ohio-state.edu">
           Nature
           <span class="u-visually-hidden">
           </span>
           Conferences
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature events" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/natureevents/">
           Nature
           <span class="u-visually-hidden">
           </span>
           events
          </a>
         </li>
        </ul>
       </div>
       <div class="c-footer__group">
        <h3 class="c-footer__heading u-mt-0">
         Regional websites
        </h3>
        <ul class="c-footer__list">
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature africa" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/natafrica">
           Nature Africa
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature china" data-track-label="link" href="http://www.naturechina.com">
           Nature China
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature india" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/nindia">
           Nature India
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature Italy" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/natitaly">
           Nature Italy
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature japan" data-track-label="link" href="https://www-natureasia-com.proxy.lib.ohio-state.edu/ja-jp/">
           Nature Japan
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature korea" data-track-label="link" href="https://www-natureasia-com.proxy.lib.ohio-state.edu/ko-kr/">
           Nature Korea
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="nature middle east" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/nmiddleeast/">
           Nature Middle East
          </a>
         </li>
        </ul>
       </div>
      </div>
     </div>
     <div class="c-footer__container">
      <ul class="c-footer__links">
       <li class="c-footer__item">
        <a class="c-footer__link" data-track="click" data-track-action="privacy policy" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/info/privacy">
         Privacy
                Policy
        </a>
       </li>
       <li class="c-footer__item">
        <a class="c-footer__link" data-track="click" data-track-action="use of cookies" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/info/cookies">
         Use
                of cookies
        </a>
       </li>
       <li class="c-footer__item">
        <button class="optanon-toggle-display c-footer__link" data-cc-action="preferences" data-track="click" data-track-action="manage cookies" data-track-label="link" onclick="javascript:;">
         Your privacy choices/Manage cookies
        </button>
       </li>
       <li class="c-footer__item">
        <a class="c-footer__link" data-track="click" data-track-action="legal notice" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/info/legal-notice">
         Legal
                notice
        </a>
       </li>
       <li class="c-footer__item">
        <a class="c-footer__link" data-track="click" data-track-action="accessibility statement" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/info/accessibility-statement">
         Accessibility
                statement
        </a>
       </li>
       <li class="c-footer__item">
        <a class="c-footer__link" data-track="click" data-track-action="terms and conditions" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/info/terms-and-conditions">
         Terms &amp; Conditions
        </a>
       </li>
       <li class="c-footer__item">
        <a class="c-footer__link" data-track="click" data-track-action="california privacy statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/ccpa">
         Your US state privacy rights
        </a>
       </li>
      </ul>
     </div>
    </div>
    <div class="c-footer__container">
     <a class="c-footer__link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/">
      <img alt="Springer Nature" height="20" loading="lazy" src="/static/images/logos/sn-logo-white-ea63208b81.svg" width="200"/>
     </a>
     <p class="c-footer__legal" data-test="copyright">
      © 2023 Springer Nature Limited
     </p>
    </div>
   </div>
   <div aria-hidden="true" class="u-visually-hidden">
    <!--?xml version="1.0" encoding="UTF-8"?-->
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
     <defs>
      <path d="M0 .74h56.72v55.24H0z" id="a">
      </path>
     </defs>
     <symbol id="icon-access" viewbox="0 0 18 18">
      <path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-account" viewbox="0 0 18 18">
      <path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-alert" viewbox="0 0 18 18">
      <path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-arrow-broad" viewbox="0 0 16 16">
      <path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)">
      </path>
     </symbol>
     <symbol id="icon-arrow-down" viewbox="0 0 16 16">
      <path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-arrow-left" viewbox="0 0 16 16">
      <path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-arrow-right" viewbox="0 0 16 16">
      <path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-arrow-sub" viewbox="0 0 16 16">
      <path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-arrow-up" viewbox="0 0 16 16">
      <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-article" viewbox="0 0 18 18">
      <path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-audio" viewbox="0 0 18 18">
      <path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-block" viewbox="0 0 24 24">
      <path d="m0 0h24v24h-24z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-book" viewbox="0 0 18 18">
      <path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-broad" viewbox="0 0 24 24">
      <path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)">
      </path>
     </symbol>
     <symbol id="icon-calendar" viewbox="0 0 18 18">
      <path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-cart" viewbox="0 0 18 18">
      <path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z">
      </path>
     </symbol>
     <symbol id="icon-chevron-less" viewbox="0 0 10 10">
      <path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)">
      </path>
     </symbol>
     <symbol id="icon-chevron-more" viewbox="0 0 10 10">
      <path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
      </path>
     </symbol>
     <symbol id="icon-chevron-right" viewbox="0 0 10 10">
      <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
      </path>
     </symbol>
     <symbol id="icon-circle-fill" viewbox="0 0 16 16">
      <path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-circle" viewbox="0 0 16 16">
      <path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-citation" viewbox="0 0 18 18">
      <path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-close" viewbox="0 0 16 16">
      <path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-collections" viewbox="0 0 18 18">
      <path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-compare" viewbox="0 0 18 18">
      <path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-download-file" viewbox="0 0 18 18">
      <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-download" viewbox="0 0 16 16">
      <path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-editors" viewbox="0 0 18 18">
      <path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-email" viewbox="0 0 18 18">
      <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-error" viewbox="0 0 18 18">
      <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-ethics" viewbox="0 0 18 18">
      <path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-expand">
      <path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-explore" viewbox="0 0 18 18">
      <path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-filter" viewbox="0 0 16 16">
      <path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z">
      </path>
     </symbol>
     <symbol id="icon-home" viewbox="0 0 18 18">
      <path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-image" viewbox="0 0 18 18">
      <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-info" viewbox="0 0 18 18">
      <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-institution" viewbox="0 0 18 18">
      <path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-location" viewbox="0 0 18 18">
      <path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-minus" viewbox="0 0 16 16">
      <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-newsletter" viewbox="0 0 18 18">
      <path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-orcid" viewbox="0 0 18 18">
      <path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-plus" viewbox="0 0 16 16">
      <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-print" viewbox="0 0 18 18">
      <path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-search" viewbox="0 0 22 22">
      <path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-social-facebook" viewbox="0 0 24 24">
      <path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-social-twitter" viewbox="0 0 24 24">
      <path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-social-youtube" viewbox="0 0 24 24">
      <path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-subject-medicine" viewbox="0 0 18 18">
      <path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-success" viewbox="0 0 18 18">
      <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-table" viewbox="0 0 18 18">
      <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-tick-circle" viewbox="0 0 24 24">
      <path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-tick" viewbox="0 0 16 16">
      <path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-update" viewbox="0 0 18 18">
      <path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-upload" viewbox="0 0 18 18">
      <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-video" viewbox="0 0 18 18">
      <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-warning" viewbox="0 0 18 18">
      <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-altmetric">
      <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm-1.886 9.684-1.101 1.845a1 1 0 0 1-.728.479l-.13.008H3.056a9.001 9.001 0 0 0 17.886 0l-4.564-.001-2.779 4.156c-.454.68-1.467.55-1.758-.179l-.038-.113-1.69-6.195ZM12 3a9.001 9.001 0 0 0-8.947 8.016h4.533l2.017-3.375c.452-.757 1.592-.6 1.824.25l1.73 6.345 1.858-2.777a1 1 0 0 1 .707-.436l.124-.008h5.1A9.001 9.001 0 0 0 12 3Z" fill-rule="nonzero">
      </path>
     </symbol>
     <symbol id="icon-checklist-banner" viewbox="0 0 56.69 56.69">
      <path d="M0 0h56.69v56.69H0z" style="fill:none">
      </path>
      <clippath id="b">
       <use style="overflow:visible" xlink:href="#a">
       </use>
      </clippath>
      <path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round">
      </path>
      <path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round">
      </path>
     </symbol>
     <symbol id="icon-chevron-down" viewbox="0 0 16 16">
      <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
      </path>
     </symbol>
     <symbol id="icon-citations">
      <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-eds-checklist" viewbox="0 0 32 32">
      <path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z">
      </path>
     </symbol>
     <symbol id="icon-eds-i-external-link-medium" viewbox="0 0 24 24">
      <path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z">
      </path>
     </symbol>
     <symbol id="icon-eds-i-info-filled-medium" viewbox="0 0 24 24">
      <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z">
      </path>
     </symbol>
     <symbol id="icon-eds-menu" viewbox="0 0 24 24">
      <path d="M21.09 5c.503 0 .91.448.91 1s-.407 1-.91 1H2.91C2.406 7 2 6.552 2 6s.407-1 .91-1h18.18Zm-3.817 6c.401 0 .727.448.727 1s-.326 1-.727 1H2.727C2.326 13 2 12.552 2 12s.326-1 .727-1h14.546Zm3.818 6c.502 0 .909.448.909 1s-.407 1-.91 1H2.91c-.503 0-.91-.448-.91-1s.407-1 .91-1h18.18Z" fill-rule="nonzero">
      </path>
     </symbol>
     <symbol id="icon-eds-search" viewbox="0 0 24 24">
      <path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z" fill-rule="nonzero">
      </path>
     </symbol>
     <symbol id="icon-eds-small-arrow-right" viewbox="0 0 16 16">
      <g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
       <path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035">
       </path>
      </g>
     </symbol>
     <symbol id="icon-eds-user-single" viewbox="0 0 24 24">
      <path d="M12 12c5.498 0 10 4.001 10 9a1 1 0 0 1-2 0c0-3.838-3.557-7-8-7s-8 3.162-8 7a1 1 0 0 1-2 0c0-4.999 4.502-9 10-9Zm0-11a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm0 2a3 3 0 1 1 0 6 3 3 0 0 1 0-6Z" fill-rule="nonzero">
      </path>
     </symbol>
     <symbol id="icon-email-new" viewbox="0 0 24 24">
      <path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z">
      </path>
     </symbol>
     <symbol id="icon-expand-image" viewbox="0 0 18 18">
      <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-github" viewbox="0 0 100 100">
      <path clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill-rule="evenodd">
      </path>
     </symbol>
     <symbol id="icon-mentions">
      <g fill-rule="evenodd" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
       <path d="M22 15.255A9.373 9.373 0 0 1 8.745 2L22 15.255ZM15.477 8.523l4.215-4.215">
       </path>
       <path d="m7 13-5 9h10l-1-5">
       </path>
      </g>
     </symbol>
     <symbol id="icon-metrics-accesses">
      <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM7.708 13.308c2.004 0 3.969 1.198 5.802 2.995l.23.23a2.285 2.285 0 0 1 .009 3.233C11.853 21.693 9.799 23 7.707 23c-2.091 0-4.14-1.305-6.033-3.226a2.285 2.285 0 0 1-.007-3.233c1.9-1.93 3.949-3.233 6.04-3.233Zm0 2c-1.396 0-3.064 1.062-4.623 2.644a.285.285 0 0 0 .007.41C4.642 19.938 6.311 21 7.707 21c1.397 0 3.069-1.065 4.623-2.644a.285.285 0 0 0 0-.404l-.23-.229c-1.487-1.451-3.064-2.415-4.393-2.415Zm-.036 1.077a1.77 1.77 0 1 1 .126 3.537 1.77 1.77 0 0 1-.126-3.537Zm.072 1.538a.23.23 0 1 0-.017.461.23.23 0 0 0 .017-.46Z" fill-rule="nonzero">
      </path>
     </symbol>
     <symbol id="icon-metrics">
      <path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z" fill-rule="nonzero">
      </path>
     </symbol>
     <symbol id="icon-springer-arrow-left">
      <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z">
      </path>
     </symbol>
     <symbol id="icon-springer-arrow-right">
      <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z">
      </path>
     </symbol>
     <symbol id="icon-submit-open" viewbox="0 0 16 17">
      <path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero">
      </path>
     </symbol>
    </svg>
   </div>
  </footer>
  <div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif" data-component-expirydays="30" data-component-id="nature-briefing-banner" data-component-trigger-scroll-percentage="15" data-track="in-view" data-track-action="in-view" data-track-category="nature briefing" data-track-label="Briefing banner visible: Flagship">
   <div class="c-site-messages__banner-large">
    <div class="c-site-messages__close-container">
     <button class="c-site-messages__close" data-track="click" data-track-category="nature briefing" data-track-label="Briefing banner dismiss: Flagship">
      <svg aria-hidden="true" focusable="false" height="25px" version="1.1" viewbox="0 0 25 25" width="25px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
       <title>
        Close banner
       </title>
       <defs>
       </defs>
       <g fill="none" fill-rule="evenodd" stroke="none" stroke-width="1">
        <rect height="25" opacity="0" width="25" x="0" y="0">
        </rect>
        <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff">
        </path>
       </g>
      </svg>
      <span class="visually-hidden">
       Close
      </span>
     </button>
    </div>
    <div class="c-site-messages__form-container">
     <div class="grid grid-12 last">
      <div class="grid grid-4">
       <img alt="Nature Briefing" height="40" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250"/>
       <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">
        Sign up for the
        <em>
         Nature Briefing
        </em>
        newsletter — what matters in science, free to your inbox daily.
       </p>
      </div>
      <div class="grid grid-8 last">
       <form action="https://briefer.public.springernature.app/" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship" method="post">
        <input id="briefing-banner-signup-form-input-track-originReferralPoint" name="track_originReferralPoint" type="hidden" value="MainBriefingBanner"/>
        <input id="briefing-banner-signup-form-input-track-formType" name="track_formType" type="hidden" value="DirectEmailBanner"/>
        <input id="gdpr_tick" name="gdpr_tick" type="hidden" value="false"/>
        <input id="marketing" name="marketing" type="hidden" value="false"/>
        <input id="marketing_tick" name="marketing_tick" type="hidden" value="false"/>
        <input id="brieferEntryPoint" name="brieferEntryPoint" type="hidden" value="MainBriefingBanner"/>
        <label class="nature-briefing-banner__email-label" for="emailAddress">
         Email address
        </label>
        <div class="nature-briefing-banner__email-wrapper">
         <input class="nature-briefing-banner__email-input box-sizing text14" data-test-element="briefing-emailbanner-email-input" id="emailAddress" name="emailAddress" placeholder="e.g. jo.smith@university.ac.uk" required="" type="email" value=""/>
         <input id="defaultNewsletter" name="N:nature_briefing_daily" type="hidden" value="true"/>
         <button class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button" type="submit">
          Sign up
         </button>
        </div>
        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
         <input class="nature-briefing-banner__checkbox-checkbox" data-test-element="briefing-emailbanner-gdpr-checkbox" id="gdpr-briefing-banner-checkbox" name="gdpr" required="" type="checkbox" value="true"/>
         <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">
          I agree my information will be processed in accordance with the
          <em>
           Nature
          </em>
          and Springer Nature Limited
          <a href="https://www-nature-com.proxy.lib.ohio-state.edu/info/privacy">
           Privacy Policy
          </a>
          .
         </label>
        </div>
       </form>
      </div>
     </div>
    </div>
   </div>
   <div class="c-site-messages__banner-small">
    <div class="c-site-messages__close-container">
     <button class="c-site-messages__close" data-track="click" data-track-category="nature briefing" data-track-label="Briefing banner dismiss: Flagship">
      <svg aria-hidden="true" focusable="false" height="25px" version="1.1" viewbox="0 0 25 25" width="25px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
       <title>
        Close banner
       </title>
       <defs>
       </defs>
       <g fill="none" fill-rule="evenodd" stroke="none" stroke-width="1">
        <rect height="25" opacity="0" width="25" x="0" y="0">
        </rect>
        <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff">
        </path>
       </g>
      </svg>
      <span class="visually-hidden">
       Close
      </span>
     </button>
    </div>
    <div class="c-site-messages__content text14">
     <span class="c-site-messages--nature-briefing__strapline strong">
      Get the most important science stories of the day, free in your inbox.
     </span>
     <a class="nature-briefing__link text14 sans-serif" data-test-element="briefing-banner-link" data-track="click" data-track-category="nature briefing" data-track-label="Small-screen banner CTA to site" href="https://www-nature-com.proxy.lib.ohio-state.edu/briefing/signup/?brieferEntryPoint=MainBriefingBanner" rel="noreferrer noopener" target="_blank">
      Sign up for Nature Briefing
     </a>
    </div>
   </div>
  </div>
  <noscript>
   <img alt="" height="0" hidden="" src="https://verify-nature-com.proxy.lib.ohio-state.edu/verify/nature.png" style="display: none" width="0"/>
  </noscript>
  <script async="" src="//content.readcube.com/ping?doi=10.1038/s41591-018-0107-6&amp;format=js&amp;last_modified=2018-08-13">
  </script>
  <img alt="" class="u-visually-hidden" height="1" src="/u3vgf9xs/article/s41591-018-0107-6" width="1"/>
  <div class="c-cookie-banner">
   <div class="c-cookie-banner__container">
    <p>
     This website sets only cookies which are necessary for it to function. They are used to enable core functionality such as security, network management and accessibility. These cookies cannot be switched off in our systems. You may disable these by changing your browser settings, but this may affect how the website functions. Please view our privacy policy for further details on how we process your information.
     <button class="c-cookie-banner__dismiss">
      Dismiss
     </button>
    </p>
   </div>
  </div>
  <script src="https://verify-nature-com.proxy.lib.ohio-state.edu/verify/nature.min.js">
  </script>
 </body>
</html>
