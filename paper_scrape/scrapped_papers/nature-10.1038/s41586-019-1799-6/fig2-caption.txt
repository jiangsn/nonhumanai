a, The ROC curve of the AI system on the UK screening data. The AUC is 0.889 (95% CI 0.871, 0.907; n = 25,856 patients). Also shown are the sensitivity and specificity pairs for the human decisions made in clinical practice. Cases were considered positive if they received a biopsy-confirmed diagnosis of cancer within 39 months of screening. The consensus decision represents the standard of care in the UK, and will involve input from between two and three expert readers. The inset shows a magnification of the grey shaded region. AI system operating points were selected on a separate validation dataset: point i was intended to match the sensitivity and exceed the specificity of the first reader; points ii and iii were selected to attain non-inferiority for both the sensitivity and specificity of the second reader and consensus opinion, respectively. b, The ROC curve of the AI system on the US screening data. When trained on both datasets (solid curve), the AUC is 0.8107 (95% CI 0.791, 0.831; n = 3,097 patients). When trained on only the UK dataset (dotted curve), the AUC is 0.757 (95% CI 0.732, 0.780). Also shown are the sensitivity and specificity achieved by radiologists in clinical practice using BI-RADS35. Cases were considered positive if they received a biopsy-confirmed diagnosis of cancer within 27 months of screening. AI system operating points were chosen, using a separate validation dataset, to exceed the sensitivity and specificity of the average reader. Negative cases were upweighted to account for the sampling protocol (see Methods section ‘Inverse probability weighting’). Extended Data Figure 1 shows an unweighted analysis. See Extended Data Table 2a for statistical comparisons of sensitivity and specificity.