Main:
Breast cancer is the second leading cause of death from cancer in women3, but early detection and treatment can considerably improve outcomes1,4,5. As a consequence, many developed nations have implemented large-scale mammography screening programmes. Major medical and governmental organizations recommend screening for all women starting between the ages of 40 and 506,7,8. In the USA and UK combined, over 42 million exams are performed each year9,10. Despite the widespread adoption of mammography, interpretation of these images remains challenging. The accuracy achieved by experts in cancer detection varies widely, and the performance of even the best clinicians leaves room for improvement11,12. False positives can lead to patient anxiety13, unnecessary follow-up and invasive diagnostic procedures. Cancers that are missed at screening may not be identified until they are more advanced and less amenable to treatment14. AI may be uniquely poised to help with this challenge. Studies have demonstrated the ability of AI to meet or exceed the performance of human experts on several tasks of medical-image analysis15,16,17,18,19. As a shortage of mammography professionals threatens the availability and adequacy of breast-screening services around the world20,21,22,23, the scalability of AI could improve access to high-quality care for all. Computer-aided detection (CAD) software for mammography was introduced in the 1990s, and several assistive tools have been approved for medical use24. Despite early promise25,26, this generation of software failed to improve the performance of readers in real-world settings12,27,28. More recently, the field has seen a renaissance owing to the success of deep learning. A few studies have characterized systems for breast cancer prediction with stand-alone performance that approaches that of human experts29,30. However, the existing work has several limitations. Most studies are based on small, enriched datasets with limited follow-up, and few have compared performance to readers in actual clinical practice—instead relying on laboratory-based simulations of the reading environment. So far there has been little evidence of the ability of AI systems to translate between different screening populations and settings without additional training data31. Critically, the pervasive use of follow-up intervals that are no longer than 12 months29,30,32,33 means that more subtle cancers that are not identified until the next screen may be ignored. In this study, we evaluate the performance of a new AI system for breast cancer prediction using two large, clinically representative datasets from the UK and the USA. We compare the predictions of the system to those made by readers in routine clinical practice and show that performance exceeds that of individual radiologists. These observations are confirmed with an independently conducted reader study. Furthermore, we show how this system might be integrated into screening workflows, and provide evidence that the system can generalize across continents. Figure 1 shows an overview of the project. Fig. 1: Development of an AI system to detect cancer in screening mammograms.Datasets representative of the UK and US breast cancer screening populations were curated from three screening centres in the UK and one centre in the USA. Outcomes were derived from the biopsy record and longitudinal follow-up. An AI system was trained to identify the presence of breast cancer from a set of screening mammograms, and was evaluated in three primary ways: first, AI predictions were compared with the historical decisions made in clinical practice; second, to evaluate the generalizability across populations, a version of the AI system was developed using only the UK data and retested on the US data; and finally, the performance of the AI system was compared to that of six independent radiologists using a subset of the US test set. 

Datasets from cancer screening programmes:
A deep learning model for identifying breast cancer in screening mammograms was developed and evaluated using two large datasets from the UK and the USA. We report results on test sets that were not used to train or tune the AI system. The UK test set consisted of screening mammograms that were collected between 2012 and 2015 from 25,856 women at two screening centres in England, where women are screened every three years. It included 785 women who had a biopsy, and 414 women with cancer that was diagnosed within 39 months of imaging. This was a random sample of 10% of all women with screening mammograms at these sites during this time period. The UK cohort resembled the broader screening population in age and disease characteristics (Extended Data Table 1a). The test set from the USA, where women are screened every one to two years, consisted of screening mammograms that were collected between 2001 and 2018 from 3,097 women at one academic medical centre. We included images from all 1,511 women who were biopsied during this time period and a random subset of women who never underwent biopsy (Methods). Among the women who received a biopsy, 686 were diagnosed with cancer within 27 months of imaging. Breast cancer outcome was determined on the basis of multiple years of follow-up (Fig. 1). We chose the follow-up duration on the basis of the screening interval in the country of origin for each dataset. In a similar manner to previous work34, we augmented each interval with a three-month buffer to account for variability in scheduling and latency of follow-up. Cases that were designated as cancer-positive were accompanied by a biopsy-confirmed diagnosis within the follow-up period. Cases labelled as cancer-negative had at least one follow-up non-cancer screen; cases without this follow-up were excluded from the test set. 

Retrospective clinical comparison:
We used biopsy-confirmed breast cancer outcomes to evaluate the predictions of the AI system as well as the original decisions made by readers in the course of clinical practice. Human performance was computed on the basis of the clinician’s decision to recall the patient for further diagnostic investigation. The receiver operating characteristic (ROC) curve of the AI system is shown in Fig. 2. Fig. 2: Performance of the AI system and clinical readers in breast cancer prediction.a, The ROC curve of the AI system on the UK screening data. The AUC is 0.889 (95% CI 0.871, 0.907; n = 25,856 patients). Also shown are the sensitivity and specificity pairs for the human decisions made in clinical practice. Cases were considered positive if they received a biopsy-confirmed diagnosis of cancer within 39 months of screening. The consensus decision represents the standard of care in the UK, and will involve input from between two and three expert readers. The inset shows a magnification of the grey shaded region. AI system operating points were selected on a separate validation dataset: point i was intended to match the sensitivity and exceed the specificity of the first reader; points ii and iii were selected to attain non-inferiority for both the sensitivity and specificity of the second reader and consensus opinion, respectively. b, The ROC curve of the AI system on the US screening data. When trained on both datasets (solid curve), the AUC is 0.8107 (95% CI 0.791, 0.831; n = 3,097 patients). When trained on only the UK dataset (dotted curve), the AUC is 0.757 (95% CI 0.732, 0.780). Also shown are the sensitivity and specificity achieved by radiologists in clinical practice using BI-RADS35. Cases were considered positive if they received a biopsy-confirmed diagnosis of cancer within 27 months of screening. AI system operating points were chosen, using a separate validation dataset, to exceed the sensitivity and specificity of the average reader. Negative cases were upweighted to account for the sampling protocol (see Methods section ‘Inverse probability weighting’). Extended Data Figure 1 shows an unweighted analysis. See Extended Data Table 2a for statistical comparisons of sensitivity and specificity. In the UK, each mammogram is interpreted by two readers, and in cases of disagreement, an arbitration process may invoke a third opinion. These interpretations occur serially, such that each reader has access to the opinions of previous readers. The records of these decisions yield three benchmarks of human performance for cancer prediction. Compared to the first reader, the AI system demonstrated a statistically significant improvement in absolute specificity of 1.2% (95% confidence interval (CI) 0.29%, 2.1%; P = 0.0096 for superiority) and an improvement in absolute sensitivity of 2.7% (95% CI −3%, 8.5%; P = 0.004 for non-inferiority at a pre-specified 5% margin; Extended Data Table 2a). Compared to the second reader, the AI system showed non-inferiority (at a 5% margin) for both specificity (P < 0.001) and sensitivity (P = 0.02). Likewise, the AI system showed non-inferiority (at a 5% margin) to the consensus judgment for specificity (P < 0.001) and sensitivity (P = 0.0039). In the standard screening protocol in the USA, each mammogram is interpreted by a single radiologist. We used the BI-RADS35 score that was assigned to each case in the original screening context as a proxy for human cancer prediction (see Methods section ‘Interpreting clinical reads’). Compared to the typical reader, the AI system demonstrated statistically significant improvements in absolute specificity of 5.7% (95% CI 2.6%, 8.6%; P < 0.001) and in absolute sensitivity of 9.4% (95% CI 4.5%, 13.9%; P < 0.001; Extended Data Table 2a). 

Generalization across populations:
To evaluate the ability of the AI system to generalize across populations and screening settings, we trained the same architecture using only the UK dataset and applied it to the US test set (Fig. 2b). Even without exposure to the US training data, the ROC curve of the AI system encompasses the point that indicates the average performance of US radiologists. Again, the AI system showed improved specificity (+3.5%, P = 0.0212) and sensitivity (+8.1%, P = 0.0006; Extended Data Table 2b) compared with radiologists. 

Comparison with a reader study:
In a reader study that was conducted by an external clinical research organization, six US-board-certified radiologists who were compliant with the requirements of the Mammography Quality Standards Act (MQSA) interpreted 500 mammograms that were randomly sampled from the US test set. Where data were available, readers were equipped with contextual information typically available in the clinical setting, including the patient’s age, breast cancer history, and previous screening mammograms. Among the 500 cases selected for this study, 125 had biopsy-proven cancer within 27 months, 125 had a negative biopsy within 27 months and 250 were not biopsied (Extended Data Table 3). These proportions were chosen to increase the difficulty of the screening task and increase statistical power. (Such enrichment is typical in observer studies36.) Readers rated each case using the forced BI-RADS35 scale, and BI-RADS scores were compared to ground-truth outcomes to fit an ROC curve for each reader. The scores of the AI system were treated in the same manner (Fig. 3). Fig. 3: Performance of the AI system in breast cancer prediction compared to six independent readers.a, Six readers rated each case (n = 465) using the six-point BI-RADS scale. A fitted ROC curve for each of the readers is compared to the ROC curve of the AI system (see Methods section ‘Statistical analysis’). For reference, a non-parametric ROC curve is presented in tandem. Cases were considered positive (n = 113) if they received a pathology-confirmed diagnosis of cancer within 27 months of the time of screening. Note that this sample of cases was enriched for patients who received a negative biopsy result (n = 119), making this a more-challenging population for screening. The mean reader AUC was 0.625 (s.d. 0.032), whereas the AUC for the AI system was 0.740 (95% CI 0.696, 0.794). The AI system exceeded human performance by a significant margin (ΔAUC = +0.115, 95% CI 0.055, 0.175; P = 0.0002 by two-sided ORH method (see Methods section ‘Statistical analysis’)). For results using a 12-month interval, see Extended Data Fig. 2. b, Pooled results from all six readers from a. c, Pooled results (n = 408) from all 6 readers using a 12-month interval for cancer definition. Cases were considered positive (n = 56) if they received a pathology-confirmed cancer diagnosis within one year (Extended Data Table 3). The AI system exceeded the average performance of radiologists by a significant margin (change in area under curve (ΔAUC) = +0.115, 95% CI 0.055, 0.175; P = 0.0002). Similar results were observed when a follow-up period of one year was used instead of 27 months (Fig. 3c, Extended Data Fig. 2). In addition to producing a classification decision for the entire case, the AI system was designed to highlight specific areas of suspicion for malignancy. Likewise, the readers in our study supplied rectangular region-of-interest (ROI) annotations surrounding concerning findings. We used multi-localization receiver operating characteristic (mLROC) analysis37 to compare the ability of the readers and the AI system to identify malignant lesions within each case (see Methods section ‘Localization analysis’). We summarized each mLROC plot by computing the partial area under the curve (pAUC) in the false-positive fraction interval from 0 to 0.138 (Extended Data Fig. 3). The AI system exceeded human performance by a significant margin (ΔpAUC = +0.0192, 95% CI 0.0086, 0.0298; P = 0.0004). 

Potential clinical applications:
The classifications made by the AI system could be used to reduce the workload involved in the double-reading process that is used in the UK, while preserving the standard of care. We simulated this scenario by omitting the second reader and any ensuing arbitration when the decision of the AI system agreed with that of the first reader. In these cases, the opinion of the first reader was treated as final. In cases of disagreement, the second and consensus opinions were invoked as usual. This combination of human and machine results in performance equivalent to that of the traditional double-reading process, but saves 88% of the effort of the second reader (Extended Data Table 4a). The AI system could also be used to provide automated, immediate feedback in the screening setting. To identify normal cases with high confidence, we used a very-low decision threshold. For the UK data, we achieved a negative predictive value (NPV) of 99.99% while retaining a specificity of 41.15%. Similarly, for the US data, we achieved a NPV of 99.90% while retaining a specificity of 34.79%. These data suggest that it may be feasible to dismiss 35–41% of normal cases if we allow for one cancer in every 1,000–10,000 negative predictions (NPV 99.90–99.99% in USA–UK). By comparison, consensus double reading in our UK dataset included one cancer in every 182 cases that were deemed normal. To identify cancer cases with high confidence, we used a very-high decision threshold. For the UK data, we achieved a positive predictive value (PPV) of 85.6% while retaining a sensitivity of 41.2%. Similarly, for the US data, we achieved a PPV of 82.4% while retaining a sensitivity of 29.8%. These data suggest that it may be feasible to rapidly prioritize 30–40% of cancer cases, with approximately five out of six follow-ups leading to a diagnosis of cancer. By comparison, in our study only 22.8% of UK cases that were recalled by consensus double reading and 4.9% of US cases that were recalled by single reading were ultimately diagnosed with cancer. 

Performance breakdown:
Comparing the errors of the AI system with errors from clinical reads revealed many cases in which the AI system correctly identified cancer whereas the reader did not, and vice versa (Supplementary Table 1). Most of the cases in which only the AI system identified cancer were invasive (Extended Data Table 5). On the other hand, cases in which only the reader identified cancer were split more evenly between in situ and invasive. Further breakdowns by invasive cancer size, grade and molecular markers show no clear biases (Supplementary Table 2). We also considered the disagreement between the AI system and the six radiologists that participated in the US reader study. Figure 4a shows a sample cancer case that was missed by all six radiologists, but correctly identified by the AI system. Figure 4b shows a sample cancer case that was caught by all six radiologists, but missed by the AI system. Although we were unable to determine clear patterns among these instances, the presence of such edge cases suggests potentially complementary roles for the AI system and human readers in reaching accurate conclusions. Fig. 4: Discrepancies between the AI system and human readers.a, A sample cancer case that was missed by all six readers in the US reader study, but correctly identified by the AI system. The malignancy, outlined in yellow, is a small, irregular mass with associated microcalcifications in the lower inner right breast. b, A sample cancer case that was caught by all six readers in the US reader study, but missed by the AI system. The malignancy is a dense mass in the lower inner right breast. Left, mediolateral oblique view; right, craniocaudal view. We compared the performance of the 20 individual readers best represented in the UK clinical dataset with that of the AI system (Supplementary Table 3). The results of this analysis suggest that the aggregate comparison presented above is not unduly influenced by any particular readers. Breakdowns by cancer type, grade and lesion size suggest no apparent difference in the distribution of cancers detected by the AI system and human readers (Extended Data Table 6a). On the US test set, a breakdown by cancer type (Extended Data Table 6b) shows that the sensitivity advantage of the AI system is concentrated on the identification of invasive cancers (for example, invasive lobular or ductal carcinoma) rather than in situ cancer (for example, ductal carcinoma in situ). A breakdown by BI-RADS35 breast density category shows that performance gains apply equally across the spectrum of breast tissue types that is represented in this dataset (Extended Data Table 6c). 

Discussion:
In this study we present an AI system that outperforms radiologists on a clinically relevant task of breast cancer identification. These results held across two large datasets that are representative of different screening populations and practices. In the UK, the AI system showed specificity superior to that of the first reader. Sensitivity at the same operating point was non-inferior. Consensus double reading has been shown to improve performance compared to single reading39, and represents the current standard of care in the UK and many European countries40. Our system did not outperform this benchmark, but was statistically non-inferior to the second reader and consensus opinion. In the USA, the AI system exhibited specificity and sensitivity superior to that of radiologists practising in an academic medical centre. This trend was confirmed in an externally conducted reader study, which showed that the scores of the AI system stratified cases better than the BI-RADS ratings (the standard scale for mammography assessment in the USA) that were assigned by each of the six readers. Notably, the human readers (both in the clinic and our reader study) had access to patient history and previous mammograms when making screening decisions. The US clinical readers may have also had access to breast tomosynthesis images. By contrast, the AI system only processed the most recent mammogram. These comparisons are not without limitations. Although the UK dataset mirrored the nationwide screening population in age and cancer prevalence (Extended Data Table 1a), the same cannot be said of the US dataset, which was drawn from a single screening centre and enriched for cancer cases. By chance, the vast majority of images used in this study were acquired on devices made by Hologic. Future research should assess the performance of the AI system across a variety of manufacturers in a more systematic way. In our reader study, all of the radiologists were eligible to interpret screening mammograms in the USA, but did not uniformly receive fellowship training in breast imaging. It is possible that a higher benchmark for performance could have been obtained with readers who were more specialized41. To obtain high-quality ground-truth labels, we used extended follow-up intervals that were chosen to encompass a subsequent round of screening in each country. Although there is some precedent in clinical trials34 and targeted cohort studies42, this step is not usually taken during systematic evaluation of AI systems for breast cancer detection. In retrospective datasets with shorter follow-up intervals, outcome labels tend to be skewed in favour of readers. As they are gatekeepers for biopsy, asymptomatic cases will only receive a cancer diagnosis if a mammogram raises the suspicions of a reader. A longer follow-up interval decouples the ground-truth labels from reader opinions (Extended Data Fig. 4) and includes cancers that may have been initially missed by human eyes. The use of an extended interval makes cancer prediction a more challenging task. Cancers that are diagnosed years later may include new growths for which there could be no mammographic evidence in the original images. Consequently, the sensitivity values presented here are lower than what has been reported for 12-month intervals2 (Extended Data Fig. 5). We present early evidence of the ability of the AI system to generalize across populations and screening protocols. We retrained the system using exclusively UK data, and then measured performance on unseen US data. In this context, the system continued to outperform radiologists, albeit by a smaller margin. This suggests that in future clinical deployments, the system might offer strong baseline performance, but could benefit from fine-tuning with local data. The optimal use of the AI system within clinical workflows remains to be determined. The specificity advantage exhibited by the system suggests that it could help to reduce recall rates and unnecessary biopsies. The improvement in sensitivity exhibited in the US data shows that the AI system may be capable of detecting cancers earlier than the standard of care. An analysis of the localization performance of the AI system suggests it holds early promise for flagging suspicious regions for review by experts. Notably, the additional cancers identified by the AI system tended to be invasive rather than in situ disease. Beyond improving reader performance, the technology described here may have a number of other clinical applications. Through simulation, we suggest how the system could obviate the need for double reading in 88% of UK screening cases, while maintaining a similar level of accuracy to the standard protocol. We also explore how high-confidence operating points can be used to triage high-risk cases and dismiss low-risk cases. These analyses highlight the potential of this technology to deliver screening results in a sustainable manner despite workforce shortages in countries such as the UK43. Prospective clinical studies will be required to understand the full extent to which this technology can benefit patient care. 

Methods:


Ethical approval:
Use of the UK dataset for research collaborations by both commercial and non-commercial organizations received ethical approval (REC reference 14/SC/0258). The US data were fully de-identified and released only after an Institutional Review Board approval (STU00206925). 

The UK dataset:
The UK dataset was collected from three breast screening sites in the UK National Health Service Breast Screening Programme (NHSBSP). The NHSBSP invites women aged between 50 and 70 who are registered with a general practitioner (GP) for mammographic screening every three years. Women who are not registered with a GP, or who are older than 70, can self-refer to the screening programme. In the UK, the screening programme uses double reading: each mammogram is read by two radiologists, who are asked to decide whether to recall the woman for additional follow-up. When there is disagreement, an arbitration process takes place. The data were initially compiled by OPTIMAM (Cancer Research UK) between 2010 and 2018, from St George’s Hospital (London), Jarvis Breast Centre (Guildford) and Addenbrooke’s Hospital (Cambridge). The collected data included screening and follow-up mammograms (comprising mediolateral oblique and craniocaudal views of the left and right breasts), all radiologist opinions (including the arbitration result, if applicable) and the metadata associated with follow-up treatment. The mammograms and associated metadata of 137,291 women were considered for inclusion in the study. Of these, 123,964 women had screening images and uncorrupted metadata. Exams that were recalled for reasons other than radiographic evidence of malignancy, or episodes that were not part of routine screening, were excluded. In total, 121,850 women had at least one eligible exam. Women who were below the age of 47 at the time of the screen were excluded from validation and test sets, leaving 121,455 women. Finally, women for whom there was no exam with sufficient follow-up were excluded from validation and test sets. This last step resulted in the exclusion of 5,990 of 31,766 test-set cases (19%); see Supplementary Fig. 1. The test set is a random sample of 10% of all women who were screened at two sites (St George’s Hospital and Jarvis Breast Centre) between 2012 and 2015. Insufficient data were provided to apply the sampling procedure to the third site. In assembling the test set, we randomly selected a single eligible screening mammogram from the record of each woman. For women with a positive biopsy, eligible mammograms were those conducted in the 39 months before the date of biopsy. For women who never had a positive biopsy, eligible mammograms were accompanied by a non-suspicious mammogram at least 21 months later. The final test set consisted of 25,856 women (see Supplementary Fig. 1). When compared to the UK national breast cancer screening service, we observed a very similar distribution of cancer prevalence, age and, cancer type (see Extended Data Table 1a). Digital mammograms were acquired predominantly on devices manufactured by Hologic (95%), followed by General Electric (4%) and Siemens (1%). 

The US dataset:
The US dataset was collected from Northwestern Memorial Hospital (Chicago) between 2001 and 2018. In the USA, each screening mammogram is typically read by a single radiologist, and screens are conducted annually or biannually. The breast radiologists at this hospital receive fellowship training and only interpret breast-imaging studies. Their experience levels ranged from 1 to 30 years. The American College of Radiology (ACR) recommends that women start routine screening at the age of 40; other organizations, including the United States Preventive Services Task Force (USPSTF), recommend that screening begins at the age of 50 for women with an average risk of breast cancer6,7,8. The US dataset included records from all women that underwent a breast biopsy between 2001 and 2018. It also included a random sample of approximately 5% of all women who participated in screening, but were never biopsied. This heuristic was used in order to capture all cancer cases (to enhance statistical power) and to curate a rich set of benign findings on which to train and test the AI system. The data-processing steps involved in constructing the dataset are summarized in Supplementary Fig. 2. Among women with a completed mammogram order, we collected records from all women with a pathology report that contained the term ‘breast’. Among women that lacked such a pathology report, those whose records bore an International Classification of Diseases (ICD) code indicative of breast cancer were excluded. Approximately 5% of this unbiopsied negative population was sampled. After de-identification and transfer, women were excluded if their metadata were unavailable or corrupted. The women in the dataset were split randomly among train (55%), validation (15%) and test (30%) sets. For testing, a single case was chosen for each woman, following a similar procedure as for the UK dataset. In women who underwent biopsy, we randomly chose a case from the 27 months preceding the date of biopsy. For women who did not undergo biopsy, one screening mammogram was randomly chosen from among those with a follow-up event at least 21 months later. Cases were considered complete if they possessed the four standard screening views (mediolateral oblique and craniocaudal views of the left and right breasts), acquired for screening intent. Again, the vast majority of the studies were acquired using Hologic (including Lorad-branded) devices (99%); the other manufacturers (Siemens and General Electric) together constituted less than 1% of studies. The radiology reports associated with cases in the test set were used to flag and exclude cases that involved breast implants or were recalled for technical reasons. To compare the AI system against the clinical reads performed at this site, we employed clinicians to manually extract BI-RADS scores from the original radiology reports. There were some cases for which the original radiology report could not be located, even if a subsequent cancer diagnosis was confirmed by biopsy. This might have happened, for example, if the screening case was imported from an outside institution. Such cases were excluded from the clinical reader comparison. 

Randomization and blinding:
Patients were randomized into training, validation, and test sets by applying a hash function to the de-identified medical record number. Set assignment was based on the value of the resulting integer modulo 100. For the UK data, values of 0–9 were reserved for the test set. For the US data, values of 0–29 were reserved for the test set. Test set sizes were chosen to produce, in expectation, a sufficient number of positives to power statistical comparisons on the metric of sensitivity. The US and UK test sets were held back from AI system development, which only took place on the training and validation sets. Investigators did not access test set data until models, hyperparameters, and operating point thresholds were finalized. None of the readers who interpreted the images had knowledge of any aspect of the AI system. 

Inverse probability weighting:
The US test set includes images from all biopsied women, but only a random subset of women who never underwent biopsy. This enrichment allowed us to accrue more positives in light of the low baseline prevalence of breast cancer, but led to underrepresentation of normal cases. We accounted for this sampling process by using inverse probability weighting to obtain unbiased estimates of human and AI system performance in the screening population44,45. We acquired images from 7,522 of the 143,238 women who underwent mammography screening but had no cancer diagnosis or biopsy record. Accordingly, we upweighted cases from women who never underwent biopsy by a factor of 19.04. Further sampling occurred when selecting one case per patient: to enrich for difficult cases, we preferentially chose cases from the timeframe preceding a biopsy (if one occurred). Although this sampling increases the diversity of benign findings, it again shifts the distribution from what would be observed in a typical screening interval. To better reflect the prevalence that results when negative cases are randomly selected, we estimated additional factors by Monte Carlo simulation. Choosing one case per patient with our preferential sampling mechanism yielded 872 cases that were biopsied within 27 months, and 1,662 cases that were not (Supplementary Fig. 2). However, 100 trials of pure random sampling yielded on average 557.54 and 2,056.46 cases, respectively. Accordingly, cases associated with negative biopsies were downweighted by 557.54/872 = 0.64. Cases that were not biopsied were upweighted by another 2,056.46/1,662 = 1.24, leading to a final weight of 19.04 × 1.24 = 23.61.Cancer-positive cases carried a weight of 1.0. The final sample weights were used in sensitivity, specificity and ROC calculations. 

Histopathological outcomes:
In the UK dataset, benign and malignant classifications (given directly in the metadata) followed NHSBSP definitions46. To derive the outcome labels for the US dataset, pathology reports were reviewed by US-board-certified pathologists and categorized according to the findings they contained. An effort was made to harmonize this categorization with UK definitions. Malignant pathologies included ductal carcinoma in situ, microinvasive carcinoma, invasive ductal carcinoma, invasive lobular carcinoma, special-type invasive carcinoma (including tubular, mucinous and cribriform carcinomas), intraductal papillary carcinoma, non-primary breast cancers (including lymphoma and phyllodes) and inflammatory carcinoma. Women who received a biopsy that found any of these malignant pathologies were considered to have a diagnosis of cancer. Benign pathologies included lobular carcinoma in situ, radial scar, columnar cell changes, atypical lobular hyperplasia, atypical ductal hyperplasia, cyst, sclerosing adenosis, fibroadenoma, papilloma, periductal mastitis and usual ductal hyperplasia. None of these findings were considered to be cancerous. 

Interpreting clinical reads:
In the UK screening setting, readers categorize mammograms from asymptomatic women as normal or abnormal, with a third option for technical recall owing to inadequate image quality. An abnormal result at the conclusion of the double-reading process results in further diagnostic assessment. We treat mammograms deemed abnormal as a prediction of malignancy. Cases in which the consensus judgment recalled the patient for technical reasons were excluded from analysis, as the images were presumed to be incomplete or unreliable. Cases in which any single reader recommended technical recall were excluded from the corresponding reader comparison. In the US screening setting, radiologists attach a BI-RADS35 score to each mammogram. A score of 0 is deemed ‘incomplete’, and will later be refined on the basis of follow-up imaging or repeat mammography to address technical issues. For computation of sensitivity and specificity, we dichotomized the BI-RADS assessments in line with previous work34. Scores of 0, 4 and 5 were treated as positive predictions if the recommendation was based on mammographic findings, not on technical grounds or patient symptoms alone. Cases of technical recall were excluded from analysis, as the images were presumed to be incomplete or unreliable. BI-RADS scores were manually extracted from the free-text radiology reports. Cases for which the BI-RADS score was unavailable were excluded from the reader comparison. In both datasets, the original readers had access to contextual information that is normally available in clinical practice. This includes the patient’s family history of cancer, prior screening and diagnostic imaging, and radiology or pathology notes from past examinations. By contrast, only the age of the patient was made available to the AI system. 

Overview of the AI system:
The AI system consisted of an ensemble of three deep learning models, each operating on a different level of analysis (individual lesions, individual breasts and the full case). Each model produces a cancer risk score between 0 and 1 for the entire mammography case. The final prediction of the system was the mean of the predictions from the three independent models. A detailed description of the AI system is available in the Supplementary Methods and Supplementary Fig. 3. 

Selection of operating points:
The AI system natively produces a continuous score that represents the likelihood of cancer being present. To support comparisons with the predictions of human readers, we thresholded this score to produce analogous binary screening decisions. For each clinical benchmark, we used the validation set to choose a distinct operating point; this amounts to a score threshold that separates positive and negative decisions. To better simulate prospective deployment, the test sets were never used in selecting operating points. The UK dataset contains three clinical benchmarks—the first reader, second reader and consensus. This last decision is the outcome of the double-reading process and represents the standard of care in the UK. For the first reader, we chose an operating point aimed at demonstrating statistical superiority in specificity and non-inferiority for sensitivity. For the second reader and consensus reader, we chose an operating point aimed at demonstrating statistical non-inferiority for both sensitivity and specificity. The US dataset contains a single operating point for comparison, which corresponds to the radiologist using the BI-RADS rubric for evaluation. In this case, we used the validation set to choose an operating point aimed at achieving superiority for both sensitivity and specificity. 

Reader study:
For the reader study, six US-board-certified radiologists interpreted a sample of 500 cases from 500 women in the test set. All radiologists were compliant with MQSA requirements for interpreting mammography and had an average of 10 years of clinical experience (Extended Data Table 7b). Two of them were fellowship-trained in breast imaging. The sample of cases was stratified to contain 50% normal cases, 25% biopsy-confirmed negative cases and 25% biopsy-confirmed positive cases. A detailed description of the case composition of the reader study can be found in Extended Data Table 3. Readers were not informed of the enrichment levels in the dataset. Readers recorded their assessments on a 21CFR11-compliant electronic case report form within the Ambra Health (New York, NY) viewer v3.18.7.0R. They interpreted the images using 5MP MSQA-compliant displays. Each reader interpreted the cases in a unique randomized order. For each study, readers were asked to first report a BI-RADS35 5th edition score using the values 0, 1 and 2, as if they were interpreting the screening mammogram in routine practice. They were then asked to render a forced diagnostic BI-RADS score using the values 1, 2, 3, 4A, 4B, 4C or 5. Readers also gave a finer-grained score between 0 and 100 that was indicative of their suspicion that the case contains a malignancy. In addition to the four standard mammographic screening images, clinical context was provided to better simulate the screening setting. Readers were presented with the preamble of the de-identified radiology report that was produced by the radiologist who originally interpreted the study. This contained information such as the age of the patient and their family history of cancer. The information was manually reviewed to ensure that no impression or findings were included. Where possible (in 43% of cases), previous imaging was made available to the readers. Readers could review up to four sets of previous screening exams that were acquired between 1 and 4 years earlier, accompanied by de-identified radiologist reports. If prior imaging was available, the study was read twice by each reader—first without the prior information, and then immediately after, with the prior information present. The system ensured that readers could not update their initial assessment after the prior information was presented. For cases for which previous exams were available, the final reader assessment (given after having reviewed the prior exams) was used for the analysis. Cases in which at least half of the readers indicated concerns with image quality were excluded from the analysis. Cases in which breast implants were noted were also excluded. The final analysis was performed on the remaining 465 cases. 

Localization analysis:
For this purpose, we considered all screening exams from the reader study for which cancer developed within 12 months. See Extended Data Table 3 for a detailed description of how the dataset was constructed. To collect ground-truth localizations, two board-certified radiologists inspected each case, using follow-up data to identify the location of malignant lesions. Instances of disagreement were resolved by one radiologist with fellowship training in breast imaging. To identify the precise location of the cancerous tissue, radiologists consulted subsequent diagnostic mammograms, radiology reports, biopsy notes, pathology reports and post-biopsy mammograms. Rectangular bounding boxes were drawn around the locations of subsequent positive biopsies in all views in which the finding was visible. In cases in which no mammographic finding was visible, the location where the lesion later appeared was highlighted. Of the 56 cancers considered for analysis, location information could be obtained with confidence in 53 cases; three cases were excluded owing to ambiguity in the index examination and the absence of follow-up images. On average, there were 2.018 ground-truth regions per cancer-positive case. In the reader study, readers supplied rectangular ROI annotations surrounding suspicious findings in all cases to which they assigned a BI-RADS score of 3 or higher. A limit of six ROIs per case was enforced. On average, the readers supplied 2.04 annotations per suspicious case. In addition to an overall cancer likelihood score, the AI system produces a ranked list of rectangular bounding boxes for each case. To conduct a fair comparison, we allowed only the top two bounding boxes from the AI system to match the number of ROIs produced by the readers. To compare the localization performance of the AI system with that of the readers, we used a method inspired by location receiver operating characteristic (LROC) analysis37. LROC analysis differs from traditional ROC analysis in that the ordinate is a sensitivity measure that factors in localization accuracy. Although LROC analysis traditionally involves a single finding per case37,47, we permitted multiple unranked findings to match the format of our data. We use the term multi-localization ROC analysis (mLROC) to describe our approach. For each threshold, a cancer case was considered a true positive if its case-wide score exceeded this threshold and at least one culprit area was correctly localized in any of the four mammogram views. Correct localization required an intersection-over-union (IoU) of 0.1 with the ground-truth ROI. False positives were defined as usual. CAD systems are often evaluated on the basis of whether the centre of their marking falls within the boundary of a ground-truth annotation48. This is potentially problematic as it does not properly penalize predicted bounding boxes that are so large as to be non-specific, but whose centre nevertheless happens to fall within the target region. Similarly, large ground-truth annotations associated with diffuse findings might be overly generous to the CAD system. We prefer the IoU metric because it balances these considerations. We chose a threshold of 0.1 to account for the fact that indistinct margins on mammography findings lead to ROI annotations of vastly different sizes depending on subjective factors of the annotator (see Supplementary Fig. 4). Similar work in three-dimensional chest computed tomography18 used any pixel overlap to qualify for correct localization. Likewise, an FDA-approved software device for the detection of wrist fractures reports statistics in which true positives require at least one pixel of overlap49. An IoU value of 0.1 is strict by these standards. 

Statistical analysis:
To evaluate the stand-alone performance of the AI system, the AUC-ROC was estimated using the normalized Wilcoxon (Mann–Whitney) U statistic50. This is the standard non-parametric method used by most modern software libraries. For the UK dataset, non-parametric confidence intervals on the AUC were computed with DeLong’s method51,52. For the US dataset, in which each sample carried a scalar weight, the bootstrap was used with 1,000 replications. For both datasets, we compared the sensitivity and specificity of the readers with that of a thresholded score from the AI system. For the UK dataset, we knew the pseudo-identity of each reader, so statistics were adjusted for the clustered nature of the data using Obuchowski’s method for paired binomial proportions53,54. Confidence intervals on the difference are Wald intervals55 and a Wald test was used for non-inferiority56. Both used the Obuchowski variance estimate. For the US dataset, in which each sample carried a scalar inverse probability weight45, we used resampling methods57 to compare the sensitivity and specificity of the AI system with those of the pool of radiologists. Confidence intervals on the difference were generated with the bootstrap method with 1,000 replications. A P value on the difference was generated through the use of a permutation test58. In each of 10,000 trials, the reader and AI system scores were randomly interchanged for each case, yielding a reader–AI system difference sampled from the null distribution. A two-sided P value was computed by comparing the observed statistic to the empirical quantiles of the randomization distribution. In the reader study, each reader graded each case using a forced BI-RADS protocol (a score of 0 was not permitted), and the resulting values were treated as a 6-point index of suspicion for malignancy. Scores of 1 and 2 were collapsed into the lowest category of suspicion; scores 3, 4a, 4b, 4c and 5 were treated independently as increasing levels of suspicion. Because none of the BI-RADS operating points reached the high-sensitivity regime (see Fig. 3), to avoid bias from non-parametric analysis59 we fitted parametric ROC curves to the data using the proper binormal model60. This issue was not alleviated by using the readers’ ratings for their suspicion of malignancy, which showed very strong correspondence with the BI-RADS scores (Supplementary Fig. 5). As BI-RADS is used in actual screening practice, we chose to focus on these scores for their superior clinical relevance. In a similar fashion, we fitted a parametric ROC curve to discretized AI system scores on the same data. The performance of the AI system was compared to that of the panel of radiologists using methods for the analysis of multi-reader multi-case (MRMC) studies that are standard in the radiology community61. More specifically, we compared the AUC-ROC and pAUC-mLROC for the AI system to those of the average radiologist using the ORH procedure62,63. Originally formulated for the comparison of multiple imaging modalities, this analysis has been adapted to the setting in which the population of radiologists operate on a single modality and interest lies in comparing their performance to that of a stand-alone algorithm61. The jackknife method was used to estimate the covariance terms in the model. Computation of P values and confidence intervals was conducted in Python using the numpy and scipy packages, and benchmarked against a reference implementation in the RJafroc library for the R computing language (https://cran.r-project.org/web/packages/RJafroc/index.html). Our primary comparisons numbered seven in total: sensitivity and specificity for the UK first reader; sensitivity and specificity for the US clinical radiologist; sensitivity and specificity for the US clinical radiologist against a model trained using only UK data; and the AUC-ROC in the reader study. For comparisons with the clinical reads, the choice of superiority or non-inferiority was based on what seemed attainable from simulations conducted on the validation set. For non-inferiority comparisons, a 5% absolute margin was pre-specified before the test set was inspected. We used a statistical significance threshold of 0.05. All seven P values survived correction for multiple comparisons using the Holm–Bonferroni method64. 

Reporting summary:
Further information on research design is available in the Nature Research Reporting Summary linked to this paper.