<table class="data last-table">
 <thead class="c-article-table-head">
  <tr>
   <th class="u-text-left">
    <p>
     Agent
    </p>
   </th>
   <th class="u-text-left">
    <p>
     Median (%)
    </p>
   </th>
   <th class="u-text-left">
    <p>
     Mean (%)
    </p>
   </th>
   <th class="u-text-left">
    <p>
     Environment frames
    </p>
   </th>
   <th class="u-text-left">
    <p>
     Training time
    </p>
   </th>
   <th class="u-text-left">
    <p>
     Training steps
    </p>
   </th>
  </tr>
 </thead>
 <tbody>
  <tr>
   <td class="u-text-left">
    <p>
     Ape-X
     <sup>
      <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41586-020-03051-4#ref-CR20" id="ref-link-section-d261098888e2344" title="Horgan, D. et al. Distributed prioritized experience replay. In International Conference on Learning Representations (2018).">
       20
      </a>
     </sup>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     434.1
    </p>
   </td>
   <td class="u-text-left">
    <p>
     1,695.6
    </p>
   </td>
   <td class="u-text-left">
    <p>
     22.8 billion
    </p>
   </td>
   <td class="u-text-left">
    <p>
     5 days
    </p>
   </td>
   <td class="u-text-left">
    <p>
     8.64 million
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     R2D2
     <sup>
      <a aria-label="Reference 19" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41586-020-03051-4#ref-CR19" id="ref-link-section-d261098888e2382" title="Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. &amp; Munos, R. Recurrent experience replay in distributed reinforcement learning. In International Conference on Learning Representations (2019).">
       19
      </a>
     </sup>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     1,920.6
    </p>
   </td>
   <td class="u-text-left">
    <p>
     4,024.9
    </p>
   </td>
   <td class="u-text-left">
    <p>
     37.5 billion
    </p>
   </td>
   <td class="u-text-left">
    <p>
     5 days
    </p>
   </td>
   <td class="u-text-left">
    <p>
     2.16 million
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     MuZero
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      2,041.1
     </b>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      4,999.2
     </b>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     20.0 billion
    </p>
   </td>
   <td class="u-text-left">
    <p>
     12 hours
    </p>
   </td>
   <td class="u-text-left">
    <p>
     1 million
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     IMPALA
     <sup>
      <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41586-020-03051-4#ref-CR18" id="ref-link-section-d261098888e2458" title="Espeholt, L. et al. IMPALA: scalable distributed deep-RL with importance weighted actor-learner architectures. In Proc. International Conference on Machine Learning, ICML Vol. 80 (eds Dy, J. &amp; Krause, A.) 1407–1416 (2018).">
       18
      </a>
     </sup>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     191.8
    </p>
   </td>
   <td class="u-text-left">
    <p>
     957.6
    </p>
   </td>
   <td class="u-text-left">
    <p>
     200 million
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     Rainbow
     <sup>
      <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41586-020-03051-4#ref-CR36" id="ref-link-section-d261098888e2496" title="Hessel, M. et al. Rainbow: combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence (2018).">
       36
      </a>
     </sup>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     231.1
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
   <td class="u-text-left">
    <p>
     200 million
    </p>
   </td>
   <td class="u-text-left">
    <p>
     10 days
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     UNREAL
     <sup>
      a
     </sup>
     <sup>
      <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41586-020-03051-4#ref-CR42" id="ref-link-section-d261098888e2537" title="Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at 
                  https://arxiv.org/abs/1611.05397
                  
                 (2016).">
       42
      </a>
     </sup>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     250
     <sup>
      a
     </sup>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     880
     <sup>
      a
     </sup>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     250 million
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     LASER
     <sup>
      <a aria-label="Reference 37" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="/articles/s41586-020-03051-4#ref-CR37" id="ref-link-section-d261098888e2579" title="Schmitt, S., Hessel, M. &amp; Simonyan, K. Off-policy actor-critic with shared experience replay. Preprint at 
                  https://arxiv.org/abs/1909.11583
                  
                 (2019).">
       37
      </a>
     </sup>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     431
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
   <td class="u-text-left">
    <p>
     200 million
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
   <td class="u-text-left">
    <p>
     –
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     MuZero Reanalyze
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      731.1
     </b>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      2,168.9
     </b>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     200 million
    </p>
   </td>
   <td class="u-text-left">
    <p>
     12 hours
    </p>
   </td>
   <td class="u-text-left">
    <p>
     1 million
    </p>
   </td>
  </tr>
 </tbody>
</table>
