a, Scaling with search time per move in Go, comparing the learned model with the ground truth simulator. Both networks were trained at 800 simulations per search, equivalent to 0.1 s per search. Remarkably, the learned model is able to scale well to up to two orders of magnitude longer searches than seen during training. b, Scaling of final human normalized mean score in Atari with the number of simulations per search. The network was trained at 50 simulations per search. Dark line indicates mean score and the shaded regions indicate the 25th to 75th and 5th to 95th percentiles. The learned model’s performance increases up to 100 simulations per search. Beyond, even when scaling to much longer searches than during training, the learned model’s performance remains stable and decreases only slightly. This contrasts with the much better scaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. c, Comparison of MCTS-based training with Q-learning in the MuZero framework on Ms. Pac-Man, keeping network size and amount of training constant. The state-of-the-art Q-learning algorithm R2D2 is shown as a baseline. Our Q-learning implementation reaches the same final score as R2D2, but improves slower and results in much lower final performance compared with MCTS-based training. d, Different networks trained at different numbers of simulations (sims) per move, but all evaluated at 50 simulations per move. Networks trained with more simulations per move improve faster, consistent with ablation (b), where the policy improvement is larger when using more simulations per move. Surprisingly, MuZero can learn effectively even when training with less simulations per move than are enough to cover all eight possible actions in Ms. Pac-Man.