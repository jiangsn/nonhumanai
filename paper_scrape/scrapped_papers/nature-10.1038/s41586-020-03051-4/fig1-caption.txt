a, How MuZero uses its model to plan. The model consists of three connected components for representation, dynamics and prediction. Given a previous hidden state sk−1 and a candidate action ak, the dynamics function g produces an immediate reward rk and a new hidden state sk. The policy pk and value function vk are computed from the hidden state sk by a prediction function f. The initial hidden state s0 is obtained by passing the past observations (for example, the Go board or Atari screen) into a representation function h. b, How MuZero acts in the environment. An MCTS is performed at each timestep t, as described in a. An action at+1 is sampled from the search policy πt, which is proportional to the visit count for each action from the root node. The environment receives the action and generates a new observation ot+1 and reward ut+1. At the end of the episode, the trajectory data are stored into a replay buffer. c, How MuZero trains its model. A trajectory is sampled from the replay buffer. For the initial step, the representation function h receives as input the past observations o1, ..., ot from the selected trajectory. The model is subsequently unrolled recurrently for K steps. At each step k, the dynamics function g receives as input the hidden state sk−1 from the previous step and the real action at+k. The parameters of the representation, dynamics and prediction functions are jointly trained, end to end, by backpropagation through time, to predict three quantities: the policy pk ≈ πt+k, value function vk ≈ zt+k and reward rk ≈ ut+k, where zt+k is a sample return: either the final reward (board games) or n-step return (Atari). Schematic Go boards at the top of the figure represent the sequence of observations.