SECTION 1. Introduction: With the advent of sophisticated image and video synthesis techniques, it has become increasingly easier to generate high-quality convincing fake videos. Deepfakes are a new genre of synthetic videos, in which a subject’s face is modified into a target face in order to simulate the target subject in a certain context and create convincingly realistic footage of events that never occurred. Video manipulation methods like Face2Face [49], Neural Textures [48] and FaceSwap [26] operate end-to-end on a source video and target face and require minimal human expertise to generate fake videos in real-time. The intent of generating such videos can be harmless and have advanced the research of synthetic video generation for movies, storytelling and modern-day streaming services. However, they can also be used maliciously to spread disinformation, harass individuals or defame famous personalities [45], The extensive spread of fake videos through social media platforms has raised significant concerns worldwide, particularly hampering the credibility of digital media. Figure 1. 
Adversarial Deepfakes for XceptionNet [40] detector. Top: Frames of of a fake video generated by Face2Face being correctly identified as fake by the detector. Bottom: Corresponding frames of the adversarially modified fake video being classified as real by the detector. 
To address the threats imposed by Deepfakes, the machine learning community has proposed several countermeasures to identify forgeries in digital media. Recent state-of-the-art methods for detecting manipulated facial content in videos rely on Convolutional Neural Networks (CNNs) [17], [40], [1], [2], [30], [39], A typical Deepfake detector consists of a face-tracking method, following which the cropped face is passed on to a CNN-based classifier for classification as real or fake [1], [13]. Some of the recent DeepFake detection methods use models operate on a sequence of frames as opposed to a single frame to exploit temporal dependencies in videos [15]. While the above neural network based detectors achieve promising results in accurately detecting manipulated videos, in this paper we demonstrate that they are susceptible to adversarial examples which can fool the detectors to classify fake videos as real1. An adversarial example is an intentionally perturbed input that can fool a victim classification model [46]. Even though several works have demonstrated that neural networks are vulnerable to adversarial inputs (Section 2.3), we want to explicitly raise this issue that has been ignored by existing works on Deepfake detection (Section 2.2). Since fake video generation can potentially be used for malicious purposes, it is critical to address the vulnerability of Deepfake detectors to adversarial inputs. To this end, we quantitatively assess the vulnerability of state-of-the-art Deepfake detectors to adversarial examples. Our proposed methods can augment existing techniques for generating fake videos, such that they can bypass a given fake video detector. We generate adversarial examples for each frame of a given fake video and combine them together to synthesize an adversarially modified video that gets classified as real by the victim Deepfake detector. We demonstrate that it is possible to construct fake videos that are robust to image and video compression codecs, making them a real world threat since videos shared over social media are usually compressed. More alarmingly, we demonstrate that it is possible to craft robust adversarial Deepfakes in black-box settings, where the adversary may not be aware of the classification model used by the detector. Finally, we discuss normative points about how the community should approach the problem of Deepfake detection. 

SECTION 2. Background: 2.1. Generating Manipulated Videos: Until recently, the ease of generating manipulated videos has been limited by manual editing tools. However, since the advent of deep learning and inexpensive computing services, there has been significant work in developing new techniques for automatic digital forgery. In our work, we generate adversarial examples for fake videos synthesized using FaceSwap (FS) [26], Face2Face (F2F) [49], DeepFakes (DF) [16] and NeuralTextures (NT) [48]. We perform our experiments on this FaceForesics++ dataset [40], which is a curated dataset of manipulated videos containing facial forgery using the above methods. Another recently proposed dataset containing videos with facial forgery is the Deep-Fake Detection Challenge (DFDC) Dataset [17], which we utilize when evaluating our attacks against sequence based detection frameworks (Section 3.1). 2.2. Detecting Manipulated Videos: Traditionally, multimedia forensics investigated the authenticity of images [51], [10], [21] using hand-engineered features and/or a-priori knowledge of the statistical and physical properties of natural photographs. However, video synthesis methods can be trained to bypass hand-engineered detectors by modifying their training objective. We direct readers to [7], [9] for an overview of counter-forensic attacks to bypass traditional (non-deep learning based) methods of detecting forgeries in multimedia content. More recent works have employed CNN-based approaches that decompose videos into frames to automatically extract salient and discriminative visual features pertinent to Deepfakes. Some efforts have focused on segmenting the entire input image to detect facial tampering resulting from face swapping [56], face morphing [38] and splicing attacks [5], [6]. Other works [28], [29], [1], [23], [40], [41] have focused on detecting face manipulation artifacts resulting from Deepfake generation methods. The authors of [29] reported that eye blinking is not well reproduced in fake videos, and therefore proposed a temporal approach using a CNN + Recurrent Neural Network(RNN) based model to detect a lack of eye blinking when exposing deepfakes. Similarly, [54] used the inconsistency in head pose to detect fake videos. However, this form of detection can be circumvented by purposely incorporating images with closed eyes and a variety of head poses in training [50], [18]. The Deepfake detectors proposed in [40], [1], [17] model Deepfake detection as a per-frame binary classification problem. The authors of [40] demonstrated that XceptionNet can outperform several alternative classifiers in detecting forgeries in both uncompressed and compressed videos, and identifying forged regions in them. In our work, we expose the vulnerability of such state-of-the-art Deepfake detectors. Since the task is to specifically detect facial manipulation, these models incorporate domain knowledge by using a face tracking method [49] to track the face in the video. The face is then cropped from the original frame and fed as input to classification model to be labelled as Real or Fake. Experimentally, the authors of [40] demonstrate that incorporation of domain knowledge helps improve classification accuracy as opposed to using the entire image as input to the classifier. The best performing classifiers amongst others studied by [40] were both CNN based models: XceptionNet [13] and MesoNet [1]. More recently, some detectors have also focused on exploiting temporal dependencies while detecting DeepFake videos. Such detectors work on sequence of frames as opposed to a single frame using a CNN + RNN model or a 3-D CNN model. One such model based on a 3-D EfficientNet [47] architecture, was used by the third place winner [15] of the recently conducted DeepFake Detection Challenge (DFDC) [17]. The first two winning submissions were CNN based per-frame classification models similar to ones described above. We evaluate our attacks against the 3D CNN model to expose the vulnerability of temporal Deepfake detectors. 2.3. Adversarial Examples: Adversarial examples are intentionally designed inputs to a machine learning (ML) model that cause the model to make a mistake [46]. Prior work has shown a series of first-order gradient-based attacks to be fairly effective in fooling DNN based models in both image [35], [34], [22], [31], [11], [44], [43], audio [12], [37], [33] and text [20], [8], [32] domains. The objective of such adversarial attacks is to find a good trajectory that (i) maximally changes the value of the model’s output and (ii) pushes the sample towards a low-density region. This is equivalent to the ML model’s gradient with respect to input features. Prior work on defenses [53] against adversarial attacks, propose to perform random operations over the input images, e.g., random cropping and JPEG compression. However, such defenses are shown to be vulnerable to attack algorithms that are aware of the randomization approach. Particularly, one line of adversarial attack [3], [4] computes the expected value of gradients for each of the sub-sampled networks/inputs and performs attacks that are robust against compression. 

SECTION 3. Methodology: 3.1. Victim Models: Deepfake Detectors: Frame-by-Frame detectors: To demonstrate the effectiveness of our attack on Deepfake detectors, we first choose detectors which rely on frame level CNN based classification models. These victim detectors work on the frame level and classify each frame independently as either Real or Fake using the following two-step pipeline:
A face tracking model [49] extracts the bounding box of the face in a given frame. The cropped face is then resized appropriately and passed as input to a CNN based classifier to be labelled as either real or fake.  In our work, we consider two victim CNN classifiers: XceptionNet [13] and MesoNet [1]. Detectors based on the above pipeline have been shown to achieve state-of-the-art performance in Deepfake detection as reported in [17], [40], [55]. The accuracy of such models on the FaceForensics++ Dataset [40] is reported in Table 1. Sequence based models: We also demonstrate the effectiveness of our attacks on detectors that utilize temporal dependencies. Such detection methods typically use a CNN + RNN or a 3D-CNN architecture to classify a sequence of frames as opposed to a single frame. A 3D-CNN architecture performs convolutions across height, width and time axis thereby exploiting temporal dependencies. In Section 5, we evaluate our attacks against one such detection method [15] that uses a 3-D EfficientNet [47] CNN model for classifying a sequence of face-crops obtained from a face tracking model. In this model, a 3-D convolution is added to each block of the EfficientNet model to perform convolutions across time. The length of the input sequence to the model is 7 frames and the step between frames is 1/15 of a second. This 3-D CNN model was used by the third place winner of the recently conducted DFDC challenge. Figure 2. 
An overview of our attack pipeline to generate Adversarial Deepfakes. We generate an adversarial example for each frame in the given fake video and combine them together to create an adversan ally modified fake video. 
3.2. Threat Model: Given a facially manipulated (fake) video input and a victim Deepfake detector, our task is to adversarially modify the fake video such that most of the frames get classified as Real by the Deepfake detector, while ensuring that the adversarial modification is quasi-imperceptible. Distortion Metric: To ensure imperceptibility of the adversarial modification, the Lp norm is a widely used distance metric for measuring the distortion between the adversarial and original inputs. The authors of [22] recommend constraining the maximum distortion of any individual pixel by a given threshold e, i.e., constraining the perturbation using an L∞ metric. Additionally, Fast Gradient Sign Method (FGSM) [22] based attacks, which are optimized for the L∞ metric, are more time-efficient than attacks which optimize for L2 or L0 metrics. Since each video can be composed of thousands of individual frames, time-efficiency becomes an important consideration to ensure the proposed attack can be reliably used in practice. Therefore, in this work, we use the L∞ distortion metric for constraining our adversarial perturbation and optimize for it using gradient sign based methods. Notation: We follow the notation previously used in [11], [36]: Define F to be the full neural network (classifier) including the softmax function, Z(x) = z to be the output of all layers except the softmax (so z are the logits), and
F(x)=softmax(Z(x))=yView Source\begin{equation*}F(x) = softmax\left( {Z(x)} \right) = y\end{equation*} The classifier assigns the label C(x) = arg maxi(F(x)i) to input x. Problem Formulation: Mathematically, for any given frame x0 of a fake video, and a victim frame-forgery detector model C, we aim to find an adversarial frame xadu such that,
C(xadv)=Realand∥xadv−x0∥∞<εView Source\begin{equation*}C\left( {{x_{adv}}} \right) = Real\,{\text{and}}\,{\left\| {{x_{adv}} - {x_0}} \right\|_\infty } < \varepsilon \end{equation*} Attack Pipeline: An overview of the process of generating adversarial fake videos is depicted in Figure 2. For any given frame, we craft an adversarial example for the cropped face, such that after going through some image transformations (normalization and resizing), it gets classified as Real by the classifier. The adversarial face is then placed in the bounding box of face-crop in the original frame, and the process is repeated for all frames of the video to create an adversarially modified fake video. In the following sections, we consider our attack pipeline under various settings and goals. Note that, the proposed attacks can also be applied on detectors that operate on entire frames as opposed to face-crops. We choose face-crop based victim models because they have been shown to outperform detectors that operate on entire frames for detecting facial-forgeries. 3.3. White-box Attack: In this setting, we assume that the attacker has complete access to the detector model, including the face extraction pipeline and the architecture and parameters of the classification model. To construct adversarial examples using the attack pipeline described above, we use the iterative gradient sign method [27] to optimize the following loss function:
Minimizeloss(x′)whereloss(x′)=max(Z(x′)Fake−Z(x′)Real,0)(1)View Source\begin{equation*}\begin{array}{l} {{\text{Minimize}}\,loss\left( {{x^\prime }} \right)\,{\text{where}}} \\ {loss\left( {{x^\prime }} \right) = max\left( {Z{{\left( {{x^\prime }} \right)}_{Fake}} - Z{{\left( {{x^\prime }} \right)}_{Real}},0} \right)} \end{array}\tag{1}\end{equation*} Here, Z(x)y is the final score for label y before the softmax operation in the classifier C. Minimizing the above loss function maximizes the score for our target label Real. The loss function we use is recommended in [11] because it is empirically found to generate less distorted adversarial samples and is robust against defensive distillation. We use the iterative gradient sign method to optimize the above objective while constraining the magnitude of the perturbation as follows:
xi=xi−1−clipε(α⋅sign(∇loss(xi−1)))(2)View Source\begin{equation*}{x_i} = {x_{i - 1}} - {\operatorname{clip} _\varepsilon }\left( {\alpha \cdot \operatorname{sign} \left( {\nabla loss\left( {{x_{i - 1}}} \right)} \right)} \right)\tag{2}\end{equation*} We continue gradient descent iterations until success or until a given number number of maximum iterations, whichever occurs earlier. In our experiments, we demonstrate that while we are able to achieve an average attack success rate of 99.05% when we save videos with uncompressed frames, the perturbation is not robust against video compression codecs like MJPEG. In the following section, we discuss our approach to overcome this limitation of our attack. 3.4. Robust White-box Attack: Generally, videos uploaded to social networks and other media sharing websites are compressed. Standard operations like compression and resizing are known for removing adversarial perturbations from an image [19], [14], [24]. To ensure that the adversarial videos remain effective even after compression, we craft adversarial examples that are robust over a given distribution of input transformations [4]. Given a distribution of input transformations T, input image x, and target class y, our objective is as follows:
xadv=argmaxxEt∼T[F(t(x))y] s.t. ∥x−x0∥∞<εView Source\begin{equation*}{x_{adv}} = \arg ma{x_x}{\mathbb{E}_{t\sim T}}\left[ {F{{(t(x))}_y}} \right]{\text{ s}}{\text{.t}}{\text{. }}{\left\| {x - {x_0}} \right\|_\infty } < \varepsilon \end{equation*} That is, we want to maximize the expected probability of target class y over the distribution of input transforms T. To solve the above problem, we update the loss function given in Equation 1 to be an expectation over input transforms T as follows:
loss(x)=Et∼T[max(Z(t(x))Fake−Z(t(x))Real,0)]View Source\begin{equation*}loss(x) = {\mathbb{E}_{t\sim T}}\left[ {max\left( {Z{{(t(x))}_{Fake}} - Z{{(t(x))}_{Real}},0} \right)} \right]\end{equation*} Following the law of large numbers, we estimate the above loss functions for n samples as:
loss(x)=1n∑ti∼T[max(Z(ti(x))Fake−Z(ti(x))Real,0)]View Source\begin{equation*}loss(x) = \frac{1}{n}\sum\limits_{{t_i}\sim T} {\left[ {max\left( {Z{{\left( {{t_i}(x)} \right)}_{Fake}} - Z{{\left( {{t_i}(x)} \right)}_{Real}},0} \right)} \right]} \end{equation*} Since the above loss function is a sum of differentiable functions, it is tractable to compute the gradient of the loss w.r.t. to the input x. We minimize this loss using the iterative gradient sign method given by Equation 2. We iterate until a given a number number of maximum iterations or until the attack is successful under the sampled set of transformation functions, whichever happens first. Next we describe the class of input transformation functions we consider for the distribution T: Gaussian Blur: Convolution of the original image with a Gaussian kernel k. This transform is given by t(x) = k * x where * is the convolution operator. Gaussian Noise Addition: Addition of Gaussian noise sampled from Θ∼N(0,σ) to the input image. This transform is given by t(x) = x + Θ Translation: We pad the image on all four sides by zeros and shift the pixels horizontally and vertically by a given amount. Let tx be the transform in the x axis and ty be the transform in the y axis, then t(x)=x′H,W,C s.t x′[i, j, c] = x[i + tx, j + ty, c] Downsizing and Upsizing: The image is first downsized by a factor r and then up-sampled by the same factor using bilinear re-sampling. The details of the hyper-parameter search distribution used for these transforms can be found in the Section 4.1. 3.5. Black-box Attack: In the black-box setting, we consider the more challenging threat model in which the adversary does not have access to the classification network architecture and parameters. We assume that the attacker has knowledge of the detection pipeline structure and the face tracking model. However, the attacker can solely query the classification model as a blackbox function to obtain the probabilities of the frame being Real or Fake. Hence there is a need to estimate the gradient of the loss function by querying the model and observing the change in output for different inputs, since we cannot backpropagate through the network. We base our algorithm for efficiently estimating the gradient from queries on the Natural Evolutionary Strategies (NES) approach of [52], [25]. Since we do not have access to the pre-softmax outputs Z, we aim to maximize the class probability F(x)y of the target class y. Rather than maximizing the objective function directly, NES maximizes the expected value of the function under a search distribution π(θ|x). That is, our objective is:
Maximize:Eπ(θ|x)[F(θ)y]View Source\begin{equation*}Maximize:\,{\mathbb{E}_{\pi \left( {\left. \theta \right|x} \right)}}\left[ {F{{(\theta )}_y}} \right]\end{equation*} This allows efficient gradient estimation in fewer queries as compared to finite-difference methods. From [52], we know the gradient of expectation can be derived as follows:
∇xEπ(θ∣x)[F(θ)y]=Eπ(θ∣x)[F(θ)y∇xlog(π(θ∣x))]View Source\begin{equation*}{\nabla _x}{\mathbb{E}_{\pi (\theta \mid x)}}\left[ {F{{(\theta )}_y}} \right] = {\mathbb{E}_{\pi (\theta \mid x)}}\left[ {F{{(\theta )}_y}{\nabla _x}\log (\pi (\theta \mid x))} \right]\end{equation*} Similar to [25], [52], we choose a search distribution π(θ|x) of random Gaussian noise around the current image x. That is, θ = x + σδ where δ∼N(0,I). Estimating the gradient with a population of n samples yields the following variance reduced gradient estimate:
∇E[F(θ)]≈1σn∑i=1nδiF(θ+σδi)yView Source\begin{equation*}\nabla \mathbb{E}\left[ {F(\theta )} \right] \approx \frac{1}{{\sigma n}}\sum\limits_{i = 1}^n {{\delta _i}F{{\left( {\theta + \sigma {\delta _i}} \right)}_y}} \end{equation*} We use antithetic sampling to generate δi similar to [42], [25]. That is, instead of generating n values δ∼N(0,I), we sample Gaussian noise for i∈{1,…,n2} and set δj = − δn−j+1 for j∈{(n2+1),…,n}. This optimization has been empirically shown to improve performance of NES. Algorthim 1 details our implementation of estimating gradients using NES. The transformation distribution T in the algorithm just contains an identity function i.e., T = {I(x)} for the black-box attack described in this section. After estimating the gradient, we move the input in the direction of this gradient using iterative gradient sign updates to increase the probability of target class:
xi=xi−1+clipε(α⋅sign(∇F(xi−1)y))(3)View Source\begin{equation*}{x_i} = {x_{i - 1}} + {\operatorname{clip} _\varepsilon }\left( {\alpha \cdot \operatorname{sign} \left( {\nabla F{{\left( {{x_{i - 1}}} \right)}_y}} \right)} \right)\tag{3}\end{equation*} 3.6. Robust Black-box Attack: To ensure robustness of adversarial videos to compression, we incorporate Expectation over Transforms (Section 3.4) method in the black-box setting for constructing adversarial videos. To craft adversarial examples that are robust under a given set of input transformations T, we maximize the expected value of the function under a search distribution π(θ|x) and our distribution of input transforms T. That is, our objective is to maximize:
Et∼T[Eπ(θ∣x)[F(t(θ))y]]View Source\begin{equation*}{\mathbb{E}_{t\sim T}}\left[ {{\mathbb{E}_{\pi (\theta \mid x)}}\left[ {F{{(t(\theta ))}_y}} \right]} \right]\end{equation*} Following the derivation in the previous section, the gradient of the above expectation can be estimated using a population of size n by iterative sampling of ti and δi:
∇E[F(θ)]≈1σn∑i=1,ti∼TnδiF(ti(θ+σδi))yView Source\begin{equation*}\nabla \mathbb{E}[F(\theta )] \approx \frac{1}{{\sigma n}}\sum\limits_{i = 1,{t_i}\sim T}^n {{\delta _i}F{{\left( {{t_i}\left( {\theta + \sigma {\delta _i}} \right)} \right)}_y}} \end{equation*} Algorithm 1 
NES Gradient Estimate
We use the same class of transformation functions listed in Section 3.4 for the distribution T. Algorithm 1 details our implementation for estimating gradients for crafting robust adversarial examples. We follow the same update rule given by Equation 3 to generate adversarial frames. We iterate until a given a number of maximum iterations or until the attack is successful under the sampled set of transformation functions. 

SECTION 4. Experiments: Dataset and Models: We evaluate our proposed attack algorithm on two pre-trained victim models: XceptionNet [13] and MesoNet [1]. In our experiments, we perform our attack on the test set of the FaceForensics++ Dataset [40], consisting of manipulated videos from the four methods described in Section 2.1. We construct adversarially modified fake videos on the FaceForensics++ test set, which contains 70 videos (total 29,764 frames) from each of the four manipulation techniques. For simplicity, our experiments are performed on high quality (HQ) videos, which apply a light compression on raw videos. The accuracy of the detector models for detecting facially manipulated videos on this test set is reported in Table 1. We will be releasing code for all our attack algorithms in PyTorch2. Table 1. 
Accuracy of Deepfake detectors on the FaceForensics++ HQ Dataset as reported in [40], The results are for the entire high-quality compressed test set generated using four manipulation techniques (DF: DeepFakes, F2F: Face2Face, FS: FaceSwap and NT: NeuralTextures).
Evaluation Metrics: Once the adversarial frames are generated, we combine them and save the adversarial videos in the following formats:
Uncompressed (Raw): Video is stored as a sequence of uncompressed images. Compressed (MJPEG): Video is saved as a sequence of JPEG compressed frames. Compressed (H.264): Video is saved in the commonly used mp4 format that applies temporal compression across frames.  We conduct our primary evaluation on the Raw and MJPEG video formats across all attacks. We also study the effectiveness of our white box robust attack using different compression levels in the H264 codec. We report the following metrics for evaluating our attacks: Success Rate (SR): The percentage of frames in the adversarial videos that get classified to our target label Real. We report: SR-U- Attack success rate on uncompressed adversarial videos saved in Raw format; and SR-C- Attack success rate on compressed adversarial videos saved in MJPEG format. Accuracy: The percentage of frames in videos that get classified to their original label Fake by the detector. We report Acc-C- accuracy of the detector on compressed adversarial videos. Mean distortion (L∞): The average L∞ distortion between the adversarial and original frames. The pixel values are scaled in the range [0,1], so changing a pixel from full-on to full-off in a grayscale image would result in L∞ distortion of 1 (not 255). 4.1. White-box Setting: To craft adversarial examples in the white-box setting, in our attack pipeline, we implement differentiable image pre-processing (resizing and normalization) layers for the CNN. This allows us to backpropagate gradients all the way to the cropped face in-order to generate the adversarial image that can be placed back in the frame. We set the maximum number of iterations to 100, learning rate α to 1/255 and max L∞ constraint ϵ to 16/255 for both our attack methods described in Sections 3.3 and 3.4. Table 2. 
Success Rate of White-box attack on XceptionNet and MesoNet. We report the average L∞ distortion between the adversarial and original frames and the attack success rate on uncompressed (SR-U) and compressed (SR-C) videos. Acc-C denotes the accuracy of the detector on compressed adversarial videos.
Table 2 shows the results of the white-box attack (Section 3.3). We are able to generate adversarial videos with an average success rate of 99.85% for fooling XceptionNet and 98.15% for MesoNet when adversarial videos are saved in the Raw format. However, the attack average success rate drops to 58.46% for XceptionNet and 92.72% for MesoNet when MJPEG compression is used. This result is coherent with past works [19], [14], [24] that employ JPEG compression and image transformations to defend against adversarial examples. Figure 3. 
Randomly selected frames of Adversarial Deepfakes from successful attacks. The frame from the dataset in the first column is correctly identified as Fake by the detectors, while the corresponding frames generated by each of our attacks are labelled as Real with a probability of 1. Video examples are linked in the footnote on the first page. 
Robust White-Box: For our robust white box attack, we sample 12 transformation functions from the distribution T for estimating the gradient in each iteration. This includes three functions from each of the four transformations listed in Section 3.4. Table 3 shows the search distribution for different hyper-parameters of the transformation functions. Table 3. 
Search distribution of hyper-parameters of different transformations used for our Robust White box attack. During training, we sample three functions from each of the transforms to estimate the gradient of our expectation over transforms.
Table 4. 
Success Rate of Robust White-box attack on XceptionNet and MesoNet. Acc-C denotes the accuracy of the detector on compressed adversarial videos.
Table 4 shows the results of our robust white-box attack. It can be seen that robust white-box is effective in both Raw and MJPEG formats. The average distortion between original and adversarial frames in the robust attack is higher as compared to the non-robust white-box attack. We achieve an average success rate (SR-C) of 98.07% and 99.83% for XceptionNet and MesoNet respectively in the compressed video format. Additionally, to assess the gain obtained by incorporating the transformation functions, we compare the robust white-box attack against the non-robust white-box attack at the same level of distortion in Table 5. We observe a significant improvement in attack success rate on compressed videos (SR-C) when using the robust attack as opposed to the simple white-box attack (84.96% vs 74.69% across all datasets at L∞ norm of 0.008). Table 5. 
Comparison of white-box and robust white-box attacks at the same magnitude of L∞ norm of the adversarial perturbation. Acc-C denotes the accuracy of the detector on compressed adversarial videos.
We also study the effectiveness of our robust white box attack under different levels of compression in the H.264 format which is widely used for sharing videos over the internet. Figure 4 shows the average success rate of our attack across all datasets for different quantization parameter c used for saving the video in H.264 format. The higher the quantization factor, the higher is the compression level. In [40], fake videos are saved in HQ and LQ formats which use c = 23 and c = 40 respectively. It can be seen that even at very high compression levels (c = 40), our attack is able to achieve 80.39% and 90.50% attack success rate for XceptionNet and MesoNet respectively, without any additional hyper-parameter tuning for this experiment. Figure 4. 
Attack success rate vs Quantization factor used for compression in H264 codec for robust white box attack. 
4.2. Black-box Setting: We construct adversarial examples in the black-box setting using the methods described in Sections 3.5 and 3.6. The number of samples n in the search distribution for estimating gradients using NES is set to 20 for black-box attacks and 80 for robust black-box to account for sampling different transformation functions ti. We set the maximum number of iterations to 100, learning rate α to 1/255 and max L∞ constraint ϵ to 16/255. Table 6 shows the results of our Black-box attack (Section 3.5) without robust transforms. Note that the average L∞ norm of the perturbation across all datasets and models is higher than our white-box attacks. We are able to generate adversarial videos with an average success rate of 97.04% for XceptionNet and 86.70% for MesoNet when adversarial videos are saved in the Raw format. Similar to our observation in the white-box setting, the success rate drops significantly in the compressed format for this attack. The average number of queries to the victim model for each frame is 985 for this attack. Robust Black-box: We perform robust black-box attack using the algorithm described in (Section 3.6). For simplicity, during the robust black-box attack we use the same hyper-parameters for creating a distribution of transformation functions T (Table 3) as those in our robust white-box attack. The average number of network queries for fooling each frame is 2153 for our robust black-box attack. Table 7 shows the results for our robust black-box attack. We observe a significant improvement in the attack success rate for XceptionNet when we save adversarial videos in the compressed format as compared to that in the naive black-box attack setting. When attacking MesoNet in robust blackbox setting, we do not observe a significant improvement even though overall success rate is higher when using robust transforms. Table 6. 
Success Rate of of Black-box attack on XceptionNet and MesoNet. Acc-C denotes the accuracy of the detector on compressed adversarial videos.
Table 7. 
Success Rate of Robust Black-box attack on XceptionNet and MesoNet. Acc-C denotes the accuracy of the detector on compressed adversarial videos.


SECTION 5. Evaluation on Sequence Based Detector: We consider the 3D CNN based detector described in Section 3.1. The detector performs 3D convolution on a sequence of face-crops from 7 consecutive frames. We perform our attacks on the pre-trained model checkpoint (trained on DFDC [17] train set) released by the NTech-Lab team [15]. We evaluate our attacks on the DeepFake videos from the DFDC public validation set which contains 200 Fake videos. We report the accuracy of the detector on the 7-frame sequences from this test set in the first row of Table 8. Similar to our attacks on frame-by-frame detectors, in the white-box setting we back-propagate the loss through the entire model to obtain gradients with respect to the input frames for crafting the adversarial frames. While both white-box and robust white-box attacks achieve 100% success rate on uncompressed videos, the robust white-box attack performs significantly better on the compressed videos and is able to completely fool the detector. As compared to frame-by-frame detectors, a higher magnitude of perturbation is required to fool this sequence model in both the white-box attacks. In the black-box attack setting, while we achieve similar attack success rates on uncompressed videos as the frame-by-frame detectors, the attack success rate drops after compression. The robust black-box attack helps improve robustness of adversarial perturbations to compression as observed by higher success rates on compressed videos (51.02% vs 24.43% SR-C). Table 8. 
Evaluation of different attacks on a sequence based detector on the DFDC validation dataset. The first row indicates the performance of the classifier on benign (non adversarial) videos.


SECTION 6. Discussion and Conclusion: The intent of Deepfake generation can be malicious and their detection is a security concern. Current works on DNN-based Deepfake detection assume a non-adaptive adversary whose aim is to fool the human-eye by generating a realistic fake video. To use these detectors in practice, we argue that it is essential to evaluate them against an adaptive adversary who is aware of the defense being present and is intentionally trying to fool the defense. In this paper, we show that the current state-of-the-art methods for Deepfake detection can be easily bypassed if the adversary has complete or even partial knowledge of the detector. Therefore, there is a need for developing provably robust detectors that are evaluated under different attack scenarios and attacker capabilities. In order to use DNN based classifiers as detectors, ensuring robustness to adversarial examples is necessary but not sufficient. A well-equipped attacker may devise other methods to by-pass the detector: For example, an attacker can modify the training objective of the Deepfake generator to include a loss term corresponding to the detector score. Classifiers trained in a supervised manner on existing Deepfake generation methods, cannot be reliably secure against novel Deepfake generation methods not seen during training. We recommend approaches similar to Adversarial Training [22] to train robust Deepfake detectors. That is, during training, an adaptive adversary continues to generate novel Deepfakes that can bypass the current state of the detector and the detector continues improving in order to detect the new Deepfakes. In conclusion, we highlight that adversarial examples are a practical concern for current neural network based Deepfake detectors and therefore recommend future work on designing provably robust Deepfake detectors. 
ACKNOWLEDGEMENTS: This work was supported by ARO under award number W911NF-19-1-0317 and SRC under Task ID: 2899.001.