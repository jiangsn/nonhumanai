SECTION I. Introduction: In computer vision and graphics there are many image-to-image translation tasks, including inpainting [17], [26], super resolution [10], [19], colorization [36], [37], style transfer [11], [15], [25] and so on. This cross-domain image-to-image translation topic has become a major concern of researchers. In many cases, given a paired dataset, it is possible to solve the problem with conditional image translation [18], [22], [30]. However, it is difficult and expensive to obtain the paired samples. In addition, there are cases where supervision is not possible. The goal of the unsupervised image translation is to learn the transformation from a source domain to a target domain given unpaired training data. Recent works have yielded impressive results in the GANs-based unsupervised image-to-image translation [1], [8], [16], [20], [23], [27], [29], [34], [38]. It can be largely classified into two types. The first is the style transfer task. This problem is to change low-level information such as color or texture while maintaining high-level information such as content or geometric structure. Style transfer and conditional GANs-based methods have yielded excellent results in this research area. The second is the object transfiguration task. Unlike the style transfer task, this focuses on changing high-level information while keeping the low-level information. CycleGAN [38], the most representative unsupervised image translation method, failed to change the high-level semantic meaning due to the network structure specialized for style transfer. To solve the unsupervised image-to-image translation problem, UNIT [23] made a shared-latent space assumption. It assumes a pair of corresponding images in different domains can be mapped to a same latent code in a shared-latent space. MUNIT [16] proposed a multimodal unsupervised image-to-image translation framework. To achieve many-to-many cross domain mapping, it mitigates a fully shared latent space assumption in UNIT by decomposing a shared-latent space across domains and each domain-specific part for the style code. UNIT and MUNIT experimentally showed impressive animal image translation from a cropped dataset centered on the head. When the training image dataset is spatially unnormalized, it makes the problem more difficult because the absence of correspondences between the shared semantic parts. In our experiments, we show that these methods often fail in various image-to-image translation applications with strong geometric change. Recently, SAGAN [35] showed that the self-attention module is complementary to convolutions and helps with modeling long range, multi-level dependencies across image regions. Despite the success of the self-attention module in non-conditional GANs, the effectiveness of the self-attention module for unsupervised image-to-image translation has not been validated. In this paper, we propose a unpaired image-to-image translation model with self-attention networks which allows long range dependency modeling for image translation task with strong geometry change. In experiments, we show superiority of the proposed method compared to existing state-of-the-art unsupervised image-to-image translation tasks. The source code and our results are online: https://github.com/itsss/img2img_sa. 

SECTION II. Self-Attention GANS: SAGAN [35] showed that the self-attention module is complementary to convolutions and it helps with long range modeling, multi-level dependencies across image regions. Attention mechanisms have become a important part of models that must capture global dependencies [2], [7], [13], [24], [32], [33].  Figure 1. Self Attention Networks. [35] ⊗ means Matrix multiplication.  Self attention networks adapt a non-local block [31] to introduce the self-attention to the GAN networks, can enable both the generator and discriminator to efficiently model relationships between widely separated spatial regions. The non-local mechanisms also have become a important part of image generation [3]–​[6], [9], [12]. In the self attention module (Figure 1.), image features from the previous hidden layer x are firstly transformed into two feature spaces f and g to calculate the attention. βj,i=exp(sij)∑Ni=1exp(sij),where sij=f(xi)Tg(xj),View Source\begin{equation*}\beta_{j,i}=\frac{\exp(s_{ij})}{\sum_{i=1}^{N}\exp(s_{ij})},  where\  s_{ij}=f(x_{i})^{T}g(x_{j}),\end{equation*} where f(x)=Wfx,g(x)=Wgx and βj,i indicates the extent to which the model attends to the ith location when synthesizing the jth region. Then the output of the attention layer is 0=(o1,o2, …, oj, …, oN), where, oj=∑i=1Nβj,ih(xi), h(xi)=WhxiView Source\begin{equation*}o_{j}=\sum_{i=1}^{N}\beta_{j,i}h(x_{i}),\ h(x_{i})=W_{h}x_{i}\end{equation*} In the above formulation, Wg,Wf and Wh are the learned weights parameters, which are implemented as 1×1 convolutions. 

SECTION III. Methods: A. Unpaired Image-to-Image Translation with Self Attention Networks: We propose an unsupervised image-to-image translation model with self-attention networks that allows long range dependency modeling for image translation tasks with strong geometry change. Combined with self-attention, the generator can translate images in which fine details at every position are carefully coordinated with fine details in distant portions of the image. Furthermore, the discriminator can also more accurately enforce complicated geometric constraints on the global image structure. In this paper, our network architecture is devised by combining several self-attention blocks into the generator and discriminator of the Multimodal Unsupervised Image-to-Image Translation [16](MUNIT) model. To explore the effect of the proposed self-attention mechanism, we built several SAGAN blocks by adding the self-attention mechanism to different stages of the generator and discriminator. For the generator, the self-attention layers are placed before the downsampling layer in the encoder and before the upsampling layer in the decoder, respectively. For the discriminator, it is added before the downsampling layer. Figure 2. shows architecture of our autoencoder model with self-attention networks. B. Loss Function: The full objective of our model comprises a bidirectional reconstruction loss function and an adversarial loss function. Same as in [16], our model consists of an encoder Ei and a decoder Gi for each domain. The latent code of each autoencoder is divided into a content code ci and a style code si, where (ci, si)=(Eci(xi), Esi(xi))=Ei(xi). Image-toimage translation can be performed by exchanging encoderdecoder pairs. Bidirectional Reconstruction Loss Bidirectional reconstruction loss includes image reconstruction loss and latent reconstruction loss. The image reconstruction loss formula is as follows: Lx1recon=Ex1∼p(x1)[||G1(Ec1(x1), Es1(x1))−X1||1].View Source\begin{equation*}\mathcal{L}_{recon}^{x_{1}}=\mathbb{E}_{x_{1}\sim p(x_{1})}[||G_{1}(E_{1}^{c}(x_{1}),\ E_{1}^{s}(x_{1}))-X_{1}||_{1}].\end{equation*} We should be able to reconstruct an image sampled from the data distribution after encoding and decoding. The latent reconstruction loss formula is as follows: Lc1recon=Ec1∼p(c1),s2∼q(s2)[∥Ec2(G2(c1,s2))−c1∥1]Ls2recon=Ec1∼p(c1),s2∼q(s2)[∥Es2(G2(c1,s2))−s2∥1]View Source\begin{align*}&
\mathcal{L}_{\text {recon}}^{c_{1}}=\mathbb{E}_{c_{1} \sim p\left(c_{1}\right), s_{2} \sim q\left(s_{2}\right)}\left[\left\|E_{2}^{c}\left(G_{2}\left(c_{1}, s_{2}\right)\right)-c_{1}\right\|_{1}\right] \\&
\mathcal{L}_{\text {recon}}^{s_{2}}=\mathbb{E}_{c_{1} \sim p\left(c_{1}\right), s_{2} \sim q\left(s_{2}\right)}\left[\left\|E_{2}^{s}\left(G_{2}\left(c_{1}, s_{2}\right)\right)-s_{2}\right\|_{1}\right]
\end{align*} Given a latent code (content an style) from the latent distribution, we should be able to reconstruct it after decoding and encoding. Adversarial Loss The adversarial loss formula is as follows: Lx2GAN=Ec1∼p(c1),s2∼q(s2)[log(1−D2(G2(c1, s2)))]+Ex2∼p(x2)[logD2(x2)].View Source\begin{equation*}\mathcal{L}_{GAN}^{x_{2}}=\mathbb{E}_{c_{1}\sim p(c_{1}),s_{2}\sim q(s_{2})}[\log(1-D_{2}(G_{2}(c_{1},\ s_{2})))]+\mathbb{E}_{x_{2}\sim p(x_{2})}[\log D_{2}(x_{2})].\end{equation*} To match the distribution between the translated and target domain, we employ the adversarial loss. Full objective The total loss formula is as follows:  Figure 2. Architecture of our Network Autoencoder Model  minE1,E2,G1,G2maxD1,D2L(E1,E2,G1,G2,D1,D2)=Lx1GAN+Lx2GAN+λx(Lx1recon+Lx2recon)+λc(Lc1recon+Lc2recon)+λs(Ls1recon+Ls2recon).View Source\begin{align*}&\min _{E_{1}, E_{2}, G_{1}, G_{2}} \max _{D_{1}, D_{2}} \mathcal{L}\left(E_{1}, E_{2}, G_{1}, G_{2}, D_{1}, D_{2}\right)=\\&\mathcal{L}_{G A N}^{x_{1}}+\mathcal{L}_{G A N}^{x_{2}}+\lambda_{x}\left(\mathcal{L}_{r e c o n}^{x_{1}}+\mathcal{L}_{r e c o n}^{x_{2}}\right)+\\&\lambda_{c}\left(\mathcal{L}_{r e c o n}^{c_{1}}+\mathcal{L}_{r e c o n}^{c_{2}}\right)+\lambda_{s}\left(\mathcal{L}_{r e c o n}^{s_{1}}+\mathcal{L}_{r e c o n}^{s_{2}}\right).\end{align*} 

SECTION IV. Experimental Results: In this section, we compared the performance of our model against various unsupervised image-to-image translation models (CycleGAN [38], DRIT [21], UNIT [23], MUNIT [16]). In order to evaluate visual quality of translated images, we performed a user study. A. Implementation Details: We used the MUNIT default setting for experiments. We used the Adam optimizer with β1=0.05,β2=0.999. Initial learning rate of 0.0001 and the learning rate is decreased by half every 100,000 iterations. We used a batch size of 1 and set the loss weights to λx=10,λc=1, λs=1. We trained our networks on four TITAN X accelerators. We trained it over 1,000,000 epochs for around 5 days. B. Datasets: We used cat2dog, face2dog, face2cat, portrait and edges2shoes for test our network. cat2dog: This datasets are used in DRIT [21]. This dataset contains cat(871) and dog(1,364). face2dog: This dataset contains faces (CelebA dataset, 202,599) and dog (cat2dog dataset, 1,364).  Figure 3. Examples of Unsupervised image translation from cat (cat2dog Dataset, Domain A) to dog (cat2dog Dataset, Domain B) using various network structures. CycleGAN, DRIT, UNIT, MUNIT are all trained to 64×64 resolution using the default settings from the official implementations.  face2cat: This dataset contains faces (CelebA dataset, 202,599) and cat (cat2dog dataset, 871). portrait: This datasets are used in DRIT [21]. This dataset contains portrait(1,814) and face photo(6,452). edge2shoes: This dataset are used in MUNIT [16]. This dataset contains edges(50,025) and shoes(50,025). C. Comparision with Previous Works: cat2dog In the process of changing the image of a cat(domain A) to a dog(domain B) image(Figure 3.), CycleGAN is unable to generate a dog image, since it only takes the color from the image. In the case of DRIT, there is a problem that the image is broken, and it is hard to see it as a dog image reflecting the shape and direction of a cat and dog.  Figure 4. Examples of Unsupervised image translation from human face(CelebA Dataset, Domain A) to cat (cat2dog Dataset, Domain B) using various network structures. CycleGAN, DRIT, UNIT, MUNIT are all trained to 64×64 resolution using the default settings from the official implementations.   Figure 5. Examples of Unsupervised image translation from human face(CelebA Dataset, Domain A) to dog (cat2dog Dataset, Domain B) using various network structures. CycleGAN, DRIT, UNIT, MUNIT are all trained to 64×64 resolution using the default settings from the official implementations.  face2cat and face2dog In the process of translating the human face image(domain A) to cat and dog(domain B), CycleGAN and DRIT could not obtain the desired results. Most translated results were distorted. In the case of UNIT and MUNIT, there was a tendency to leave the shape of human face or be distorted in the translated image (See Figure 4. and Figure 5.).  Figure 6. Reverse of Figure 4. Examples of Unsupervised image translation from cat(cat2dog Dataset, Domain A) to human(CelebA Dataset, Domain B) using various network structures. CycleGAN, DRIT, UNIT, MUNIT are all trained to 64×64 resolution using the default settings from the official implementations.   Figure 7. Reverse of Figure 5. Examples of Unsupervised image translation from dog(cat2dog Dataset, Domain A) to human(CelebA Dataset, Domain B) using various network structures. CycleGAN, DRIT, UNIT, MUNIT are all trained to 64×64 resolution using the default settings from the official implementations.  cat2face and dog2face we also experimented for the translations from cats and dogs to human faces. In this experiments, the proposed method showed much better results than the previous works (See Figure 6. and Figure 7.). portrait Even at the stage of changing the portrait (domain A) shown in Figure 8. to a face(domain B), CycleGAN has not been able to convert portrait photos to face at all. In the case of DRIT, conversion is not performed by generating irrelevant images. In the case of UNIT and MUNIT, there is a problem that the image is distorted although it reflects the shape.  Figure 8. Examples of Unsupervised image translation from portrait(portrait Dataset, Domain A) to human face(portrait Dataset, Domain B) using various network structures. CycleGAN, DRIT, UNIT, MUNIT are all trained to 64×64 resolution using the default settings from the official implementations.   Figure 9. Examples of Unsupervised image translation from edges(edges2shoes Dataset, Domain A) to shoes (edges2shoes Dataset, Domain B) using various network structures. CycleGAN, DRIT, UNIT, MUNIT are all trained to 64×64 resolution using the default settings from the official implementations.   Figure 10. Ablation-study result of four self attention techniques.  edges2shoes In the process of translating from the edges image(domain A) to a shoes(domain B) image, our model generated more realistic results keeping the pose and style of A domain than the results from other models (See Figure 9). D. Ablation Study: In this section, the other experiments are conducted to evaluate the effectiveness of the self-attention(SA) networks in our unsupervised image-to-image translation model. In Figure 10. self attention unsupervised image-to-image translation models “SA on downsampling layer (DS-layer) “SA on upsampling layer (US-layer)” and “SA on DS-layers ×3/US-layers ×3” are compared with a our “SA on DS-layer/US-layer” model. In case of “SA on DS-layer “SA on US-layer” and “SA on DS-layers ×3/US-layers ×3 we could not obtained the well-translated results. However, “SA on DS-layer/USlayer” model generated more realistic images than other methods. Based on this experiments, we applied “SA on DS-layer/US-layer” to our model. E. User Study: For the qualitative evaluation, we also conducted a user study on 80 participants. The results of this study are summarized as follows. First, 192 images were selected randomly in the questionnaires, and the questionnaires were used to select the best image that reflects the pose of input Image and the appearance of target domain well. Figure 11. shows that our method yields quantitatively much more superior results than the existing GAN models.  Figure 11. User-study result of five image-to-image translation algorithms.   Table 1 Quantitative Evaluation On 7 Image Translation Examples. We Used Frechet Inception Distance(Fid) To Measure The Performance Of Various Network Structures. F. Quantitative Evaluation Analysis: We used Frchet Inception Distance (FID) [14] to measure the distance between the data distributions of the source and target domains using the features extracted by the inception networks [28]. The lower FID score indicates that the data distribution of two domains are similar. Table I. shows the results of the FID score analysis, and we can see that our model translated more similar images than other image-toimage translation methods. 

SECTION V. Conclusions: In this paper, we proposed a method about unsupervised image-to-image translation with self-attention networks, in which long range dependency helps to not only capture strong geometric change but also generate details using cues from all feature locations. In experiments, we showed superiority of the proposed method compared to existing state-of-the-art unsupervised image-to-image translation methods. 
ACKNOWLEDGEMENTS: We would like to thank our sponsors, especially, Sejong Academy of Science and Arts(SASA) in Korea. We also thanks to Artificial Intelligence Research Institute(AIRI) in Korea. This research was funded by the Korea Foundation for the Advancement of Science&Creativity(KOFAC) Science High School Student R&E support program.