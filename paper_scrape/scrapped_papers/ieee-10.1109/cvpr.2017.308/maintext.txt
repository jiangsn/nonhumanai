SECTION 1. Introduction: Typography is the technology to design the special text effects to render the character into an original and unique artwork. These amazing text styles include basic effects such as shadows, outlines, colors and sophisticated effects such as burning flames, flowing smokes, multicolored neon-s, as shown in Fig. 1. Texts decorated by well-designed special effects become much more attractive. It can also better reflect the thoughts and emotions from the designer. The beauty and elegance of text effects are well appreciated, making it widely used in the publishing and advertisement. However, creating vivid text effects requires a series of subtle processes by an experienced designer using the editing software: determine color styles, warp textures to match text shapes and adjust the transparency for visual plausibleness, etc. These advanced editing skills are far beyond the abilities of most casual users. This practical requirement motivates our work: We investigate an approach to automatically transfer various fantastic text effects designed by artists onto raw plain texts, as shown in Fig. 1.
Figure 1. Overview: Our method takes as input the source text image S, its counterpart stylized image S′ and the target text image T, then automatically generates the target stylized image T′ with the special effects as in S′. 
Text effects transfer is a brand new sub-topic of style transfer. Style transfer can be related to color and texture transfer, respectively. Color transfer matches global [27] or local [31] color distributions of the target and source images. Texture transfer relies on texture synthesis technologies, where the texture generation is constrained by guidance images. Meanwhile, texture synthesis can be divided into two categories: non-parametric methods [7], [6], [15], [35] and parametric methods [14], [9], [17], [11]. The former generates new textures by resampling pixels or patches from the original texture, while the latter models textures using statistical measurements and produces a new texture that shares the same parametric results with the original one. From a technical perspective, it is quite challenging and impractical to directly exploit the traditional style transfer methods to generate new text effects. The challenges lie in three aspects: (i) The extreme diversity of the text effects and character shapes: The style diversity makes the transfer task difficult to model uniformly. Further, the algorithm should be robust to the tremendous variety of characters. (i-i) The complicated composition of style elements: A text effects image often contains multiple intertwined style elements (we call them text sub-effects) that have very different textures and structures (see denim fabric example in Fig. 1) and need specialized treatments. (iii) The simpleness of guidance images: The raw plain text as guidance gives few hints on how to place different sub-effects. Textures in the white text and black background regions may not hold the stationarity. This makes the traditional nonparametric texture-by-numbers method [12] fail, which has assumed textures to be stationary in each region of the guidance map. Meanwhile, the plain text image provides little semantic information. This makes the recent successful parametric deep-based style transfer methods [10], [17] lose their advantages of representing high-level semantic information. For these reasons, conventional style transfer methods for general styles perform poorly on text effects. In this paper, we propose a novel text effects transfer algorithm to address these challenges. The key idea is to analyze and model the distance-based essential characteristics of high-quality text effects and to leverage them to guide the synthesis process. The characteristics are summarized based on the analytics over dozens of well-designed text effects into a general prior. This prior guides our style transfer process to synthesize different sub-effects adaptively and to simulate their spatial distribution. All measurements are carefully designed to achieve certain robustness to the character shape. In addition, we further consider the psycho-visual factor to enhance image naturalness. In summary, our contributions are threefold:
We raise a brand new topic of text effects transfer that turns plain texts into fantastic artworks, which enjoys wide application scenarios such as picture creation on social networks and commercial graphic design. We perform investigation and analysis on well-designed typography and summarize the key distance-based characteristics for high-quality text effects. We model these characteristics mathematically to form a general prior that can be used to significantly improve the style transfer process for texts. We propose the first method to generate compelling text effects, which share both similar local texture patterns and the global spatial distribution with the source example, while preserving image naturalness.  

SECTION 2. Related Work: Color Transfer: Pioneering color transfer methods [27], [25] transfer color between images by matching their global color distributions. Subsequently, local color transfer is achieved based on segmentation [31], [32] or user interaction [34] and it is further improved using fine-grained patch [30] or pixel [29], [24] correspondences. Recently, color transfer [36] and colorization [16], [37] using deep neural networks have drawn people's attentions. Non-Parametric Texture Synthesis and Transfer: E-fros and Lueng [7] proposed a pioneering pixel-by-pixel synthesis approach based on sampling similar patches. The subsequent works improve it in quality and speed by synthesizing patches rather than pixels. To handle the overlapped regions of neighboring patches for seamlessness, Liang et al. [19] proposed to blend patches, and Efros and Freeman [6] used dynamic programming to find an optimal separatrix in overlapped regions, which is further improved via graph cut [15]. Unlike previous methods that synthesize textures in a local manner, recent techniques synthesize globally using objective functions. A coherence-based function [35] is proposed to synthesize textures in an iterative coarse-to-fine fashion. This method performs patch matching and voting operations alternately and achieves good local structures. It is then extended to adapt to non-stationary textures through patch geometric and photometric transformations [2], [5]. Texture transfer, also known as Image Analogies [12], generates textures but also keeps the structure of the target image. Structures are usually preserved by reducing the differences between the source and target guidance maps [12], [23]. In [21], texture boundaries are synthesized in priority to constrain the structure. Frigo et al. [8] proposed an adaptive patch partition to precisely capture source textures and preserve target structures, followed by a Markov Random Field (MRF) function for global texture synthesis. Parametric Texture Synthesis and Transfer: The idea of modelling textures using statistical measurements has led to the development of textons and its variants [14], [26]. Nowadays, deep-based texture synthesis [9] starts trending due to the great descriptive ability of deep neural networks. Gatys et al. proposed to use Gram-matrix in the Convolutional Neural Networks (CNNs) feature space to represent textures [9] and adapt it to style transfer by incorporating content similarities [10]. This work presented the remarkable generic painting transfer technique and attracted many follow-ups in loss function improvement [20], [28] and algorithm acceleration [13], [33]. Recently, methods that replace the Gram-matrix by MRF regularizer is proposed for photographic synthesis [17] and semantic texture transfer [3]. Meanwhile, Generative Adversarial Networks (GANs) [11] provide another idea for texture generation by using discriminator and generator networks, which iteratively improve the model by playing a minimax game. Its extension, the conditional GANs [22], fulfils the challenging task of generating images from abstract semantic labels. Li and Wand [18] further showed that their Markovian GANs has certain advantages over the Gram-matrix-based methods [10], [33] in coherent texture preservation. 

SECTION 3. Proposed Method: In this section, we first formulate our text effects transfer problem. Visual analytic is then presented on our observation of the high correlation between patch patterns (i.e. color and scale) and their spatial distributions in text effects images (Sec. 3.1). Based on this observation, we extract text effects statistics from the source images (Sec. 3.2) and employ it to adapt the texture synthesis algorithm for high-quality text effects transfer (Sec. 3.3).
Figure 2. Statistics of the text effects images. (a)(c) Flame and denim fabric text effects. (b)(d) Textures with similar distances to the text skeleton (in white) tend to have similar patterns. (e) Pixels are divided into N=16 classes using different partition modes. (f)-(g) High correlation between pixel colors and distances: Pixels are distinguished from each other by their distances in RGB space. (h)-(i) High correlation between patch scales and distances: Patches with similar distances have uniform responses to changes of their size. 
3.1. Problem Formulation and Analysis: Text effects transfer takes as input a set of three images, the source raw text image S, the source stylized image S′ and the target raw text image T, then automatically produces the target stylized image T′ with the text effects such that S:S′::T:T′. For our patch-based algorithm, in the following we use p and q to denote the pixels in T/T′ and S/S′, respectively, and use P(p) and P′(p) to represent the patches centered at p in T and T′, respectively. The same goes for patches Q(q) and Q′(q) in S and S′. To transfer arbitrary text effects automatically, we have to face a variety of text effects, the complex composition of text sub-effects and the simpleness of guidance maps. Thus, it is a quite challenging task. How to synthesize preferable text effects is still an open question. That is, (i) what is the essential factor to make text effects preferable? and (ii) how can we capture these characteristics for synthesis? Fortunately, after mathematically analyzing dozens of text effects created by designers, we find the clue: the high correlation between patch patterns (i.e. color and scale) and their distances to text skeletons. We note that textures with similar distances to the text skeleton tend to share similar patterns. It is schematically illustrated in Figs. 2(b)(d) where the patches with the same distance to the skeleton (in white) are marked by the same color. This observation is consistent with the texture adaptation based on text shapes for readability conducted by designers in the real world. To quantitatively verify this correlation, we divide pixels/patches in text effects images into N classes based on their distance to the text skeleton (distance calculation will be given in Sec. 3.2.2) and use the differentiation of N partitions as the measurement. Two examples of the distance-based partition are shown in the top row of Fig. 2(e), where we exploit different colors to denote each class. For the pixel color, taking flame image for example, by marking each point in RGB space (Fig. 2(f)) with its class-color, we note that the points in Fig. 2(g) with the same class-color appear in the neighborhood, which means pixel colors have strong correlation with their distance values. Thus, we quantify it as the classification accuracy,
corr(color, dist)=1−ϵ,(1)View Source\begin{equation*} corr (\text{color},\ \text{dist}) =1-\epsilon,\tag{1}
\end{equation*} where ϵ is the training error obtained by training SVM [4] to classify pixel colors given our distance-based partition. The mean correlation between pixel colors and the distance values on 30 text effects images created by designers is 0.147. We also provide other three partition modes (as shown in the bottom row of Fig. 2(e)) as well as the random partition mode for comparison in the experiments. Their mean correlations with pixel colors over 30 test images are shown in the second row of Table 1. As expected, the distance is the most important factor to depict pixel colors.
Table 1. Correlations between patch patterns and different modes.
For the patch scale, we enumerate patch sizes and calculate the difference between each patch and its best match, which forms a response curve at different scaling. Taking denim fabric image for example, Figs. 2(h) and (i) show N response curves of patch sizes in distance and random modes, respectively. Each point on the curve gives the mean and standard deviation of patch differences at a certain patch size in the corresponding class. We find that: (i) Response curves in distance mode are more diverse (high inter-curve standard deviations σinter), which means different sub-effects are well distinguished by their distance values. (ii) Points on response curves in distance mode have lower standard deviations (low intra-curve standard deviations σintra), which means patches with similar distances react uniformly to scale changing and possibly share common optimal scale for description. Considering these two aspects, we evaluate the correlation with the patch scale by,
corr(scale, dist)=σinter/σintra.(2)View Source\begin{equation*} corr (\text{scale},\ \text{dist}) =\sigma_{\text{inter}}/\sigma_{\text{intra}}. \tag{2}
\end{equation*} Here σinter and σintra are defined as sum of standard deviations σ with different patch sizes. Given a patch size, σ is standard deviation of mean patch differences of N points in different curves for σinter and is mean of standard ss of N points for σintra. The bottom row of Table 1 shows the mean correlations between the patch scale and different modes on 30 images where distance owns the highest value. Based on the results for pixel colors and patch scales, we obtain the conclusion that the high correlation between patch patterns and their distances is reasonable essential characteristics for high-quality text effects. 3.2. Text Effects Statistics Estimation: We now convert the aforementioned analysis into patch statistics that can be directly used as the transfer guidance. Specifically, we detect the optimal scales for source patches, and estimate their normalized distances to the text skeleton. Then we are able to derive the posterior probability of the optimal scale for each patch based on its spatial position. 3.2.1 Optimal Patch Scale DetectionInspired by [8], we propose a simple yet effective approach to detect the optimal patch scale scal(q) to depict texture patterns round q. Given a predefined downsample factor s, we start from the max (roughest) scale L to filter source patches and let the screened patches pass to a finer scale.
Figure 3. Detected optimal patch scales for the flame image. 
We use a fixed patch size of m×m and resize the image to accomplish multiple scales. Let Sℓ be the downsampled source S with a scale rate of 1/sℓ−1 and Qℓ(q) be the patch centered at q/sℓ−1 in Sℓ.S′ℓ and Q′ℓ(q) are similarly defined. If q^ is the correspondence of q at scale ℓ such that
q^=argmin∥Qℓ(q)−Qℓ(q^)∥2+∥Q′ℓ(q)−Q′ℓ(q^)∥2,(3)View Source\begin{equation*}
\hat{q}= \arg\min \Vert Q_{\ell}(q)-Q_{\ell}(\hat{q})\Vert^{2}+\Vert Q_{\ell}^{\prime}(q)-Q_{\ell}^{\prime}(\hat{q})\Vert^{2},\tag{3}
\end{equation*} then our filter criterion at scale ℓ is
ζℓ(q, q^)=(σℓ+dℓ(q, q^)−−−−−−−√>ω),(4)View Source\begin{equation*}
\zeta_{\ell}(q,\ \hat{q})=(\sigma_{\ell}+\sqrt{d_{\ell}(q,\ \hat{q})} > \omega),\tag{4}
\end{equation*} where σℓ=Var(Q′ℓ(q))−−−−−−−−−√/2. Patches that satisfy the filter criterion pass through to finer scale ℓ−1, while the filter residues set ℓ as their optimal scales. An example of the optimal scales for the flame image is shown in Fig. 3(a). It is found that the textured region near the character requires finer patch scales than the outer flat region. For better visualization, we show the optimal scale of the patch Q(q) by resizing it at a scale rate of sscal(q)−1 in Fig. 3(b). 3.2.2 Robust Normalized Distance EstimationHere we first define some concepts. In the text image, its text region is denoted by Ω. The skeleton skel (Ω) is a kernel path within Ω. We use dist(q, A) to denote the distance between q and its nearest pixel in set A. We are going to calculate dist(q, skel(Ω)). For q on the text contour δΩ, the distance is also known as the text width or radius r(q). Fig. 4(b) gives the visual interpretation. We extract skel(Ω) from S using morphology operations. To ensure the distance invariant to the text width, we aim to normalize the distance so that the normalized text width equals to 1. Simply dividing the distance by the text width is unreliable because the inaccurate of the obtained skel(Ω) leads to errors both in the numerator and denominator as well. To address this issue, we estimate corrected text width r~(q) based on statistics and use the accurate dist(q, δΩ) to derive normalized dist~(q, skel(Ω)). Specifically, we sort r(q), ∀q∈δΩ and obtain their rankings rank(q). We observe that the relation between r(q) and rank(q) can be well modelled by linear regression, as shown in Figs. 4(d). From Figs. 4(b)(d), we discover that outliers assemble at small values. We empirically assume the leftmost 20% points are outliers and eliminate them by
r~(q)=max(dist(q, skel(Ω)), 0.2k|δΩ|+b),(5)View Source\begin{equation*}
\tilde{r}(q)=\max(\text{dist} (q,\ \text{skel} (\Omega)),\ 0.2k\vert \delta\Omega\vert +b),\tag{5}
\end{equation*} where k, b are linear regression coefficients, |δΩ| is the pixel number of δΩ. Finally, the normalized distance is obtained,
dist~(q, skel(Ω))={1+ dist (q, δΩ)/r¯¯, if q∉Ω1−dist (q, δΩ)/r~(q⊥), other,(6)View Source\begin{equation*}
\tilde{\text{dist}} (q,\ \text{skel} (\Omega)) =\begin{cases}
1+\ \text{dist}\ (q,\ \delta\Omega)/\overline{r},\ \text{if}\ q\not\in\Omega\\
1- \text{dist}\ (q,\ \delta\Omega)/\tilde{r}(q_{\perp}),\ \text{other}
\end{cases},\tag{6}\end{equation*} where q⊥∈δΩ is the nearest pixel to q along δΩ and r¯¯=0.5k|δΩ|+b is the mean text width.
Figure 4. Robust normalized distance estimation. (a) The text image. (b) Our detected text skeleton and the notation definition. (c) The estimated normalized distance. The distance of the pixels on the text boundary to the text skeleton are normalized to 1 (colored by magenta). (d) The statistics of the text width. 
For simplicity, we omit skel(Ω) and use dist(q) to refer to dist~(q, skel(Ω)) in the following. 3.2.3 Optimal Scale Posterior Probability EstimationIn this section, we derive the posterior probability of the optimal patch scale to model the aforementioned high correlation between patch patterns and their spatial distributions. We uniformly quantify all distances into 100 bins and denote bin(q) as the bin q belongs to. Then, a 2-d histogram hist(ℓ, x) is computed:
hist(ℓ, x)=∑qψ(scal(q)=ℓ∧bin(q)=x),(7)View Source\begin{equation*} hist (\ell,\ x)=\sum_{q}\psi(\text{scal} (q)=\ell\wedge \text{bin} (q)=x),\tag{7}
\end{equation*} where ψ(⋅) is 1 when the argument is true and 0 otherwise. And the joint probability of the distance and the optimal scale can be estimated as,
P(ℓ, x)=hist(ℓ, x)/∑ℓ,xhist(ℓ, x).(8)View Source\begin{equation*}
\mathcal{P}(\ell,\ x)=hist(\ell,\ x)/\sum_{\ell, x}hist(\ell,\ x).\tag{8}
\end{equation*} Finally, the posterior probability P(ℓ|bin(q)) for ℓ being the appropriate scale to depict the patches with distances corresponding to bin(q) can be deduced:
P(ℓ|bin(p))=P(ℓ, bin(p))/∑ℓP(ℓ, bin(p)).(9)View Source\begin{equation*}
\mathcal{P}(\ell\vert \text{bin} (p))=\mathcal{P}(\ell,\ \text{bin} (p)) / \sum_{\ell}\mathcal{P}(\ell,\ \text{bin} (p)). \tag{9}
\end{equation*} We assume the target images share the same posterior probability with the source image. And we will use this probability to select patch scales statistically for texture synthesis to adapt extremely various text effects. 3.3. Text Effect Transfer: In this section, we describe how we adapt conventional texture synthesis method to dealing with the challenging text effects. We build on the texture synthesis method of Wexler et al. [35] and its variants [5] using random search and propagation as in PatchMatch [1], [2]. We refer to these papers for details of the base algorithm. We apply character shape constrains to the patch appearance measurement to build our baseline, and further incorporate estimated text effects statistics to accomplish adaptive multi-scale style transfer (Sec. 3.3.2). Then a distribution term is introduced to adjust the spatial distribution of the text sub-effects (Sec. 3.3.3). Finally, we propose a psycho-visual term that prevents texture over-repetitiveness for naturalness (Sec. 3.3.4). 3.3.1 Objective FunctionWe augment the texture synthesis objective function in [35] by including a distribution term and a psycho-visual term. And our objective function takes the following form,
minq∑pEapp(p, q)+λ1Edist(p, q)+λ2Epsy(p, q),(10)View Source\begin{equation*}
\min_{q}\sum_{p}E_{\text{app}}(p,\ q)+\lambda_{1}E_{\text{dist}}(p,\ q)+\lambda_{2}E_{\text{psy}}(p,\ q),\tag{10}
\end{equation*} where p is the center position of a target patch in T and T′,q is the center position of the corresponding source patch in S and S′. The three terms Eapp, Edist and Epsy are the appearance, distribution and psycho-visual terms, respectively, which are weighted by λ1 and λ2 to together make up the patch distance. 3.3.2 Appearance Term: Texture Style TransferThe original texture synthesis algorithm of Wexler et al. [35] minimizes the Sum of the Squared Differences (SS-D) of two patches sampled from texture image pair S′/T′. We adapt it to texture transfer tasks by applying additional SSD of two patches sampled from the text image pair S/T:
Eapp(p, q)=λ3∥P(p)−Q(q)∥2+∥P′(p)−Q′(q)∥2,(11)View Source\begin{equation*}
E_{\text{app}}(p,\ q)=\lambda_{3}\Vert P(p)-Q(q)\Vert^{2}+\Vert P^{\prime}(p)-Q^{\prime}(q)\Vert^{2},\tag{11}
\end{equation*} where λ3 is a weight that compromises between the color difference and character shape difference. We take the objective function that only minimizes the appearance term in Eq. (11) as our baseline. Stylized texts often contain multiple sub-effects with different optimal representation scales. Thus, in addition to the baseline, we propose the adaptive scale-aware patch distance by incorporating the estimated posterior probability,
Eapp(p, q)=λ3∑ℓP(ℓ|bin(p))∥Pℓ(p)−Qℓ(q)∥2+∑ℓP(ℓ|bin(p))∥P′ℓ(p)−Q′ℓ(q)∥2.(12)View Source\begin{align*}
E_{\text{app}}(p,\ q)& = \lambda_{3} \sum_{\ell}\mathcal{P}(\ell\vert \text{bin} (p))\Vert P_{\ell}(p)- Q_{\ell}(q)\Vert^{2}\tag{12}\\
&+ \sum_{\ell}\mathcal{P}(\ell\vert \text{bin} (p))\Vert P_{\ell}^{\prime}(p)- Q_{\ell}^{\prime}(q)\Vert^{2}.
\end{align*} The posterior probability helps to explore patches through multiple appropriate scales for better textures synthesis. 3.3.3 Distribution Term: Spatial Style TransferThe distribution of sub-effects highly correlates with their distances to the text skeleton. Based on this prior, we introduce a distribution term,
Edist(p, q)=(dist(p)−dist(q))2/max(1, dist2(p)),(13)View Source\begin{equation*}
E_{\text{dist}}(p,\ q)= (\text{dist} (p)- \text{dist} (q))^{2}/ \max(1,\ \text{dist}^{{2}} (p)), \tag{13}
\end{equation*} which encourages the text effects of the target to share similar distribution with the source image, thereby realizing a spatial style transfer. To ensure that the cost is invariant to the image scale, we add the denominator max(1, dist2(p)). 3.3.4 Psycho-Visual Term: Naturalness PreservationTexture over-repetitiveness can seriously reduce human subjective evaluation in the aesthetics. Therefore, we aim to penalize certain source patches to be selected repetitiously. Let Φ(q) be the set of pixels that currently find q as their correspondence and \vert \Phi(q)\vert be the size of the set. We define the psycho-visual term as,
\begin{equation*}
E_{\text{psy}}(p,\ q)=\vert \Phi(q)\vert. \tag{14}
\end{equation*}View Source\begin{equation*}
E_{\text{psy}}(p,\ q)=\vert \Phi(q)\vert. \tag{14}
\end{equation*} From the perspective of q, we can better understand this repetitiveness penalty:
\begin{equation*}
\sum_{p}\vert \Phi(q)\vert =\sum_{q}\sum_{p\in\Phi(q)}\vert \Phi(q)\vert =\sum_{q}\vert \Phi(q)\vert ^{2}. \tag{15}
\end{equation*}View Source\begin{equation*}
\sum_{p}\vert \Phi(q)\vert =\sum_{q}\sum_{p\in\Phi(q)}\vert \Phi(q)\vert =\sum_{q}\vert \Phi(q)\vert ^{2}. \tag{15}
\end{equation*} Since \sum_{q}\vert \Phi(q)\vert =\vert T\vert is constant, Eq. (15) reaches the minimum when all \vert \Phi(q)\vert equals. It means our psycho-visual term encourages source patches to be used evenly. 3.3.5 Function OptimizationWe follow the iterative coarse-to-fine matching and voting steps as in [35]. In the matching step, PatchMatch algorithm [1], [2] is adopted. We update \Phi(q) after each iteration of search and propagation for the psycho-visual term. Meanwhile, the initialization of T^{\prime} plays an important role in the final results, since our guidance map provides very few constraints on textures. We vote the source patches that are searched to only minimize Eq. (13) to form our initial guess of T^{\prime}. This simple strategy improves the final results significantly as shown in Fig. 6.
Figure 5. Effects of the multi-scale strategy. (a) Results using single-scale 5\times 5 patches. (b) Results using single-scale 15 \times 15 patches. (c) Results using joint 5 \times 5 patches over 5 scales. 
Figure 6. Effects of the distribution term. (a) Results without distribution term. (b) Results obtained by random initialization and optimization with distribution term. (c) Results obtained by both initialization and optimization with distribution term. 


SECTION 4. Analysis: Appearance Term: The advantages of the proposed appearance term lie in two aspects: (i) Preserve coarse grained texture structures. (ii) Preserve texture details. We show in Figs. 5(a) and (b) the denim fabric style generated using single-scale 5\times 5 and 15\times 15 patches, respectively. Small patches capture very limited contextual information, thus it cannot guarantee the structure continuity. As can be seen in Fig. 5(a), sewing threads look cracked and are not along the uniform directions. However, choosing large patches leads to smoothing out tiny thread residues as in Fig. 5(b). These problems are well solved by jointly using 5\times 5 patches over 5 scales as in Fig. 5(c), where the overall shape is well preserved and the details like sewing threads look more vivid. Distribution Term: The distribution term ensures the sub-effects in the target image and the source example are similarly distributed, which is the basis of our assumption in Sec. 3.2.3 that the posterior probabilities \mathcal{P}(\ell\vert x) in T^{\prime} and S^{\prime} are the same. Fig. 6 shows the effects of the distribution term on the flame style. Without distribution constraints, the flames appear randomly in the black background. The distribution term adjusts the flames to better match their spatial distribution as that in the source example. Psycho-Visual Term: The effects of our psycho-visual term are shown in Fig. 7. The lava textures synthesized without the psycho-visual penalty (Fig. 7(a)) densely repeat the red cracks in several regions, which causes obvious unnaturalness. By increasing the penalty, the reuse of the same source textures is greatly restrained (Fig. 7(b)) and our method tends to agilely combine different source patches to create brand-new textures (Fig. 7(c)). Thus, the psycho-visual term can effectively penalize texture over-repetitiveness and encourage new texture creation.
Figure 7. Effects of the psycho-visual term, which penalizes texture over-repetitiveness and encourages new texture creation. 
Figure 8. Comparison with state-of-the-art methods on various text effects. From top to bottom: Neon, smoke, denim fabric. (a) Input source text effects with their raw text counterparts in the lower-left corner. (b) Target text. (c) Results of image analogies [12]. (d) Results of split and match [8]. (e) Results of neural doodles [3]. (f) Results of our baseline method. (g) Results of the proposed method. 
Combination of the Three Term: It is worth noting that the proposed three terms are complementary: First, the appearance and distribution terms emphasize local texture patterns and global text sub-effects distributions, respectively. The former depicts low-level color features while the latter exploits complementary mid-level position features. Second, the appearance and distribution terms jointly evaluate objective patch similarities. Meanwhile, the psycho-visual term complements these two terms by incorporating aesthetic subjective evaluations. 

SECTION 5. Experimental Results: In the experiment, the patch size is 5 \times 5 and the max scale L=5. We build an image pyramid of 10 levels with a fixed coarsest size (32 \times 32). At level \ell, joint patches over scales from \ell to min(10,\ \ell+L-1) are used. The weights \lambda_{1},\ \lambda_{2} and \lambda_{3} to balance different terms are set to 0.01, 0.005 and 10, respectively. The parameter \omega for the filter criterion is 0.3 in our implementation1. In Fig. 8, we present a comparison of our algorithm with three state-of-the-art style transfer techniques as well as our baseline. The first method is the pioneering Image Analogies [12]. The textures in their results repeat locally and look disordered globally with evident patch boundaries. The second method is our implementation of Split and Match [8], which synthesizes textures using adaptive patch sizes. The original method directly transfers the style in S^{\prime} to T without the help of S. To make a fair comparison, we incorporate the guidance by using S instead of S^{\prime} in the split stage. This method fails to generate textures in the background and produces plain stylized results. The third method, Neural Doodle [3], is based on the combination of MRF and CNN [17] and incorporates semantic maps for analogy guidance. While the color palette of the example text effects is transferred, fine textures are poorly synthesized. The text shape is lost as well. Our baseline transfers fine textures but fails to keep the overall sub-effects distribution and generates artifacts in the background. By comparison, the proposed method outperforms state-of-the-art methods, preserving both local textures and the global sub-effects distribution. In Fig. 9, we present an illustration of style transfer from six very different text effects to three representative characters (Chinese, alphabetic, handwriting). This experiment covers challenging transformations between styles, languages and fonts. Thanks to distance normalization and multi-scale strategy, our algorithm accomplishes to transfer the text effects regardless of character shapes and texture scales, providing a solid tool for artistic typography.
Figure 9. Apply different text effects to representative characters (Chinese, alphabetic, handwriting). 
Figure 10. An overview of our flame typography library. The bigger image at the top left corner serves as the example to generate the other 774 characters. The whole library as well as the other stylized libraries can be found in the supplementary material. 
Finally, we show our flame typography library including as much as 775 frequently used Chinese characters. Due to the space limitation, only the first 32 of them are presented in Fig. 10. The whole library as well as the other typography libraries are included in our supplementary material. The extensive synthesis results demonstrate the robustness of our method to varied character shapes. 

SECTION 6. Conclusion: In this paper, we raise the text effects transfer problem and propose a novel statistics-based method to solve it. We convert the high correlation between the sub-effects patterns and their relative spatial distribution to the text skeletons into soft constraints for text effects generation. An objective function with three complementary terms is proposed to jointly consider the local multi-scale texture, global sub-effects distribution and visual naturalness. We validate the effectiveness and robustness of our method by comparisons with state-of-the-art style transfer algorithms and extensive artistic typography generations. Future work will concentrate on the composition of the stylized texts and the background photos.