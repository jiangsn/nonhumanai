Figure 1. Block-NeRF is a method that enables large-scale scene reconstruction by representing the environment using multiple compact NeRFs that each fit into memory. At inference time, Block-NeRF seamlessly combines renderings of the relevant NeRFs for the given area. In this example, we reconstruct the alamo square neighborhood in san francisco using data collected over 3 months. Block-NeRF can update individual blocks of the environment without retraining on the entire scene, as demonstrated by the construction on the right. Video results can be found on the project website waymo.com/research/block-nerf. 


SECTION 1. Introduction: Recent advancements in neural rendering such as Neural Radiance Fields [40] have enabled photo-realistic reconstruction and novel view synthesis given a set of posed camera images [3], [38], [44]. Earlier works tended to focus on small-scale and object-centric reconstruction. Though some methods now address scenes the size of a single room or building, these are generally still limited and do not naïvely scale up to city-scale environments. Applying these methods to large environments typically leads to significant artifacts and low visual fidelity due to limited model capacity. Reconstructing large-scale environments enables several important use-cases in domains such as autonomous driving [30], [43], [69] and aerial surveying [14], [33]. For example, a high-fidelity map of the operating domain can serve as a prior for robot navigation. Large-scale scene reconstructions can be used for closed-loop robotic simulations [13]. Autonomous driving systems are commonly evaluated by re-simulating previously encountered scenarios. Any deviation from the recorded encounter, however, may change the vehi-cle's trajectory, requiring high-fidelity novel view renderings along the altered path. Scene conditioned NeRFs can further augment simulation scenarios by changing environmental lighting conditions, such as camera exposure, weather, or time of day. Reconstructing such large-scale environments introduces additional challenges, including the presence of transient objects (cars and pedestrians), limitations in model capacity, along with memory and compute constraints. Furthermore, training data for such large environments is highly unlikely to be collected in a single capture under consistent conditions. Rather, data for different parts of the environment may need to be sourced from different data collection efforts, in-troducing variance in both scene geometry (e.g., construction work and parked cars), as well as appearance (e.g., weather conditions and time of day). We extend NeRF with appearance embeddings and learned pose refinement to address the environmental changes and pose errors in the collected data. We additionally add exposure conditioning to provide the ability to modify the exposure during inference. We refer to this modified model as a Block-NeRF. Scaling up the network capacity of Block-NeRF enables the ability to represent in-creasingly large scenes. However this approach comes with a number of limitations; rendering time scales with the size of the network, networks can no longer fit on a single compute device, and updating or expanding the environment requires retraining the entire network. To address these challenges, we propose dividing up large environments into individually trained Block-NeRFs, which are then rendered and combined dynamically at inference time. Modeling these Block-NeRFs independently allows for maximum flexibility, scales up to arbitrarily large en-vironments and provides the ability to update or introduce new regions in a piecewise manner without retraining the entire environment as demonstrated in Figure 1. To com-pute a target view, only a subset of the Block-NeRFs are rendered and then composited based on their geographic lo-cation compared to the camera. To allow for more seamless compositing, we propose an appearance matching technique which brings different Block-NeRFs into visual alignment by optimizing their appearance embeddings. 

SECTION 2. Related Work: 2.1. Large Scale 3D Reconstruction: Researchers have been developing and refining techniques for 3D reconstruction from large image collections for decades [1], [16], [31], [46], [56], [78], and much current work re-lies on mature and robust software implementations such as COLMAP to perform this task [54]. Nearly all of these reconstruction methods share a common pipeline: extract 2D im-age features (such as SIFT [37]), match these features across different images, and jointly optimize a set of 3D points and camera poses to be consistent with these matches (the well-explored problem of bundle adjustment [23], [64]). Extending this pipeline to city-scale data is largely a matter of imple-menting highly robust and parallelized versions of these algorithms, as explored in work such as Photo Tourism [56] and Building Rome in a Day [1]. Core graphics research has also explored breaking up scenes for fast high quality rendering [36].
Figure 2. The scene is split into multiple Block-NeRFs that are each trained on data within some radius (dotted orange line) of a specific Block-NeRF origin coordinate (orange dot). To render a target view in the scene, the visibility maps are computed for all of the NeRFs within a given radius. Block-NeRFs with low visibility are discarded (bottom Block-NeRF) and the color output is rendered for the remaining blocks. The renderings are then merged based on each block origin's distance to the target view. 
These approaches typically output a camera pose for each input image and a sparse 3D point cloud. To get a complete 3D scene model, these outputs must be further processed by a dense multi-view stereo algorithm (e.g., PMVS [18]) to produce a dense point cloud or triangle mesh. This process presents its own scaling difficulties [17]. The resulting 3D models often contain artifacts or holes in areas with limited texture or specular reflections as they are challenging to triangulate across images. As such, they frequently require further postprocessing to create models that can be used to render convincing imagery [55]. However, this task is mainly the domain of novel view synthesis, and 3D reconstruction techniques primarily focus on geometric accuracy. In contrast, our approach does not rely on large-scale SfM to produce camera poses, instead performing odome-try using various sensors on the vehicle as the images are collected [63]. 2.2. Novel View Synthesis: Given a set of input images of a given scene and their camera poses, novel view synthesis seeks to render observed scene content from previously unobserved viewpoints, al-lowing a user to navigate through a recreated environment with high visual fidelity. Geometry-based Image Reprojection. Many approaches to view synthesis start by applying traditional 3D reconstruction techniques to build a point cloud or triangle mesh representing the scene. This geometric “proxy” is then used to reproject pixels from the input images into new camera views, where they are blended by heuristic [6] or learning-based methods [24], [51], [52]. This approach has been scaled to long trajectories of first-person video [29], panoramas collected along a city street [28], and single landmarks from the Photo Tourism dataset [39]. Methods reliant on geometry proxies are limited by the quality of the initial 3D reconstruction, which hurts their performance in scenes with complex geometry or reflectance effects. Volumetric Scene Representations. Recent view synthe-sis work has focused on unifying reconstruction and rendering and learning this pipeline end-to-end, typically using a volumetric scene representation. Methods for rendering small baseline view interpolation often use feed-forward networks to learn a mapping directly from input images to an output volume [15], [77], while methods such as Neural Volumes [35] that target larger-baseline view synthesis run a global optimization over all input images to reconstruct every new scene, similar to traditional bundle adjustment. Neural Radiance Fields (NeRF) [40] combines this single-scene optimization setting with a neural scene representation capable of representing complex scenes much more effi-ciently than a discrete 3D voxel grid; however, its rendering model scales very poorly to large-scale scenes in terms of compute. Followup work has proposed making NeRF more efficient by partitioning space into smaller regions, each containing its own lightweight NeRF network [42], [47], [48]. Unlike our method, these network ensembles must be trained jointly, limiting their flexibility. Another approach is to pro-vide extra capacity in the form of a coarse 3D grid of latent codes [34]. This approach has also been applied to compress detailed 3D shapes into neural signed distance functions [61] and to represent large scenes using occupancy networks [45]. Concurrent works Mega-NeRF [65] and CityNeRF [67] utilize NeRFs to represent large scenes. Mega-NeRF splits data captured from drones into multiple partitions to train specialized NeRFs. CityNeRF learns a multi-scale represen-tation from satellite imagery. We build our Block-NeRF implementation on top of mip-NeRF [3], which improves aliasing issues that hurt NeRF's performance in scenes where the input images observe the scene from many different distances. We incorporate techniques from NeRF in the Wild (NeRF-W) [38], which adds a latent code per training image to handle inconsistent scene appearance when applying NeRF to landmarks from the Photo Tourism dataset. NeRF-W creates a separate NeRF for each landmark from thousands of images, whereas our approach combines many NeRFs to reconstruct a coherent large environment from millions of images. Our model also incorporates a learned camera pose refinement which has been explored in previous works [32], [58], [66], [70], [71]. Some NeRF-based methods use segmentation data to isolate and reconstruct static [68] or moving objects (such as people or cars) [43], [74] across video sequences. As we focus primarily on reconstructing the environment itself, we choose to simply mask out dynamic objects during training.
Figure 3. Our model is an extension of the model presented in mip-NeRF [3]. The first MLP f_{\sigma}fσ predicts the density \sigmaσ for a position x in space. The network also outputs a feature vector that is concatenated with viewing direction d, the exposure level, and an appearance embedding. These are fed into a second MLP f_{c}fc that outputs the color for the point. We additionally train a visibility network f_{v}fv to predict whether a point in space was visible in the training views, which is used for culling block-nerfs during inference.  2.3. Urban Scene Camera Simulation: Camera simulation has become a popular data source for training and validating autonomous driving systems on interactive platforms [2], [27]. Early works [13], [19], [50], [53] syn-thesized data from scripted scenarios and manually created 3D assets. These methods suffered from domain mismatch and limited scene-level diversity. Several recent works tackle the simulation-to-reality gaps by minimizing the distribution shifts in the simulation and rendering pipeline. Kar et al. [26] and Devaranjan et al. [12] proposed to minimize the scene-level distribution shift from rendered outputs to real camera sensor data through a learned scenario generation frame-work. Richter et al. [49] leveraged intermediate rendering buffers in the graphics pipeline to improve photorealism of synthetically generated camera images. Towards the goal of building photo-realistic and scalable camera simulation, prior methods [9], [30], [69] leverage rich multi-sensor driving data collected during a single drive to reconstruct 3D scenes for object injection [9] and novel view synthesis [69] using modern machine learning techniques, in-cluding image GANs for 2D neural rendering. Relying on a sophisticated surfel reconstruction pipeline, SurfelGAN [69] is still susceptible to errors in graphical reconstruction and can suffer from the limited range and vertical field-of-view of LiDAR scans. In contrast to existing efforts, our work tackles the 3D rendering problem and is capable of modeling the real camera data captured from multiple drives under varying environmental conditions, such as weather and time of day, which is a prerequisite for reconstructing large-scale areas. 

SECTION 3. Background: We build upon NeRF [40] and its extension mip-NeRF [3]. Here, we summarize relevant parts of these methods. For details, please refer to the original papers. 3.1. NeRF and mip-NeRF Preliminaries: Neural Radiance Fields (NeRF) [40] is a coordinate-based neural scene representation that is optimized through a dif-ferentiable rendering loss to reproduce the appearance of a set of input images from known camera poses. After opti-mization, the NeRF model can be used to render previously unseen viewpoints. The NeRF scene representation is a pair of multilayer perceptrons (MLPs). The first MLP f_{\sigma}fσ takes in a 3D position \mathbf{x}x and outputs volume density \sigmaσ and a feature vector. This feature vector is concatenated with a 2D viewing direction \mathbf{d}d and fed into the second MLP f_{c}fc, which outputs an RGB color \mathbf{c}c. This architecture ensures that the output color can vary when observed from different angles, allowing NeRF to represent reflections and glossy materials, but that the underlying geometry represented by \sigmaσ is only a function of position. Each pixel in an image corresponds to a ray \mathbf{r}(t)=\mathbf{o}+ t\mathbf{d}r(t)=o+td through 3D space. To calculate the color of \mathbf{r}r, NeRF randomly samples distances \{t_{i}\}_{i=0}^{N}{ti}Ni=0 along the ray and passes the points \mathbf{r}(t_{i})r(ti) and direction \mathbf{d}d through its MLPs to calculate \sigma_{i}σi and \mathbf{c}_{i}ci. The resulting output color is
\begin{align*}\mathbf{c}_{\text{out}} &=\sum_{i=1}^N w_i \mathbf{c}_i, \quad \text { where } w_i=T_i\left(1-e^{-\Delta_i \sigma_i}\right),\tag{1}\\
T_i &=\exp \left(-\sum_{j< i} \Delta_j \sigma_j\right), \quad \Delta_i=t_i-t_{i-1}.\tag{2}\end{align*}coutTi=∑i=1Nwici, where wi=Ti(1−e−Δiσi),=exp(−∑j<iΔjσj),Δi=ti−ti−1.(1)(2)View Source\begin{align*}\mathbf{c}_{\text{out}} &=\sum_{i=1}^N w_i \mathbf{c}_i, \quad \text { where } w_i=T_i\left(1-e^{-\Delta_i \sigma_i}\right),\tag{1}\\
T_i &=\exp \left(-\sum_{j< i} \Delta_j \sigma_j\right), \quad \Delta_i=t_i-t_{i-1}.\tag{2}\end{align*} The full implementation of NeRF iteratively resamples the points t_{i}ti (by treating the weights w_{i}wi as a probability distribution) in order to better concentrate samples in areas of high density. To enable the NeRF MLPs to represent higher frequency detail [62], the inputs x and \mathbf{d}d are each preprocessed by a componentwise sinusoidal positional encoding \gamma_{\text{PF}}γPF:
\begin{equation*}\gamma_{\text{PE}}(z)=\left[\sin \left(2^0 z\right), \cos \left(2^0 z\right), \ldots, \sin \left(2^{L-1} z\right), \cos \left(2^{L-1} z\right)\right]\tag{3}\end{equation*}γPE(z)=[sin(20z),cos(20z),…,sin(2L−1z),cos(2L−1z)](3)View Source\begin{equation*}\gamma_{\text{PE}}(z)=\left[\sin \left(2^0 z\right), \cos \left(2^0 z\right), \ldots, \sin \left(2^{L-1} z\right), \cos \left(2^{L-1} z\right)\right]\tag{3}\end{equation*}
where LL is the number of levels of positional encoding. NeRF's MLP f_{\sigma}fσ takes a single 3D point as input. How-ever, this ignores both the relative footprint of the corre-sponding image pixel and the length of the interval [t_{i-1},\ t_{i}][ti−1, ti] along the ray \mathbf{r}r containing the point, resulting in aliasing artifacts when rendering novel camera trajectories. Mip-NeRF [3] remedies this issue by using the projected pixel footprint to sample conical frustums along the ray rather than intervals. To feed these frustums into the MLP, mip-NeRF approximates each of them as Gaussian distributions with parameters \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i} and replaces the positional encoding \gamma_{\text{PE}} with its expectation over the input Gaussian
\begin{equation*}\gamma_{\operatorname{IPE}}(\boldsymbol{\mu}, \boldsymbol{\Sigma})=\mathbb{E}_{\boldsymbol{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})}\left[\gamma_{\text{PE}}(\boldsymbol{X})\right],\tag{4}\end{equation*}View Source\begin{equation*}\gamma_{\operatorname{IPE}}(\boldsymbol{\mu}, \boldsymbol{\Sigma})=\mathbb{E}_{\boldsymbol{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})}\left[\gamma_{\text{PE}}(\boldsymbol{X})\right],\tag{4}\end{equation*}
referred to as an integrated positional encoding. 

SECTION 4. Method: Training a single NeRF does not scale when trying to represent scenes as large as cities. We instead propose split-ting the environment into a set of Block-NeRFs that can be independently trained in parallel and composited during inference. This independence enables the ability to expand the environment with additional Block-NeRFs or update blocks without retraining the entire environment (see Figure 1). We dynamically select relevant Block-NeRFs for rendering, which are then composited in a smooth manner when traversing the scene. To aid with this compositing, we optimize the appearances codes to match lighting conditions and use interpolation weights computed based on each Block-NeRF's distance to the novel view. 4.1. Block Size and Placement: The individual Block-NeRFs should be arranged such that they collectively achieve full coverage of the target en-vironment. We typically place one Block-NeRF at each intersection, covering the intersection itself and any con-nected street 75% of the way until it converges into the next intersection (see Figure 1). This results in a 50% overlap between any two adjacent blocks on the connecting street segment, making appearance alignment easier between them. We make sure to train each Block-NeRF on data that is con-fined to a geographic area. This can be automated and only relies on basic map data, such as OpenStreetMap [22]. Other placement heuristics are conceivable. For example, for some of our experiments, we place Block-NeRFs along a street segment at uniform distances and define the block size to be a sphere around the origin of the blocks (see Figure 2). 4.2. Training Individual Block-NeRFs: 4.2.1 Appearance EmbeddingsGiven that different parts of our data may be captured under different environmental conditions, we follow NeRF-W [38] and use Generative Latent Optimization [5] to optimize per-image appearance embedding vectors, as shown in Figure. This allows the NeRF to explain away several appearance-changing conditions, such as varying weather and lighting. We can additionally manipulate these appearance embed-dings to interpolate between different conditions observed in the training data (such as cloudy versus clear skies, or day and night). Examples of rendering with different appear-ances can be seen in Figure 4. In § 4.3.3, we use test-time optimization over these embeddings to match the appear-ance of adjacent Block-NeRFs, which is important when combining multiple renderings.
Figure 4. The appearance codes allow the model to represent different lighting and weather conditions.  4.2.2 Learned Pose RefinementAlthough we assume that camera poses are provided, we find it advantageous to learn regularized pose offsets for further alignment. Pose refinement has been explored in previous NeRF based models [32], [58], [66], [71]. These offsets are learned per driving segment and include both a translation and a rotation component. We optimize these offsets jointly with the NeRF itself, significantly regularizing the offsets in the early phase of training to allow the network to first learn a rough structure prior to modifying the poses. 4.2.3 Exposure InputTraining images may be captured across a wide range of exposure levels, which can impact NeRF training if left unaccounted for. We find that feeding the camera exposure information to the appearance prediction part of the model allows the NeRF to compensate for the visual differences (see Figure 3). Specifically, the exposure information is processed as \gamma_{\text{PE}}(\text { shutter speed } \times \text { analog gain } / t) where \gamma_{\text{PE}} is a sinusoidal positional encoding with 4 levels, and t is a scaling factor (we use 1,000 in practice). An example of different learned exposures can be found in Figure 5. 4.2.4 Transient ObjectsWhile the appearance embeddings account for variation in appearance, we assume that the scene geometry is consis-tent across the training data. Movable objects (e.g. cars, pedestrians) typically violate this assumption. We therefore use a semantic segmentation model [10] to ignore masks of common movable objects during training. Note that this does not account for changes in otherwise static parts of the environment, e.g. construction, it accommodates most common types of geometric inconsistency. 4.2.5 Visibility PredictionWhen merging multiple Block-NeRFs, it can be useful to know whether a specific region of space was visible to a given NeRF during training. We extend our model with an additional small MLP f_{v} that is trained to learn an approx-imation of the visibility of a sampled point (see Figure 3). For each sample along a training ray, f_{v} takes in the lo-cation and view direction and regresses the corresponding transmittance of the point (T_{i} in Equation 2). The model is trained alongside f_{\sigma}, which provides supervision. Trans-mittance represents how visible a point is from a particular input camera: points in free space or on the surface of the first intersected object will have transmittance near 1, and points inside or behind the first visible object will have trans-mittance near 0. If a point is seen from some viewpoints but not others, the regressed transmittance value will be the average over all training cameras and lie between zero and one, indicating that the point is partially observed. Our visi-bility prediction is similar to the visibility fields proposed by Srinivasan et al. [57]. However, they used an MLP to predict visibility to environment lighting to recover a relightable NeRF model, while we predict visibility to training rays. The visibility network is small and can be run indepen-dently from the color and density networks. This proves useful when merging multiple NeRFs, since it can help to determine whether a specific NeRF is likely to produce meaningful outputs for a given location, as explained in § 4.3.1. The visibility predictions can also be used to determine locations to perform appearance matching between two NeRFs, as detailed in § 4.3.3. 4.3. Merging Multiple Block-NeRFs: 4.3.1 Block-NeRF SelectionThe environment can be composed of an arbitrary number of Block-NeRFs. For efficiency, we utilize two filtering mechanisms to only render relevant blocks for the given target viewpoint. We only consider Block-NeRFs that are within a set radius of the target viewpoint. Additionally, for each of these candidates, we compute the associated visibility. If the mean visibility is below a threshold, we discard the Block-NeRF. An example of visibility filtering is provided in Figure 2. Visibility can be computed quickly because its network is independent of the color network, and it does not need to be rendered at the target image resolution. After filtering, there are typically one to three Block-NeRFs left to merge.
Figure 5. Our model is conditioned on exposure, which helps account for exposure changes present in the training data. This allows users to alter the appearance of the output images in a human-interpretable manner during inference.  4.3.2 Block-NeRF CompositingWe render color images from each of the filtered Block-NeRFs and interpolate between them using inverse distance weighting between the camera origin c and the centers x_{{i}} of each Block-NeRF. Specifically, we calculate the respective weights as w_{i}\propto distance (c,\ x_{i})^{-p}, where p influences the rate of blending between Block-NeRF renders. The inter-polation is done in 2D image space and produces smooth transitions between Block-NeRFs. We also explore other interpolation methods in § 5.4. 4.3.3 Appearance MatchingWe can control the appearance of our learned models by an appearance latent code after the Block-NeRF has been trained. These codes are randomly initialized during training and the same code therefore typically leads to different appearances when fed into different Block-NeRFs. This is undesirable when compositing as it may lead to inconsis-tencies between views. Given a target appearance in one of the Block-NeRFs, we match its appearance in the remaining blocks. To this end, we first select a 3D matching location between pairs of adjacent Block-NeRFs. The visibility pre-diction at this location should be high for both Block-NeRFs. Given the matching location, we freeze the Block-NeRF net-work weights and only optimize the appearance code of the target in order to reduce the \ell_{2} loss between the respective area renders. This optimization is quick, converging within 100 iterations. While not necessarily yielding perfect align-ment, this procedure aligns most global and low-frequency attributes of the scene, such as time of day, color balance, and weather, which is a prerequisite for successful compositing. Figure 6 shows an example optimization, where appearance matching turns a daytime scene into nighttime to match the adjacent Block-NeRF. Starting from a root Block-NeRF, we propagate the optimized appearance through the scene by iteratively optimizing the appearance of its neighbors. If mul-tiple blocks surrounding a target Block-NeRF have already been optimized, we consider each of them when computing the loss. 

SECTION 5. Results and Experiments: In this section we will discuss our datasets and exper-iments. We provide the architectural and optimization specifics in the supplement. The supplement also provides comparisons to reconstructions from COLMAP [54], a traditional Structure from Motion approach. This reconstruction is sparse and fails to represent reflective surfaces and the sky. 5.1. Datasets: We perform experiments on datasets that we collected for novel view synthesis of large-scale scenes using data collection vehicles driving on public roads. Existing public large-scale driving datasets are not designed for the task of view synthesis. For example, some datasets lack sufficient camera coverage (e.g., KITTI [21], Cityscapes [11]) or pri-oritize visual diversity over repeated observations of a target area (e.g., NuScenes [7], Waymo Open Dataset [60], Argov-erse [8]). Instead, these datasets are typically designed for tasks such as object detection and tracking. Our dataset includes both long-term sequence data (100 s or more) and distinct sequences captured repeatedly in a particular target area over a period of several months. We use image data captured by 12 cameras, where 8 cameras mounted on the roof of the car provide a 360^{\circ} surround view, and 4 cameras located at the front of the vehicle point forward and sideways. Each camera captures images at 10 Hz and stores a scalar exposure value. The vehicle pose is known and all cameras are calibrated. We calculate the corresponding camera ray origins and directions in a com-mon coordinate system, accounting for the rolling shutter of the cameras. As described in § 4.2.4, we use a semantic segmentation model [10] to detect movable objects. San Francisco Alamo Square Dataset. We select San Francisco's Alamo Square neighborhood as the target area for our scalability experiments. The dataset spans an area of approximately 960 \mathrm{m}\times 570\mathrm{m}, and was recorded in June, July, and August of 2021. We divide this dataset into 35 Block-NeRFs. Example renderings and Block-NeRF place-ments can be seen in Figure 1. To best appreciate the scale of the reconstruction, please refer to supplementary videos. Each Block-NeRF was trained on data from 38 to 48 differ-ent data collection runs, adding up to a total driving time of 18 to 28 minutes each. After filtering out some redundant image captures (e.g. stationary captures), each Block-NeRF is trained on between 64,575 to 108,216 images. The over-all dataset is composed of 13.4 h of driving time sourced from 1,330 different data collection runs, with a total of 2,818,745 training images.
Figure 6. When rendering scenes based on multiple Block-NeRFs, we use appearance matching to obtain a consistent appearance across the scene. Given a fixed target appearance for one of the Block-NeRFs (left image), we optimize the appearances of the adjacent Block-NeRFs to match. In this example, appearance matching produces a consistent night appearance across Block-NeRFs. 
San Francisco Mission Bay Dataset. We choose San Francisco's Mission Bay District as the target area for our baseline, block size, and placement experiments. Mission Bay is an urban environment with challenging geometry and reflective facades. We identified a long stretch on Third Street with far-range visibility, making it an interesting test case. Notably, this dataset was recorded in a single capture in November 2020, with consistent environmental conditions al-lowing for simple evaluation. This dataset was recorded over 100 s, in which the data collection vehicle traveled 1.08 km and captured 12,000 total images from 12 cameras. We will release this single-capture dataset to aid reproducibility. 5.2. Model Ablations:  Table 1. Ablations of different Block-NeRF components on a single intersection in the alamo square dataset. We show the performance of mip-NeRF as a baseline, as well as the effect of removing individual components from our method
We ablate our model modifications on a single intersection from the Alamo Square dataset. We report PSNR, SSIM, and LPIPS [76] metrics for the test image reconstructions in Table 1. The test images are split in half vertically, with the appearance embeddings being optimized on one half and tested on the other. We also provide qualitative examples in Figure 7. Mip-NeRF alone fails to properly reconstruct the scene and is prone to adding non-existent geometry and cloudy artifacts to explain the differences in appearance. When our method is not trained with appearance embed-dings, these artifacts are still present. If our method is not trained with pose optimization, the resulting scene is blurrier and can contain duplicated objects due to pose misalignment. Finally, the exposure input marginally improves the reconstruction, but more importantly provides us with the ability to change the exposure during inference. 5.3. Block-NeRF Size and Placement:  Table 2. Comparison of different numbers of Block-NeRFs for reconstructing the mission bay dataset. Splitting the scene into multiple Block-NeRFs improves the reconstruction accuracy, even when holding the total number of weights constant (bottom section). The number of blocks determines the size of the area each block is trained on and the relative compute expense at inference time
We compare performance on our Mission Bay dataset versus the number of Block-NeRFs used. We show details in Table 2, where depending on granularity, the Block-NeRF sizes range from as small as 54 m to as large as 544 m. We ensure that each pair of adjacent blocks overlaps by 50% and compare other overlap percentages in the supplement. All were evaluated on the same set of held-out test images spanning the entire trajectory. We consider two regimes, one where each Block-NeRF contains the same number of weights (top section) and one where the total number of weights across all Block-NeRFs is fixed (bottom section). In both cases, we observe that increasing the number of models improves the reconstruction metrics. In terms of computational expense, parallelization during training is trivial as each model can be optimized independently across devices. At inference, our method only requires rendering Block-NeRFs near the target view. Depending on the scene and NeRF layout, we typically render between one to three NeRFs. We report the relative compute expense in each setting without assuming any parallelization, which would also be possible and lead to an additional speed-up. We find that splitting the scene into multiple lower capacity models can reduce the overall computational cost as not all of the models need to be evaluated (see bottom section of Table 2).
Figure 7. Model ablation results on multi segment data. Appearance embeddings help the network avoid adding cloudy geometry to explain away changes in the environment like weather and lighting. Removing exposure slightly decreases the accuracy. The pose optimization helps sharpen the results and removes ghosting from repeated objects, as observed with the telephone pole in the first row.  5.4. Interpolation Methods:  Table 3. Comparison of interpolation methods. For our flythrough video results, we opt for 2D inverse distance weighting (IDW) as it produces temporally consistent results
We explore interpolation techniques in Table 3. The sim-ple method of only rendering the nearest Block-NeRF to the camera requires the least amount of compute but results in harsh jumps when transitioning between blocks. These transitions can be smoothed by using inverse distance weighting (IDW) between the camera and Block-NeRF centers, as described in § 4.3.2. We also explored a variant of IDW where the interpolation was performed over projected 3D points predicted by the expected Block-NeRF depth. This method suffers when the depth prediction is incorrect, leading to artifacts and temporal incoherence. Finally, we experiment with weighing the Block-NeRFs based on per-pixel and per-image predicted visibility. This produces sharper reconstructions of further-away areas but is prone to temporal inconsistency. Therefore, these methods are best used only when rendering still images. We provide further details in the supplement. 

SECTION 6. Limitations and Future Work: The proposed method filters out transient objects during training via masking using a segmentation model. Objects that are not properly masked can cause artifacts, such as remaining shadows of cars that have been removed from the scene. Temporal inconsistencies in the training data, such as changing vegetation or temporary construction work, break our assumptions and may result in blurred renderings. The inability to handle dynamic objects currently limits ap-plications to closed-loop robotic simulation. These issues could be addressed by learning transient objects [38] or di-rectly modeling dynamic objects [43], [68]. Our model does not sample distant objects with the same density as nearby objects. This issue with sampling unbounded volumetric representations can lead to blurrier reconstructions. Techniques proposed in \text{NeRF}++ [75] and Mip-NeRF 360 [4] could potentially be used to produce sharper renderings of distant objects. In many applications, real-time rendering is key. NeRFs, however, are computationally expensive to render. NeRF caching techniques [20], [25], [73] or sparse voxel grids [34] could enable real-time Block-NeRF rendering. Recent work has demonstrated techniques to speed up NeRF training by multiple orders of magnitude [41], [59], [72]. 

SECTION 7. Conclusion: We propose Block-NeRF, a method that reconstructs arbitrarily large environments using NeRFs. We demonstrate the efficacy of the method by building an entire neighborhood in San Francisco from 2.8M images, forming the largest neu-ral scene representation to date. We accomplish this scale by splitting our representation into multiple blocks that can be optimized independently. At such a scale, the data col-lected will necessarily have transient objects and variations in appearance, which we account for by modifying the underlying NeRF architecture. We hope that this can inspire future work in large-scale scene reconstruction using modern neural rendering methods.