Figure 3. Our model is an extension of the model presented in mip-NeRF [3]. The first MLP f_{\sigma}fσ predicts the density \sigmaσ for a position x in space. The network also outputs a feature vector that is concatenated with viewing direction d, the exposure level, and an appearance embedding. These are fed into a second MLP f_{c}fc that outputs the color for the point. We additionally train a visibility network f_{v}fv to predict whether a point in space was visible in the training views, which is used for culling block-nerfs during inference.