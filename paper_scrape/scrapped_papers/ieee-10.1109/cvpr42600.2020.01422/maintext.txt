Figure 1: Seeing through obstructions. We present a learning-based method for recovering clean images from a given short sequence of images taken by a moving camera through obstructing elements such as (a) windows, (b) fence, or (c) raindrop. 


SECTION 1. Introduction: Taking clean photographs through reflective surfaces (such as windows) or occluding elements (such as fences) is challenging as the captured images inevitably contain both the scenes of interests and the obstructions caused by reflections or occlusions. An effective solution to recover the underlying clean image is thus of great interest for improving the quality of the images captured under such conditions or allowing computers to form a correct physical interpretation of the scene, e.g., enabling a robot to navigate in a scene with windows safely. Recent efforts have been focused on automatically removing unwanted reflections or occlusions from a single image [2], [8], [16], [17], [27], [38], [43], [45]. These methods either leverage the ghosting cues [30] or adopt learning-based approaches to capture the prior of natural images [8], [16], [38], [43], [45]. While impressive results have been shown, separating the clean background from reflection/occlusions is fundamentally ill-posed and often requires a high-level semantic understanding of the scene to perform well. In particular, the performance of learning-based methods degrades significantly for out-of-distribution images. To tackle these challenges, multi-frame approaches have been proposed for reflection/occlusion removal. The core idea is to exploit the fact that the background scene and the occluding elements are located at different depths with respect to the camera (e.g., virtual depth of window reflections). Consequently, taking multiple images from a slightly moving camera reveals the motion differences between the two layers [3], [9], [12], [21], [24], [34]. A number of approaches exploit such cues for reflection or fence removal from a video [1], [3], [6], [9], [12], [21], [24], [26], [3]â€“1, [3]4. Xue et al. [42] propose a unified computational framework for obstruction removal and show impressive results on several natural sequences. The formulation, however, requires a computationally expensive optimization process and relies on strict assumptions of brightness constancy or accurate motion estimation. To alleviate these issues, recent work [1] explores model-free methods by using a generic 3D convolutional neural network (CNN). Yet, the CNN-based methods do not produce results with comparable quality as optimization-based algorithms on real input sequences. In this work, we propose a multi-frame obstruction removal algorithm that exploits the advantages of both optimization-based and learning-based methods. Inspired by the optimization-based approach [42], the proposed algorithm alternates between the dense motion estimation and the background/obstruction layer reconstruction steps in a coarse-to-fine manner. The explicit modeling of dense motion allows us to progressively recover detailed content in the respective layers. Instead of relying on hand-crafted objectives for solving the layers, we exploit the learning-based method for fusing flow-warped images to accommodate potential violations of brightness constancy and errors in flow estimation. We train our fusion network using a synthetically generated dataset and demonstrate it transfers well to unseen real-world sequences. In addition, we present an online optimization process to further improve the visual quality of particular testing sequences. Finally, we demonstrate that the proposed method performs favorably against existing algorithms on a wide variety of challenging sequences and applications. Our framework builds upon the optimization-based formulation of [26], [42] but differs in that our model is purely data-driven and does not rely on classical assumptions such as brightness constancy [26], [42], accurate flow fields [21], or planar surface [12] in the scene. When these assumptions are violated (e.g., occlusion/dis-occlusion, motion blur, inaccurate flow), classical approaches may fail to reconstruct clear foreground and background layers. On the other hand, data-driven approaches learn from diverse training data and can tolerate errors when these assumptions are violated. The contributions of this work include:
We present a learning-based method that integrates the optimization-based formulation for robustly reconstructing background/obstruction layers. We demonstrate that combining model pre-training using synthetically generated data and fine-tuning with real testing sequence (in an unsupervised manner) leads to state-of-the-art performance. We show our model with minimum design changes can be applied to various obstruction removal problems.  

SECTION 2. Related Work: Multi-Frame Reflection Removal: Existing methods often exploit the differences of motion patterns between the background and reflection layers [12], [42] and impose natural image priors [10], [12], [42]. These methods differ in their way of modeling the motion fields, e.g., SIFT flow [21], homography [12], and dense optical flow [42]. Recent advances include optimizing temporal coherence [26] and learning-based layer decomposition [1]. Compared to learning a generic CNN [1], our method explicitly models the dense flow fields of the background and obstruction layers to obtain sharper and cleaner results on real sequences. Single-Image Reflection Removal: A number of approaches have been proposed to remove unwanted reflections with only one single image as input. Existing methods exploit various cues, including ghosting effect [30], blurriness caused by depth-of-field [22], [36], image priors (either hand-designed [2] or learned from data [43], [45]), and the defocus-disparity cues from dual pixel sensors [28]. Despite the demonstrated success, reflection removal from a single image remains challenging due to the nature of this highly ill-posed problem and the lack of motion cues. Our work instead utilizes the motion cues from image sequences captured with a slightly moving camera for separating the background and reflection layers. Occlusion and Fence Removal: Occlusion removal aims to eliminate the captured obstructions, e.g., fence or raindrops on an image or sequences, and provide a clear view of the scene. Existing methods detect fence patterns by exploiting visual parallax [25], dense flow field [42], disparity maps [18], or using a graph-cut [44]. One recent work leverages a CNN for fence segmentation [6] and recovers the occluded pixels using optical flow. Our method also learns deep CNNs for optical flow estimation and background image reconstruction. Instead of focusing on fence removal, our formulation is more general and applicable to different obstruction removal tasks. Video Completion: Video completion aims to fill in plausi- ble content in missing regions of a video [14], with applications ranging from object removal, full-frame video stabilization, and watermark/transcript removal. State-of-the-art methods estimate the flow fields in both known and missing regions to constrain the content synthesis [13], [40], and generate temporally coherent results. The obstruction removal problem resembles a video completion task. However, the crucial difference is that no manual mask selection is required for removing the fences/obstructions from videos. Layer Decomposition: Image layer decomposition is a long-standing problem in computer vision, e.g., intrinsic image [4], [46], depth, normal estimation [15], relighting [7], and inverse rendering [23], [29]. Our method is inspired by the development of the approaches for these layer decomposition, particularly in the ways of leveraging both the physical image formation constraints and data-driven priors. Online Optimization: Learning from the test data has been an effective way to reduce the domain discrepancy between the training/testing distributions. Examples include using geometric constraints [5], self-supervised losses [33], and online template update [19]. Similar to these methods, we apply online optimization to fine-tune our back-ground/obstruction reconstruction network on a particular test sequence to further improve the separation. Our unsupervised loss directly measures how well the recovered background/obstruction and the dense flow fields explain all the input frames.
Figure 2: Algorithmic overview. we reconstruct the background/reflection layers in a coarse-to-fine manner. At the coarsest level, we estimate uniform flow fields for both the background and reflection layers and then reconstruct coarse background/reflection layers by averaging the aligned frames. At level l, we apply (1) back-ground/reflection layer reconstruction modules to reconstruct the background/reflection layer, and (2) use the PWC-net to predict the refined flow fields for both layers. Our framework progressively reconstructs the back-ground/reflection layers and flow fields until the finest level. 


SECTION 3. Proposed Method: Given a sequence {It}Tt=1 of T frames, the goal is to decompose each frame Ik into two layers, one for the (clean) background and the other for obstruction caused by fense/raindrops/occlusion. Decomposing an image sequence into background and obstruction layers is difficult as it involves solving two tightly coupled problems: optical flow decomposition and layer reconstruction. Without a good flow decomposition, the layers cannot be reconstructed faithfully due to the misalignment from inaccurate motion estimation. On the other hand, without well-reconstructed background and obstruction layers, the optical flow cannot be accurately estimated because of the mixed content. Due to the nature of this chicken-and-egg problem, there is no ground to start with because we do not have information for both flows and layers. In this work, we propose to learn deep CNNs to address the challenges. Our proposed method mainly consists of three modules: 1) initial flow decomposition, 2) background and obstruction layer reconstruction, and 3) optical flow refinement. Our method takes T frames as input and aims to decompose the keyframe frame Ik into a background layer Bk and reflection layer Rk at a time. We reconstruct the output images in a coarse-to-fine manner within an L-level hierarchy. First, we estimate the flows at the coarsest level from the initial flow decomposition module (Section 3.1). We then progressively reconstruct the back-ground/obstruction layers (Section 3.2) and refine optical flows (Section 3.3) until the last level. Figure 2 shows an overview of our method. Our unified framework can be applied to several layer decomposition problems, such as reflection/obstruction/fence/rain removal. Without loss of generality, we use the reflection removal task as an example to introduce our algorithm. We describe the details of the three modules in the following sections. 3.1. Initial Flow Decomposition: We first predict the flow for both background and reflection layers at the coarsest level (l=0), which is the essential starting point of our algorithm. Instead of estimating dense flow fields, we propose to learn a uniform motion vector for each layer. Our initial flow decomposition network consists of two sub-modules: 1) a feature extractor, and 2) a layer flow estimator. The feature extractor first generates feature maps for all the input frames at a 1/2LÃ— spatial resolution. Then, we construct a cost volume between frame j and frame k via a correlation layer [32]:
CVjk(x1,Â x2)=cj(x1)âŠ¤ck(x2),(1)View Source\begin{equation*}
CV_{jk}(\mathrm{x}_{1},\ \mathrm{x}_{2})=c_{j}(\mathrm{x}_{1})^{\top}c_{k}(\mathrm{x}_{2}),
\tag{1}
\end{equation*} where cj and ck are the extracted features of frame j and k, respectively, and x indicates the pixel index. Since the spatial resolution is quite small at this level, we set the search range of the correlation layer to only 4 pixels. The cost volume CV is then concatenated with the feature cj and fed into the layer flow estimator. The layer flow estimator uses the global average pooling and fully-connected layers to generate two global motion vectors. Finally, we tile the global motion vectors into two uniform flow fields (at a 1/2LÃ— spatial resolution): {V0B,jâ†’k} for the background layer and {V0R,jâ†’k} for the reflection layer. We provide the detailed architecture of our initial flow decomposition module in the supplementary material. 3.2. Background/reflection Layer Reconstruction: The layer reconstruction module aims to reconstruct the clean background image Bk and the reflection image Rk. Although the two tasks of background and reflection reconstruction are similar in their goals, the characteristics of the background and reflection layers are quite different. For example, the background layers are often more dominant in appearance but could be occluded in some frames. On the other hand, the reflection layers are often blurry and darker. Consequently, we train two independent networks for reconstructing the background and reflection layers. The two networks have the same architecture but do not share the network parameters. In the following, we only describe the network for background layer reconstruction; the reflection layer is reconstructed in a similar fashion.
Figure 3: Overview of layer reconstruction module. At level l, we first upsample the background flows {Vlâˆ’1B,jâ†’k} from level lâˆ’1 to warp and align the input frames {Ilt} with the keyframe Ilk. We then compute the difference maps between the background-registered frames and the keyframe. The background reconstruction network takes as input the background-registered frames {I~lB,jâ†’k}, the difference maps {DlB,jâ†’k}, the invalid masks {MlB,jâ†’k}, the upsampled background (Bslâˆ’1k)â†‘2, the reflection layers (Rlâˆ’1k)â†‘2, and learns to predict the residual map of the background keyframe. We add the predicted residual map to the upsampled background frame (Blâˆ’1k)â†‘2 and produce the reconstructed background frame Blk at level l. For the reflection layer reconstruction, we use the same architecture but learn a different set of network parameters. 
We reconstruct the background layer in a coarse-to-fine fashion. At the coarsest level (l=0), we first use the flow fields estimated from the initial flow decomposition module to align the neighboring frames. Then, we compute the average of all the background-registered frames as the predicted background image:
B0k=1Tâˆ‘j=1TW(I0j,Â V0B,jâ†’k),(2)View Source\begin{equation*}
B_{k}^{0}=\frac{1}{T}\sum_{j=1}^{T}\mathrm{W}(I_{j}^{0},\ V_{B,j\rightarrow k}^{0}),
\tag{2}
\end{equation*} where I0j is the frame j downsampled to level 0, and W() is the bilinear sampling operation. At the l-th level, the network takes as input the reconstructed background image Blâˆ’1k, reflection image Rlâˆ’1k, background optical flows {Vlâˆ’1B,kâ†’j} from the previous level as well as the input frames {Ilt} at the current level. The model aims to reconstruct the background image of the keyframe Blk at the current level. We first upsample the background flow fields {Vlâˆ’1B,kâ†’j} by 2Ã— and align all the input frames {Ilj} to the keyframe {Ilk}:
IÂ¯Â¯Â¯lB,jâ†’k=W(Ilj,Â (Vlâˆ’1B,jâ†’k)â†‘2),(3)View Source\begin{equation*}
\overline{I}_{B,j\rightarrow k}^{l}=W(I_{j}^{l},\ (V_{B,j\rightarrow k}^{l-1})\uparrow_{2}),
\tag{3}
\end{equation*} where ()â†‘2 denotes the 2Ã— bilinear upsampling operator. As some pixels may become invalid due to occlusion or the warping from outside image boundaries, we also compute a difference map DlB,jâ†’k=|IlB,jâ†’kâˆ’Ilk| and a warping invalid masks MlB,jâ†’k as additional cues for the network to reduce the warping artifacts. We concatenate the registered frames, difference maps, invalid masks, and the upsampled background and reflection layers from the previous level as the input feature to the background reconstruction network. The network then reconstructs a background image Blk via residual learning:
Blk=gB({I~lJ3,jâ†’k},{DlB,jâ†’k},{MlB,jâ†’k},(Blâˆ’1k)â†‘2,(Rlâˆ’1k)â†‘2)+(Blâˆ’1k)â†‘2,(4)View Source\begin{align*}
B_{k}^{l}=g_{B}&(\{\tilde{I}_{J3,j\rightarrow k}^{l}\}, \{D_{B,j\rightarrow k}^{l}\}, \{M_{B,j\rightarrow k}^{l}\}, (B_{k}^{l-1})\uparrow 2,\\
&(R_{k}^{l-1})\uparrow 2)+(B_{k}^{l-1})\uparrow 2,
\tag{4}
\end{align*} where gb is the background reconstruction network. Note that the reflection layer is also involved in the reconstruction of the background layer, which couples the background and reflection reconstruction networks together for joint training. Figure 3 illustrates an overview of the background reconstruction network at the l-th level. The detailed network configuration is provided in the supplementary material. 3.3. Optical Flow Refinement: After reconstructing all the background images Bl, we then learn to refine the background optical flows. We use the pre-trained PWC-Net [32] to estimate the flow fields between a paired of background images:
VlB,jâ†’k=PWC(Blj,Â Blk),(5)View Source\begin{equation*}
V_{B,j\rightarrow k}^{l}= \mathbf{PWC}(B_{j}^{l},\ B_{k}^{l}),
\tag{5}
\end{equation*} where PWC is the pre-trained PWC-Net. Note that the PWC-Net is fixed and not updated with the other submodules of our model. 3.4. Network Training: To improve training stability, we employ a two-stage training procedure. At the first stage, we train the initial flow decomposition network with the following loss:
Ldec=âˆ‘k=1Tâˆ‘J=1,jâ‰ kTâˆ¥V0B,jâ†’kPWC(B^j,B^k)â†“2Lâˆ¥1+âˆ¥V0R,jâ†’kâˆ’PWC(R^j,R^k)â†“2Lâˆ¥1,(6)View Source\begin{align*}
\mathcal{L}_{\mathrm{dec}}=\sum_{k=1}^{T}\sum_{J=1,j\neq k}^{T}&\Vert V_{B,j\rightarrow k}^{0}\mathrm{PWC}(\hat{B}_{j},\hat{B}_{k})\downarrow^{2^{L}}\Vert_{1}+\\
&\Vert V_{R,j\rightarrow k}^{0}- \mathrm{PWC}(\hat{R}_{j},\hat{R}_{k})\downarrow^{2^{L}}\Vert_{1},
\tag{6}
\end{align*} where â†“ is the bilinear downsampling operator, B^ and R^ denote the ground-truth background and reflection layers, respectively. We use the pre-trained PWC-Net to compute optical flows and downsample the flows by 2LÃ— as the ground-truth to train the initial flow decomposition network. Next, we freeze the initial flow decomposition network and train the layer reconstruction networks with an image reconstruction loss:
Limg=1TÃ—Lâˆ‘t=1Tâˆ‘l=0L(âˆ¥B^ltâˆ’Bltâˆ¥1+âˆ¥R^ltâˆ’Rltâˆ¥1),(7)View Source\begin{align*}
\mathcal{L}_{\mathrm{img}}=\frac{1}{T\times L}\sum_{t=1}&^{T}\sum_{l=0}^{L}(\Vert\hat{B}_{t}^{l}-B_{t}^{l}\Vert_{1}+\Vert\hat{R}_{t}^{l}-R_{t}^{l}\Vert_{1}),
\tag{7}\\
\end{align*} and a gradient loss:
Lgrad=1TÃ—Lâˆ‘t=1Tâˆ‘l=0L(âˆ¥âˆ‡B^ltÎ¹âˆ’âˆ‡Bltâˆ¥1+âˆ¥âˆ‡R^ltâˆ’âˆ‡Rltâˆ¥1))(8)View Source\begin{equation*}
\mathcal{L}_{\mathrm{grad}}=\frac{1}{T\times L}\sum_{t=1}^{T}\sum_{l=0}^{L}(\Vert\nabla \hat{B}_{t}^{l}\iota-\nabla B_{t}^{l}\Vert_{1}+\Vert\nabla\hat{R}_{t}^{l}-\nabla R_{t}^{l}\Vert_{1}))
\tag{8}
\end{equation*} where âˆ‡ is the spatial gradient operator. The gradient loss encourages the network to reconstruct faithful edges to further improve visual quality. The overall loss for training the layer reconstruction networks is:
L=Limg+Î»gradLgrad,(9)View Source\begin{equation*}
\mathcal{L}=\mathcal{L}_{\mathrm{img}}+\lambda_{grad}\mathcal{L}_{\mathrm{grad}},
\tag{9}
\end{equation*} where the weight Î»grad is empirically set to 1 in all our experiments. We train both the initial flow decomposition and layer reconstruction networks with the Adam optimizer [20] with a batch size of 2. We set the learning rate to 10â€“4 for the first 100k iterations and then decrease to 10â€“5 for another 100k iterations. 3.5. Synthetic Sequence Generation: Since collecting real sequences with ground-truth reflection and background layers is very difficult, we use the Vimeo-90k dataset [41] to synthesize sequences for training. Out of the 91,701 sequences in the Vimeo-90k training set, we randomly select two sequences as the background and reflection layers. First, we warp the sequences using random homography transformations. We then randomly crop the sequences to a spatial resolution of 320Ã—192 pixels. Finally, the composition is applied frame by frame using the realistic reflection image synthesis model proposed by previous work [8], [45]. More details about the synthetic data generation are provided in the supplementary material. 3.6. Online Optimization: We observe that the model trained on our synthetic dataset may not perform well on real-world sequences. Therefore, we propose an online refinement method to fine-tune our pre-trained model with real sequences by optimizing an unsupervised warping consistency loss:
Lwarp=âˆ‘k=1Tâˆ‘j=0,jâ‰ kTâˆ‘l=0Lâˆ¥Iljâˆ’(WW(Blk,VlB,jâ†’k)+WW(Rlk,Â VlR,jâ†’k))âˆ¥1.(10)View Source\begin{align*}
\mathcal{L}_{\mathrm{warp}}=\sum_{k=1}^{T} \sum_{j=0,j\neq k}^{T} \sum_{l=0}^{L}\Vert I_{j}^{l}-&(\pmb{W}(B_{k}^{l}, V_{B,j\rightarrow k}^{l})+\\
&\pmb{W}(R_{k}^{l},\ V_{R,j\rightarrow k}^{l}))\Vert_{1}.
\tag{10}
\end{align*} The consistency loss enhances fidelity by enforcing that the predicted background and reflection layers should be warped back and composited into the original input frames. In addition, we also incorporate the total variation loss:
Ltv=âˆ‘t=1Tâˆ‘l=0L(âˆ¥âˆ‡Bltâˆ¥1+âˆ¥âˆ‡Rltâˆ¥1),(11)View Source\begin{equation*}
\mathcal{L}_{tv}=\sum_{t=1}^{T}\sum_{l=0}^{L}(\Vert \nabla B_{t}^{l}\Vert_{1}+\Vert \nabla R_{t}^{l}\Vert_{1}),
\tag{11}
\end{equation*} which encourages the network to generate natural images by following the sparse gradient image prior. The overall loss of online optimization is:
Lonline=Lwarp+Î»tvLtv,(12)View Source\begin{equation*}
\mathcal{L}_{online}=\mathcal{L}_{\mathrm{warp}}+\lambda_{tv}\mathcal{L}_{tv},
\tag{12}
\end{equation*} where the weight Î»tv is empirically set to 0.1 in all our experiments. Note that we freeze the weight of the PWC-Net and only update the background/reflection layer reconstruction modules. We fine-tune our model on every single input sequence for 1k iterations, which takes about 20 minutes for a sequence with a 1296Ã—864 spatial resolution. We use only five frames in the sequence for fine-tuning. 3.7. Extension to Other Obstruction Removal: The proposed framework can be easily modified to handle other obstruction removal tasks, such as fence or raindrop removal. First, we remove the image reconstruction network for the obstruction (i.e., reflection) layer and only predict the background layers. Second, the background image reconstruction network outputs an additional channel as the alpha map for segmenting the obstruction layer. We do not estimate flow fields for the obstruction layer as the flow estimation network cannot handle the repetitive structures (e.g., fence) or tiny objects (e.g., raindrops) well and often predicts noisy flows. With such a design change, our model is able to perform well on the fence and raindrop removal tasks. We use the fence segmentation dataset [6] and alpha matting dataset [39] to train our model for both tasks. 

SECTION 4. Experiments and Analysis: We present the main findings in this section and include more results in the supplementary material.
Figure 4: Quantitative evaluation on controlled sequences. For each sequence, we show the keyframe (left) and recovered background (middle) and reflection/occluder (right). We report the NCC scores of recovered backgrounds and reflections for quantitative comparisons. 
Table 1: Quantitative comparison of reflection removal methods on synthetic sequences. We compare the proposed method with existing reflection removal approaches on a synthetic dataset with 100 sequences, where each sequence contains five consecutive frames. For the single-image based methods [8], [16], [38], [43], [45], we generate the results frame-by-frame. For multi-frame algorithms [1], [12], [21] and our method, we use five input frames to generate the results
4.1. Comparisons with State-of-the-Arts: Controlled SequencesWe first evaluate on the con- trolled sequences provided by Xue et al [42], which contain three videos with ground-truth background and reflection layers. We compare the proposed method with Li and Brown [21], Guo et al. [12], Xue et al. [42], and Alayrac et al. [1]. Figure 4 shows our recovered background and reflection/obstruction layers and the normalized crosscorrelation (NCC) scores [35], [42]. Our method performs favorably against other approaches on the Toy and Hanoi sequences and shows comparable scores to Xue et al. [42] on the Stone sequence. Synthetic SequencesWe synthesize 100 sequences by the method described in Section 3.5 from the Vimeo-90k test set. We compare our approach with five single-image reflection removal methods [8], [16], [38], [43], [45], and three multi-frame approaches [1], [12], [21]. We use the default parameters of each method to generate the results. Since Alayrac et al. [1] do not release the source code or pre-trained model, we re-implement their model and train on our training dataset. Table 1 shows the average PSNR, SSIM [37], NCC, and LMSE [11] metrics. The proposed method obtains the best scores on all the evaluation metrics for both background and reflection layers. Real SequencesIn Figure 5, we present visual compar- isons of real input sequences from [42]. Our method is able to separate the reflection layers and reconstruct clear and sharp background images than other approaches [1], [21], [26], [42]. Figure 6 shows two examples where the inputs contain obstruction such as texts on the glass or raindrops. Our method can remove the obstruction layer and reconstruct clear background images. More visual comparisons are available in the supplementary material. 4.2. Analysis and Discussion: In this section, we analyze several key design choices of the proposed framework. We also provide the execution time and show a failure case of our method. Initial Flow DecompositionWe demonstrate that the uni- form flow initialization plays an important role in our algorithm. We train our model with the following settings: 1) removing the initial flow decomposition network, where the flows at the coarsest level are set to zero, and 2) predicting spatially-varying dense flow fields as the initial flows.
Figure 5: Visual comparison of background-reflection separation on natural sequences. More results can be found in the supplementary material. *results are in lower resolution. 
Figure 6: Recovering occluded scenes by raindops. 
Table 2(a) reports the validation loss of Equation (9) on our Vimeo-90k validation set, where the model with uniform flow prediction achieves a much lower validation loss compared to the alternatives. Initializing the flow fields to zero makes it difficult for the following levels to decompose the background and reflection layers. On the contrary, estimating dense flow fields at the coarsest level may result in noisy predictions and lead to inconsistent layer separation. Our uniform flow prediction strikes a balance and serves as a good initial prediction to facilitate the following background reconstruction and flow refinement steps. Image Reconstruction NetworkTo demonstrate the effec- tiveness of the image reconstruction network, we replace it with a temporal filter to fuse the neighbor frames, which are warped and aligned by the optical flows. We show in Table 2(b) that both the temporal mean and median filters result in large errors (in terms of the validation loss of Equation (9)) as the errors are accumulated across levels. In contrast, our image reconstruction network learns to reduce warping and alignment errors and generates clean foreground and background images. Online OptimizationTable 2(c) shows that both the net- work pre-training with synthetic data and online optimization with real data are beneficial to the performance of our model. In Figure 7, we show that the model without pretraining cannot separate the reflection well on the real input sequence. Without online optimization, the background image contains residuals from the reflection layer. After online optimization, our method is able to reconstruct both background and reflection layers well. Running TimeWe evaluate the execution time of two optimization-based algorithms [12], [21] and a recent CNN-based method [1] with different input sequences resolutions on a computer with Intel Core i7-8550U CPU and NVIDIA TITAN Xp GPU. Table 3 shows that our method without the online optimization step runs faster than optimization-based algorithms. Alayrac et al. [1] use a 3D CNN architecture without explicit motion estimation, which results in a faster inference speed. In contrast, our method computes bi-directional optical flows for every pair of input frames in a coarse-to-fine manner, which is slower but achieves much better reconstruction performance.
Figure 7: Effect of online optimization and pre-training. Both steps are crucial to achieving high-quality results. 
Table 2: Ablations. We analyze the design choices of the proposed method and report the validation loss of equation (9) on the synthetic reflection-background vimeo-90k test set
Table 3: Running time comparison (in seconds). Cpu: intel core i7-8550U, GPU: NVIDIA TITAN xp. * denotes methods using gpu
Failure CaseWe show a failure case of our algorithm in Figure 8, where our method does not separate the reflection layer well. This example is particularly challenging as there are two layers of reflections: the top part contains the wooden beams, and the bottom part comes from the street behind the camera. As the motion of the wooden beams is close to the background image, our method can only separate the street scenes in the reflection layer. 

SECTION 5. Conclusions: We have presented a novel method for multi-frame reflections and obstructions removal. Our key insight is to leverage a CNN to reconstruct background and reflection layers from flow-warped images. Integrating optical flow estimation and coarse-to-fine refinement enable our model to robustly recover the underlying clean image from challenging real-world sequences. Our method can be applied to different tasks such as fence or raindrop removal with minimum changes in our design. We also show that online optimization on testing sequences leads to improved visual quality. Extensive visual comparisons and quantitative evaluation demonstrate that our approach performs well on a wide variety of scenes.
Figure 8: A failure case. Our method fails to recover the correct flow fields for each layer, leading to ineffective reflection removal. 

ACKNOWLEDGMENTS: This work is supported in part by NSF CAREER (#1149783), NSF CRII (#1755785), MOST 109-2634-F-002-032, MediaTek Inc. and gifts from Adobe, Toyota, Panasonic, Samsung, NEC, Verisk, and Nvidia.