SECTION 1. Introduction: 3D content creation is a challenging, mostly manual task which requires both artistic modeling skills and technical knowledge. Efforts to automate 3D modeling can save substantial production costs or allow for faster and more diverse content creation. Photogrammetry [48], [56] is a popular technique to assist in this process, where multiple photos of an object are converted into a 3D model. Game studios leverage photogrammetry to quickly build highly detailed virtual landscapes [21]. However, this is a multi-stage process, including multi-view stereo [52] to align cameras and find correspondences, geometric simplification, texture parameterization, material baking and delighting. This complex pipeline has many steps with conflicting optimization goals and errors that propagate between stages. Artists rely on a plethora of software tools and significant manual adjustments to reach the desired quality of the final 3D model.
Figure 1. We reconstruct a triangular mesh with unknown topology, spatially-varying materials, and lighting from a set of multi-view images. We show examples of scene manipulation using off-the-shelf modeling tools, enabled by our reconstructed 3d model. 
Our goal is to frame this process as an inverse rendering task, and optimize as many steps as possible jointly, driven by the quality of the rendered images of the reconstructed model, compared to the captured input imagery. Recent work approaches 3D reconstruction with neural rendering, and provides high quality novel view synthesis [39]. However, these methods typically produce representations that entangle geometry, materials and lighting into neural networks, and thus cannot easily support scene editing operations. Furthermore, to use them in traditional graphics engines, one needs to extract geometry from the network using methods like Marching Cubes which may lead to poor surface quality, particularly at low triangle counts. Recent neural methods can disentangle shape, materials, and lighting [3], [68], [70], but sacrifice reconstruction quality. Also, the materials encoded in neural networks cannot easily be edited or extracted in a form compatible with traditional game engines. In contrast, we reconstruct 3D content compatible with traditional graphics engines, supporting relighting and scene editing. In this paper, we present a highly efficient inverse rendering method capable of extracting triangular meshes of unknown topology, with spatially-varying materials and lighting from multi-view images. We assume that the object is illuminated under one unknown environment lighting condition, and that we have corresponding camera poses and masks indicating the object in these images, as in past work [3]. Our approach learns topology and vertex positions for a surface mesh without requiring any initial guess for the 3D geometry. The heart of our method is a differentiable surface model based on a deformable tetra-hedral mesh [54], which we extend to support spatially-varying materials and high dynamic range (HDR) environment lighting, through a novel differentiable split sum approximation. We optimize geometry, materials and lighting (50M+ parameters) jointly using a highly optimized differentiable rasterizer with deferred shading [22], [30]. The resulting 3D model can be deployed without conversion on any device supporting triangle rendering, including phones and web browsers, and renders at interactive rates. Experiments show our extracted models used in scene editing (e.g., Figure 1), material decomposition, and high quality view interpolation, all running at interactive rates in triangle-based renderers (rasterizers and path tracers). 

SECTION 2. Related Work: 2.1. Multi-View 3d Reconstruction: Classical methods for multi-view 3D reconstruction either exploit inter-image correspondences [1], [11], [12], [52] to estimate depth maps or use voxel grids to represent shapes [10], [53]. The former methods typically fuse depth maps into point clouds, optionally generating meshes [27]. They rely heavily on the quality of matching, and errors are hard to rectify during post-processing. The latter methods estimate occupancy and color for each voxel and are often limited by the cubic memory requirement. Neural implicit representations leverage differentiable rendering to reconstruct 3D geometry with appearance from image collections [24], [39], [43]. NeRF [39] and follow-ups [15], [38], [41], [42], [49], [50], [60], [62], [66], [69], use volumetric representations and compute radiance by ray marching through a neurally encoded 5D light field. While achieving impressive results on novel view synthesis, geometric quality suffers from the ambiguity of volume rendering [69]. Surface-based rendering methods [43], [64] use implicit differentiation to obtain gradients, optimizing the underlying surface directly. Unisurf [47] is a hybrid approach that gradually reduces the sampling region, encouraging a volumetric representation to converge to a surface, and NeuS [59] provides an unbiased conversion from SDF into density for volume rendering. Common for all methods is that they rely on ray marching for rendering, which is computationally expensive both during training and inference. While implicit surfaces can be converted to meshes for fast inference, this introduces additional error not accounted for during optimization [54]. We optimize the end-to-end image loss of an explicit mesh representation, supporting intrinsic decomposition of shape, materials and lighting by design, and utilizing efficient differentiable rasterization [30]. Explicit surface representations are proposed to estimate explicit 3D mesh from images [7], [8], [14], [23], [34], [35], [54]. Most approaches assume a given, fixed mesh topology [7], [8], [23], [35], but this is improved in recent work [14], [34], [54]. In particular, DMTet [54] directly optimizes the surface mesh using a differentiable marching tetrahedral layer. However, it focuses on training with 3D supervision. In this work, we extend DMTet to 2D supervision, using differentiable rendering to jointly optimize topology, materials, and lighting. 2.2. Brdf and Lighting Estimation: Beyond geometry, several techniques estimate surface radiometric properties from images. Previous work on BTF and SVBRDF estimation rely on special viewing configurations, lighting patterns or complex capturing setups [2, 5, 16-18,20,31,51,61]. Recent methods exploit neural networks to predict BRDF from images [13], [19], [32], [33], [37], [44]. Differentiable rendering methods [7], [8], [22], [35], [7]1 learn to predict geometry, SVBRDF and, in some cases, lighting via 2D image loss. Still, their shape is generally deformed from a sphere and cannot represent arbitrary topology. Neural implicit representations successfully estimate lighting and BRDF from image collections. Bi et al. [2] and NeRV [57] model light transport to support advanced lighting effects, e.g., shadows, but have very high computational cost. Most related to our work are neural 3D reconstruction methods for jointly estimating shape, BRDFs and lighting from images [3], [4], [68], [70], while providing an intrinsic decomposition of these terms. Illumination is represented using mixtures of spherical Gaussians (NeRD [3], PhySG [68]), or low resolution envmaps (NeRFactor [70]), in both cases limited to low frequency illumination. In contrast, we propose a differentiable split sum lighting model, also adopted by the concurrent work Neural-PIL [4]. These neural implicit methods use multiple MLPs to factorize the representation, resulting in long training and inference times. Furthermore, these methods forgo the vast ecosystem of available 3D modeling and rendering tools, “rein-venting the wheel” for tasks such as rendering, scene editing [63] and simulation. In contrast, our output is directly compatible with existing renderers and modeling tools. We optimize an explicit surface mesh, BRDF parameters, and lighting stored in an HDR probe, achieving faster training speed and better decomposition results. Table 1 shows a high level comparison of the methods.
Figure 2. Overview of our approach. We learn topology, materials, and environment map lighting jointly from 2d supervision. We leverage differentiable marching tetrahedrons to directly optimize topology of a triangle mesh. While the topology is drastically changing, we learn materials through volumetric texturing, efficiently encoded using an mlp with positional encoding. Finally, we introduce a differentiable version of the split sum approximation for environment lighting. Our output representation is a triangle mesh with spatially varying 2d textures and a high dynamic range environment map, which can be used unmodified in standard game engines. The system is trained end-to-end, supervised by loss in image space, with gradient-based optimization of all stages. Spot model by keegan crane. 
Table 1. Taxonomy of methods. Nv: neural volume, ns: neural surface. Factorize indicates if the method supports some decomposition of geometry, materials, and lighting.


SECTION 3. Our Approach: We present a method for 3D reconstruction supervised by multi-view images of an object illuminated under one unknown environment lighting condition, together with known camera poses and background segmentation masks. The target representation consists of triangle meshes, spatially-varying materials (stored in 2D textures) and lighting (a high dynamic range environment probe). We carefully design the optimization task to explicitly render triangle meshes, while robustly handling arbitrary topology. Hence, unlike most recent work using neural implicit surface or volumetric representations, we directly optimize the target shape representation. Concretely, we adapt Deep Marching Tetrahedra [54] (DMTet) to work in the setting of 2D supervision, and jointly optimize shape, materials, and lighting. At each optimization step, the shape representation - parameters of a signed distance field (SDF) defined on a grid with corresponding per-vertex offsets - is converted to a triangular surface mesh using a marching tetrahedra layer. Next, we render the extracted surface mesh in a differentiable rasterizer with deferred shading, and compute loss in image space on the rendered image compared to a reference image. Finally, the loss gradients are back-propagated to update the shape, textures and lighting parameters. Our approach is summarized in Figure 2 and each step is described in detail below; Section 3.1 outlines our topology optimization, Section 3.2 presents the spatially-varying shading model, and our approach for reconstructing all-frequency environment lighting is described in Section 3.3. Optimization task Let \phiϕ denote our optimization parameters (i.e., SDF values and vertex offsets representing the shape, spatially varying material and light probe parameters). For a given camera pose, cc, the differentiable renderer produces an image I_{\phi}(c)Iϕ(c). The reference image I_{\text{ref}}(c)Iref(c) is a view from the same camera. Given a loss function LL, we minimize the empirical risk
\begin{equation*}\mathop{\arg\min}\limits_{\phi} \mathbb{E}_{c}[L(I_{\phi}(c), I_{\text{ref}}(c))]\tag{1}\end{equation*}argminϕEc[L(Iϕ(c),Iref(c))](1)View Source\begin{equation*}\mathop{\arg\min}\limits_{\phi} \mathbb{E}_{c}[L(I_{\phi}(c), I_{\text{ref}}(c))]\tag{1}\end{equation*} using Adam [28] based on gradients w.r.t. the optimization parameters, \partial L/\partial\phi∂L/∂ϕ, which are obtained through differentiable rendering. Our renderer uses physically based shading and produces images with high dynamic range. Therefore, the objective function must be robust to the full range of floating-point values. Following recent work in differentiable rendering [22], our loss function is L=L_{\text{image}}+ L_{\text{mask}}+\lambda L_{\text{reg}}L=Limage+Lmask+λLreg, an image space loss, L_{\text{image}}(L_{1}Limage(L1 norm on tone mapped colors), a mask loss, L_{\text{mask}}Lmask (squared L_{2}L2) and a regularizer LL reg (Equation 2) to improve geometry. Please refer to the supplemental material for details. Assumptions For performance reasons we use a differentiable rasterizer with deferred shading [22], hence reflections, refractions (e.g., glass), and translucency are not supported. During optimization, we only renderer direct lighting without shadows. Our shading model uses a diffuse Lambertian lobe and a specular, isotropic microfacet GGX lobe, which is commonly used in modern game engines [26], [29]. Both dielectrics and metal materials are supported. We note that our approach directly generalizes to a differentiable path tracer [45], [46], but at a significantly increased computational cost. 3.1. Learning Topology: Volumetric and implicit shape representations (e.g., SDFs) can be converted to meshes through Marching Cubes [36] (MC) in a post-processing step. However, MC inevitably imposes discretization errors. As a result, the output mesh quality, particularly at the moderate triangle counts typically used in real-time rendering, is often not sufficient. Similarly, simplifying dense extracted meshes using decimation tools may introduce errors in rendered appearance. To avoid these issues, we explicitly render triangle meshes during optimization. We leverage Deep Marching Tetrahedra [54] (DMTet) in a 2D supervision setting through differentiable rendering. DMTet is a hybrid 3D representation that represents a shape with a discrete SDF defined on vertices of a deformable tetrahedral grid. The SDF is converted to triangular mesh using a differentiable marching tetrahedra layer (MT), as illustrated in Figure 4. The loss, in our case computed on renderings of the 3D model, is back-propagated to the implicit field to update the surface topology. This allows us to directly optimize the surface mesh and rendered appearance end-to-end.
Figure 3. Triangle mesh extraction from a set of 256 rendered images w/ masks. Damicornis model from the smithsonian 3d repository [55], we extracted meshes from nerf and neus using marching cubes for a target triangle count of roughly 50k triangles and optimized the example in our pipeline for a similar count. We show renderings of the extracted meshes in a path tracer and report the chamfer loss. We note that neus, which optimizes a surface representation, significantly improves on the volumetric representation used by nerf for this example. Furthermore, our end-to-end optimization of a triangle mesh improves both the visual quality and the chamfer loss at a fixed triangle count. When drastically increasing the triangle count in the neus mesh extraction (from 53k to 900k triangles), the quality improves significantly, indicating that neus has a high quality internal surface representation. Still, our mesh with 53k triangles is on par with the high resolution neus output, indicating the benefit of directly optimizing the mesh representation. 
Figure 4. Marching tetrahedra extracts faces from a tetrahedral grid with grid vertices, v_{i}^{\prime}= v_{i}+\Delta v_{i}v′i=vi+Δvi and scalar SDF values, s_{i}si. For tets with \text{sign}(s_{i})\neqsign(si)≠ sign (s_{j})(sj), faces are extracted, and the face vertices, v_{ij}vij, are determined by by linear interpolation. 
We illustrate the advantage of end-to-end learning in Figure 3, where we compare our meshes to those generated by competing methods. While NeRF [39] (volumetric rep.) and NeuS [59] (implicit surface rep.) provide high quality view interpolation, the quality loss introduced in the MC step is significant at low triangle counts. Given a tetrahedral grid with vertex positions vv, DMTet learns SDF values, ss, and deformation vectors \Delta vΔv. The SDF values and deformations can either be stored explicitly as values per grid vertex, or implicitly [43], [47] by a neural network. At each optimization step, the SDF is first converted to a triangular surface mesh using MT, which is shown to be differentiable w.r.t. SDF and can change surface topology in DMTet [54]. Next, the extracted mesh is rendered using a differentiable rasterizer to produce an output image, and image-space loss gradients are back-propagated to the SDF values and offsets (or network weights). A neural SDF representation can act as a smoothness prior, which can be beneficial in producing well-formed shapes. Directly optimizing per-vertex attributes, on the other hand, can capture higher frequency detail and is faster to train. In practice, the optimal choice of parametrization depends on the ambiguity of geometry in multi-view images. We provide detailed analysis in the supplemental materials. To reduce floaters and internal geometry, we regularize the SDF values of DMTet similar to Liao et al. [34]. Given the binary cross-entropy HH, sigmoid function \sigmaσ, and the sign function sign (x)(x), we define the regularizer as
\begin{equation*}
L_{\text{reg}}=\sum_{i,j\in \mathbb{S}_{e}}H(\sigma(s_{i}), \text{sign}(s_{j}))+H(\sigma(s_{j}), \text{sign}(s_{i})),\tag{2}\end{equation*}Lreg=∑i,j∈SeH(σ(si),sign(sj))+H(σ(sj),sign(si)),(2)View Source\begin{equation*}
L_{\text{reg}}=\sum_{i,j\in \mathbb{S}_{e}}H(\sigma(s_{i}), \text{sign}(s_{j}))+H(\sigma(s_{j}), \text{sign}(s_{i})),\tag{2}\end{equation*}
where we sum over the set of unique edges, \mathbb{S}_{e}Se, in the tetrahedral grid, for which sign (s_{i})\neq(si)≠ sign (s_{j})(sj). Intuitively, this reduces the number of sign flips and simplifies the surface, penalizing internal geometry or floaters. We ablate the choice of regularization loss in the supplemental material. 3.2. Shading Model: Material Model We follow previous work in differentiable rendering [22] and use the physically-based (PBR) material model from Disney [6]. This lets us easily import game assets and render our optimized models directly in existing engines without modifications. This material model combines a diffuse term with an isotropic, specular GGX lobe [58]. Referring to Figure 5, the diffuse lobe parameters \boldsymbol{k}_{d}kd are provided as a four-component texture, where the optional fourth channel \alpha represents transparency. The specular lobe is described by a roughness value, r, for the GGX normal distribution function and a metalness factor, m, which interpolates between plastic and metallic appearance by computing a specular highlight color according to \boldsymbol{k}_{s}=(1-m)\cdot 0.04+m\cdot \boldsymbol{k}_{d} [26]. Following a standard convention, we store these values in a texture \boldsymbol{k}_{\text{orm}}=(0, r, m), where o is left unused. Finally, we include a tangent space normal map, n, to capture high frequency shading detail. We regularize material parameters using a smoothness loss [70], please refer to our supplemental material for details.
Figure 5. We represent 3d models as a triangular mesh and a set of spatially varying materials following a standard pbr model. 
Texturing Automatic texture parametrization for surface meshes is an active research area in computer graphics. We optimize topology, which requires continually updating the parametrization, potentially introducing discontinuities into the training process. To robustly handle texturing during topology optimization, we leverage volumetric texturing, and use world space position to index into our texture. This ensures that the mapping varies smoothly with both vertex translations and changing topology. The memory footprint of volumetric textures grows cubically, which is unmanageable for our target resolution. We therefore extend the approach of PhySG [68], using a multilayer perceptron (MLP) to encode all material parameters in a compact representation. This representation can adaptively allocate detail near the 2D manifold representing the surface mesh, which is a small subset of the dense 3D volume. More formally, we let a positional encoding + MLP represent a mapping \mathbf{x}\rightarrow(k_{d},\boldsymbol{k}_{\text{orm}}, \mathbf{n}), e.g., given a world space position \mathbf{x}, compute the base color, \boldsymbol{k}_{d}, specular parameters, \boldsymbol{k}_{\text{orm}} (roughness, metalness), and a tangent space normal perturbation, n. We leverage the tiny-cuda-nn framework [40], which provides efficient kernels for hash-grid positional encoding [41] and MLP evaluations. Once the topology and MLP texture representation have converged, we re-parametrize the model: we generate unique texture coordinates using xatlas [65] and sample the MLP on the surface mesh to initialize 2D textures, then continue the optimization with fixed topology. Referring to Figure 6, this effectively removes texture seams introduced by the (u, v) -parametrization, and may also increase texture detail as we can use high resolution 2D textures for each of \boldsymbol{k}_{d}, \boldsymbol{k}_{\text{orm}}, and n. This results in 2D textures compatible with standard 3D tools and game engines.
Figure 6. Sampling out the volumetric representation to create 2d textures results in texture seams (left). However, further optimization (right), quickly removes the seams automatically. 
3.3. Image Based Lighting: We adopt an image based lighting model, where the scene environment light is given by a high-resolution cube map. Following the rendering equation [25], we compute the outgoing radiance L(\omega_{o}) in direction \omega_{o} by:
\begin{equation*}
L(\omega_{o})=\int_{\Omega}L_{i}(\omega_{i})f(\omega_{\mathrm{i}}, \omega_{o})(\omega_{i}\cdot \mathbf{n})d\omega_{i}.\tag{3}\end{equation*}View Source\begin{equation*}
L(\omega_{o})=\int_{\Omega}L_{i}(\omega_{i})f(\omega_{\mathrm{i}}, \omega_{o})(\omega_{i}\cdot \mathbf{n})d\omega_{i}.\tag{3}\end{equation*} This is an integral of the product of the incident radiance, L_{i}(\omega_{i}) from direction \omega_{i} and the BSDF f(\omega_{i}, \omega_{o}). The integration domain is the hemisphere \Omega around the surface intersection normal, n. Below, we focus on the specular part of the outgoing radiance, where, in our case, the BSDF is a Cook-Torrance microfacet specular shading model [9] according to:
\begin{equation*}
f(\omega_{i}, \omega_{o})=\dfrac{DGF}{4(\omega_{o}\cdot \mathbf{n})(\omega_{i}\cdot \mathbf{n})},\tag{4}\end{equation*}View Source\begin{equation*}
f(\omega_{i}, \omega_{o})=\dfrac{DGF}{4(\omega_{o}\cdot \mathbf{n})(\omega_{i}\cdot \mathbf{n})},\tag{4}\end{equation*}
where D, G and F are functions representing the GGX [58] normal distribution (NDF), geometric attenuation and Fresnel term, respectively. High quality estimates of image based lighting can be obtained by Monte Carlo integration. For low noise levels, large sample counts are required, which is typically too expensive for interactive applications. Thus, spherical Gaussians (SG) and spherical harmonics (SH) are common approximations for image based lighting [3], [7], [68]. They allow for control over the lighting frequency through varying the number of SG lobes (or SH coefficients), and are efficient representations for low to medium frequency lighting. However, representing high frequency and highly specular materials requires many SG lobes, which comes at a high runtime cost and hurts training stability. We instead draw inspiration from real-time rendering, where the split sum approximation [26] is a popular, efficient method for all-frequency image based lighting. Here, the lighting integral from Equation 3 is approximated as:
\begin{equation*}
L(\omega_{o})\approx\int_{\Omega}f(\omega_{i}, \omega_{o})(\omega_{i}\cdot \mathbf{n})d\omega_{i}\int_{\Omega}L_{i}(\omega_{i})D(\omega_{i}, \omega_{o})(\omega_{i}\cdot \mathbf{n})d\omega_{i}.\tag{5}\end{equation*}View Source\begin{equation*}
L(\omega_{o})\approx\int_{\Omega}f(\omega_{i}, \omega_{o})(\omega_{i}\cdot \mathbf{n})d\omega_{i}\int_{\Omega}L_{i}(\omega_{i})D(\omega_{i}, \omega_{o})(\omega_{i}\cdot \mathbf{n})d\omega_{i}.\tag{5}\end{equation*}
Figure 7. Relighting quality for a scene from the nerfactor dataset, with our examples relit using blender, and nerfactor results generated using the public code. 
The first term represents the integral of the specular BSDF with a solid white environment light. It only depends on the parameters \cos\theta=\omega_{i}\cdot \mathbf{n} and the roughness r of the BSDF, and can be precomputed and stored in a 2D lookup texture. The second term represents the integral of the incoming radiance with the specular NDF, D. Following Karis [26], this term is also pre-integrated and represented by a filtered cubemap. In each mip-level, the environment map is integrated against D for a fixed roughness value (increased roughness at coarser mips). The split sum approach is popular for its modest runtime cost, using only two texture lookups: query the 2D lookup texture representing the first term based on (r, \cos\theta) and the mip pyramid at level r, in direction \omega_{o}. This should be compared to evaluating SG products with hundreds of lobes for each shading point. Furthermore, it uses the standard GGX parametrization, which means that we can relight our extracted models with different kinds of light sources (point, area lights etc.) and use our reconstructed materials with no modifications in most game engines and modeling tools. We introduce a differentiable version of the split sum shading model to learn environment lighting from image observations through differentiable rendering. We let the texels of a cube map (typical resolution 6 × 512 × 512) be the trainable parameters. The base level represents the pre-integrated lighting for the lowest supported roughness value, and each smaller mip-level is constructed from the base level using the pre-filtering approach from Karis [26]. To obtain texel gradients, we express the lighting computations using PyTorch's auto-differentiation. However, the pre-filtering of the second term in Equation 5 must be updated in each training iteration, and therefore warrant a specialized CUDA implementation to reduce the training cost. This term can either be estimated through Monte-Carlo integration (BSDF importance sampling), or by pre-filtering the environment map in a solid-angle footprint derived from the NDF. To reduce noise, at the cost of introducing some bias, we chose the latter approach. Please refer to our supplemental material for implementation details. We additionally create a single filtered low-resolution (6 × 16 × 16) cube map representing the diffuse lighting. The process is identical to the filtered specular probe, sharing the same trainable parameters, average-pooled to the mip level with roughness r=1. The pre-filtering of the diffuse term only uses the cosine term, \omega_{i}\cdot \mathbf{n}. The two filtering steps are fully differentiable and are performed at each training step.
Figure 8. To highlight the benefits of our explicit representation, we insert two reconstructed models into the cornell box. Note that the objects accurately interact with the scene lighting, and cast shadows (e.g., the green wall). Next, we use our reconstructed hotdog model in a soft-body physics simulation, dropping red jelly on the plate. We run the entire simulation (21 frames) on both the reference 3d mesh and our reconstructed mesh, and display the last frame. Note that these applications are not currently feasible for neural volumetric representations. 


SECTION 4. Experiments: In the following, we evaluate our system for a variety of applications. To emphasize our approach's explicit decomposition into a triangle mesh and materials, we show re-lighting, editing, and simulation using off-the shelf tools. We also compare to recent neural methods supporting factorization: NeRD [3] and NeRFactor [70]. While not our main focus, we include view interpolation results to establish a baseline comparison to state-of-the-art methods. Finally, we compare our split-sum approximation against spherical Gaussians for image-based lighting. 4.1. Scene Editing and Simulation: Our factorized scene representation enables advanced scene editing. Previous work using density-based neural representations only supports rudimentary relighting and simple forms of scene edits [3], [68], [70]. In Figure 7 we compare the relighting quality of our reconstructed model, rendered using the Blender Cycles path tracer, with the results of NeRFactor (rendered by evaluating a neural network). A quantitative summary is provided in Table 2, where we also measure the quality of the reconstructed albedo textures. We note that our method produces more detailed results and outperforms NeRFactor in all metrics. Our artifacts come mainly from the mismatch between training (using a rasterizer), and inference (using full global illumination). In areas with strong shadowing or color bleeding, our material and geometry quality suffer. A differentiable path tracer [46] can likely improve the material separation in our pipeline, but would require significantly more computations.
Table 2. Relighting quality for nerfactor's synthetic dataset. The reported image metrics are the arithmetic mean over 8 validation views and 8 light probes for all 4 test scenes. We also show metrics for the \boldsymbol{k}_{d} (albedo) textures. Following nerfactor, we note that the scale factor between material and lighting is indeterminable and therefore normalize albedo images by the average intensity of the reference before measuring errors.
Our representation can be directly deployed in the vast collection of 3D content generation tools available for triangle meshes. This greatly facilitates scene editing, which is still very challenging for neural volumetric representations [70]. We show advanced scene editing examples in Figure 8, where we add our reconstructed models from the NeRFactor dataset to the Cornell box and use them in a softbody simulation. Note that our models receive scene illumination, cast accurate shadows, and robustly act as colliders for virtual objects. In Figure 1, and the supplemental video, we show another example, where an object is reconstructed from real-world photographs and then used as a collider for a virtual cloth object. The combined scene is then rendered using our extracted environment light. Note that shading of the virtual object looks plausible given the reference photo. We also show material editing on the same example. 4.2. View Interpolation: Synthetic datasets We show results for the NeRF realistic synthetic image dataset in Table 3 and a visual example of the Materials scene in Figure 9. Per-scene results and visual examples are included in our supplemental material, where we also include Chamfer loss on extracted meshes. Our method consistently performs on par with NeRF, with better quality in some scenes. The margins are smaller for perceptually based image metrics (SSIM and LPIPS). We speculate that density-based volume approaches can more efficiently minimize PSNR than our opaque meshes. However, the effect of slightly moving a silhouette edge will not be as detrimental to a perceptual metric. The Drums and Ship scenes are failure cases for our method. We assume mostly direct lighting, with no significant global illumination effects, and these scenes contain both significant intra-scene reflections, refractions, and caustics. Interestingly, while material reconstruction suffers, we still note high quality results for view interpolation.
Figure 9. Our result on the materials scene, reconstructed from 100 images from the nerf synthetic dataset. 
Table 3. Average results for the eight scenes in the nerf realistic synthetic dataset. Each scene consists of 100 training images, and 200 test images, with masks and known camera poses. Results from nerf are taken from Table 4 of the nerf paper [39]. Physg and mip-nerf were retrained using public source code.
Table 4. View interpolation error metrics on nerfactor's variant of the nerf synthetic dataset. The reported image metrics are the arithmetic mean over the eight validation images of all four scenes.
Given that we factorize into explicit shape, materials and lighting, we have slightly lower quality on novel view synthesis than methods specialized for view-interpolation. To put this in context, in Table 4 we compare our aproach against NeRFactor, which performs a similar factorization, and our approach. We observe a 4.21 dB PSNR image quality reduction for NeRFactor compared to the NeRF baseline. This is consistent with NeRD [3] which do not provide source code but report a 4.17 dB quality drop for their factorized representation on a subset of the NeRF synthetic dataset. In contrast, our quality is significantly higher, while still providing the flexibility of a factorized representation. Real-world datasets NeRD [3] provides a small dataset of real-world photo scans with auto-generated (inaccurate) coverage masks and diverse camera placement. Visual and quantitative results are shown in Figure 10, where we have masked out the background for the reference object. Due to inconsistencies in the dataset, both NeRF and NeRD struggle to find sharp geometry borders with transparent “floaters” and holes. In contrast, we get sharp silhouettes and a significant boost in image quality. The results reported for NeRD are for their volumetric representation. Note that NeRD can generate output meshes as a post-processing step, but at a significant quality loss (visual comparison included in our supplemental material).
Figure 10. Reconstruction from photographs (datasets from nerd), comparing our results with nerd and nerf. Images in the two rightmost colums were provided by the nerd authors. We score higher in terms of image metrics, most likely due to our mesh representation enforcing opaque geometry, where competing algorithms rely on volumetric opacity. Despite inconsistencies in camera poses and masks, our results remain sharp while nerf and nerd suffer from floating or missing geometry. 
4.3. Comparing Spherical Gaussians and Split Sum: In Figure 11, we compare our differentiable split sum environment lighting approximation, from Section 3.3 against the commonly used spherical Gaussian (SG) model. Split sum captures the lighting much more faithfully across all frequencies, while still having a lower runtime cost. In our implementation, we observe a 5 × reduction of optimization time compared to SG with 128 lobes. At inference, evaluating the split sum approximation is extremely fast, requiring just two texture lookups. 

SECTION 5. Limitations and Conclusions: Our main limitation is the simplified shading model, not accounting for global illumination or shadows. This choice is intentional to accelerate optimization, but is a limiting factor for material extraction and relighting. With the current progress in differentiable path tracing, we look forward to this limitation being lifted in future work. We additionally rely on alpha masks to separate foreground from background. While our method seems quite robust to corrupted masks, it would be beneficial to further incorporate this step into the system. Other limitations include the static lighting assumption, not optimizing camera poses, and high compute resource and memory consumption during training. Apart from deepfakes, which are common to all scene reconstruction methods, we are not aware of and do not fore-see nefarious use cases of our method.
Figure 11. Environment lighting approximated with spherical gaussians using 128 lobes vs. Split sum. The training set consists of 256 path traced images with monte carlo sampled environment lighting using a high resolution hdr probe. We assume known geometry and optimize materials and lighting using identical settings for both methods. Reported image metrics are the arithmetic mean of the 16 (novel) views in the test set. Note that the split sum approximation is able to capture high frequency lighting. Probe from poly haven [67]. 
In summary, we show results on par with state-of-the-art for view synthesis and material factorization, while directly optimizing an explicit representation: triangle meshes with materials and environment lighting. Our representation is, by design, directly compatible with modern 3D engines and modeling tools, which enables a vast array of applications and simplifies artist workflows. We perform end-to-end optimization driven by appearance of the rendered model, while previous work often sidestep the error from mesh extraction through Marching Cubes. Our method can be applied as an appearance-aware converter from a (neural) volumetric or SDF representation to triangular 3D models with materials, complementing many recent techniques.