SECTION 1. Introduction: The resolution and quality of images produced by generative methods, especially generative adversarial networks (GAN) [13], are improving rapidly [20], [26], [4]. The current state-of-the-art method for high-resolution image synthesis is StyleGAN [21], which has been shown to work reliably on a variety of datasets. Our work focuses on fixing its characteristic artifacts and improving the result quality further. The distinguishing feature of StyleGAN [21] is its unconventional generator architecture. Instead of feeding the input latent code \mathbf{z}\in \mathcal{Z} only to the beginning of a the network, the mapping network f first transforms it to an intermediate latent code \mathbf{w}\in \mathcal{W}. Affine transforms then produce styles that control the layers of the synthesis network g via adaptive instance normalization (AdaIN) [18], [8], [11], [7]. Additionally, stochastic variation is facilitated by providing additional random noise maps to the synthesis network. It has been demonstrated [21], [33] that this design allows the intermediate latent space \mathcal{W} to be much less entangled than the input latent space \mathcal{Z}. In this paper, we focus all analysis solely on \mathcal{W}, as it is the relevant latent space from the synthesis network's point of view. Many observers have noticed characteristic artifacts in images generated by StyleGAN [3]. We identify two causes for these artifacts, and describe changes in architecture and training methods that eliminate them. First, we investigate the origin of common blob-like artifacts, and find that the generator creates them to circumvent a design flaw in its architecture. In Section 2, we redesign the normalization used in the generator, which removes the artifacts. Second, we analyze artifacts related to progressive growing [20] that has been highly successful in stabilizing high-resolution GAN training. We propose an alternative design that achieves the same goal — training starts by focusing on low-resolution images and then progressively shifts focus to higher and higher resolutions — without changing the network topology during training. This new design also allows us to reason about the effective resolution of the generated images, which turns out to be lower than expected, motivating a capacity increase (Section 4). Quantitative analysis of the quality of images produced using generative methods continues to be a challenging topic. Fréchet inception distance (FID) [17] measures differences in the density of two distributions in the high-dimensional feature space of an InceptionV3 classifier [34]. Precision and Recall (P&R) [31], [22] provide additional visibility by explicitly quantifying the percentage of generated images that are similar to training data and the percentage of training data that can be generated, respectively. We use these metrics to quantify the improvements. Both FID and P&R are based on classifier networks that have recently been shown to focus on textures rather than shapes [10], and consequently, the metrics do not accurately capture all aspects of image quality. We observe that the perceptual path length (PPL) metric [21], originally introduced as a method for estimating the quality of latent space interpolations, correlates with consistency and stability of shapes. Based on this, we regularize the synthesis network to favor smooth mappings (Section 3) and achieve a clear improvement in quality. To counter its computational expense, we also propose executing all regularizations less frequently, observing that this can be done without compromising effectiveness.
Figure 1. Instance normalization causes water droplet -like artifacts in stylegan images. These are not always obvious in the generated images, but if we look at the activations inside the generator network, the problem is always there, in all feature maps starting from the 64x64 resolution. It is a systemic problem that plagues all stylegan images. 
Finally, we find that projection of images to the latent space \mathcal{W} works significantly better with the new, pathlength regularized StyleGAN2 generator than with the original StyleGAN. This makes it easier to attribute a generated image to its source (Section 5). Our implementation and trained models are available at https://github.com/NVlabs/stylegan2 

SECTION 2. Removing Normalization Artifacts: We begin by observing that most images generated by StyleGAN exhibit characteristic blob-shaped artifacts that resemble water droplets. As shown in Figure 1, even when the droplet may not be obvious in the final image, it is present in the intermediate feature maps of the generator.1 The anomaly starts to appear around 64×64 resolution, is present in all feature maps, and becomes progressively stronger at higher resolutions. The existence of such a consistent artifact is puzzling, as the discriminator should be able to detect it. We pinpoint the problem to the AdaIN operation that normalizes the mean and variance of each feature map separately, thereby potentially destroying any information found in the magnitudes of the features relative to each other. We hypothesize that the droplet artifact is a result of the generator intentionally sneaking signal strength information past instance normalization: by creating a strong, localized spike that dominates the statistics, the generator can effectively scale the signal as it likes elsewhere. Our hypothesis is supported by the finding that when the normalization step is removed from the generator, as detailed below, the droplet artifacts disappear completely. 2.1. Generator Architecture Revisited: We will first revise several details of the StyleGAN generator to better facilitate our redesigned normalization. These changes have either a neutral or small positive effect on their own in terms of quality metrics. Figure 2a shows the original StyleGAN synthesis network g [21], and in Figure 2b we expand the diagram to full detail by showing the weights and biases and breaking the AdaIN operation to its two constituent parts: normalization and modulation. This allows us to re-draw the conceptual gray boxes so that each box indicates the part of the network where one style is active (i.e., “style block”). Interestingly, the original StyleGAN applies bias and noise within the style block, causing their relative impact to be inversely proportional to the current style's magnitudes. We observe that more predictable results are obtained by moving these operations outside the style block, where they operate on normalized data. Furthermore, we notice that after this change it is sufficient for the normalization and modulation to operate on the standard deviation alone (i.e., the mean is not needed). The application of bias, noise, and normalization to the constant input can also be safely removed without observable drawbacks. This variant is shown in Figure 2c, and serves as a starting point for our redesigned normalization. 2.2. Instance Normalization Revisited: One of the main strengths of StyleGAN is the ability to control the generated images via style mixing, i.e., by feeding a different latent w to different layers at inference time. In practice, style modulation may amplify certain feature maps by an order of magnitude or more. For style mixing to work, we must explicitly counteract this amplification on a per-sample basis-otherwise the subsequent layers would not be able to operate on the data in a meaningful way. If we were willing to sacrifice scale-specific controls (see video), we could simply remove the normalization, thus removing the artifacts and also improving FID slightly [22]. We will now propose a better alternative that removes the artifacts while retaining full controllability. The main idea is to base normalization on the expected statistics of the incoming feature maps, but without explicit forcing.
Figure 2. We redesign the architecture of the StyleGAN synthesis network. (a) The original StyleGAN, where A denotes a learned affine transform from \mathcal{W} that produces a style and B is a noise broadcast operation. (b) The same diagram with full detail. Here we have broken the adain to explicit normalization followed by modulation, both operating on the mean and standard deviation per feature map. We have also annotated the learned weights (w), biases (b), and constant input (c), and redrawn the gray boxes so that one style is active per box. The activation function (leaky relu) is always applied right after adding the bias. (c) We make several changes to the original architecture that are justified in the main text. We remove some redundant operations at the beginning, move the addition of b and B to be outside active area of a style, and adjust only the standard deviation per feature map. (d) The revised architecture enables us to replace instance normalization with a “demodulation” operation, which we apply to the weights associated with each convolution layer. 
Recall that a style block in Figure 2c consists of modulation, convolution, and normalization. Let us start by considering the effect of a modulation followed by a convolution. The modulation scales each input feature map of the convolution based on the incoming style, which can alternatively be implemented by scaling the convolution weights:\begin{equation*}
w_{ijk}^{\prime}=s_{i}\cdot w_{ijk},
\tag{1}
\end{equation*}View Source\begin{equation*}
w_{ijk}^{\prime}=s_{i}\cdot w_{ijk},
\tag{1}
\end{equation*} where w and w^{\prime} are the original and modulated weights, respectively, s_{i} is the scale corresponding to the ith input feature map, and j and k enumerate the output feature maps and spatial footprint of the convolution, respectively. Now, the purpose of instance normalization is to essentially remove the effect of s from the statistics of the con-volution's output feature maps. We observe that this goal can be achieved more directly. Let us assume that the input activations are i.i. \mathrm{d}. random variables with unit standard deviation. After modulation and convolution, the output activations have standard deviation of\begin{equation*}
\sigma_{j}=\sqrt{\sum_{i,k}w_{ijk^{2}}'},
\tag{2}
\end{equation*}View Source\begin{equation*}
\sigma_{j}=\sqrt{\sum_{i,k}w_{ijk^{2}}'},
\tag{2}
\end{equation*} i.e., the outputs are scaled by the L_{2} norm of the corresponding weights. The subsequent normalization aims to restore the outputs back to unit standard deviation. Based on Equation 2, this is achieved if we scale (“demodulate”) each output feature map j by 1/\sigma_{j}. Alternatively, we can again bake this into the convolution weights:\begin{equation*}
w_{ijk}^{\prime\prime}=w_{ijk}^{\prime}/\sqrt{\sum_{i,k} w_{ijk}^{\prime}{}^{2}+\epsilon},
\tag{3}
\end{equation*}View Source\begin{equation*}
w_{ijk}^{\prime\prime}=w_{ijk}^{\prime}/\sqrt{\sum_{i,k} w_{ijk}^{\prime}{}^{2}+\epsilon},
\tag{3}
\end{equation*} where \epsilon is a small constant to avoid numerical issues. We have now baked the entire style block to a single convolution layer whose weights are adjusted based on s using Equations 1 and 3 (Figure 2d). Compared to instance normalization, our demodulation technique is weaker because it is based on statistical assumptions about the signal instead of actual contents of the feature maps. Similar statistical analysis has been extensively used in modern network initializers [12], [16], but we are not aware of it being previously used as a replacement for data-dependent normalization. Our demodulation is also related to weight normalization [32] that performs the same calculation as a part of reparameterizing the weight tensor. Prior work has identified weight normalization as beneficial in the context of GAN training [38]. Our new design removes the characteristic artifacts (Figure 3) while retaining full controllability, as demonstrated in the accompanying video. FID remains largely unaffected (Table 1, rows A, B), but there is a notable shift from precision to recall. We argue that this is generally desirable, since recall can be traded into precision via truncation, whereas the opposite is not true [22]. In practice our design can be implemented efficiently using grouped convolutions, as detailed in Appendix B. To avoid having to account for the activation function in Equation 3, we scale our activation functions so that they retain the expected signal variance.
Figure 3. Replacing normalization with demodulation removes the characteristic artifacts from images and activations. 
Table 1. Main results. For each training run, we selected the training snapshot with the lowest fid. We computed each metric 10 times with different random seeds and report their average. Path length corresponds to the PPL metric, computed based on path endpoints in \mathcal{W} [21], without the central crop used by karras et al. [21]. The FFHQ dataset contains 70k images, and the discriminator saw 25M images during training. For LSUN CAR the numbers were 893k and 57m. ↑ indicates that higher is better, and ↓ that lower is better.


SECTION 3. Image Quality and Generator Smoothness: While GAN metrics such as FID or Precision and Recall (P&R) successfully capture many aspects of the generator, they continue to have somewhat of a blind spot for image quality. For an example, refer to Figures 3 and 4 in the Supplement that contrast generators with identical FID and P&R scores but markedly different overall quality.2
Figure 4. Connection between perceptual path length and image quality using baseline StyleGAN (config a) with LSUN cat. (a) Random examples with low PPL (≤ 10th percentile). (b) Examples with high PPL (≥ 90th percentile). There is a clear correlation between PPL scores and semantic consistency of the images. 
Figure 5. (a) Distribution of PPL scores of individual images generated using baseline StyleGAN (config a) with LSUN cat (fid = 8.53, ppl = 924). The percentile ranges corresponding to Figure 4 are highlighted in orange. (b) StyleGAN2 (config f) improves the PPL distribution considerably (showing a snapshot with the same FID = 8.53, PPL = 387). 
We observe a correlation between perceived image quality and perceptual path length (PPL) [21], a metric that was originally introduced for quantifying the smoothness of the mapping from a latent space to the output image by measuring average LPIPS distances [44] between generated images under small perturbations in latent space. Again consulting Figures 3 and 4 in the Supplement, a smaller PPL (smoother generator mapping) appears to correlate with higher over-all image quality, whereas other metrics are blind to the change. Figure 4 examines this correlation more closely through per-image PPL scores on LSUN Cat, computed by sampling the latent space around \mathbf{w}\sim f(\mathbf{z}). Low scores are indeed indicative of high-quality images, and vice versa. Figure 5a shows the corresponding histogram and reveals the long tail of the distribution. The overall PPL for the model is simply the expected value of these per-image PPL scores. We always compute PPL for the entire image, as opposed to Karras et al. [21] who use a smaller central crop. It is not immediately obvious why a low PPL should correlate with image quality. We hypothesize that during training, as the discriminator penalizes broken images, the most direct way for the generator to improve is to effectively stretch the region of latent space that yields good images. This would lead to the low-quality images being squeezed into small latent space regions of rapid change. While this improves the average output quality in the short term, the accumulating distortions impair the training dynamics and consequently the final image quality. Clearly, we cannot simply encourage minimal PPL since that would guide the generator toward a degenerate solution with zero recall. Instead, we will describe a new regularizer that aims for a smoother generator mapping without this drawback. As the resulting regularization term is somewhat expensive to compute, we first describe a general optimization that applies to any regularization technique. 3.1. Lazy Regularization: Typically the main loss function (e.g., logistic loss [13]) and regularization terms (e.g., R1[25]) are written as a single expression and are thus optimized simultaneously. We observe that the regularization terms can be computed less frequently than the main loss function, thus greatly diminishing their computational cost and the overall memory usage. Table 1, row c shows that no harm is caused when R_{1} regularization is performed only once every 16 minibatches, and we adopt the same strategy for our new regularizer as well. Appendix B gives implementation details. 3.2. Path Length Regularization: We would like to encourage that a fixed-size step in \mathcal{W} results in a non-zero, fixed-magnitude change in the image. We can measure the deviation from this ideal empirically by stepping into random directions in the image space and observing the corresponding \mathbf{w} gradients. These gradients should have close to an equal length regardless of \mathbf{w} or the image-space direction, indicating that the mapping from the latent space to image space is well-conditioned [28]. At a single \mathbf{w}\in \mathcal{W}, the local metric scaling properties of the generator mapping q(\mathbf{w}):\mathcal{W}\mapsto \mathcal{Y} are captured by the Jacobian matrix \mathbf{J}_{\mathrm{w}}=\partial g(\mathbf{w})/\partial \mathbf{w}. Motivated by the desire to preserve the expected lengths of vectors regardless of the direction, we formulate our regularizer as\begin{equation*}
\mathbb{E}_{\mathrm{w},\mathrm{y}\sim \mathcal{N}(0,\mathbf{I})}(\Vert \mathbf{J}_{\mathbf{w}}^{T}\mathbf{y}\Vert_{2}-a)^{2},
\tag{4}
\end{equation*}View Source\begin{equation*}
\mathbb{E}_{\mathrm{w},\mathrm{y}\sim \mathcal{N}(0,\mathbf{I})}(\Vert \mathbf{J}_{\mathbf{w}}^{T}\mathbf{y}\Vert_{2}-a)^{2},
\tag{4}
\end{equation*} where \mathbf{y} are random images with normally distributed pixel intensities, and \mathbf{w}\sim f(\mathbf{z}), where \mathbf{z} are normally distributed. We show in Appendix C that, in high dimensions, this prior is minimized when \mathbf{J}_{\mathbf{w}} is orthogonal (up to a global scale) at any \mathbf{w}. An orthogonal matrix preserves lengths and introduces no squeezing along any dimension. To avoid explicit computation of the Jacobian matrix, we use the identity \mathbf{J}_{\mathbf{w}}^{T}\mathbf{y}=\nabla_{\mathbf{w}}(g(\mathbf{w})\cdot \mathbf{y}), which is efficiently computable using standard backpropagation [5]. The constant a is set dynamically during optimization as the long-running exponential moving average of the lengths \Vert \mathbf{J}_{\mathbf{w}}^{T}\mathbf{y}\Vert_{2}, allowing the optimization to find a suitable global scale by itself. Our regularizer is closely related to the Jacobian clamping regularizer presented by Odena et al. [28]. Practical differences include that we compute the products \mathbf{J}_{\mathbf{w}}^{T}\mathbf{y} analytically whereas they use finite differences for estimating \mathbf{J}_{\mathbf{w}}\boldsymbol{\delta} with \mathcal{Z}\ni\boldsymbol{\delta}\sim \mathcal{N}(0,\ \mathbf{I}). It should be noted that spectral normalization [26] of the generator [40] only constrains the largest singular value, posing no constraints on the others and hence not necessarily leading to better conditioning. We find that enabling spectral normalization in addition to our contributions— or instead of them — invariably compromises FID, as detailed in Appendix E. In practice, we notice that path length regularization leads to more reliable and consistently behaving models, making architecture exploration easier. We also observe that the smoother generator is significantly easier to invert (Section 5). Figure 5b shows that path length regularization clearly tightens the distribution of per-image PPL scores, without pushing the mode to zero. However, Table 1, row \mathrm{D} points toward a tradeoff between FID and PPL in datasets that are less structured than FFHQ. 

SECTION 4. Progressive Growing Revisited: Progressive growing [20] has been very successful in stabilizing high-resolution image synthesis, but it causes its own characteristic artifacts. The key issue is that the progressively grown generator appears to have a strong location preference for details; the accompanying video shows that when features like teeth or eyes should move smoothly over the image, they may instead remain stuck in place before jumping to the next preferred location. Figure 6 shows a related artifact. We believe the problem is that in progressive growing each resolution serves momentarily as the output resolution, forcing it to generate maximal frequency details, which then leads to the trained network to have excessively high frequencies in the intermediate layers, compromising shift invariance [43]. Appendix A shows an example. These issues prompt us to search for an alternative formulation that would retain the benefits of progressive growing without the drawbacks.
Figure 6. Progressive growing leads to “phase” artifacts. In this example the teeth do not follow the pose but stay aligned to the camera, as indicated by the blue line. 
Figure 7. Three generator (above the dashed line) and discriminator architectures. Up and down denote bilinear up and downsampling, respectively. In residual networks these also include 1×1 convolutions to adjust the number of feature maps. Trgb and frgb convert between RGB and high-dimensional per-pixel data. Architectures used in configs E and F are shown in green. 
4.1. Alternative Network Architectures: While StyleGAN uses simple feedforward designs in the generator (synthesis network) and discriminator, there is a vast body of work dedicated to the study of better network architectures. Skip connections [29], [19], residual networks [15], [14], [26], and hierarchical methods [6], [41], [42] have proven highly successful also in the context of generative methods. As such, we decided to re-evaluate the network design of StyleGAN and search for an architecture that produces high-quality images without progressive growing. Figure 7a shows MSG-GAN [19], which connects the matching resolutions of the generator and discriminator using multiple skip connections. The MSG-GAN generator is modified to output a mipmap [37] instead of an image, and a similar representation is computed for each real im-
age as well. In Figure 7b we simplify this design by upsampling and summing the contributions of RGB outputs corresponding to different resolutions. In the discriminator, we similarly provide the downsampled image to each resolution block of the discriminator. We use bilinear filtering in all up and downsampling operations. In Figure 7c we further modify the design to use residual connections.3 This design is similar to LAPGAN [6] without the per-resolution discriminators employed by Denton et al.
Table 2. Comparison of generator and discriminator architectures without progressive growing. The combination of generator with output skips and residual discriminator corresponds to configuration E in the main result table.
Table 2 compares three generator and three discriminator architectures: original feedforward networks as used in StyleGAN, skip connections, and residual networks, all trained without progressive growing. FID and PPL are provided for each of the 9 combinations. We can see two broad trends: skip connections in the generator drastically improve PPL in all configurations, and a residual discriminator network is clearly beneficial for FID. The latter is perhaps not surprising since the structure of discriminator resembles classifiers where residual architectures are known to be helpful. However, a residual architecture was harmful in the generator—the lone exception was FID in LSUN Car when both networks were residual. For the rest of the paper we use a skip generator and a residual discriminator, without progressive growing. This corresponds to configuration E in Table 1, and it significantly improves FID and PPL. 4.2. Resolution Usage: The key aspect of progressive growing, which we would like to preserve, is that the generator will initially focus on low-resolution features and then slowly shift its attention to finer details. The architectures in Figure 7 make it possible for the generator to first output low resolution images that are not affected by the higher-resolution layers in a significant way, and later shift the focus to the higher-resolution layers as the training proceeds. Since this is not enforced in any way, the generator will do it only if it is beneficial. To analyze the behavior in practice, we need to quantify how strongly the generator relies on particular resolutions over the course of training.
Figure 8. Contribution of each resolution to the output of the generator as a function of training time. The vertical axis shows a breakdown of the relative standard deviations of different resolutions, and the horizontal axis corresponds to training progress, measured in millions of training images shown to the discriminator. We can see that in the beginning the network focuses on low-resolution images and progressively shifts its focus on larger resolutions as training progresses. In (a) the generator basically outputs a 5122 image with some minor sharpening for 10242, while in (b) the larger network focuses more on the high-resolution details. 
Since the skip generator (Figure 7b) forms the image by explicitly summing RGB values from multiple resolutions, we can estimate the relative importance of the corresponding layers by measuring how much they contribute to the final image. In Figure 8a, we plot the standard deviation of the pixel values produced by each tRGB layer as a function of training time. We calculate the standard deviations over 1024 random samples of w and normalize the values so that they sum to 100%. At the start of training, we can see that the new skip generator behaves similar to progressive growing-now achieved without changing the network topology. It would thus be reasonable to expect the highest resolution to dominate towards the end of the training. The plot, however, shows that this fails to happen in practice, which indicates that the generator may not be able to “fully utilize” the target resolution. To verify this, we inspected the generated images manually and noticed that they generally lack some of the pixel-level detail that is present in the training data—the images could be described as being sharpened versions of 5122 images instead of true 10242 images. This leads us to hypothesize that there is a capacity problem in our networks, which we test by doubling the number of feature maps in the highest-resolution layers of both networks.4 This brings the behavior more in line with expectations: Figure 8b shows a significant increase in the contribution of the highest-resolution layers, and Table 1, row F shows that FID and Recall improve markedly. The last row shows that baseline StyleGAN also benefits from additional capacity, but its quality remains far below StyleGAN2.
Table 3. Improvement in LSUN datasets measured using FID and ppl. We trained CAR for 57M images, CAT for 88M, CHURCH for 48M, and HORSE for 100M images.
Table 3 compares StyleGAN and StyleGAN2 in four LSUN categories, again showing clear improvements in FID and significant advances in PPL. It is possible that further increases in the size could provide additional benefits. 

SECTION 5. Projection of Images to Latent Space: Inverting the synthesis network g is an interesting problem that has many applications. Manipulating a given image in the latent feature space requires finding a matching latent code w for it first. Previous research [1], [9] suggests that instead of finding a common latent code w, the results improve if a separate w is chosen for each layer of the generator. The same approach was used in an early encoder implementation [27]. While extending the latent space in this fashion finds a closer match to a given image, it also enables projecting arbitrary images that should have no latent representation. Instead, we concentrate on finding latent codes in the original, unextended latent space, as these correspond to images that the generator could have produced. Our projection method differs from previous methods in two ways. First, we add ramped-down noise to the latent code during optimization in order to explore the latent space more comprehensively. Second, we also optimize the stochastic noise inputs of the StyleGAN generator, regularizing them to ensure they do not end up carrying coherent signal. The regularization is based on enforcing the autocorrelation coefficients of the noise maps to match those of unit Gaussian noise over multiple scales. Details of our projection method can be found in Appendix D. 5.1. Attribution of Generated Images: Detection of manipulated or generated images is a very important task. At present, classifier-based methods can quite reliably detect generated images, regardless of their exact origin [24], [39], [35], [45], [36]. However, given the rapid pace of progress in generative methods, this may not be a lasting situation. Besides general detection of fake images, we may also consider a more limited form of the problem: being able to attribute a fake image to its specific source [2]. With StyleGAN, this amounts to checking if there exists a \mathbf{w}\in \mathcal{W} that re-synthesis the image in question.
Figure 9. Example images and their projected and re-synthesized counterparts. For each configuration, top row shows the target images and bottom row shows the synthesis of the corresponding projected latent vector and noise inputs. With the baseline StyleGAN, projection often finds a reasonably close match for generated images, but especially the backgrounds differ from the originals. The images generated using StyleGAN2 can be projected almost perfectly back into generator inputs, while projected real images (from the training set) show clear differences to the originals, as expected. All tests were done using the same projection method and hyperparameters. 
Figure 10. LPIPS distance histograms between original and projected images for generated (blue) and real images (orange). Despite the higher image quality of our improved generator, it is much easier to project the generated images into its latent space w. The same projection method was used in all cases. 
We measure how well the projection succeeds by computing the LPIPS [44] distance between original and resynthesized image as D_{\mathrm{LPIPS}}[\pmb{x}, g(\tilde{g}^{-1}(\pmb{x}))], where x is the image being analyzed and \tilde{g}^{-1} denotes the approximate projection operation. Figure 10 shows histograms of these distances for LSUN Car and FFHQ datasets using the original StyleGAN and StyleGAN2, and Figure 9 shows example projections. The images generated using StyleGAN2 can be projected into \mathcal{W} so well that they can be almost unambiguously attributed to the generating network. However, with the original StyleGAN, even though it should technically be possible to find a matching latent code, it appears that the mapping from \mathcal{W} to images is too complex for this to succeed reliably in practice. We find it encouraging that StyleGAN2 makes source attribution easier even though the image quality has improved significantly. 

SECTION 6. Conclusions and Future Work: We have identified and fixed several image quality issues in StyleGAN, improving the quality further and considerably advancing the state of the art in several datasets. In some cases the improvements are more clearly seen in motion, as demonstrated in the accompanying video. Appendix A includes further examples of results obtainable using our method. Despite the improved quality, StyleGAN2 makes it easier to attribute a generated image to its source. Training performance has also improved. At 10242 resolution, the original StyleGAN (config A in Table 1) trains at 37 images per second on NVIDIA DGX-1 with 8 Tesla V100 GPUs, while our config E trains 40% faster at 61 img/s. Most of the speedup comes from simplified dataflow due to weight demodulation, lazy regularization, and code optimizations. StyleGAN2 (config F, larger networks) trains at 31 img/s, and is thus only slightly more expensive to train than original StyleGAN. Its total training time was 9 days for FFHQ and 13 days for LSUN Car. The entire project, including all exploration, consumed 132 MWh of electricity, of which 0.68 MWh went into training the final FFHQ model. In total, we used about 51 single-GPU years of computation (Volta class GPU). A more detailed discussion is available in Appendix F. In the future, it could be fruitful to study further improvements to the path length regularization, e.g., by replacing the pixel-space L2 distance with a data-driven feature-space metric. Considering the practical deployment of GANs, we feel that it will be important to find new ways to reduce the training data requirements. This is especially crucial in applications where it is infeasible to acquire tens of thousands of training samples, and with datasets that include a lot of intrinsic variation. 
ACKNOWLEDGEMENTS: We thank Ming-Yu Liu for an early review, Timo Viitanen for help with the public release, David Luebke for in-depth discussions and helpful comments, and Tero Kuosmanen for technical support with the compute infrastructure.