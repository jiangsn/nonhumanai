SECTION 1. Introduction: Neural text-to-speech (TTS) models, especially autoregressive TTS models, produce naturally sounding speech for in-domain text [1]–​[3]. However, these models can suffer from pronunciation issues such as missing and repeated words for out-of-domain text, especially in long utterances. A typical neural TTS model consists of an encoder that maps text inputs to hidden states, a decoder that generates mel-spectograms or waveforms from the hidden states, and an alignment mechanism or a duration source that maps the encoder states to decoder inputs [1]–​[7]. Autoregressive TTS models rely on the attention mechanism [8],[9] to align text and speech, typically using a content based attention mechanism [1], [3]. Although recent works have improved alignments by using both content and location sensitive attention [2], such models still suffer from alignment problems on long utterances [6]. In contrast, parallel (non-autoregressive) TTS models factor out durations from the decoding process, thereby requiring durations as input for each token. These models generally rely on external aligners [4] like the Montreal Forced Aligner (MFA) [10], or on durations extracted from a pre-trained autoregressive model(forced aligner) [5],[7],[11] like Tacotron 2 [2]. In addition to the dependency on external alignments, these models can suffer from poor training efficiency, require carefully engineered training schedules to prevent unstable learning, and may be difficult to extend to new languages either because pre-existing aligners are unavailable or their output does not exactly fit the desired format. Ideally, we would like the alignment to be trained end-to-end as part of the TTS model to significantly simplify the training pipeline. We would also like the alignments to converge rapidly as the rest of the TTS pipeline depends on it. Most importantly, the output quality should be better (at least no worse) than if we were to train on alignments provided by external sources. This work leverages the alignment framework proposed in RAD-TTS [12] to simplify alignment learning in several TTS models1. We demonstrate its ability to convert all TTS models to a simpler end-to-end alignment pipeline with better convergence rates and improved robustness to long utterances. We improve prior work on alignments in autoregressive TTS systems [1]–​[3] by adding a constraint that directly maximizes the likelihood of text given speech mel-spectrograms. We demonstrate that this approach can also be used to learn alignments online in parallel TTS models [4],[7],[12], eliminating the need for external aligners. In addition, we further examine the effect of a simple static alignment prior for guiding alignment attention learning [12],[13]. In summary, our results2 show that TTS models trained with our alignment learning framework have fewer repeated and missing words during inference, improved stability on long sequence synthesis, and improved overall speech quality based on human evaluation. 

SECTION 2. Alignment Learning Framework: We extend the alignment learning approach proposed in RAD-TTS [12] to be more broadly applicable to various TTS models, especially autoregressive models. Our alignment framework is presented in Figure 1. It takes the encoded text input Φ∈RCtxt×N and aligns it to mel-spectrograms X∈RCmel×T where T is number of mel frames and N is the text length. 2.1. Unsupervised alignment learning objective: To learn the alignment between mel-spectrograms(X) and text(Φ), we use the alignment learning objective proposed in RAD-TTS [12]. This objective maximizes the likelihood of text given mel-spectrograms using the forward-sum algorithm used in Hidden Markov Models [14]. In our formulation, we constrain the alignment between text and speech to be monotonic, in order to avoid missing or repeating tokens. The following equation summarizes the conditional likelihood of text:
P(S(Φ)|X;θ)=∑s∈S(Φ)∏t=1TP(st|xt;θ)(1)View Source\begin{equation*}P(S(\Phi )|X;\theta ) = \sum\limits_{{\mathbf{s}} \in S(\Phi )} {\prod\limits_{t = 1}^T P } \left( {{s_t}|{x_t};\theta } \right)\tag{1}\end{equation*}
where s is a specific alignment between mels and text (eg: s1 = ϕ1, s2 = ϕ1, s3 = ϕ2, …, sT = ϕN), S(Φ) is the set of all possible valid monotonic alignments, P (st|xt) is the likelihood of a specific text token st = ϕi aligned for mel frame xt at timestep t. It is important to note that the above formulation of the alignment learning objective is applicable to both autoregressive and parallel models. We define the forward sum objective that maximizes (1) as LForwardSum. Following RAD-TTS, we use the efficient, off-the-shelf CTC [15] implementation from PyTorch to compute this objective. Fig. 1: 
Overview of the alignment learning: autoregressive models use sequential attention mechanism to generate alignments between text and mels. Parallel models encode text and mel using 1D convolutions and use pairwise L2 distance to compute alignments. Alignments represent the distribution P (st|xt) used in alignment objective (Eq. 1). 
2.2. Autoregressive TTS Models: Autoregressive TTS models typically use a sequential attention mechanism to learn online alignments. TTS models such as Tacotron [1] and Flowtron [3] use a content based attention mechanism that relies only on decoder inputs and the current attention hidden state to compute an attention map between encoder and decoder steps. Other autoregressive models use a location relative attention mechanism [16] to promote forward movement of alignments [2]. Although alignment learning in these autoregressive models is tightly coupled with the decoder and can be learned with the mel-spectrogram reconstruction objective, it has been observed that the likelihood of a misstep in the alignment increases with the length of the utterance. This results in catastrophic failure on long sequences and out-of-domain text [17]. The application of the unsupervised objective described in Sec 2.1 improves both convergence speed during training and robustness during inference. Our autoregressive setup uses the standard stateful content based attention mechanism for Flowtron [3] and a hybrid attention mechanism with both content and location based features for Tacotron2 [2]. The location sensitive term (Eq. 4) uses features computed from attention weights at previous decoder timesteps. We use the Tacotron2 encoder to obtain the sequence of encoded text representations (ϕenci)Ni=1 and an attention RNN to produce a sequence of hidden states ht. A simple architecture is used to compute the alignment energies et,i for text token si at timestep t for mel xt using the tanh attention [9]. The attention weights are computed with soft-max over the text domain using the alignment energies. The following equations summarize the attention mechanism:
(ht)Tt=1=RNN(ht−1,xt−1,ct−1)ct=∑αt,iϕencift=F(αt−1)et,i=−vTtanh(Wht+Vϕenci+Uft,i)P(st=ϕi|xt)=αt,i=Softmax(−et)i,(2)(3)(4)(5)(6)View Source\begin{align*} & \left( {{h_t}} \right)_{t = 1}^T = \operatorname{RNN} \left( {{h_{t - 1}},{x_{t - 1}},{c_{t - 1}}} \right)\tag{2} \\ & {c_t} = \sum {{\alpha _{t,i}}} \phi _i^{enc}\tag{3} \\ & {f_t} = F\left( {{\alpha _{t - 1}}} \right)\tag{4} \\ & {e_{t,i}} = - {v^T}\tanh \left( {W{h_t} + V\phi _i^{enc} + U{f_{t,i}}} \right)\tag{5} \\ & P\left( {{s_t} = {\phi _i}|{x_t}} \right) = {\alpha _{t,i}} = Soft\max {\left( { - {e_t}} \right)_i},\tag{6}\end{align*}
where ft is the location relative term for location sensitive attention F (cumulative attention from [2] using a concatenation of the attention weights from the previous timestep and the cumulative attention weights). The attention weights model the distribution P (st = ϕi|xt), which is exactly the right-most term in Eq 1, and we incorporate it as the alignment loss:
Lalign=LForwardSum.(7)View Source\begin{equation*}{\mathcal{L}_{align\,}} = {\mathcal{L}_{ForwardSum}}.\tag{7}\end{equation*} 2.3. Parallel TTS Models: As parallel TTS models have durations factored out from the decoder, the alignment learning module can be decoupled from the mel decoder as a standalone aligner. This provides a lot of flexibility in choosing the architecture to formulate the distribution P (st|xt), where st is a random variable for a text token aligned at timestep t for mel frame xt. Similar to GlowTTS [6] and RAD-TTS [12], we compute the soft alignment distribution based on the learned pairwise affinity between all text tokens and mel frames, which is normalized with softmax across the text domain:
Di,j=distL2(ϕenci,xencj),Asoft=softmax(−D, dim =0).(8)(9)View Source\begin{align*} & {D_{i,j}} = dis{t_{L2}}\left( {\phi _i^{enc},x_j^{enc}} \right),\tag{8} \\ & {\mathcal{A}_{soft}} = \operatorname{softmax} ( - D,{\text{ dim }} = 0).\tag{9}\end{align*} We use two simple convolutional encoders from RAD-TTS [12] for encoding text Φ as Φenc and mel-spectograms X as Xenc with 2 and 3 1D convolution layers respectively. In Section 3, we demonstrate that the same architecture works well with different parallel TTS models such as FastPitch and FastSpeech2. Parallel models require alignments to be specified beforehand, typically in the form of the number of output frames for every input phoneme, equivalent to a binary alignment map. However, attention models produce soft alignment maps, constituting a train-test domain gap. Following [6], [12], we use Viterbi algorithm to find the most likely monotonic path through the soft alignment map in order to convert soft alignments (Asoft) to hard alignments (Ahard). We further close the gap between soft and hard alignments by forcing Asoft to match Ahard as much as possible by minimizing their KL-divergence(Lbin):
Lbin=Ahard⊙logAsoft,Lalign=LForwardSum+Lbin.(10)(11)View Source\begin{align*} & {\mathcal{L}_{bin\,}} = {\mathcal{A}_{hard\,}} \odot \log {\mathcal{A}_{soft}},\tag{10} \\ & {\mathcal{L}_{align\,}} = {\mathcal{L}_{ForwardSum\,}} + {\mathcal{L}_{bin}}.\tag{11}\end{align*}
where ⊙ is Hadamard product, Lalign is final alignment loss. 2.4. Alignment Acceleration: Faster convergence of alignments means faster training of the TTS model, as training the decoder needs a stable alignment. The length of mel-spectrograms is known upfront during training, hence we use a static 2D prior [12], that is wider near the center and narrower near the corners to accelerate the alignment by making far-off-diagonal elements less probable. This idea has been explored by Tachibana et al [13] by introducing a new loss promoting near-diagonal alignments. We believe auxiliary loss [13] should yield similar results to our static prior approach. We apply the prior (fB) over the alignment (P (s | X =xt)) to obtain the following posterior:
fB(k,α,β)=(NK)B(k+α)B(N−k+β)B(α,β)Pposterior(Φ=ϕk|X=xt)=P(Φ=ϕk|X=xt)⊙fB(k,ωt,ω(T−t+1))(12)(13)View Source\begin{align*} & {f_B}(k,\alpha ,\beta ) = \binom{N}{K}\frac{{B(k + \alpha )B(N - k + \beta )}}{{B(\alpha ,\beta )}}\tag{12} \\ & {P_{posterior\,}}\left( {\Phi = {\phi _k}|X = {x_t}} \right) = \\ &\quad P\left( {\Phi = {\phi _k}|X = {x_t}} \right) \odot {f_B}(k,\omega t,\omega (T - t + 1))\tag{13}\end{align*}
for k = {0,…, N}, where α, β are hyperparameters of beta function B(•, •), N is number of tokens and ω is scaling factor controlling width of prior: lower the ω, wider the width. 

SECTION 3. Experiments: We evaluate the effectiveness of the alignment learning framework by comparing its performance in terms of convergence speed, distance from human annotated ground truth durations, and speech quality. We use the LJ Speech dataset3 (LJ) [18] for all our experiments. For autoregressive models, we compare with the baseline alignment methods therein. For parallel models, we compare with alignment method that relies on an external TTS model (Tacotron2) or external aligner (MFA). Fig. 2: 
MCD-DTW convergence rate improvements in TTS models with the alignment learning framework. 
3.1. Convergence: Rate We use the mean mel-cepstral distance(MCD) [17],[19] to compare convergence rates. MCD compares the distance between synthesized and ground truth mel-spectrograms aligned temporally with dynamic time warping. We observe in Figure 2 that using the static prior described in Section 2.4 significantly improves the convergence rate of Tacotron2. Parallel models such as RAD-TTS, FastPitch and FastSpeech2 with the alignment framework (no dependency on external aligners) converge at the same rate as their baseline models using a forced aligner. Flowtron benefits the most from using the alignment framework. It has two autoregressive flows running in opposing directions, each with its own learned alignment. If the alignment in the first flow fails, the second flow, which depends on the output of the first, will fail as well. Thus, the baseline flowtron training is time consuming as it must train each flow step in succession. By using just the attention prior, we are now able to train at least two flows simultaneously, with further improvements in convergence speed with the addition of alignment learning Lalign objective (described in Sec 2.1). Fig. 3: 
Converged soft alignments for Flowtron, Tacotron2. Alignment framework provides sharper, more connected alignments. 
3.2. Alignment Sharpness: We visually inspect alignment matrices for a random validation sample in Figure 3. The objective Lalign consistently improves the sharpness and connectedness of the paths, leading to continuous speech without repeating or missing words. 3.3. Duration Analysis: To observe the influence of the alignment loss on the quality of alignments, we compare phoneme durations extracted from model alignments to manually annotated durations (due to lack of ground truth (GT) durations) from 10 samples of the LJ test set. For autoregressive models, we extract binarized alignments from soft alignments using monotonic argmax, iterating through phonemes and identifying the phoneme with maximum attention weights among the current and next phonemes. We use this binarized alignment to extract durations for each phoneme. Figure 4 shows average L1 distance between durations extracted from the models with respect to ground truth annotated durations. The proposed framework leads to faster convergence rate and alignments that are closer to GT. 3.4. Pairwise Opinion Scores: We crowd-sourced pairwise preference scores to subjectively compare models. Listeners were pre-screened with a hearing test based on sinusoid counting. To perform pairwise ranking, raters were repeatedly given two synthesized utterances of the same text, picked at random from 100 LJ test samples. Both were synthesized with the same architecture: one being the baseline, and other with alignment framework. The listeners were shown the text and asked to select samples with best overall quality, defined by accuracy of text, pleasantness, and naturalness. Approximately 200 scores per model were collected. Table 1 shows pairwise preference scores of models trained with alignment framework over baseline. It shows that alignment framework consistently improves over all baselines. Table 1: 
Pairwise preference scores by human raters, shown with 95% confidence intervals. Scores above 0.5 indicate models trained with Lalign were preferred by majority of raters.
Fig. 4: 
L1 distance between ground truth alignments and those extracted during training for Flowtron and Tacotron 2. Both use different batch sizes and are thus plotted separately. 
3.5. Robustness to Errors on Long Utterances: We measure character error rate (CER) between synthesized and input texts using an external speech recognition model to evaluate the robustness of the alignments on long utterances. We use 14, 045 full sentences from the LibriTTS dataset [20]. We synthesize speech with models trained on LJ Speech, and transcribe it with Jasper [21]. Figure 5 shows that autoregressive models with Lalign have a lower CER, providing evidence that the alignment objective results in more robust speech for long utterances. Parallel models such as RAD-TTS use a duration predictor and do not suffer from alignment issues, and hence have a much lower CER than autoregressive models. Fig. 5: 
Character error rate of models at different text lengths. Alignment framework results in fewer errors on long prompts. 


SECTION 4. Conclusion: We present an alignment framework that is broadly applicable to various TTS architectures, both autoregressive and parallel. This framework combines the forward-sum, Viterbi algorithm and a simple prior to make attention-based online alignment learning stable and fast-converging. It eliminates the need for forced aligners which are expensive to use and often not readily available for certain languages. Our experiments demonstrate improvements in speech quality based on human pairwise comparisons, reduced alignment failures, faster convergence, and robustness to errors in synthesis of long text sequences.