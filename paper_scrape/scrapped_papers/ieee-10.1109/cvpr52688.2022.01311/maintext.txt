SECTION 1. Introduction: Visual alignment, also known as the correspondence or registration problem, is a critical element in much of computer vision, including optical flow, 3D matching, medical imaging, tracking and augmented reality. While much recent progress has been made on pairwise alignment (aligning image A to image B) [2,14,22,34,51,57,58,60,68-71,75], the problem of global joint alignment (aligning all images across a dataset) has not received as much attention. Yet, joint alignment is crucial for tasks requiring a common reference frame, such as automatic keypoint annotation, augmented reality or edit propagation (see Figure 1 bottom row). There is also evidence that training on jointly aligned datasets (such as FFHQ [42], AFHQ [15], CelebA-HQ [40]) can produce higher quality generative models than training on unaligned data. In this paper, we take inspiration from a series of classic works on automatic joint image set alignment. In particular, we are motivated by the seminal unsupervised Congealing method of Learned-Miller [48] which showed that a set of images could be brought into alignment by continually warping them toward a common, updating mode. While Congealing can work surprisingly well on simple binary images, such as MNIST digits, the direct pixel-level alignment is not powerful enough to handle most datasets with significant appearance and pose variation.
Figure 1. Given an input dataset of unaligned images, our gangealing algorithm discovers dense correspondences between all images. Top row: images from lsun cats and the dataset's average image. Second row: our learned transformations of the input images. Third row: dense correspondences learned by gangealing. Bottom row: by annotating the average transformed image, we can propagate user edits to images and videos. Please see our project page for detailed video results: www.wpeebles.com/gangealing. 
To address these limitations, we propose GANgealing: a GAN-Supervised algorithm that learns transformations of input images to bring them into better joint alignment. The key is in employing the latent space of a GAN (trained on the unaligned data) to automatically generate paired training data for a Spatial Transformer [35]. Crucially, in our proposed GAN-Supervised Learning framework, both the Spatial Transformer and the target images are learned jointly. Our Spatial Transformer is trained exclusively with GAN images and generalizes to real images at test time. We show results spanning eight datasets-LSUN Bicycles, Cats, Cars, Dogs, Horses and TVs [87], In-The-Wild CelebA [52] and CUB [83] - that demonstrate our GANgealing algorithm is able to discover accurate, dense correspondences across datasets. We show our Spatial Transformers are useful in image editing and augmented reality tasks. Quantitatively, GANgealing significantly outperforms past self-supervised dense correspondence methods, nearly doubling key point transfer accuracy (PCK [4]) on many SPair-71K [59] categories. Moreover, GANgealing sometimes matches and even exceeds state-of-the-art correspondence-supervised methods. 

SECTION 2. Related Work: PreTrained Gans for Vision: Prior work has explored the use of GAN s [27], [67] in vision tasks such as classification [10], [12], [55], [74], [84], segmentation [56], [79], [82], [90] and representation learning [7], [20], [21], [23], [36], as well as 3D vision and graphics tasks [28], [64], [72], [89]. Likewise, we share the goal of leveraging the power of pre-trained deep generative models for vision tasks. However, the relevant past methods follow a common two-stage paradigm of (1) synthesizing a GAN-generated dataset and (2) training a discriminative model on the fixed dataset. In contrast, our GAN-Supervised Learning approach learns both the discriminative model as well as the GAN-generated data jointly end-to-end. We do not rely on handcrafted pixel space augmentations [12], [36], human-labeled data [28], [72], [79], [89], [90] or post-processing of GAN-generated datasets using domain knowledge [10], [56], [82], [89]. Joint Image Set Alignment: Average images have long been used to visualize joint alignment of image sets of the same semantic content (e.g., [78], [95]), with the seminal work of Congealing [32], [48] establishing unsupervised joint alignment as a research problem. Congealing uses sequential optimization to gradually minimize the entropy of the intensity distribution of a set of images by continuously warping each image via a parametric transformation (e.g., affine). It produces impressive results on well-structured datasets, such as digits, but struggles with more complex data. Subsequent work in this area assumes the data lies on a low-rank subspace [44], [66] or factorizes images as a composition of color, appearance and shape [62] to establish dense correspondences between instances of the same object category. FlowWeb [92] uses cycle consistency constraints to estimate a fully-connected correspondence flow graph. Every method above assumes that it is possible to align all images to a single central mode in the data. Joint visual alignment and clustering was proposed in AverageExplorer [95] but as a user-driven data interaction tool. Bounding box supervision has been used to align and cluster multiple modes within object categories [19]. Automated transformation-invariant clustering methods [24], [25] can align images in a collection before comparing them but work only in limited domains. Recently, Monnier et al. [63] showed that warps could be predicted with a network instead, removing the need for per-image optimization; this opened the door for simultaneous alignment and clustering of large-scale collections. Unlike our approach, these methods assume images can be aligned with simple (e.g., affine) color transformations; this assumption breaks down for complex datasets like LSUN. Spatial Transformer Networks (STNs)A Spatial Transformer module [35] is one way to incorporate learnable geometric transformations in a deep learning framework. It regresses a set of warp parameters, where the warp and grid sampling functions are differentiable to enable backpropagation. STNs have seen success in discriminative tasks (e.g., classification) and applications such as robust filter learning [16], [37], view synthesis [26], [65], [93] and 3D representation learning [39], [86], [91]. Inverse Compositional STN s (IC-STN s) [49] advocate an iterative image alignment framework in the spirit of the classical Lukas- Kanade algorithm [6], [54]. Prior work has incorporated STNs in generative models for geometry-texture disentanglement [85] and image compositing [50]. In contrast, we use a generative model to directly produce training data for STNs. 

SECTION 3. GAN-Supervised Learning: In this section, we present GAN-Supervised Learning. Under this framework, (x,y) pairs are sampled from a pre-trained GAN generator, where x is a random sample from the GAN and y is the sample obtained by applying a learned latent manipulation to x's latent code. These pairs are used to train a network f6: x→y. This framework minimizes the following loss: where ℓ is a reconstruction loss. In vanilla supervised learning, fθ is learned onfixed (x, y) pairs. In contrast, in GAN-Supervised Learning, both fθ and the targets y are learned jointly end-to-end. At test time, we are free to evaluate fθ on real inputs.
L(fθ,y)=ℓ(fθ(x),y),(1)View Source\begin{equation*}\mathcal{L}(f_{\theta},\boldsymbol{y})=\ell(f_{\theta}(\boldsymbol{x}),\boldsymbol{y}),\tag{1}\end{equation*}
Figure 2. Gangealing overview. We first train a generator G on unaligned data. We create a synthetically-generated dataset for alignment by learning a mode c in the generator's latent space. We use this dataset to train a spatial transformer network T to map from unaligned to corresponding aligned images using a perceptual loss [38]. The spatial transformer generalizes to align real images automatically. 
3.1. Dense Visual Alignment: Here, we show how GAN-Supervised Learning can be applied to Congealing [48] a classic unsupervised alignment algorithm. In this instantiation, f6 is a Spatial Transformer Network [35] T, and we describe our parameterization of inputs x and learned targets y below. We call our algorithm GANgealing. We present an overview in Figure 2. GANgealing begins by training a latent variable gener-ative model G on an unaligned input dataset. We refer to the input latent vector to G as w∈R512. With G trained, we are free to draw samples from the unaligned distribution by computing x=G(w) for randomly sampled w∼W, where W denotes the distribution over latents. Now, consider a fixed latent vector c∈R512. This vector corresponds to a fixed synthetic image G(c) from the original unaligned distribution. A simple idea in the vein of traditional Congealing is to use G(c) as the target mode y-i.e., we learn a Spatial Transformer T that is trained to warp every random unaligned image x=G(w) to the same target image y=G(c). Since G is differentiable in its input, we can optimize c and hence learn the target we wish to congeal towards. Specifically, we can optimize the following loss with respect to both T's parameters and the target image's latent vector c jointly:
Lalign(T,c)=ℓ(T(G(w)),G(c)),(2)View Source\begin{equation*}\mathcal{L}_{\text{align}}(T, \mathbf{c})=\ell(T(G(\mathbf{w})), G(\mathbf{c})),\tag{2}\end{equation*}
where ℓ is some distance function between two images. By minimizing L with respect to the target latent vector c, GANgealing encourages c to find a pose that makes T's job as easy as possible. If the current value of c corresponds to a pose that cannot be reached from most images via the transformations predicted by T, then it can be adjusted via gradient descent to a different vector that is “reachable” by more images. This simple approach is reasonable for datasets with limited diversity; however, in the presence of significant appearance and pose variation, it is not reasonable to expect that every unaligned sample can be aligned to the exact same target image. Hence, optimizing the above loss does not produce good results in general (see Table 3). Instead of using the same target G(c) for every randomly sampled image G(w, it would be ideal if we could construct a per-sample target that retains the appearance of G(w) but where the pose and orientation of the object in the target image is roughly identical across targets. To accomplish this, given G(w), we produce the corresponding target by setting just a portion of the w vector equal to the target vector c. Specifically, let mix (c, w)∈R512 refer to the latent vector whose first entries are taken from c and remaining entries are taken from w. By sampling new w vectors, we can create an infinite pool of paired data where the input is the unaligned image x=G(w) and the target y=G (mix (c, w)) shares the appearance of G(w, but is in a learned, fixed pose. This gives rise to the GAN gealing loss function:
\begin{equation*}\mathcal{L}_{\text{align}}(T, \mathbf{c})=\ell(T(\underbrace{G(\mathbf{w})}_{\boldsymbol{x}}), \underbrace{G(\text{mix}(\mathbf{c},\mathbf{w}))})_{\boldsymbol{y}},\tag{3}\end{equation*}View Source\begin{equation*}\mathcal{L}_{\text{align}}(T, \mathbf{c})=\ell(T(\underbrace{G(\mathbf{w})}_{\boldsymbol{x}}), \underbrace{G(\text{mix}(\mathbf{c},\mathbf{w}))})_{\boldsymbol{y}},\tag{3}\end{equation*}
where \ell is a perceptual loss function [38]. In this paper, we opt to use StyleGAN2 [43] as our choice of G, but in principle other GAN architectures could be used with our method. An advantage of using StyleGAN2 is that it possesses some innate style-pose disentanglement that we can leverage to construct the per-image target described above. Specifically, we can construct the per-sample targets G (mix (\mathbf{c}, \mathbf{w})) by using style mixing [42] - c is supplied to the first few inputs to the synthesis generator that roughly control pose and w is fed into the later layers that roughly control texture. See Table 3 for a quantitative ablation of the mixing” cutoff point” where we begin to feed in \mathbf{w} (i.e., the cutoff point is chosen as a layer index in \mathcal{W}^{+} space [1]). Spatial Transformer ParameterizationRecall that a Spatial Transformer T takes as input an image and regresses and applies a (reverse) sampling grid \mathbf{g}\in \mathbb{R}^{H\times W\times 2} to the input image. Hence, one must choose how to constrain the g regressed by T. In this paper, we explore a T that performs similarity transformations (rotation, uniform scale, horizontal shift and vertical shift). We also explore an arbitrarily expressive T that directly regresses unconstrained perpixel flow fields g ‘ Our final T is a composition of the similarity Spatial Transformer into the unconstrained Spatial Transformer, which we found worked best. In contrast to prior work [50], [63], we do not find multistage training necessary and train our composed T end-to-end. Finally, our Spatial Transformer is also capable of performing horizontal flips at test time-please refer to Supplement B.4 for details. When using the unconstrained T, it can be beneficial to add a total variation regularizer that encourages the predicted flow to be smooth to mitigate degenerate solutions: \mathcal{L}_{\text{TV}}(T)=\mathcal{L}_{\text{Huber}}(\Delta_{x}\mathbf{g})+\mathcal{L}_{\text{Huber}}(\Delta_{y}\mathbf{g}), where \mathcal{L}_{\text{Huber}} denotes the Huber loss and \Delta_{x} and \triangle_{v} denote the partial derivative w.r.t. x and y coordinates under finite differences. We also use a regularizer that encourages the flow to not deviate from the identity transformation: \mathcal{L}_{\mathrm{I}}(T)=\Vert \mathbf{g}\Vert _{2}^{2}. Parameterization of CIn practice, we do not backpropagate gradients directly into c. Instead, we parameterize \mathbf{c} as a linear combination of the topN principal directions of \mathcal{W} space [29], [77]:
\begin{equation*}\mathbf{c}=\bar{\mathbf{w}}+\sum_{i=1}^{N}\alpha_{i}\mathbf{d}_{i},\tag{4}\end{equation*}View Source\begin{equation*}\mathbf{c}=\bar{\mathbf{w}}+\sum_{i=1}^{N}\alpha_{i}\mathbf{d}_{i},\tag{4}\end{equation*}
where \overline{\mathrm{w}} is the empirical mean w vector, \mathrm{d}_{i} is the i-th principal direction and \alpha_{i} is the learned scalar coefficient of the direction. Instead of optimizing \mathcal{L} w.r.t. c directly, we optimize it w.r.t. the coefficients \{\alpha_{i}\}_{i=1}^{N}. The motivation for this reparameterization is that StyleGAN's \mathcal{W} space is highly expressive. Hence, in the absence of additional constraints, naive optimization of c can yield poor target images off the manifold of natural images. Decreasing N keeps c on the manifold and prevents degenerate solutions. See Table 3 for an ablation of N. Our final GANgealing objective is given by:
\begin{align*}\mathcal{L}(T, \mathbf{c})=\mathbb{E}_{\mathbf{w}\sim \mathcal{W}}[\mathcal{L}_{\text{align}}& (T, \mathbf{c})\\
& +\lambda_{\text{TV}}\mathcal{L}_{\text{TV}}(T)+\lambda_{I}\mathcal{L}_{\mathrm{I}}(T)].\tag{5}\end{align*}View Source\begin{align*}\mathcal{L}(T, \mathbf{c})=\mathbb{E}_{\mathbf{w}\sim \mathcal{W}}[\mathcal{L}_{\text{align}}& (T, \mathbf{c})\\
& +\lambda_{\text{TV}}\mathcal{L}_{\text{TV}}(T)+\lambda_{I}\mathcal{L}_{\mathrm{I}}(T)].\tag{5}\end{align*} We set the loss weighting \lambda_{\text{TV}} at either 1000 or 2500 (depending on choice of l) and the loss weighting \lambda_{l} at 1. See Supplement B for additional details and hyperparameters. 3.2. Joint Alignment and Clustering: GANgealing as described so far can handle highly-multimodal data (e.g., LSUN Bicycles, Cats, etc.). Some datasets, such as LSUN Horses, feature extremely diverse poses that cannot be represented well by a single mode in the data. To handle this situation, GANgealing can be adapted into a clustering algorithm by simply learning more than one target latent c. Let K refer to the number of c vectors (clusters) we wish to learn. Since each c captures a specific mode in the data, learning multiple \{\mathrm{c}_{k}\}_{k=1}^{K} would enable us to learn multiple modes. Now, each \mathrm{c}_{k} will learn its own set of \alpha coefficients. Similarly, we will now have K Spatial Transformers, one for each mode being learned. This variant of GANgealing amounts to simultaneously clustering the data and learning dense correspondence between all images within each cluster. To encourage each \mathrm{c}_{k} and T_{k} pair to specialize in a particular mode, we include a hard-assignment step to assign unaligned synthetic images to modes:
\begin{equation*}\mathcal{L}_{\text{align}}^{K}(T,\mathbf{c})=\min \mathcal{L}_{\text{align}}(T_{k},\mathbf{c}_{k})\tag{6}\end{equation*}View Source\begin{equation*}\mathcal{L}_{\text{align}}^{K}(T,\mathbf{c})=\min \mathcal{L}_{\text{align}}(T_{k},\mathbf{c}_{k})\tag{6}\end{equation*} Note that the K=1 case is equivalent to the previously described unimodal case. At test time, we can assign an input fake image G(\mathrm{w}) to its corresponding cluster index k^{\ast}=\arg \min k\mathcal{L}_{\text{align}}(T_{k},\ \mathrm{c}_{k}). Then, we can warp it with the Spatial Transformer T_{k^{\ast}}. However, a problem arises in that we cannot compute this cluster assignment for input real images-the assignment step requires computing \mathcal{L}_{\text{align}}, which itself requires knowledge of the input image's corresponding w vector. The most obvious solution to this problem is to perform GAN inversion [8], [11], [94] on input real images x to obtain a latent vector w such that G(\mathbf{w})\approx \boldsymbol{x}. However, accurate GAN inversion for non-face datasets remains somewhat challenging and slow, despite re-cent progress [3], [33]. Instead, we opt to train a classifier that directly predicts the cluster assignment of an input image. We train the classifier using a standard crossentropy loss on (input fake image, target cluster) pairs (G(\mathrm{w}),\ k^{\ast}), where k^{\ast} is obtained using the above assignment step. We initialize the classifier with the weights of T (replacing the warp head with a randomly-initialized classification head). As with the Spatial Transformer, the classifier generalizes well to real images despite being trained exclusively on fake samples.
Figure 3. Dense correspondence results on eight datasets. For each dataset, the top row shows unaligned images and the dataset average image. The middle row shows our learned alignment of the input images. The bottom row shows dense correspondences between the images. For our clustering models (lsun horses and cars), we show results for one selected cluster. See supplement f for uncurated results. 


SECTION 4. Experiments: In this section, we present quantitative and qualitative results of GANgealing on eight datasets: LSUN Bicycles, Cats, Cars, Dogs, Horses and TVs [87], In-The-Wild CelebA [52] and CUB-200-2011 [83]. These datasets feature significant diversity in appearance, pose and occlusion of objects. Only LSUN Cars and Horses use clustering (K=4)1; for all other datasets we use unimodal GANgealing (K=1). Note that all figures except Figure 2 show our method applied to real images-not GAN samples. Please see www.wpeebles.com/gangealing for full results. 4.1. Propagation from Congealed Space: With the Spatial Transformer T trained, it is trivial to identify dense correspondences between real input images x. A particularly convenient way to find dense correspondences between a set of images is by propagating from our congealed coordinate space. As described earlier, T both regresses and applies a sampling grid g to an input image. Because we use reverse sampling, this grid tells us where each point in the congealed image T(x) maps to in the original image x. This enables us to propagate anything from the congealed coordinate space-dense labels, sparse keypoints, etc. If a user annotates a single congealed image (or the average congealed image) they can then propagate those labels to an entire dataset by simply predicting the grid g for each image x in their dataset via a forward pass through T. Figures 1 and 3 show visual results for all eight datasets-our method can find accurate dense correspondences in the presence of significant appearance and pose diversity. GANgealing accurately handles diverse morphologies of birds, cats with varying facial expressions and bikes in different orientations.
Figure 4. Image editing with gangealing. By annotating just a single image per-category (our average transformed image), a user can propagate their edits to any image or video in the same category. 
Image EditingOur average congealed image is a template that can propagate any user edit to images of the same category. For example, by drawing cartoon eyes or overlaying a Batman mask on our average congealed cat, a user can effortlessly propagate their edits to massive numbers of cat images with forward passes of T. We show editing results on several datasets in Figures 4 and 1. Augmented RealityJust as we can propagate dense correspondences to images, we can also propagate to individual video frames. Surprisingly, we find that GANgealing yields remarkably smooth and consistent results when applied out-of-the-box to videos per-frame without leveraging any temporal information. This enables mixed reality applications like dense tracking and filters. GANgealing can outperform supervised methods like RAFT [75] - please see www.wpeebles.com/gangealing for results. 4.2. Direct Image-to-Image Correspondence: In addition to propagating correspondences from congealed space to unaligned images, we can also find dense correspondences directly between any pair of images x_{A} and x_{B}. At a high level, this merely involves applying the forward warp that maps points in x_{A} to points in T(x_{A}) and composing it with the reverse warp that maps points in the congealed coordinate space back to x_{B}. Please refer to Supplement B.7 for details.
Figure 5. PCK {@}\alpha_{\mathbf{bbox}} on various SPair-71K categories for \alpha_{\mathbf{bbox}} between 10−1 and 10−2. we report the average threshold (maximum distance for a correspondence to be deemed correct) in pixels for 256×256 images beneath each plot. Gangealing outperforms state-of-the-art supervised methods for very precise thresholds (< 2 pixel error tolerance), sometimes by substantial margins. 
Quantitative ResultsWe evaluate GANgealing with PCK - Transfer. Given a source image x_{A}, target image x_{E} and ground-truth keypoints for both images, PCK- Transfer measures the percentage of keypoints transferred from x_{A} to x_{B} that lie within a certain radius of the ground-truth keypoints in x_{E}. We evaluate PCK on SPair-71K [59] and CUB. For SPair, we use the \alpha_{\text{bbox}} threshold in keeping with prior works. Under this threshold, a predicted keypoint is deemed to be correctly transferred if it is within a radius \alpha_{\text{bbox}}\max(H_{\text{bbox}}, W_{\text{bbox}}) of the ground truth, where H_{\text{bbox}} and W_{\text{bbox}} are the height and width of the object bounding box in the target image. For each SPair category, we train a StyleGAN2 on the corresponding LSUN category2 2_{-} the GANs are trained on 256×256 center-cropped images. We then train a Spatial Transformer using GANgealing and directly evaluate on SPair. For CUB, we first pretrain a StyleGAN2 with ADA [41] on the NABirds dataset [81] and finetune it with FreezeD [61] on the training split of CUB, using the same image pre-processing and dataset splits as ACSM [46] for a fair comparison. When T performs a horizontal flip for one image in a pair, we permute our model's predictions for keypoints with a left versus right distinction. Spair-71k ResultsWe compare against several self-supervised and state-of-the-art supervised methods on the challenging SPair-71K dataset in Table 1, using the standard \alpha_{\text{bbox}}=0.1 threshold. Our method significantly outperforms prior self-supervised methods on several categories, nearly doubling the best prior self-supervised method's PCK on SPair Bicycles and Cats. GANgealing performs on par with and even outperforms state-of-the-art correspondence-supervised methods on several categories. We increase the previous best PCK on Bicycles achieved by Cost Aggregation Transformers [14] from 34.7% to 37.5% and perform comparably on Cats and TVs.
Table 1. Pck-transfer@αbbox = 0.1 results on spair-71k categories (test split).
High-Precision Spair-71k ResultsThe usual \alpha_{\text{bbox}} =0.1 threshold reported by most papers using SPair deems a correspondence correct if it is localized within roughly 10 to 20 pixels of the ground truth for 256×256 images (depending on the SPair category). In Figure 5, we evaluate performance over a range of thresholds between 0.1 and 0.01 (the latter of which affords a roughly 1 to 2 pixel error tolerance, again depending on category). GANgealing outperforms all supervised methods at these high-precision thresholds across all four categories tested. Notably, our LSUN Cats model improves the previous best SPair Cats PCK @ \alpha_{\text{bbox}}=0.01 achieved by SCOT [\delta 1] from 5.4% to 18.5%. On SPair TVs, we improve the best supervised PCK achieved by Dynamic Hyperpixel Flow [60] from 2.1% to 3.0%. Even on SPair Dogs, where GANgealing is outperformed by every supervised method at low-precision thresholds, we marginally outperform all baselines at the 0.01 threshold.
Figure 6. Gangealing alignment improves downstream gan training. We show random, untruncated samples from stylegan2 trained on lsun cats versus our aligned lsun cats (both models trained from scratch). Our method improves visual fidelity. 
Figure 7. Various failure modes: significant out-of-plane rotation and complex poses poorly modeled by gans. 
Cub ResultsTable 2 shows PCK results on CUB, comparing against several 2D and 3D correspondence methods that use varying amounts of supervision. GANgealing achieves 57.5% PCK, outperforming all past methods that require instance mask supervision and performing comparably with the best correspondence-supervised baseline (58.5%). AblationsWe ablate several components of GANgealing in Table 3. We find that learning the target mode \mathbf{c} is critical for complex datasets; fixing \mathrm{c}=\overline{\mathrm{w}} dramatically degrades PCK from 67% to 10.6% for our LSUN Cats model. This highlights the value of our GAN-Supervised Learning framework where both the discriminative model and targets are learned jointly. We additionally find that our baseline inspired by traditional Congealing (using a single learned target G(\mathrm{c}) for all inputs) is highly unstable and degrades PCK to as little as 7.7%. This result demonstrates the importance of our per-input alignment targets. We also ablate two choices of the perceptual loss \ell: an off-the-shelf supervised option (LPIPS [88]) and a fully-unsupervised VGG-16 [73] pre-trained with SimCLR [13] on ImageNetlK [17] (SSL)-there is no significant difference in performance between the two (\pm 0.2%). Please see Table 3 for more ablations. 4.3. Automated Gan Dataset PreProcessing: An exciting application of GANgealing is automated dataset pre-processing. Dataset alignment is an important yet costly step for many machine learning methods. GAN training in particular benefits from carefully-aligned and filtered datasets, such as FFHQ [42], AFHQ [15] and CelebA-HQ [40]. We can align input datasets using our similarity Spatial Transformer T to train generators with higher visual fidelity. We show results in Figure 6: training StyleGAN2 from scratch with our learned pre-processing of LSUN Cats yields high-quality samples reminiscent of AFHQ. As we show in Supplement E, our pre-processing accelerates GAN training significantly.
Table 2. Pck-transfer@ 0.1 on cub. Numbers for the 3d methods are reported from [46]. We sample 10,000 random pairs from the cub validation split as in [46].
Table 3. Gangealing ablations for lsun cats. We evaluate on spair-71k cats using αbbox = 0.1. Ssl refers to using a self-supervised vgg-16 as the perceptual loss ℓ. N refers to the number of w space pca coefficients learned when optimizing c. Note that the lsun cats stylegan2 generator has 14 layers.
Our Spatial Transformer has a few notable failure modes as demonstrated in Figure 7. One limitation with GANgealing is that we can only reliably propagate correspondences that are visible in our learned target mode. For example, the learned mode of our LSUN Dogs model is the upper-body of a dog-this particular model is thus incapable of finding correspondences between, e.g., paws. A potential solution to this problem is to initialize the learned mode with a user-chosen image via GAN inversion that covers all points of interest. Despite this limitation, we obtain competitive results on SPair for some categories where many keypoints are not visible in the learned mode (e.g., cats). 

SECTION 5. Limitations and Discussion: In this paper, we showed that GANs can be used to train highly competitive dense correspondence algorithms from scratch with our proposed GAN-Supervised Learning framework. We hope this paper will lead to increased adoption of GAN-Supervision for other challenging tasks.