SECTION 1. Motivation: It is an oft-told story that when a young graduate student asked Takeo Kanade what are the three most important problems in computer vision, Kanade replied: "Correspondence, correspondence, correspondence!" Indeed, most fundamental vision problems, from optical flow and tracking to action recognition and 3D reconstruction, require some notion of visual correspondence. Correspondence is the glue that links disparate visual percepts into persistent entities and underlies visual reasoning in space and time. Figure 1: 
We propose to learn a representation for visual correspondence from raw video. Without any fine-tuning, the acquired representation generalizes to various tasks involving visual correspondence, allowing for propagation of: (a) Multiple Instance Masks; (b) Pose; (c) Semantic Masks; (d) Long-Range Optical Flow; (e) Texture. 
Learning representations for visual correspondence, from pixel-wise to object-level, has been widely explored, primarily with supervised learning approaches requiring large amounts of labelled data. For learning low-level correspondence, such as optical flow, synthetic computer graphics data is often used as supervision [10], [22], [50], [62], limiting generalization to real scenes. On the other hand, approaches for learning higher-level semantic correspondence rely on human annotations [71], [19], [65], which becomes prohibitively expensive at large scale. In this work, our aim is to learn representations that support reasoning at various levels of visual correspondence (Figure 1) from scratch and without human supervision. A fertile source of free supervision is video. Because the world does not change abruptly, there is inherent visual correspondence between observations adjacent in time. The problem is how to find these correspondences and turn them into a learning signal. In a largely static world observed by a stationary camera, such as a webcam trained on the Eiffel Tower, correspondence is straightforward because nothing moves and capturing visual invariance (to weather, lighting) amounts to supervised metric learning. In the dynamic world, however, change in appearance is confounded by movement in space. Finding correspondence becomes more difficult because capturing visual invariance now requires learning to track, but tracking relies on a model of visual invariance. This paper proposes to learn to do both simultaneously, in a self-supervised manner. Figure 2: 
A Cycle in Time. Given a video, tracking along the sequence formed by a cycle in time can be self-supervised: the target is simply the beginning of the cycle. The yellow arrow between the start and end represents the differentiable learning signal. 
The key idea is that we can obtain unlimited supervision for correspondence by tracking backward and then forward (i.e. along a cycle in time) and using the inconsistency between the start and end points as the loss function (Figure 2). We perform tracking by template-matching in a learned deep feature space. To minimize the loss - i.e. to be cycle-consistent - the model must learn a feature representation that supports identifying correspondences across frames. As these features improve, the ability to track improves, inching the model toward cycle-consistency. Learning to chain correspondences in such a feature space should thus yield a visual similarity metric tolerant of local transformations in time, which can then be used at test-time as a stand-alone distance metric for correspondence. While conceptually simple, implementing objectives based on cycle-consistency can be challenging. Without additional constraints, learning can take shortcuts, making correspondences cycle-consistent but wrong [88]. In our case, a track that never moves is inherently cycle-consistent. We avoid this by forcing the tracker to re-localize the next patch in each successive frame. Furthermore, cycle- consistency may not be achievable due to sudden changes in object pose or occlusions; skip-cycles can allow for cycle- consistency by skipping frames, as in Figure 3 (right). Finally, correspondence may be poor early in training, and shorter cycles may ease learning, as in Figure 3 (left). Thus, we simultaneously learn from many kinds of cycles to induce a natural curriculum and provide better training data. The proposed formulation can be used with any differentiable tracking operation, providing a general framework for learning representations for visual correspondence from raw video. Because the method does not rely on human annotation, it can learn from the near infinite video data available online. We demonstrate the usefulness of the learned features for tasks at various levels of visual correspondence, ranging from pose, keypoint, and segmentation propagation (of objects and parts) to optical flow. 

SECTION 2. Related Work: Temporal Continuity in Visual Learning. Temporal structure serves as a useful signal for learning because the visual world is continuous and smoothly-varying. Spatiotemporal stability is thought to play a crucial role in the development of invariant representations in biological vision [83], [36], [77], [78]. For example, Wood [77] showed that for newborn chicks raised in a visual world that was not temporally smooth, object recognition abilities were severely impaired. Computational approaches for unsupervised learning have sought to leverage this continuity, such as continuous transformation learning [13], [70], "slow" feature learning [76], [91], [25] and information maximization between neighbouring patches in time [66]. Our work can be seen as slow feature learning with fixation, learned end-to- end without supervision. Self-supervised Representation Learning from Video. Learning representations from video using time as supervision has been extensively studied, both as future prediction task [15], [60], [44], [42] as well as motion estimation [2], [25], [63], [38], [40]. Our approach is most related to the methods of Wang et al. [73],[74] and Pathak et al. [47], which use off-the-shelf tools for tracking and optical flow respectively, to provide supervisory signal for training. However, representations learned in this way are inherently limited by the power of these off-the-shelf tools as well as their failure modes. We address this issue by learning the representation and the tracker jointly, and find the two learning problems to be complementary. Our work is also inspired by the innovative approach of Vondrick et al [69] where video colorization is used as a pretext self-supervised task for learning to track. While the idea is very intriguing, in Section 4 we find that colorization is a weaker source of supervision for correspondence than cycle-consistency, potentially due to the abundance of constant-color regions in natural scenes. Figure 3: 
Multiple Cycles and Skip Cycles. Cycle-consistency may not be achievable due to sudden changes in object pose or occlusions. Our solution is to optimize multiple cycles of different lengths simultaneously. This allows learning from shorter cycles when the full cycle is too difficult (left). This also allows cycles that skip frames, which can deal with momentary occlusions (right). 
Tracking. Classic approaches to tracking treat it as a matching problem, where the goal is to find a given object/patch in the next frame (see [11] for overview), and the key challenge is to track reliably over extended time periods [57], [79], [1], [27]. Starting with the seminal work of Ramanan et al. [49], researchers largely turned to "tracking as repeated recognition", where trained object detectors are applied to each frame independently [3], [28], [80], [71], [19], [35], [65]. Our work harks back to the classic tracking-by-matching methods in treating it as a correspondence problem, but uses learning to obtain a robust representation that is able to model wide range of appearance changes. Optical Flow. Correspondence at the pixel level - mapping where each pixel goes in the next frame is the optical flow estimation problem. Since the energy minimization framework of Horn and Schunck [20] and coarse- to-fine image warping by Lucas and Kanade [41], much progress has been made in optical flow estimation [46], [6], [61], [10], [22], [50], [62]. However, these methods still struggle to scale to long-range correspondence in dynamic scenes with partial observability. These issues have driven researchers to study methods for estimating long-range optical flow [56], [5], [55], [51], [52], [34]. For example, Brox and Malik [5] introduced a descriptor that matches region hierarchies and provides dense and subpixel-level estimation of flow. Our work can be viewed as enabling mid-level optical flow estimation. Mid-level Correspondence. Given our focus on finding correspondence at the patch level, our method is also related to the classic SIFT Flow [39] algorithm and other methods for finding mid-level correspondences between regions across different scenes [30], [16], [87]. More recently, researchers have studied modeling correspondence in deep feature space [64],[33], [31],[17], [53],[54]. In particular, our work draws from Rocco et al. [53], [54], who propose a differentiable soft inlier score for evaluating quality of alignment between spatial features and provides a loss for learning semantic correspondences. Most of these methods rely on learning from simulated or large-scale labeled datasets such as ImageNet, or smaller custom human-annotated data with narrow scope. We address the challenge of learning representations of correspondence without human annotations. Forward-Backward and Cycle Consistency. Our work is influenced by the classic idea of forward-backward consistency in tracking [57], [79], [1], [27], which has long been used as an evaluation metric for tracking [27] as well as a measure of uncertainty [1]. Recent work on optical flow estimation [43], [24], [68], [75], [45] also utilizes forward-backward consistency as an optimization goal. For example, Meister et al. [45] combines one-step forward and backward consistency check with pixel reconstruction loss for learning optical flows. Compared to pixel reconstruction, modeling correspondence in feature space allows us to follow and learn from longer cycles. Forward-backward consistency is a specific case of cycle-consistency, which has been widely applied as a learning objective for 3D shape matching [21], image alignment [87], [89], [88], depth estimation [86], [14], [84], and image-to-image translation [90], [4]. For example Zhou et al. [88] used 3D CAD models to render two synthetic views for pairs of training images and construct a correspondence flow 4-cycle. To the best of our knowledge, our work is the first to employ cycle-consistency across multiple steps in time. 

SECTION 3. Approach: An overview of the training procedure is presented in Figure 4a. The goal is to learn a feature space ϕ by tracking a patch pt extracted from image It backwards and then forwards in time, while minimizing the cycle-consistency loss lθ (yellow arrow). Learning ϕ relies on a simple tracking operation \mathcal{T}T, which takes as inputs the features of a current patch and a target image, and returns the image feature region with maximum similarity. Our implementation of \mathcal{T}T is shown in Figure 4b: without information of where the patch came from, \mathcal{T}T must match features encoded by ϕ to localize the next patch. As shown in Figure 4a, \mathcal{T}T can be iteratively applied backwards and then forwards through time to track along an arbitrarily long cycle. The cycle-consistency loss lθ is the euclidean distance between the spatial coordinates of initial patch pt and the patch found at the end of the cycle in It. In order to minimize lθ, the model must learn a feature space ϕ that allows for robustly measuring visual similarity between patches along the cycle. Figure 4: 
Method Overview. (a) During training, the model learns a feature space encoded by ϕ to perform tracking using tracker \mathcal{T}T. By tracking backward and then forward, we can use cycle-consistency to supervise learning of ϕ. Note that only the initial patch pt is explicitly encoded by ϕ; other patch features along the cycle are obtained by localizing image features. (b) We show one step of tracking back in time from t to t — 1. Given input image features x_{t - 1}^IxIt−1 and query patch features x_t^p,\mathcal{T}xpt,T localizes the patch x_{t - 1}^pxpt−1 in x_{t - 1}^IxIt−1. This operation is performed iteratively to track along the cycle in (a). 
Note that \mathcal{T}T is only used in training and is deliberately designed to be weak, so as to place the burden of representation on ϕ. At test time, the learned ϕ is used directly for computing correspondences. In the following, we first formalize cycle-consistent tracking loss functions and then describe our architecture for mid-level correspondence. 3.1. Cycle-Consistency Losses: We describe a formulation of cycle-consistent tracking and use it to succinctly express loss functions based on temporal cycle-consistency. 3.1.1 Recurrent Tracking FormulationConsider as inputs a sequence of video frames It-k:t and a patch pt taken from It. These pixel inputs are mapped to a feature space by an encoder ϕ, such that x_{t - k:t}^I = \phi \left( {{I_{t - k:t}}} \right)xIt−k:t=ϕ(It−k:t) and x_t^p = \phi \left( {{p_t}} \right).xpt=ϕ(pt). . Let \mathcal{T}T be a differentiable operation x_s^I \times x_t^p \mapsto x_s^p,xIs×xpt↦xps, where s and t represent time steps. The role of \mathcal{T}T is to localize the patch features x_s^pxps in image features x_s^IxIs that are most similar to x_t^pxpt. We can apply T iteratively in a forward manner i times from t — i to t — 1 :
\begin{equation*}{\mathcal{T}^{(i)}}\left( {x_{t - i}^I,\;{x^p}} \right) = \mathcal{T}\left( {x_{t - 1}^I,\;\mathcal{T}\left( {x_{t - 2}^I,\; \ldots \mathcal{T}\left( {x_{t - i}^I,\;{x^p}} \right)} \right)} \right)\end{equation*}T(i)(xIt−i,xp)=T(xIt−1,T(xIt−2,…T(xIt−i,xp)))View Source\begin{equation*}{\mathcal{T}^{(i)}}\left( {x_{t - i}^I,\;{x^p}} \right) = \mathcal{T}\left( {x_{t - 1}^I,\;\mathcal{T}\left( {x_{t - 2}^I,\; \ldots \mathcal{T}\left( {x_{t - i}^I,\;{x^p}} \right)} \right)} \right)\end{equation*} By convention, the tracker \mathcal{T}T can be applied backwards i times from time t — 1 to t — i:
\begin{equation*}{\mathcal{T}^{( - i)}}\left( {x_{t - 1}^I,\;{x^p}} \right) = \mathcal{T}\left( {x_{t - i}^I,\;\mathcal{T}\left( {x_{t - i + 1}^I,\; \ldots \mathcal{T}\left( {x_{t - 1}^I,\;{x^p}} \right)} \right)} \right)\end{equation*}T(−i)(xIt−1,xp)=T(xIt−i,T(xIt−i+1,…T(xIt−1,xp)))View Source\begin{equation*}{\mathcal{T}^{( - i)}}\left( {x_{t - 1}^I,\;{x^p}} \right) = \mathcal{T}\left( {x_{t - i}^I,\;\mathcal{T}\left( {x_{t - i + 1}^I,\; \ldots \mathcal{T}\left( {x_{t - 1}^I,\;{x^p}} \right)} \right)} \right)\end{equation*} 3.1.2 Learning ObjectivesThe following learning objectives rely on a measure of agreement {l_\theta }\left( {x_t^p,\hat x_t^p} \right)lθ(xpt,x^pt) between the initial patch and re-localized patch (defined in Section 3.2). Tracking: The cycle-consistent loss \mathcal{L}_{long}^iLilong is defined as
\begin{equation*}\mathcal{L}_{long}^i = {l_\theta }\left( {x_t^p,\;{\mathcal{T}^{(i)}}\left( {x_{t - i + 1}^I,\;{\mathcal{T}^{( - i)}}\left( {x_{t - 1}^I,\;x_t^p} \right)} \right)} \right)\;.\end{equation*}Lilong=lθ(xpt,T(i)(xIt−i+1,T(−i)(xIt−1,xpt))).View Source\begin{equation*}\mathcal{L}_{long}^i = {l_\theta }\left( {x_t^p,\;{\mathcal{T}^{(i)}}\left( {x_{t - i + 1}^I,\;{\mathcal{T}^{( - i)}}\left( {x_{t - 1}^I,\;x_t^p} \right)} \right)} \right)\;.\end{equation*} The tracker attempts to follow features backward and then forward i steps in time to re-arrive to the initial query, as depicted in Figure 4a. Skip Cycle: In addition to cycles through consecutive frames, we also allow skipping through time. We define the loss on a two-step skip-cycle as \mathcal{L}_{skip}^iLiskip: \begin{equation*}\mathcal{L}_{skip}^i = {l_\theta }\left( {x_t^p,\;\mathcal{T}\left( {x_t^I,\;\mathcal{T}\left( {x_{t - i}^I,\;x_t^p} \right)} \right)} \right).\end{equation*}Liskip=lθ(xpt,T(xIt,T(xIt−i,xpt))).View Source\begin{equation*}\mathcal{L}_{skip}^i = {l_\theta }\left( {x_t^p,\;\mathcal{T}\left( {x_t^I,\;\mathcal{T}\left( {x_{t - i}^I,\;x_t^p} \right)} \right)} \right).\end{equation*}This attempts longer-range matching by skipping to the frame i steps away. Feature Similarity: We explicitly require the query patch x_t^pxpt and localized patch \mathcal{T}\left( {x_{t - i}^I,\;x_t^p} \right)T(xIt−i,xpt) to be similar in feature space. This loss amounts to the negative Frobenius inner product between spatial feature tensors:
\begin{equation*}\mathcal{L}_{sim}^i = - \left\langle {x_t^p,\;\mathcal{T}\left( {x_{t - i}^I,\;x_t^p} \right)} \right\rangle \end{equation*}Lisim=−⟨xpt,T(xIt−i,xpt)⟩View Source\begin{equation*}\mathcal{L}_{sim}^i = - \left\langle {x_t^p,\;\mathcal{T}\left( {x_{t - i}^I,\;x_t^p} \right)} \right\rangle \end{equation*} In principle, this loss can further be formulated as the inlier loss from [54]. The overall learning objective sums over the k possible cycles, with weight λ = 0.1:
\begin{equation*}\mathcal{L} = \sum\limits_{i = 1}^k {\mathcal{L}_{sim}^i} + \lambda \mathcal{L}_{skip}^i + \lambda \mathcal{L}_{long}^i.\end{equation*}L=∑i=1kLisim+λLiskip+λLilong.View Source\begin{equation*}\mathcal{L} = \sum\limits_{i = 1}^k {\mathcal{L}_{sim}^i} + \lambda \mathcal{L}_{skip}^i + \lambda \mathcal{L}_{long}^i.\end{equation*} 3.2. Architecture for Mid-level Correspondence: The learning objective thus described can be used to train arbitrary differentiable tracking models. In practice, the architecture of the encoder determines the type of correspondence captured by the acquired representation. In this work, we are interested in a model for mid-level temporal correspondence. Accordingly, we choose the representation to be a mid-level deep feature map, coarser than pixel space but with sufficient spatial resolution to support tasks that require localization. An overview is provided in Figure 4b. 3.2.1 Spatial Feature Encoder ϕWe compute spatial features with a ResNet-50 architecture [18] without res5 (the final 3 residual blocks). We reduce the spatial stride of res4 for larger spatial outputs. Input frames are 240 × 240 pixels, randomly cropped from video frames re-scaled to have min(H, W) = 256. The size of the spatial feature of the frame is thus 30 × 30. Image patches are 80 × 80, randomly cropped from the full 240 × 240 frame, so that the feature is 10 × 10. We perform l2 normalization on the channel dimension of spatial features to facilitate computing cosine similarity. 3.2.2 Differentiable Tracker \mathcal{T}TGiven the representation from the encoder, we perform tracking with \mathcal{T}T. As illustrated in Figure 4b, the differentiable tracker is composed of three main components. Affinity function f provides a measure of similarity between coordinates of spatial features xI and xp. We denote the affinity function as f (xI ,xp) := A, such that f:{\mathbb{R}^{c \times 30 \times 30}} \times {\mathbb{R}^{c \times 10 \times 10}} \to {\mathbb{R}^{900 \times 100}}.f:Rc×30×30×Rc×10×10→R900×100. A generic choice for computing the affinity is the dot product between embeddings, referred to in recent literature as attention [67], [72] and more historically known as normalized cross-correlation [10], [35]. With spatial grid j in feature xI as xI (j) and the grid i in xp as xp(i),
\begin{equation*}A\left( {j,\;i} \right) = \frac{{\exp \left( {{x^I}{{(j)}^ \top }{x^p}\left( i \right)} \right)}}{{\sum\limits_j {\exp } \left( {{x^I}{{\left( j \right)}^ \top }{x^p}\left( i \right)} \right)}}\tag{1}\end{equation*}View Source\begin{equation*}A\left( {j,\;i} \right) = \frac{{\exp \left( {{x^I}{{(j)}^ \top }{x^p}\left( i \right)} \right)}}{{\sum\limits_j {\exp } \left( {{x^I}{{\left( j \right)}^ \top }{x^p}\left( i \right)} \right)}}\tag{1}\end{equation*}
where the similarity A(j, i) is normalized by the softmax over the spatial dimension of xI, for each xp(i). Note that the affinity function is defined for any feature dimension. Localizer g takes affinity matrix A as input and estimates localization parameters θ corresponding to the patch in feature xI which best matches xp. g is composed of two convolutional layers and one linear layer. We restrict g to output 3 parameters for the bilinear sampling grid (i.e. simpler than [23]), corresponding to 2D translation and rotation: g(A) := θ, where g : ℝ900×100 → ℝ3. The expressiveness of g is intentionally limited so as to place the burden of representation on the encoder (see Appendix B). Bilinear Sampler h uses the image feature xI and θ predicted by g to perform bilinear sampling to produce a new patch feature h(xI, θ) which is in the same size as xp, such that h : ℝc×30×30 × ℝ3 → Rc×10×10. 3.2.3 End-to-end Joint TrainingThe composition of encoder ϕ and \mathcal{T} forms a differentiable patch tracker, allowing for end-to-end training of ϕ and \mathcal{T}:
\begin{gather*} {x^I},\;{x^p} = \phi \left( I \right),\;\phi \left( p \right) \\ \mathcal{T}\left( {{x^I},\;{x^p}\,} \right) = h({x^I},\;g\left( {f\left( {{x^I},\;{x^p}} \right)} \right). \end{gather*}View Source\begin{gather*} {x^I},\;{x^p} = \phi \left( I \right),\;\phi \left( p \right) \\ \mathcal{T}\left( {{x^I},\;{x^p}\,} \right) = h({x^I},\;g\left( {f\left( {{x^I},\;{x^p}} \right)} \right). \end{gather*} Alignment Objective lθ is applied in the cycle-consistent losses \mathcal{L}_{long}^i and \mathcal{L}_{skip}^i, measuring the error in alignment between two patches. We follow the formulation introduced by [53]. Let M\left( {{\theta _{{x^p}}}} \right) correspond to the bilinear sampling grids used to form a patch feature xp from image feature xI. Assuming M\left( {{\theta _{{x^p}}}} \right) contains n sampling coordinates, the alignment objective is defined as:
\begin{equation*}{l_\theta }\left( {x_*^p,\hat x_t^p} \right) = \frac{1}{n}\sum\limits_{i = 1}^n {\left\| {M{{\left( {{\theta _{x_*^p}}} \right)}_i} - M{{\left( {{\theta _{\hat x_t^p}}} \right)}_i}} \right\|} _2^2\end{equation*}View Source\begin{equation*}{l_\theta }\left( {x_*^p,\hat x_t^p} \right) = \frac{1}{n}\sum\limits_{i = 1}^n {\left\| {M{{\left( {{\theta _{x_*^p}}} \right)}_i} - M{{\left( {{\theta _{\hat x_t^p}}} \right)}_i}} \right\|} _2^2\end{equation*} 

SECTION 4. Experiments: We report experimental results for a model trained on the VLOG dataset [12] from scratch; training on other large video datasets such as Kinetics gives similar results (see Appendix A.3). The trained representation is evaluated without fine-tuning on several challenging video propagation tasks: DAVIS-2017 [48], JHMDB [26] and Video Instance-level Parsing (VIP) [85]. Through various experiments, we show that the acquired representation generalizes to a range of visual correspondence tasks (see Figure 5). 4.1. Common Setup and Baselines: Training. We train the model on the VLOG dataset [12] without using any annotations or pre-training. The VLOG dataset contains 114K videos and the total length of the videos is 344 hours. During training, we set the number of past frames as k = 4. We train on a 4-GPU machine with a mini-batch size of 32 clips (8 clips per GPU), for 30 epochs. The model is optimized with Adam [32] with a learning rate of 0.0002 and momentum term βl = 0.5, β2 = 0.999. Inference. At test time, we use the trained encoder’s representation to compute dense correspondences for video propagation. Given initial labels of the first frame, we propagate the labels to the rest of the frames in the video. Labels are given by specified targets for the first frame of each task, with instance segmentation masks for DAVIS- 2017 [48], human pose key-points JHMDB [26], and both instance-level and semantic-level masks for VIP [85]. The labels of each pixel are discretized to C classes. For segmentation masks, C is the number of instance or semantic labels. For key-points, C is the number of key-points. We include a background class. We propagate the labels in the feature space. The labels in the first frame are one-hot vectors, while propagated labels are soft distributions. Figure 5: 
Visualizations of our propagation results. Given the labels as input in the first frame, our feature can propagate them to the rest of frames, without further fine-tuning. The labels include (a) instance masks in DAVIS-2017 [48], (b) pose key-points in JHMDB [26], (c) semantic masks in VIP [85] and even (d) texture map. 
Propagation by k-NN. Given a frame It and a frame It-1 with labels, we compute their affinity in feature space: At-1,t = f (Φ(It-1), Φ(It)) (Eq. 1). We compute label yi of pixel i in It as
\begin{equation*}{y_i} = \sum\limits_j {{A_{t - 1,t}}} \left( {j,\;i} \right){y_j},\tag{2}\end{equation*}View Source\begin{equation*}{y_i} = \sum\limits_j {{A_{t - 1,t}}} \left( {j,\;i} \right){y_j},\tag{2}\end{equation*}
where At-1,t(j, i) is the affinity between pixels i in It and j in It-1. We propagate from the top-5 pixels with the greatest affinity At-1,t(j, i) for each pixel i. Labels are propagated from It-1:t-K, as well as Ii, and averaged. Finally, we up-sample the label maps to image size. For segmentation, we use the argmax of the class distribution of each pixel. For key-points, we choose the pixel with the maximum score for each key-point type. Baselines. We compare with the following baselines:
Identity: Always copy the first frame labels. Optical Flow (FlowNet2 [22]): A state-of-the-art method for predicting optical flow with neural networks [22]. We adopt the open-source implementation which is trained with synthetic data in a supervised manner. For a target frame It, we compute the optical flow from frame It-1 to It and warp the labels in It-1 to It. SIFT Flow [39]: For a target frame It, we compute the SIFT Flow between It and its previous frames. We propagate the labels in K frames before It and the first frame via SIFT Flow warping. The propagation results are averaged to compute the labels for It. Transitive Invariance [74]: A self-supervised approach that combines multiple objectives: (i) visual tracking on raw video [73] and (ii) spatial context reasoning [9]. We use the open-sourced pre-trained VGG-16 [58] model and adopt our proposed inference procedure. DeepCluster [8]: A self-supervised approach which uses a K-means objective to iteratively update targets and learn a mapping from images to targets. It is trained on the ImageNet dataset without using annotations. We apply the trained model with VGG-16 and adopt the same inference procedure as our method. Video Colorization [69]: A self-supervised approach for label propagation. Trained on the Kinetics [29] dataset, it uses color propagation as self-supervision. The architecture is based on 3D ResNet-18. We report their results. ImageNet Pre-training [18]: The conventional setup for supervised training of ResNet-50 on ImageNet. Fully-Supervised Methods: We report fully-supervised methods for reference, which not only use ImageNet pretraining but also fine-tuning on the target dataset. Note that these methods do not always follow the inference procedure used with method, and labels of the first frame are not used for JHMDB and VIP at test time.  4.2. Instance Propagation on DAVIS-2017: We apply our model to video object segmentation on the DAVIS-2017 validation set [48]. Given the initial masks of the first frame, we propagate the masks to the rest of the frames. Note that there can be multiple instances in the first frame. We follow the standard metrics including the region similarity \mathcal{J} (IoU) and the contour-based accuracy\mathcal{F}. We set K = 7, the number of reference frames in the past. Table 1: 
Evaluation on instance mask propagation on DAVIS- 2017 [48]. We follow the standard metric on region similarity \mathcal{J} and contour-based accuracy \mathcal{F}.
We show comparisons in Table 1. Comparing to the recent Video Colorization approach [69], our method is 7.3% in \mathcal{J} and 6.7% in \mathcal{F}. Note that although we are only 4.4% better than the DeepCluster baseline in \mathcal{J}, we are better in contour accuracy \mathcal{F} by 6.2%. Thus, DeepCluster does not capture dense correspondence on the boundary as well. For fair comparisons, we also implemented our method with a ResNet-18 encoder, which has less parameters compared to the VGG-16 in [74], [8] and the 3D convolutional ResNet-18 in [69]. We observe that results are only around 2% worse than our model with ResNet-50, which is still better than the baselines. While the ImageNet pre-trained network performs better than our method on this task, we argue it is easy for the ImageNet pre-trained network to recognize objects under large variation as it benefits from curated object-centric annotation. Though our model is only trained on indoor scenes without labels, it generalizes to outdoor scenes. Although video segmentation is an important application, it does not necessarily show that the representation captures dense correspondence. 4.3. Pose Keypoint Propagation on JHMDB: To see whether our method is learning more spatially precise correspondence, we apply our model on the task of key-point propagation on the split 1 validation set of JH- MDB [26]. Given the first frame with 15 labeled human key-points, we propagate them through time. We follow the evaluation of the standard PCK metric [82], which measures the percentage of key-points close to the ground truth in different thresholds of distance. We set the number of reference frames same as experiments in DAVIS-2017. As shown in Table 2, our method outperforms all self- supervised baselines by a large margin. We observe that SIFT Flow actually performs better than other self- supervised learning methods in PCK@.1. Our method outperforms SIFT Flow by 8.7% in PCK@.1 and 9.9% in PCK@.2. Notably, our approach is only 0.7% worse than ImageNet pre-trained features in PCK@.1 and performs better in PCK@.2. Table 2: 
Evaluation on pose propagation on JHMDB [26]. We report the PCK in different thresholds.
4.4. Semantic and Instance Propagation on VIP: We apply our approach on the Video Instance-level Parsing (VIP) dataset [85], which is densely labeled with semantic masks for different human parts (e.g., hair, right arm, left arm, coat). It also has instance labels that differentiate humans. Most interestingly, the duration of a video ranges from 10 seconds to 120 seconds in the dataset, which is much longer than aforementioned datasets. We test our method on the validation set of two tasks in this dataset: (i) The first task is to propagate the semantic human part labels from the first frame to the rest of the video, and evaluate with the mean IoU metric; (ii) In the second task, the labels in the first frame are given with not only the semantic labels but also the instance identity. Thus, the model must differentiate the different arms of different human instances. We use the standard instance-level human parsing metric [37], mean Average Precision, for overlap thresholds varying from 0.1 to 0.9. Since part segments are relatively small (compared to objects in DAVIS-2017), we increase the input image size to 560 × 560 for inference, and use two reference frames, including the first frame. Semantic Propagation. As shown with the mIoU metric in Table 3, our method again exceeds all self-supervised baselines by a large margin (a [69] model is currently not available). ImageNet pre-trained models have the advantage of semantic annotation and thus do not necessarily have to perform tracking. As shown in Figure 5(c), our method is able to handle occlusions and multiple instances. Part Instance Propagation. This task is more challenging. We show the results in mean AP_{{\text{vol}}}^r in Table 3. Our method performs close to the level of ImageNet pre-trained features. We show different radial thresholds for average precision \left( {AP_{{\text{vol}}}^r} \right) in Table 4. ImageNet pre-trained features performs better under smaller thresholds and worse under larger thresholds, suggesting that it has an advantage in finding coarse correspondence while our method is more capable of spatial precision. Table 3: 
Evaluation on propagating human part labels in Video Instance-level Parsing (VIP) dataset [85]. We measure Semantic Propagation with mIoU and Part Instance Propagation in AP_{{\text{vol}}}^r.
Table 4: 
A more detailed analysis of different thresholds for Part Instance Propagation on the VIP dataset [85].
4.5. Texture Propagation: The acquired representation allows for propagation of not only instance and semantic labels, but also textures. We visualize texture propagation in Figure 5 (d); these videos are samples from DAVIS-2017 [48]. We "paint" a texture of 6 colored stripes on an the object in the first frame and propagate it to the rest of the frames using our representation. We observe that the structure of the texture is well preserved in the following frames, demonstrating that the representation allows for finding precise correspondence smoothly though time. See the project page for video examples. 4.6. Video Frame Reconstructions: Though we do not optimize for pixel-level objectives at training time, we can evaluate how well our method performs on pixel-level reconstruction. Specifically, given two images Is and It distant in time in a video, we compute coordinate-wise correspondences under the acquired representation and generate a flow field for pixel movement between Is and It. We then up sample the flow field to the same size as the image and warp it on Image Is to generate a new image I't (as shown in Figure 6). We compare the L1 distance between I't and It in RGB space and report the reconstruction errors in Table 5. For fair comparison, we perform this experiment on the DAVIS-2017 validation set, which none of the reported methods have seen. We experiment with two time gaps, 5 and 10 frames. For the smaller gap, FlowNet2 [22] performs reasonably well, whereas reconstruction degrades for larger gaps. In both cases, our method performs better than FlowNet2 and the ImageNet pre-trained network. This is encouraging: our method is not trained with pixel-level losses, yet out-performs methods trained with pixel-level tasks and human supervision. Figure 6: 
Given I1,I6 which have 5-frame gap, we compute the long-range flows between them with our representation. This flow can be used to warp I1 to generate image similar to I6. 
Table 5: 
We compute the long-range flow on two frames and warp the first one with the flow. We compare the warped frame with the second frame in L1 distance. The gaps are 5 or 10 frames.


SECTION 5. Limitations and Future Work: While in principle our method should keep improving with more data, in practice, learning seems to plateau after a moderate amount of training (i.e. 30 epochs). An important next step is thus how to better scale to larger, noisier data. A crucial component is improving robustness to occlusions and partial observability, for instance, by using a better search strategy for finding cycles at training time. Another issue is deciding what to track at training time. Picking patches at random can result in issues such as stationary background patches and tracking ambiguity - e.g. how should one track a patch containing two objects that eventually divergeƒ Jointly learning what to track may also give rise to unsupervised object detection. Finally, incorporating more context for tracking both at training and test time may be important for learning more expressive models of spatial-temporal correspondence. We hope this work is a step toward learning from the abundance of visual correspondence inherent in raw video in a scalable and end-to-end manner. While our experiments show promising results at certain levels of correspondence, much work remains to cover the full spectrum. 
ACKNOWLEDGEMENTS:: We thank members of the BAIR community for helpful discussions and feedback, and Sasha Sax and Michael Janner for draft comments. AJ is supported by the P.D. Soros Fellowship. XW is supported by the Facebook PhD Fellowship. This work was also supported, in part, by NSF grant IIS-1633310 and Berkeley DeepDrive.