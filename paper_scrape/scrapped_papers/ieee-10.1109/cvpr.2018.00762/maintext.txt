Figure 1: Dense pose estimation aims at mapping all human pixels of an RGB image to the 3D surface of the human body. We introduce DensePose-COCO, a large-scale ground-truth dataset containing manually annotated image-to-surface correspondences for 50K images, and train DensePose-rcnn to densely regress UV coordinates at multiple frames per second. Left: The image and the regressed correspondence by DensePose-RCNN. Middle: DensePose-COCO dataset annotations. Right: Partitioning and UV parametrization of the body surface. 

SECTION 1. Introduction: This work aims at pushing further the envelope of human understanding in images by establishing dense correspondences between a 2D image and a 3D, surface-based representation of the human body. We can understand this task as involving several other problems, such as object detection, pose estimation, part and instance segmentation either as special cases or prerequisites. Addressing this task has applications in problems that require going beyond plain landmark localization, such as graphics, augmented reality, or human-computer interaction, and could also be a stepping stone towards general 3D-based object understanding. The task of establishing dense correspondences from an image to a surface-based model has been addressed mostly in the setting where a depth sensor is available [43], [34], [46]. Instead, we establish dense image-to-surface correspondences using as sole input the RGB values of a single image. Several other works have recently aimed at recovering dense correspondences between pairs [3] or sets of RGB images [50], [10] in an unsupervised setting. More recently, [44] used the equivariance principle in order to align sets of images to a common coordinate system, while following the general idea of groupwise image alignment, e.g. [24], [22]. While these works target general categories, ours is focused on arguably the most important one, humans. For humans one can simplify the task by exploiting parametric deformable surface models, such as the Skinned Multi-Person Linear (SMPL) model [2], or the more recent Adam model [15] obtained through controlled 3D surface acquisition. Turning to the task of image-to-surface mapping, in [2], the authors propose a two-stage method of detecting human landmarks and fitting a parametric deformable surface model to the image through iterative minimization. In parallel to our work, [21] extend [2] to operate end-to-end, incorporating the iterative reprojection error minimization as a module of a deep network that recovers 3D camera pose and the low-dimensional body parametrization.
Figure 2: We annotate dense correspondence between images and a 3D surface model by asking the annotators to first segment the image into semantic regions and then localize each of the sampled points on any of the rendered part images. The surface coordinates of the rendered views are used to localize the collected 2D points on the 3D model. 
Our methodology differs from all these works in that we take a full-blown supervised learning approach and gather ground-truth correspondences between images and a detailed, accurate parametric surface model of the human body [28]: rather than using the SMPL model at test time we only use it as a means of defining our problem during training. Our approach can be understood as the next step in the line of works on human pose estimation [27], [1], [20], [7], [42], [19], [29]. Human part segmentation masks have been provided in a number of datasets [48], [6], [13]; these can be understood as providing a coarsened version of image-to-surface correspondence, where rather than continuous coordinates one predicts discretized part labels [35]. Surface-level supervision was only recently introduced for synthetic images in [45], while in [23] a dataset of 8515 images is annotated with keypoints and semi-automated fits of 3D models to images. In this work instead of compromising the extent and realism of our training set we introduce a novel annotation pipeline that allows us to gather ground-truth correspondences for 50K images of COCO, yielding our new DensePose-COCO dataset. Our work is closest in spirit to the recent DenseReg framework [14], where CNNs were trained to establish dense correspondences between a 3D model and images ‘in the wild’. That work focused mainly on faces, and provided evaluations on datasets with moderate pose variability. Here, however, we are facing new challenges, due to the higher complexity and flexibility of the human body, as well as the larger scale variation. We address these challenges by designing appropriate architectures (Sec. 3) that yield substantial improvements over a DenseReg-type fully convolutional architecture. By combining our approach with the recent Mask-RCNN system of [16] we show that a discriminatively trained model can efficiently recover highly-accurate correspondence fields for complex scenes involving tens of persons: on a GTX 1080 GPU our system operates at 20–26 fps for a 240×320 image or 4–5 fps for a 800×1100 image. Our contributions can be summarized in three points. Firstly, as described in Sec. 2, we introduce the first manually-collected ground truth dataset for the task, by gathering dense correspondences between the SMPL model [28] and persons appearing in the COCO dataset. This is accomplished through a novel annotation pipeline that exploits 3D surface information during annotation. Secondly, as described in Sec. 3, we use the resulting dataset to train CNN-based systems that deliver dense correspondence ‘in the wild’ by regressing body surface coordinates at any image pixel. We experiment with both fully-convolutional architectures, relying on Deeplab [4], and also with region-based systems, relying on Mask-RCNN [16], observing a superiority of the latter. We also consider cascading variants of our approach, yielding further improvements over existing architectures. Thirdly, we explore different ways of exploiting our constructed ground truth information. Our supervision signal is defined over a randomly chosen subset of image pixels per training sample. We use these sparse correspondences to train a teacher network that can inpaint the supervision signal in the rest of the image domain. Using this inpainted signal results in better performance when compared to either sparse points, or any other existing dataset, as shown experimentally in Sec. 4. Our experiments indicate that dense human pose estimation is to a large extent feasible, but still has space for improvement. Our code and the data will be made publicly available at http://DensePose.org.
Figure 3: Visualization of annotations: Image (left), U (middle) and V (right) values for the collected points. 


SECTION 2. DensePose-COCO Dataset: Gathering rich, high-quality training sets has been a catalyst for progress in the classification [40], detection and segmentation [8], [27] tasks. There currently exists no manually collected ground-truth for dense human pose estimation for real images. The works of [23], [45] can be used as surrogates, but as we show in Sec. 4 provide worse supervision. In this Section we introduce DensePose-COCO, a large-scale dataset for dense human pose estimation. DensePose-COCO provides ground-truth for 50K humans and contains more than 5 million manually annotated pairs. We first present our annotation pipeline, since our design choices may be useful for general 3D annotation. We then analyze the accuracy of the collected ground-truth, and finally introduce evaluation metrics for dense pose estimation. 2.1. Annotation System: In this work we use human annotators to establish dense correspondences from 2D images to surface-based representations of the human body. If done naively this would require manipulating a surface through rotations to find the vertices corresponding to every 2D image point, which is time-demanding and inefficient. Instead, we construct an annotation pipeline through which we can efficiently gather annotations for image-to-surface correspondence. As shown in Fig. 2, in the first stage we ask annotators to delineate regions corresponding to visible, semantically defined body parts. These include Head, Torso, Lower/Upper Arms, Lower/Upper Legs, Hands and Feet. In order to simplify the UV parametrization we design the parts to be isomorphic to a plane, partitioning the upper and lower limbs and torso into frontal-back parts. For head, hands and feet, we use the manually obtained UV fields provided in the SMPL model [28]. For other parts we obtain the unwrapping via multi-dimensional scaling applied to pairwise geodesic distances. The UV fields for the resulting 24 parts are visualized in Fig. 1 (right). We instruct the annotators to estimate the body part behind the clothes, so that for instance wearing a large skirt will not complicate the subsequent correspondence annotations. In the second stage we sample every part region with a set of roughly equidistant points obtained by running k-means over the coordinates occupied by each part and request the annotators to bring these points in correspondence with the surface. The number of sampled points varies based on the size of the part and the maximum number of sampled points per part is 14. In order to simplify this task we ‘unfold’ the part surface by providing six pre-rendered views of the same body part and allow the user to place landmarks on any of them. This allows the annotator to choose the most convenient viewpoint by selecting one among six options instead of manually rotating the surface. As the user indicates a point on any of the rendered part views, its surface coordinates are used to simultaneously show its position on the remaining views – this gives a global overview of the correspondence. We show indicative visualizations of the gathered annotations in Fig. 3. 2.2. Accuracy of Human Annotators: A common concern when gathering ground-truth is the accuracy of the human annotations, which is often seen as an upper bound of what vision algorithms can deliver. In pose estimation one typically asks multiple annotators to label the same landmark, which is then used to assess the variance in position, e.g. [27], [38]. In our case we can directly compare to the true mesh coordinates used to render a pixel, rather than first estimating a ‘consensus’ landmark location among multiple human annotators. In particular, we provide annotators with synthetic images generated through the rendering system and textures of [45]. We ask the annotators to bring the synthesized images into correspondence with the surface using our annotation tool, and for every image k estimate the geodesic distance di,k between the correct surface point, i and the point estimated by human annotators i^k: di,k=g(i,i^k),(1)View Source\begin{equation*}
d_{i, k}=g(i,\hat{i}_{k}),\tag{1}
\end{equation*} g(⋅,⋅) is the geodesic distance between two surface points. For any image k, we annotate and estimate the error on a randomly sampled set of surface points Sk and interpolate the errors on the remainder of the surface. Finally, we average the errors across all examples given to the annotators. As shown in Fig. 4 the annotation errors are substantially smaller on small surface parts with distinctive features that could help localization (face, hands, feet), while on larger uniform areas that are typically covered by clothes (torso, back, hips) the annotator errors can get larger. 2.3. Evaluation Metrics: We consider two different ways of summarizing correspondence accuracy over the whole human body, including pointwise and per-instance evaluation. Pointwise EvaluationThis approach evaluates correspondence accuracy over the whole image domain through the Ratio of Correct Point (RCP) correspondences, where a correspondence is declared correct if the geodesic distance is below a certain threshold. As the threshold t varies, we obtain a curve f(t), whose area provides us with a scalar summary of the correspondence accuracy. For any given image we have a varying set of points coming with ground-truth signals. We summarize performance on the ensemble of such points, gathered across images. We evaluate the area under the curve (AUC), AUCa=1a∫a0f(t)dt, for two different values of a=10cm, 30cm yielding AUC10 and AUC30 respectively, where AUC10 is understood as being an accuracy measure for more refined correspondence. This performance measure is easily applicable to both single- and multi-person scenarios and can deliver directly comparable values. In Fig. 5 we provide the per-part pointwise evaluation of the human annotator performance on synthetic data, which can be seen as an upper bound for the performance of our systems. Per-Instance EvaluationInspired by the object keypoint similarity (OKS) measure used for pose evaluation on the COCO dataset [27], [38], we introduce geodesic point similarity (GPS) as a correspondence matching score: GPSj=1|Pj|∑p∈Pjexp(−g(ip,i^p)22κ2),(2)View Source\begin{equation*}\text{GPS} j= \frac{1}{\vert P_{j}\vert}\sum\limits_{p\in P_{j}}\exp\left(\frac{-g(i_{p},\hat{i}_{p})^{2}}{2\kappa^{2}}\right),\tag{2}\end{equation*} where Pj is the set of ground truth points annotated on person instance j, ip is the vertex estimated by a model at point p, i^p is the ground truth vertex p and κ is a normalizing parameter. We set κ=0.255 so that a single point has a GPS value of 0.5 if its geodesic distance from the ground truth equals the average half-size of a body segment, corresponding to approximately 30 cm. Intuitively, this means that a score of GPS≈0.5 can be achieved by a perfect part segmentation model, while going above that also requires a more precise localization of a point on the surface.
Figure 4: Average human annotation error on the surface. 
Once the matching is performed, we follow the COCO challenge protocol [27], [39] and evaluate Average Precision (AP) and Average Recall (AR) at a number of GPS thresholds ranging from 0.5 to 0.95, which corresponds to the range of geodesic distances between 0 and 30 cm, We use the same range of distances to perform both per-instance and per-point evaluation. 

SECTION 3. Learning Dense Human Pose Estimation: We now turn to the task of training a deep network that predicts dense correspondences between image pixels and surface points. Such a task was recently addressed in the Dense Regression (DenseReg) system of [14] through a fully-convolutional network architecture [4]. In this Section we introduce improved architectures by combining the DenseReg approach with the Mask-RCNN architecture [16], yielding our ‘DensePose-RCNN’ system. We develop cascaded extensions of DensePose-RCNN that further improve accuracy and describe a training-based interpolation method that allows us to turn a sparse supervision signal into a denser and more effective variant. 3.1. Fully-Convolutional Dense Pose Regression: Since the human body has a complicated structure, we break it into multiple independent pieces and parametrize each piece using a local two-dimensional coordinate system, that identifies the position of any node on this surface part. Using the surface representation, a simple choice for dense image-to-surface correspondence estimation consists in using a fully convolutional network (FCN) that combines a classification and a regression task, similar to DenseReg. In a first step, we classify a pixel as belonging to either background or one among the surface parts. In a second step, a regression system indicates the exact coordinates of the pixel within the part. Intuitively, we can say that we first use appearance to make a coarse estimate of where the pixel belongs to and then align it to the exact position through some small-scale correction. Concretely, coordinate regression at an image position i can be formulated as follows: c∗=argmaxcP(c|i),[U,V]=Rc∗(i)(3)View Source\begin{equation*}
c^{\ast}= \arg\!\max\nolimits_{c}P(c\vert i),\quad [U, V]=R^{c^{\ast}}(i) \tag{3}
\end{equation*} where in the first stage we assign position i to the body part c∗ that has highest posterior probability, as calculated by the classification branch, and in the second stage we use the regressor Rc∗ that places the point i in the continuous U,V coordinates parametrization of part c∗. In our case, c can take 25 values (one is background), meaning that Px is a 25-way classification unit, and we train 24 regression functions Rc, each of which provides 2D coordinates within its respective part c. While training, we use a cross-entropy loss for part classification and a smooth L1 loss [12] for each part-specific regression function. The regression loss for a part is only considered for pixels occupied by that part.
Figure 5: Human annotation error distribution within parts. 
Figure 6: DensePose-RCNN architecture: We use a cascade of region proposal generation and feature pooling, followed by a fully-convolutional network that densely predicts discrete part labels and continuous surface coordinates. 
3.2. Region-Based Dense Pose Regression: Using an FCN makes the system particularly easy to train, but loads the same deep network with too many tasks, including part segmentation and pixel localization, while at the same time requiring scale-invariance, which becomes challenging for humans in COCO. Here we adopt the region-based approach of [36], [16], which consists in a cascade of proposing regions-of-interest (ROI), extracting region-adapted features through ROI pooling [17], [16] and feeding the resulting features into a region-specific branch. Region-based architectures decompose the complexity of the task into controllable modules and implement a scale selection mechanism through ROI-pooling. At the same time, they can be jointly trained in an end-to-end manner [36]. We adopt the settings introduced in [16], involving the construction of Feature Pyramid Network [26] features, and ROI-Align pooling, which have been shown to be important for tasks that require spatial accuracy. We adapt this architecture to our task, so as to obtain dense part labels and coordinates within each of the selected regions. As shown in Fig. 6, on top of ROI-pooling we introduce an FCN that is entirely devoted to these two tasks, generating a classification and a regression head that provide the part assignment and part coordinate predictions, as in DenseReg. For simplicity, we use the exact same architecture used in the keypoint branch of Mask-RCNN, consisting of a stack of 8 alternating 3×3 fully convolutional and ReLU layers with 512 channels. At the top of this branch we have the same classification and regression losses as in the FCN baseline, but we now use a supervision signal that is cropped within the proposed region.
Figure 7: Cross-cascading architecture: The roialign output in fig. 6 feeds into the densepose network and auxiliary networks for other tasks (masks, keypoints). Once first-stage predictions are obtained from all tasks, they are combined and fed into a second-stage refinement unit. 
3.3. Multi-Task Cascaded Architectures: Inspired by the success of recent pose estimation models based on iterative refinement [47], [31] we experiment with cascaded architectures. Cascading can improve performance both by providing context to the following stages, and also through the benefits of deep supervision [25]. As shown in Fig. 7 we do not confine ourselves to cascading within a single task, but also exploit information from related tasks, such as keypoint estimation and instance segmentation, which have successfully been addressed by the Mask-RCNN architecture [16]. This allows us to exploit task synergies and the complementary merits of different sources of supervision. 3.4. Distillation-Based Ground-Truth Interpolation: Even though we aim at dense pose estimation at test time, in every training sample we annotate only a sparse subset of the pixels, approximately 100–150 per human. This does not necessarily pose a problem during training, since we can make our classification/regression losses oblivious to points where the ground-truth correspondence was not collected, simply by not including them in the summation over the per-pixel losses [41]. However, we have observed that we obtain better results by “inpainting” the values of the supervision signal on positions that were not originally annotated. For this we adopt a learning-based approach where we firstly train a “teacher” network to reconstruct the ground-truth values wherever these are observed, and then deploy it on the full image domain, yielding a dense supervision signal. As shown in Fig. 8, we use human segmentation maps available in COCO in order to get the most accurate supervision signal possible by (a) replacing background structures with a common gray value and (b) ignoring the network's predictions outside the human region. The performance of the teacher network can therefore be understood as an upper bound on what an algorithm can deliver on real data, since we remove false positives, normalize scale and remove background variation during both training and testing. 

SECTION 4. Experiments: In all experiments we assess the methods on a test set of 1.5K images containing 2.3K humans and use 48K humans in the training set. Our test set coincides with the COCO keypoints-minival partition used by [16] and the training set with the COCO-train partition. Before assessing dense pose estimation in the wild (Sec. 4.2), we start in Sec. 4.1 with the ‘Single-Person’ setting where the input images are cropped around ground-truth boxes. This factors out the effects of detection performance and provides us with a controlled setting to assess the usefulness of the DensePose-COCO dataset. 4.1. Single-Person Dense Pose Estimation: In Sec. 4.1.1 we compare the DensePose-COCO dataset to other sources of supervision for dense pose estimation. In Sec. 4.1.2 we compare the performance of the model-based system of [2] with ours. We note that the system of [2] was not trained with the same amount of data as our model; this comparison therefore serves primarily to show the merit of our large-scale dataset for discriminative training. 4.1.1 Manual Supervision Versus SurrogatesWe start by assessing whether DensePose-COCO improves the accuracy of dense pose estimation with respect to the prior semi-automated, or synthetic supervision signals.
Figure 8: We train a ‘teacher network’ with our collected sparse supervision signal and use it to ‘inpaint’ a dense supervision signal used to train our region-based system. 
A semi-automated method is used for the ‘Unite the People’ (UP) dataset of [23], where human annotators verified the results of fitting the SMPL 3D deformable model [28] to 2D images. However, model fitting often fails in the presence of occlusions, or extreme poses, and is never guaranteed to be entirely successful – for instance, even after rejecting a large fraction of the fitting results, the feet are still often misaligned in [23]. Synthetic ground-truth can be established by rendering images using surface-based models [33], [32], [37], [11], [5], [30]. This has recently been applied to human pose in the SURREAL dataset of [45], where the SMPL model [28] was rendered with the CMU Mocap dataset poses [29]. However, domain shift can emerge because of the different statistics of rendered and natural images. Since both of these two methods use the same SMPL surface model as the one we use in our work, we can directly compare results, and also combine datasets. We render our dense coordinates and our dense part labels on the SMPL model for all 8514 images of UP dataset and 60k SURREAL models for comparison. In Fig. 10 we assess the test performance of ResNet-101 FCNs of stride 8 trained with different datasets, using a Deeplab-type architecture. During training we augment samples from all of the datasets with scaling, cropping and rotation. We observe that the surrogate datasets lead to weaker performance, while their combination yields improved results. Still, their performance is substantially lower than the one obtained by training on our DensePose dataset, while combining the DensePose with SURREAL results in a moderate drop in network performance. Based on these results we rely exclusively on the DensePose dataset for training in the remaining experiments, even though domain adaptation [9] could be used in the future to exploit synthetic sources of supervision. The last line in the table of Fig. 10 (‘DensePose*’) indicates the additional performance boost that we get by using the teacher network settings described in Sec. 3.4. Clearly, the results are not directly comparable with those of other methods, since we use additional information to remove background structures. Still, the resulting predictions are substantially closer to human performance – we can therefore confidently use our teacher network to obtain dense supervision for the experiments in Sec. 4.2. 4.1.2 FCNN-Vs Model-Based Pose EstimationIn Fig. 9 we compare our method to the SMPLify pipeline of [2], which fits the 3D SMPL model to an image based on a pre-computed set of landmark points. We use the code provided by [23] with both DeeperCut pose estimation landmark detector [18] for 14-landmark results and with the 91-landmark alternative proposed in [23].
Figure 9: Qualitative comparison between model-based single-person pose estimation of SMPLify [2] and our FCN-based result, in the absence (‘full-body images’) and presence (‘all images’) of occlusions. 
Figure 10: Single-person performance for different kinds of supervision signals used for training: DensePose leads to substantially more accurate results than surrogate datasets. DensePose* uses a figure-ground oracle at both training and test time. 
Figure 11: Results of multi-person dense correspondence labelling. Here we compare the performance of our proposed DensePose-RCNN system against the fully-convolutional alternative on realistic images from the COCO dataset including multiple persons with high variability in scales, poses and backgrounds. 
Since the whole body is visible in the MPII dataset used for training the landmark detectors, for a fair comparison we separately evaluate on images where 16/17 or 17/17 landmarks are visible and on the whole test set. We observe that while being orders of magnitude faster (0.04-0.25” vs 60-200”) our bottom-up method largely outperforms the iterative, model fitting result. As mentioned above, this difference in accuracy indicates the merit of having at our disposal DensePose-COCO for discriminative training. 4.2. Multi-Person Dense Pose Estimation: Having established the merit of the DensePose-COCO dataset, we now turn to examining the impact of network architecture on dense pose estimation in-the-wild. In Fig. 11 we summarize our experimental findings using the same RCP measure used in Fig. 10. We observe firstly that the FCN-based performance in-the-wild (curve ‘DensePose-FCN’) is now substantially lower than that of the DensePose curve in Fig. 11. Even though we apply a multi-scale testing strategy that fuses probabilities from multiple runs using input images of different scale [49], the FCN is not sufficiently robust to deal with the variability in object scale. We then observe in curve ‘DensePose-RCNN’ a big boost in performance thanks to switching to a region-based system. The networks up to here have been trained using the sparse set of points that have been manually annotated. In curve ‘DensePose-RCNN-Distillation’ we see that using the dense supervision signal delivered by our DensePose* system on the training set yields a substantial improvement. Finally, in ‘DensePose-RCNN-Cascade’ we show the performance achieved thanks to the introduction of cascading: Sec. 3.3 almost matches the ‘DensePose*’ curve of Fig. 10. This is a remarkably positive result: as described in Sec. 3.4, the ‘DensePose*’ curve corresponds to a very privileged evaluation and can be understood as an upper bound of what one can expect to obtain when operating in-the-wild. We see that our best system is marginally below that level of performance, which clearly reveals the power of the three modifications we introduce, namely region-based processing, inpainting the supervision signal, and cascading. In Table 1 we report the AP and AR metrics described in Sec. 2 as we change different choices in our architecture. We have conducted experiments using both ResNet-50 and ResNet-101 backbones and observed an only insignificant boost in performance with the larger model (first two rows in Table 1). The rest of our experiments are therefore based on the ResNet-50-FPN version of DensePose-RCNN. The following two experiments shown in the middle section of Table 1 indicate the impact on multi-task learning. Augmenting the network with the mask or keypoint branches yields improvements with any of these two auxiliary tasks. The last section of Table 1 reports improvements in dense pose estimation obtained through the cascading setup from Fig. 7. Incorporating additional guidance in particular from the keypoint branch significantly boosts performance. Our qualitative results in Fig. 12 indicate that our method is able to handle large amounts of occlusion, scale, and pose variation, regardless of the shape of the clothes.
Figure 12: Qualitative evaluation of DensePose-RCNN. Left: Input, Right: DensePose-RCNN estimates. Our system successfully estimates body pose regardless of skirts or dresses, while handling a large variability of scales, poses, and occlusions. 
Table 1: Per-instance evaluation of DensePose-RCNN performance on COCO minival. All multi-task experiments are based on ResNet-50. DensePose-ST applies cascading to the base, single-task network.


SECTION 5. Conclusion: In this work we have addressed the task of dense human pose estimation using discriminatively trained models. We introduce DensePose-COCO, a large-scale dataset of ground-truth image-surface correspondences and develop novel architectures for recovering highly-accurate dense correspondences between images and the body surface in multiple frames per second. We anticipate that this will lead to novel augmented reality or graphics tasks, and we intend to further pursue the association of images with semantic 3D object representations. 
ACKNOWLEDGEMENTS: We thank the authors of [16] for their code, P. Dollar and T.-Y. Lin for help with COCO, the authors of [28] for making the SMPL model open for research and H. Y. Güler for his help with back-end development.