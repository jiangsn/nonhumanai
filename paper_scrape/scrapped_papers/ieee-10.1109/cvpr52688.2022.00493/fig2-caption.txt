Figure 2. A detailed overview of mttr. First, the input text and video frames are passed through feature encoders and then concatenated into multimodal sequences (one per frame). A multimodal transformer then encodes the feature relations and decodes instance-level features into a set of prediction sequences. Next, corresponding mask and reference prediction sequences are generated. Finally, the predicted sequences are matched with the ground truth sequences for supervision (in training) or used to generate the final prediction (during inference).