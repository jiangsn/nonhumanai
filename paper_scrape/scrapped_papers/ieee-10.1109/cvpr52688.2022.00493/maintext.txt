SECTION 1. Introduction: Attention-based [41] deep neural networks exhibit im-pressive performance on various tasks across different fields, from computer vision [10], [27] to natural language processing [3], [8]. These advancements make networks of this sort, such as the Transformer [41], particularly interesting candidates for solving multimodal problems. By relying on the self-attention mechanism, which allows each token in a sequence to globally aggregate information from every other token, Transformers excel at modeling global dependencies and have become the cornerstone in most NLP tasks [3], [8], [35], [50]. Transformers have also started showing promise in solving computer vision tasks, from recognition [10] to object de-tection [4] and even outperforming the long-used CNNs as general-purpose vision backbones [27].
Figure 1. Given a text query and a sequence of video frames, the proposed model outputs prediction sequences for all object instances in the video prior to determining the referred instance. Here predictions with the same color and shape belong to the same sequence and attend to the same object instance in different frames. Note that the order of instance predictions for different frames remains the same. Best viewed in color. 
The referring video object segmentation task (RVOS) involves the segmentation of a text-referred object instance in the frames of a given video. Compared with the referring image segmentation task (RIS) [29], [52], in which objects are mainly referred to by their appearance, in RVOS objects can also be referred to by the actions they are performing or in which they are involved. This renders RVOS significantly harder than RIS, as text expressions that refer to actions often cannot be properly deduced from a single static frame. Furthermore, unlike their image-based counterparts, RVOS methods may be required to establish data association of the referred object across multiple frames (tracking) in order to deal with disturbances such as occlusions or motion blur. To solve these challenges and effectively align video with text, existing RVOS approaches [14], [25], [32] typically rely on complicated pipelines. In contrast, here we propose a simple, end-to-end Transformer-based approach to RVOS. Using recent advancements in Transformers for textual feature ex-traction [26], [41], visual feature extraction [10], [27], [28] and object detection [4], [45], we develop a framework that sig-nificantly outperforms existing approaches. To accomplish this, we employ a single multimodal Transformer and model the task as a sequence prediction problem. Given a video and a text query, our model generates prediction sequences for all objects in the video before determining the one the text refers to. Additionally, our method is free of text-related inductive bias modules and utilizes a simple crossentropy loss to align the video and the text. As such, it is much less complicated than previous approaches to the task. The proposed pipeline is schematically depicted in Fig. 1. First, we extract linguistic features from the text query using a standard Transformer-based text encoder, and visual features from the video frames using a spatio-temporal encoder. The features are then passed into a multimodal Transformer, which outputs several sequences of object predictions [45]. Next, to determine which of the predicted sequences best cor-responds to the referred object, we compute a text-reference score for each sequence. For this we propose a temporal segment voting scheme that allows our model to focus on more relevant parts of the video when making the decision. Our main contributions are as follows:
We present a Transformer-based RVOS framework, dubbed Multimodal Tracking Transformer (MTTR), which models the task as a parallel sequence prediction problem and outputs predictions for all objects in the video prior to selecting the one referred to by the text. Our sequence selection strategy is based on a temporal segment voting scheme, a novel reasoning scheme that allows our model to focus on more relevant parts of the video with regards to the text. The proposed method is end-to-end trainable, free of text-related inductive bias modules, and requires no additional mask refinement. As such, it greatly simplifies the RVOS pipeline compared to existing approaches. We thoroughly evaluate our method. On the A2D-Sentences and JHMDB-Sentences [12], MTTR significantly outperforms all existing methods across all metrics. We also show strong results on the public vali-dation set of Refer- YouTube-Vos [39], a challenging dataset that has yet to receive attention in the literature.  

SECTION 2. Related Work: Referring Video Object Segmentation: The RVOS task was introduced by Gavrilyuk et al. [12], whose goal was to attain pixel-level segmentation of actors and their actions in video content. To effectively aggregate and align visual, temporal and lingual information from video and text, state-of-the-art RVOS approaches typically rely on complicated pipelines [25], [30], [32], [42], [43]. Gavrilyuk et al. [12] proposed an I3D-based [5] encoder-decoder architecture that generated dynamic filters from text features and convolved them with visual features to obtain the masks. Following them, Wang et al. [42] added spatial context to the kernels with deformable convolutions [7]. For a more effective representation, VT-Capsule [30] encoded each modality in capsules [37], while ACGA [43] utilized a co-attention mechanism to enhance the multimodal features. To improve positional relation representations in the text, PRPE [32] explored a positional encoding mechanism based on polar coordinates. URVOS [39] improved tracking capabilities by performing language-based object segmentation on a key frame and then propagating its mask throughout the video. AAMN [49] utilized a top-down approach where an off-the-shelf object detector is used to localize objects in the video prior to parsing relations between visual and textual features. CMPC- V [25] achieved state-of-the-art results by constructing a temporal graph from video and text features, and applying graph convolution [18] to detect the referred entity. Transformers: The Transformer [41] was introduced as an attention-based building block for sequence-to-sequence machine translation, and since then has become the corner-stone for most NLP tasks [3], [8], [35], [50]. Unlike previous architectures, the Transformer relies entirely on the attention mechanism to draw dependencies between input and output. Recently, the introduction of Transformers to computer vision tasks has demonstrated spectacular performance. DETR [4], which utilizes a non-auto-regressive Transformer, simpli-fies the traditional object detection pipeline while achieving performance comparable to that of CNN-based detectors [36]. Given a fixed set of learned object queries, DETR rea-sons about the global context of an image and the relations between its objects and then outputs a final set of detection predictions in parallel. VisTR [45] extends the idea behind DETR to video instance segmentation. It views the task as a direct end-to-end parallel sequence prediction problem. By supervising video instances at the sequence level as a whole, VisTR is able to output an ordered sequence of masks for each instance in a video directly (i.e., natural tracking).
Figure 2. A detailed overview of mttr. First, the input text and video frames are passed through feature encoders and then concatenated into multimodal sequences (one per frame). A multimodal transformer then encodes the feature relations and decodes instance-level features into a set of prediction sequences. Next, corresponding mask and reference prediction sequences are generated. Finally, the predicted sequences are matched with the ground truth sequences for supervision (in training) or used to generate the final prediction (during inference). 
ViT [10] introduced the Transformer to image recognition by using linearly projected patches as tokens for a Trans-former encoder. Swin Transformer [27] proposed a general-purpose backbone for computer vision based on a hierarchical Transformer whose representations are computed inside shifted windows. This architecture was also extended to the video domain [28], which we adapt as our temporal encoder. Another recent relevant work is MDETR [16], a DETR-based end-to-end multimodal detector that detects objects in an image conditioned on a text query. Different from our method, their approach is designed to work on static images, and its performance largely depends on well-annotated datasets that contain aligned text and box annotations, the types of which are not available in the RVOS task. 

SECTION 3. Method: 3.1. Method Overview: Task DefinitionThe input of RVOS consists of a frame sequence V={vi}Ti=1, where vi∈RC×H0×W0, and a text query T={ti}Li=1, where ti is the ith word in the text. Then, for a subset of frames of interest VI⊆V of size TI, the goal is to segment the object referred by T in each frame in VI. We note that since producing mask annotations requires significant efforts, VI rarely contains all of the frames in V. Feature ExtractionWe begin by extracting features from each frame in the sequence V using a deep spatio-temporal encoder. Simultaneously, linguistic features are extracted from the text query T using a Transformer-based [41] text encoder. Then, the spatio- temporal and linguistic features are linearly projected to a shared dimension D. Instance PredictionIn the next step, the features of each frame of interest are flattened and separately concatenated with the text embeddings, producing a set of TI multimodal sequences. These sequences are fed in parallel into a Trans-former [4], [41]. In the Transformer's encoder layers, the textual embeddings and the visual features of each frame exchange information. Then, the decoder layers, which are fed with Nq object queries per input frame, query the mul-timodal sequences for entity-related information and store it in the object queries. Corresponding queries of different frames share the same trainable weights and are trained to attend to the same instance in the video (each one in its des-ignated frame). We refer to these queries (represented by the same unique color and shape in Figs. 1 and 2) as queries belonging to the same instance sequence. This design allows for natural tracking of each object instance in the video [45]. Output GenerationFor each output instance sequence, we generate a a corresponding mask sequence using an FPN-like [22] spatial decoder and dynamically generated conditional convolution kernels [40], [44]. Finally, we use a novel text-reference score function that, based on text associations, determines which of the object query sequences has the strongest association with the object described in T, and returns its segmentation sequence as the model's prediction. 3.2. Temporal Encoder: A suitable temporal encoder for the RVOS task should be able to extract both visual characteristics (e.g., shape, size, location) and action semantics for each instance in the video. Several previous works [12], [25], [32] utilized the Kinetics-400 [17] pre-trained I3D network [5] as their temporal encoder. However, since I3D was originally designed for action classification, using its outputs as-is for tasks that require fine details (e.g., instance segmentation) is not ideal as the features it outputs tend to suffer from spatial misalignment caused by temporal downsampling. To compensate for this side effect, past state-of-the-art approaches came up with different solutions, from auxiliary mask refinement algorithms [19], [25] to utilizing additional backbones that operate alongside the temporal encoder [14]. In contrast, our end-to-end approach does not require any additional mask refinement steps and utilizes a single backbone. Recently, the Video Swin Transformer [28] was proposed as a generalization of the Swin Tranformer [27] to the video domain. While the original Swin was designed with dense predictions (such as segmentation) in mind, Video Swin was tested mainly on action recognition benchmarks. To the best of our knowledge, we are the first to utilize it (with a slight modification) for video segmentation. As opposed to I3D, Video Swin contains just a single temporal downsampling layer and can be easily modified to output per-frame feature maps (we refer to the supplement for more details). As such, it is a much better choice for processing a full sequence of consecutive video frames for segmentation purposes. 3.3. Multimodal Transformer: For each frame of interest, the temporal encoder generates a feature map fVIt∈RH×W×Cv and the text encoder outputs a linguistic embedding vector fT∈RL×DT for the text. These visual and linguistic features are linearly pro-jected to a shared dimension D. The features of each frame are then flattened and separately concatenated with the text embeddings, resulting in a set of TI multimodal sequences, each of shape (H×W+L)×D. The multimodal sequences along with a set of Nq instance sequences are then fed in parallel into a Transformer as described earlier. Our Trans-former architecture is similar to the one used in DETR [4]. Accordingly, the problem now comes down to finding the instance sequence that attends to the text-referred object. 3.4. The Instance Segmentation Process: Our segmentation process, as shown in Fig. 2, consists of several steps. First, given FE, the updated multimodal sequences output by the last Transformer encoder layer, we extract and reshape the video-related part of each sequence (i.e., the first H×W tokens) into the set FVIE. Then, we take F1,…,n−1B, the outputs of the first n−l blocks of our temporal encoder, and hierarchically fuse them with FVIE using an FPN-like [22] spatial decoder GSeg. This process results in semantically-rich, high resolution feature maps of the video frames, denoted as FSeg.
FSeg={ftSeg}TIt=1,ftSeg∈RDs×H04×W04(1)View Source\begin{equation*}\mathcal{F}_{\text{Seg}}=\{f_{\text{Seg}}^{t}\}_{t=1}^{T_{\mathcal{I}}}, f_{\text{Seg}}^{t}\in \mathbb{R}^{D_{\mathrm{s}}\times\frac{H_{0}}{4} \times\frac{W_{0}}{4}}\tag{1}\end{equation*} Next, for each instance sequence Q={qt}TIt=1,qt∈RD output by the Transformer decoder, we use a two-layer per-ceptron Gkemel to generate a corresponding sequence of con-ditional segmentation kernels [40], [44].
Gkernal(Q)={kt}TIt=1,kt∈RDs(2)View Source\begin{equation*}
G_{\text{kernal}}(\mathcal{Q})=\{k_{t}\}_{t=1}^{T_{\mathcal{I}}}, k_{t}\in \mathbb{R}^{D_{s}}\tag{2}\end{equation*} Finally, a sequence of segmentation masks M is generated for Q by convolving each segmentation kernel with its corre-sponding frame features, followed by a bilinear upsampling operation to resize the masks into ground-truth resolution,
M={mt}TIt=1,mt=Upsample(kt∗ftSeg)∈RH0×W0.(3)View Source\begin{equation*}\mathcal{M}=\{m_{t}\}_{t=1}^{T_{\mathcal{I}}}, m_{t}=\text{Upsample} (k_{t}*f_{\text{Seg}}^{t})\in \mathbb{R}^{H_{0}\times W_{0}}.\tag{3}\end{equation*} 3.5. Instance Sequence Matching: During the training process we need to determine which of the predicted instance sequences best fits the referred ob-ject. However, if the video sequence V contains additional annotated instances, we found that supervising their detection (as negative examples) alongside that of the referred instance helps stabilize the training process. Let us denote by y the set of ground-truth sequences that are available for V, and by y^={y^i}Nqi=1 the set of the predicted instance sequences. We assume that the number of predicted sequences (Nq) is chosen to be strictly greater than the number of annotated instances (denoted Nt) and that the ground-truth sequences set is padded with ∅ (no object) to fill any missing slots. Then, we want to find a matching between the two sets [4], [45]. Accordingly, we search for a permutation σ E S N q with the lowest total cost:
σ^=argminσ∈SNq∑i=1NqCMatch(y^σ(i), yi),(4)View Source\begin{equation*}\hat{\sigma}=\underset{\sigma\in S_{N_{\mathrm{q}}}}{\arg\min} \sum_{i=1}^{N_{q}}\mathcal{C}_{\text{Match}}(\hat{y}_{\sigma(i)},\ y_{i}),\tag{4}\end{equation*}
where CMatch is a pair-wise matching cost. The optimal per-mutation o^ can be computed efficiently using the Hungarian algorithm [20]. Each ground-truth sequence is of the form
yi=(mi, ri)=({mti}TIt=1,{rti}TIt=1),(5)View Source\begin{equation*}
y_{i}=(m_{i},\ r_{i})=\left(\{m_{i}^{t}\}_{t=1}^{T_{\mathcal{I}}}, \{r_{i}^{t}\}_{t=1}^{T_{\mathcal{I}}}\right),\tag{5}\end{equation*}
where mti is a ground-truth mask, and rti∈{0,1}2 is a one-hot referring vector, i.e., the positive class means that yi corresponds to the text-referred object and that this object is visible in the corresponding video frame vt. Note that if yi is a padding sequence then mi=∅. To allow our model to produce reference predictions in the form of Eq. (5), we use a reference prediction head, denoted GRef, which consists of a single linear layer of shape D×2 followed by a softmax layer. Given a predicted object query q∈RD, this head takes q as input and outputs a reference prediction r^≡GRef(q). Thus, each prediction of our model is a pair of sequences:
y^j=(m^j,r^j)=({m^tj}TIt=1,{r^tj}TIt=1).(6)View Source\begin{equation*}\hat{y}_{j}=(\hat{m}_{j},\hat{r}_{j})=\left(\{\hat{m}_{j}^{t}\}_{t=1}^{T_{\mathcal{I}}}, \{\hat{r}_{j}^{t}\}_{t=1}^{T_{\mathcal{I}}}\right).\tag{6}\end{equation*} We define the pair-wise matching cost function as the sum
CMatch(y^j, yi)=I{mi≠∅}[λdCDice(m^j, mi)+λrCRef(r^j,ri)],(7)View Source\begin{equation*}
C_{\text{Match}}(\hat{y}_{j},\ y_{i})=\mathbb{I}_{\{m_{i}\neq\emptyset\}}[\lambda_{d}C_{\text{Dice}}(\hat{m}_{j},\ m_{i})+\lambda_{r}C_{\text{Ref}}(\hat{r}_{j},r_{i})],\tag{7}\end{equation*}
where λd,λr∈R are hyperparameters. CDice supervises the predicted mask sequence using the ground-truth mask sequence by averaging the negation of the Dice coefficients [31] of each pair of corresponding masks at every time step. We refer to the supplement for the full definition of this cost function. CRef supervises the reference predictions using the corresponding ground-truth sequence as follows
CRef(r^j, ri)=−1TI∑t=1TIr^tj⋅rti.(8)View Source\begin{equation*}
C_{\text{Ref}}(\hat{r}_{j},\ r_{i})=-\frac{1}{T_{\mathcal{I}}}\sum_{t=1}^{T_{\mathcal{I}}}\hat{r}_{j}^{t}\cdot r_{i}^{t}.\tag{8}\end{equation*} 3.6. Loss Functions: Let us denote (with a slight abuse of notation) by y^ the set of predicted instance sequences permuted according to the optimal permutation σ^∈SNQ. Then, we can define our loss function as follows:
L(y^, y)=ΣNqi=1]11{mi≠∅}LMask(m^i, mi)+LRef(r^i, ri).(9)View Source\begin{equation*} \mathcal{L}(\hat{y},\ y)=\Sigma_{i=1}^{N_{q}}]{1\!\!1}_{\{m_{i}\neq\varnothing\}}\mathcal{L}_{\text{Mask}}(\hat{m}_{i},\ m_{i})+\mathcal{L}_{\text{Ref}}(\hat{r}_{i},\ r_{i}).\tag{9}\end{equation*} Following VisTR [45], the first term, dubbed LMask, ensures mask alignment between the predicted and ground-truth se-quences. As such, this term is defined as a combination of the Dice [31] and the per-pixel Focal [23] loss functions:
LMask(m^i, mi)=λdLDice(m^i, mi)+λfLFocal(m^i, mi),(10)View Source\begin{equation*}\mathcal{L}_{\text{Mask}}(\hat{m}_{i},\ m_{i})=\lambda_{d}\mathcal{L}_{\text{Dice}}(\hat{m}_{i},\ m_{i})+\lambda_{f}\mathcal{L}_{\text{Focal}}(\hat{m}_{i},\ m_{i}),\tag{10}\end{equation*}
where λd,λf∈R are hyperparameters. Both LDice and LFocal are applied on corresponding masks at every time step, and are normalized by the number of instances inside the training batch. We refer to the supplement for the full definitions of these functions. The second loss term, denoted LRef, is a crossentropy term that supervises the sequence reference predictions:
LRef(r^i, ri)=−λr1TI∑t=1TIrti⋅log(r^ti),(11)View Source\begin{equation*} \mathcal{L}_{\text{Ref}}(\hat{r}_{i},\ r_{i})=-\lambda_{r}\frac{1}{T_{\mathcal{I}}}\sum_{t=1}^{T_{\mathcal{I}}}r_{i}^{t}\cdot\log(\hat{r}_{i}^{t}),\tag{11}\end{equation*}
where λr∈R is a hyperparameter. In practice we further downweight the terms of the negative (“unreferred”) class by a factor of 10 to account for class imbalance [4]. Also, note that the same λr and λd are used as weights in the matching cost (7) and loss functions. Intriguingly, despite LRef's simplicity and lack of explicit text-related inductive bias, it was able to deliver equivalent or even better performance compared with more complex loss functions [16] that we tested. Hence, and for the sake of simplicity, no additional loss functions are used for text supervision in our method. 3.7. Inference: For a given sample of video and text, let us denote by R={r^i}Nqi=1 the set of reference prediction sequences output by our model. Additionally, we denote by pref(r^ti) the probability of the positive (“referred”) class for a given refer-ence prediction r^ti. During inference we return the segmentation mask sequence M pred that corresponds to r^pred, the predicted reference sequence with the highest positive score:
r^pred=argmaxr^i∈R∑t=1TIpref(r^ti).(12)View Source\begin{equation*} \hat{r}_{\text{pred}}=\underset{\hat{r}_{i} \in\mathcal{R}}{\arg\max} \sum_{t=1}^{T_{\mathcal{I}}}p_{\text{ref}}(\hat{r}_{i}^{t}).\tag{12}\end{equation*} This sequence selection scheme, which we term the “temporal segment voting scheme” (TSVS), grades each prediction sequence based on the total association of its terms with the text referred object. Thus, it allows our model to focus on more relevant parts of the video (in which the referred object is visible), and disregard less relevant parts (which may depict irrelevant objects or in which the referred object is occluded) when making the decision. We refer to the supplement for further analysis of the effect of TSVS. 

SECTION 4. Experiments: To evaluate our approach, we conduct experiments on three referring video object segmentation datasets. The first two, A2D-Sentences and JHMDB-Sentences [12], were created by adding textual annotations to the original A2D [47] and JHMDB [15] datasets. Each video in A2D has 3–5 frames annotated with pixel-level segmentation masks, while in JHMDB, 2D articulated human puppet masks are avail-able for all frames. More details are in the supplement. We adopt Overall IoU, Mean IoU, and precision@K to evaluate our method on these datasets. Overall IoU computes the ratio between the total intersection and the total union area over all the test samples. Mean IoU is the averaged IoU over all the test samples. Precision@ K considers the percentage of test samples whose IoU scores are above a threshold K, where K∈[0.5,0.6,0.7,0.8,0.9]. We also compute mean average precision (mAP) over 0.50:0.05:0.95 [24].
Table 1. Comparison with state-of-the-art methods on a2d-sentences [12].
Table 2. Comparison with state-of-the-art methods on jhmdb-sentences [12].
We want to note that we found inconsistencies in the mAP metric calculation in previous studies. For example, examination of published code revealed incorrect calculation of the metric as the average of the precision @ K metric over several K values. To avoid further confusion and ensure a fair comparison, we suggest adopting the COCO API1 for mAP calculation. For reference, a full implementation of the evaluation that utilizes the API is released with our code. We further evaluate MTTR on the more challenging Refer-YouTube-VOS dataset, introduced by Seo et al. [39], who provided textual annotations for the original YouTube- Vos dataset [48]. Each video has pixel-level instance segmentation annotations for every fifth frame. The original release of Refer- YouTube- Vos contains two subsets. One subset contains first-frame expressions that describe only the first frame. The other contains full-video expressions that are based on the whole video and are, therefore, more challenging. Following the introduction of the RVOS competition2, only the more challenging subset of the dataset is publicly available now. Since ground-truth annotations are available only for the training samples and the test server is currently inaccessible, we report results on the validation samples by uploading our predictions to the competition's server3. We refer to the supplement for more details. The primary eval-uation metrics for this dataset are the average of the region similarity (J) and the contour accuracy (F) [34]. 4.1. Implementation Details: As our temporal encoder we use the smallest (“tiny”) Video Swin Transformer [28] pretrained on Kinetics-400 [17]. The original Video Swin consists of four blocks with decreasing spatial resolution. We found the output of the fourth block to be too small for small object detection and hence we only utilize the first three blocks. We use the output of the third block as the input of the multimodal Transformer, while the outputs of the earlier blocks are fed into the spatial decoder. We also modify the encoder's single temporal downsampling layer to output per-frame feature maps as required by our model. As our text encoder we use the Hugging Face [46] implementation of RoBERTa-base [26]. For A2D-Sentences [12] we feed the model windows of w=8 frames with the annotated target frame in the middle. Each frame is resized such that the shorter side is at least 320 pixels and the longer side is at most 576 pixels. For Refer- YouTube- Vos [39], we use windows of w=12 consecutive annotated frames during training, and full-length videos (up to 36 annotated frames) during evaluation. Each frame is resized such that the shorter side is at least 360 pixels and the longer side is at most 640 pixels. We do not use any segmentation-related pretraining, e.g., on COCO [24], which is known to boost segmentation performance [45]. We refer the reader to the supplement for more implementation details.
Table 3. Results on refer-youtube-vos. The upper half is evaluated on the original validation set, while the bottom half is evaluated on the public validation set. ± – ensemble.
4.2. Comparison With State-of-the-Art Methods: We compare our method with existing approaches on the A2D-Sentences dataset. For fair comparison with existing works [14], [25], our model is trained and evaluated for this purpose with windows of size 8. As shown in Tab. 1, our method significantly outperforms existing approaches across all metrics. For example, our model shows a 4.3 mAP gain over current state of the art, and an absolute improvement of 6.6% on the most stringent metric P@0.9, which demon-strates its ability to generate high-quality masks. We also note that our top configuration (w=10) achieves a mas-sive 5.7 mAP gain and 6.7% absolute improvement on both Mean and Overall IoU compared to the current state of the art. Impressively, this configuration is able to do so while processing 76 frames per second on a single RTX 3090 GPU. Following previous works [12], [25], we evaluate the generalization ability of our model by evaluating it on JHMDB-Sentences without fine-tuning. We uniformly sample three frames from each video and evaluate our best model on these frames. As shown in Tab. 2, our method generalizes well and outperforms all existing approaches. Note that all methods (including ours) produce low results on P@0.9. This can be attributed to JHMDB's [15] imprecise mask annotations which were generated by a coarse human puppet model. Finally, we report our results on the public validation set of Refer- YouTube-VOS [39] in Tab. 3. As mentioned earlier, this subset contains only the more challenging full-video ex-pressions from the original release of Refer- YouTube-VOS. Compared with existing methods [25], [39] which trained and evaluated on the full version of the dataset, our model demon-strates superior performance across all metrics despite being trained on less data and evaluated exclusively on a more challenging subset. Additionally, our method shows com-petitive performance compared with the methods that led in the 2021 RVOS competition [9], [21]. We note, however, that these methods use ensembles and are trained on additional segmentation and referring datasets [24], [29], [48], [52]. 4.3. Ablation Studies: We conduct ablation studies on A2D-Sentences to evaluate our model's design and robustness. Unless stated other-wise, we use window size w=6. An ablation study on the number of object queries can be found in the supplement. Temporal EncoderTo evaluate MTTR's performance in- dependently of the temporal encoder, we compare it with CMPC-I, the image-targeted version of CMPC-V [25]. Fol-lowing CMPC-I, we use DeepLab-ResNetl0l [6] pretrained on PASCAL- VOC [11] as a visual feature extractor. We train our model using only the target frames (i.e., without additional frames for temporal context). As shown in Tab. 4a, our method significantly surpasses CMPC- I across all metrics, with a 6.1 gain in mAP and 8.7% absolute improvement in Mean IoU. In fact, this configuration of our model surpasses all existing methods regardless of the temporal context. Temporal ContextIn Tab. 4b we study the effect of the temporal context size on MTTR's performance. A larger temporal context enables better extraction of action-related information. For this purpose, we train and evaluate our model using different window sizes. As expected, widening the temporal context leads to large performance gains, with an mAP gain of 4.3 and an absolute Mean IoU improvement of 3.7% when gradually changing the window size from 1 to 10. Intriguingly, however, peak performance on A2D-Sentences is obtained using w=10, as widening the window even further (e.g., w=12) results in a performance drop. Text EncoderTo study the effect of the selected word em- beddings on our model's performance, we train our model using two additional widely-used Transformer-based text encoders, namely BERT-base [8] and Distill-RoBERTa-base [38], a distilled version of RoBERTa [26]. Additionally, we experiment with GloVe [33] and fastText [2], two simpler word embedding methods. As shown in Tab. 4c, our model achieves comparable performance when relying on the different Transformer-based encoders, which demonstrates its robustness to this change. Unsurprisingly, however, performance is slightly worse when relying on the simpler methods. This may be explained by the fact that while Transformer-based encoders are able to dynamically encode sentence context within their output embeddings, simpler methods disregard this context and merely rely on fixed pretrained embeddings.
Figure 3. Visual examples of mttr's performance on the refer- youtube- vos [39] validation set. Best viewed in color. 
Table 4. Ablation studies on a2d-sentences [12] dataset.
Supervision of Un-Referred InstancesTo study the effect of supervising the detections of un-referred instances along-side that of the referred instance in each sample, we train different configurations of our model without supervision of un-referred instances. Intriguingly, in all such experiments our model immediately converges to a local minimum of the text loss (LRef), where the same object query is repeatedly matched with all ground-truth instances, thus leaving the rest of the object queries untrained. In some experiments our model manages to escape this local minimum after a few epochs and then achieves comparable performance with our original configuration. Nevertheless, in other experiments this phenomenon significantly hinders its final mAP score. 4.4. Qualitative Analysis: As illustrated in Fig. 3, MTTR can successfully track and segment the referred objects even in challenging situations where they are surrounded by similar instances, occluded, or completely outside of the frame in large parts of the video. 

SECTION 5. Conclusion: We introduced MTTR, a simple Transformer-based approach to RVOS that models the task as a sequence prediction problem. Our end-to-end method considerably simpli-fies existing RVOS pipelines by simultaneously processing both text and video frames in a single multimodal Trans-former. Extensive evaluation of our approach on standard benchmarks reveals that our method outperforms existing state-of-the-art methods by a large margin (e.g., a 5.7 mAP improvement on A2D-Sentences). We hope our work will inspire others to see the potential of Transformers for solving complex multimodal tasks.