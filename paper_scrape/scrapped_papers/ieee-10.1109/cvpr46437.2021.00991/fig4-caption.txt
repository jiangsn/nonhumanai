Figure 4: 
Keypoint computation pipeline. For each step, we show the first five keypoints and the synthesized images using them. Given the source image (a), our model first predicts the canonical keypoints (b). We then apply the rotation and translation estimated from the driving image to the canonical keypoints, bringing them to the target head pose (transformations illustrated as arrows). (c) The expression-aware deformation adjusts the keypoints to the target expression (e.g. closed eyes). (d) We visualize the distributions of canonical keypoints estimated from different images. Upper: the canonical keypoints from different poses of a person are similar. Lower: the canonical keypoints from different people in the same pose are different.
