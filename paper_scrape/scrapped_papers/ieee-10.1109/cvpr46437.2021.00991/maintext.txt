SECTION 1. Introduction: We study the task of generating a realistic talking-head video of a person using one source image of that person and a driving video, possibly derived from another person. The source image encodes the target person’s appearance, and the driving video dictates motions in the output video. Figure 1: 
Our method can re-create a talking-head video using only a single source image (e.g., the first frame) and a sequence of unsupervisedly-learned 3D keypoints, representing motions in the video. Our novel keypoint representation provides a compact representation of the video that is 10× more efficient than the H.264 baseline can provide. A novel 3D keypoint decomposition scheme allows re-rendering the talking-head video under different poses, simulating often missed face-to-face video conferencing experiences. Video versions of the paper figures and additional results are available at our project page, https://nvlabs.github.io/face-vid2vid. 
We propose a pure neural rendering approach, where we render a talking-head video using a deep network in the one-shot setting without using a graphics model of the 3D human head. Compared to 3D graphics-based models, 2D-based methods enjoy several advantages. First, it avoids 3D model acquisition, which is often laborious and expensive. Second, 2D-based methods can better handle the synthesis of hair, beard, etc., while acquiring detailed 3D geometries of these regions is challenging. Finally, they can directly synthesize accessories present in the source image, including eyeglasses, hats, and scarves, without their 3D models. However, existing 2D-based one-shot talking-head methods [62],[75],[86] come with their own set of limitations. Due to the absence of 3D graphics models, they can only synthesize the talking-head from the original viewpoint. They cannot render the talking-head from a novel view. Our approach addresses the fixed viewpoint limitation and achieves local free-view synthesis. One can freely change the viewpoint of the talking-head within a large neighborhood of the original viewpoint, as shown in Fig. 1(c). Our model achieves this capability by representing a video using a novel 3D keypoint representation, where person-specific and motion-related information is decomposed. Both the keypoints and their decomposition are learned unsupervisedly. Using the decomposition, we can apply 3D transformations to the person-specific representation to simulate head pose changes such as rotating the talking-head in the output video. Figure 2 gives an overview of our approach. We conduct extensive experimental validation with comparisons to state-of-the-art methods. We evaluate our method on several talking-head synthesis tasks, including video reconstruction, motion transfer, and face redirection. We also show how our approach can be used to reduce the bandwidth of video conferencing, which has become an important platform for social networking and remote collaborations. By sending only the keypoint representation and reconstructing the source video on the receiver side, we can achieve a 10x bandwidth reduction as compared to the commercial H.264 standard without compromising the visual quality. Contribution 1. A novel one-shot neural talking-head synthesis approach, which achieves better visual quality than state-of-the-art methods on the benchmark datasets. Contribution 2. Local free-view control of the output video, without the need for a 3D graphics model. Our model allows changing the viewpoint of the talking-head during synthesis. Contribution 3. Reduction in bandwidth for video streaming. We compare our approach to the commercial H.264 standard on a benchmark talking-head dataset and show that our approach can achieve 10× bandwidth reduction. 

SECTION 2. Related Works: GANs. Since its introduction by Goodfellow et al. [21], GANs have shown promising results in various areas [44], such as unconditional image synthesis [21],[23],[31],[32],[33],[45], [55], image translation [8],[12],[26],[28],[42],[43],[52],[61],[67],[77],[96], [97], text-to-image translation [56], [84], [89], image processing [17],[18],[27],[35],[36],[37],[38],[40],[66],[72],[83],[88], and video synthesis [2],[10],[34],[41],[46],[54],[57],[63],[75],[76],[95]. We focus on using GANs to synthesize talking-head videos in this work. 3D model-based talking-head synthesis. Works on transferring the facial motion of one person to another—face reenactment—can be divided into subject-dependent and subject-agnostic models. Traditional 3D-based methods usually build a subject-dependent model, which can only synthesize one subject. Moreover, they focus on transferring the expressions without the head movement [65],[69],[70],[71],[73]. This line of works starts by collecting footage of the target person to be synthesized using an RGB or RGBD sensor [70],[71]. Then a 3D model of the target person is built for the face region [6]. At test time, the new expressions are used to drive the 3D model to generate the desired motions. More recent 3D model-based methods are able to perform subject-agnostic face synthesis [19],[20],[49],[51]. While they can do an excellent job synthesizing the inner face region, they have a hard time generating realistic hair, teeth, accessories, etc. Due to the limitations, most modern face reenactment frameworks adopt the 2D approach. Another line of works [15],[68] focuses on controllable face generation, providing explicit control over the generated face from a pretrained StyleGAN [32], [33]. However, it is not clear how they can be adapted to modifying real images since the inverse mapping from images to latent codes is nontrivial. Figure 2: 
Combining appearance information from the source image, our framework can re-create a driving video by just using the expression and head pose information from the driving video. With a user-specified head pose, it can also synthesize the head pose change in the output video. 
2D-based talking-head synthesis. Again, 2D approaches can be classified into subject-dependent and subject-agnostic models. Subject-dependent models [5],[82] can only work on specific persons since the model is only trained on the target person. On the other hand, subject-agnostic models [4], [9],[11],[20],[22],[24],[29],[50],[54],[62],[64],[74],[75],[80],[86],[87],[94] only need a single image of the target person, who is not seen during training, to synthesize arbitrary motions. Siarohin et al. [62] warp extracted features from the input image, using motion fields estimated from sparse keypoints. On the other hand, Zakharov et al. [87] demonstrate that it is possible to achieve promising results using direct synthesis methods without any warping. Few-shot vid2vid [75] injects the information into their generator by dynamically determining the the parameters in the SPADE [52] modules. Zakharov et al. [86] decompose the low and high frequency components of the image and greatly accelerate the inference speed of the network. While demonstrating excellent result qualities, these methods can only synthesize fixed viewpoint videos, which produce less immersive experiences. Video compression. A number of recent works [3],[16],[25], [39], [47], [59], [81] propose using a deep network to compress arbitrary videos. The general idea is to treat the problem of video compression as one of interpolating between two neighboring keyframes. Through the use of deep networks to replace various parts of the traditional pipeline, as well as techniques such as hierarchical interpolation and joint encoding of residuals and optical flows, these prior works reduce the required bit-rate. Other works [48], [79], [85], [91] focus on restoring the quality of low bit-rate videos using deep networks. Most related to our work is DAVD-Net [91], which restores talking-head videos using information from the audio stream. Our proposed method is different from these works in a number of aspects, in both the goal as well as the method used to achieve compression. We specifically focus on videos of talking faces. People’s faces have an inherent structure—from the shape to the relative arrangement of different parts such as eyes, nose, mouth, etc. This allows us to use keypoints and associated metadata for efficient compression, an order of magnitude better than traditional codecs. Our method does not guarantee pixel-aligned output videos; however, it faithfully models facial movements and emotions. It is also better suited for video streaming as it does not use bi-directional or B-frames. Figure 3: 
Source and driving feature extraction. (a) From the source image, we extract appearance features and 3D canonical keypoints. We also estimate the head pose and the keypoint perturbations due to expressions. We use them to compute the source keypoints. (b) For the driving image, we again estimate the head pose and the expression deformations. By reusing canonical keypoints from the source image, we compute the driving keypoints. 


SECTION 3. Method: Let s be an image of a person, referred to as the source image. Let {d1, d2, …, dN} be a talking-head video, called the driving video, where di’s are the individual frames, and N is the total number of frames. Our goal is to generate an output video {y1,y2,…,yN}, where the identity in yi’s is inherited from s and the motions are derived from di’s. Several talking-head synthesis tasks fall in the above setup. When s is a frame of the driving video (e.g., the first frame: s ≡ d1.), we have a video reconstruction task. When s is not from the driving video, we have a motion transfer task. We propose a pure neural synthesis approach that does not use any 3D graphics models, such as the well-known 3D morphable model (3DMM) [6]. Our approach contains three major steps: 1) source image feature extraction, 2) driving video feature extraction, and 3) video generation. In Fig. 3, we illustrate 1) and 2), while Fig. 5 shows 3). Our key ingredient is an unsupervised approach for learning a set of 3D keypoints and their decomposition. We decompose the keypoints into two parts, one that models the facial expressions and the other that models the geometric signature of a person. These two parts are combined with the target head pose to generate the image-specific keypoints. After the keypoints are estimated, they are then used to learn a mapping function between two images. We implement these steps using a set of networks and train them jointly. In the following, we discuss the three steps in detail. 3.1. Source image feature extraction: Synthesizing a talking-head requires knowing the appearance of the person, such as the skin and eye colors. As shown in Fig. 3(a), we first apply a 3D appearance feature extraction network F to map the source image s to a 3D appearance feature volume fs. Unlike a 2D feature map, fs has three spatial dimensions: width, height, and depth. Mapping to a 3D feature volume is a crucial step in our approach. It allows us to operate the keypoints in the 3D space for rotating and translating the talking-head during synthesis. We extract a set of K 3D keypoints xc,k ∈ ℝ3 from s using a canonical 3D keypoint detection network L. We set K = 20 throughout the paper unless specified otherwise. Note that these keypoints are unsupervisedly learned and different from the common facial landmarks. We note that the extracted keypoints are meant to be independent of the face’s pose and expression. They shall only encode a person’s geometry signature in a neutral pose and expression. Next, we extract pose and expression information from the image. We use a head pose estimation network H to estimate the head pose of the person in s, parameterized by a rotation matrix Rs ∈ ℝ3×3 and a translation vector ts ∈ ℝ3. In addition, we use an expression deformation estimation network Δ to estimate a set of K 3D deformations δs,k—the deformations of keypoints from the neutral expression. Both H and Δ extract motion-related geometry information in the image. We combine the identity-specific information extracted by L with the motion-related information extracted by H and Δ to obtain the source 3D keypoints xs,k via a transformation T:
\begin{equation*}{x_{s,k}} = T\left( {{x_{c,k}},{R_s},{t_s},{\delta _{s,k}}} \right) \equiv {R_s}{x_{c,k}} + {t_s} + {\delta _{s,k}}\tag{1}\end{equation*}View Source\begin{equation*}{x_{s,k}} = T\left( {{x_{c,k}},{R_s},{t_s},{\delta _{s,k}}} \right) \equiv {R_s}{x_{c,k}} + {t_s} + {\delta _{s,k}}\tag{1}\end{equation*} The final keypoints are image-specific and contain person-signature, pose, and expression information. Figure 4 visualizes the keypoint computation pipeline. The 3D keypoint decomposition in (1) is of paramount importance to our approach. It commits to a prior decomposition of keypoints: geometry-signatures, head poses, and expressions. It helps learn manipulable representations and differs our approach from prior 2D keypoint-based neural talking-head synthesis approaches [62],[75], [86]. Also note that unlike FOMM [62], our model does not estimate Jacobians. The Jacobian represents how a local patch around the keypoint can be transformed into the corresponding patch in another image via an affine transformation. Instead of explicitly estimating them, our model assumes the head is mostly rigid and the local patch transformation can be directly derived from the head rotation via Js = Rs. Avoiding estimating Jacobians allows us to further reduce the transmission bandwidth for the video conferencing application, as detailed in Sec. 5. Figure 4: 
Keypoint computation pipeline. For each step, we show the first five keypoints and the synthesized images using them. Given the source image (a), our model first predicts the canonical keypoints (b). We then apply the rotation and translation estimated from the driving image to the canonical keypoints, bringing them to the target head pose (transformations illustrated as arrows). (c) The expression-aware deformation adjusts the keypoints to the target expression (e.g. closed eyes). (d) We visualize the distributions of canonical keypoints estimated from different images. Upper: the canonical keypoints from different poses of a person are similar. Lower: the canonical keypoints from different people in the same pose are different. 
Figure 5: 
Video synthesis. We use the source and driving keypoints to estimate K flows, wk’s. These flows are used to warp the source feature fs. The results are combined and fed to the motion field estimation network M to produce a flow composition mask m. A linear combination of m and wk’s then produces the composited flow field w, which is used to warp the 3D source feature. Finally, the generator G converts the warped feature to the output image y. 
3.2. Driving video feature extraction: We use d to denote a frame in {d1,d2,…,dN} as individual frames are processed in the same way. To extract motion-related information, we apply the head pose estimator H to get Rd and td and apply the expression deformation estimator Δ to obtain δd,k’s, as shown in Fig. 3(b). Now, instead of extracting canonical 3D keypoints from the driving image d using L, we reuse xc,k, which were extracted from the source image s. This is because the face in the output image must have the same identity as the one in the source image s. There is no need to compute them again. Finally, the identity-specific information and the motion-related information are combined to compute the driving keypoints for the driving image d in the same way we obtained source keypoints:
\begin{equation*}{x_{d,k}} = T\left( {{x_{c,k}},{R_d},{t_d},{\delta _{d,k}}} \right) = {R_d}{x_{c,k}} + {t_d} + {\delta _{d,k}}\tag{2}\end{equation*}View Source\begin{equation*}{x_{d,k}} = T\left( {{x_{c,k}},{R_d},{t_d},{\delta _{d,k}}} \right) = {R_d}{x_{c,k}} + {t_d} + {\delta _{d,k}}\tag{2}\end{equation*} We apply this processing to each frame in the driving video, and each frame can be compactly represented by Rd, td, and δd,k’s. This compact representation is very useful for low-bandwidth video conferencing. In Sec. 5, we will introduce an entropy coding scheme to further compress these quantities to reduce the bandwidth utilization. Our approach allows manual changes to the 3D head pose during synthesis. Let Ru and tu be user-specified rotation and translation, respectively. The final head pose in the output image is given by Rd ← RuRd and td ← tu + td. In video conferencing, we can change a person’s head pose in the video stream freely despite the original view angle. 3.3. Video generation: As shown in Fig. 5, we synthesize an output image by warping the source feature volume and then feeding the result to the image generator G to produce the output image y. The warping approximates the nonlinear transformation from s to d. It re-positions the source features for the synthesis task. To obtain the required warping function w, we take a bottom-up approach. We first compute the warping flow wk induced by the k-th keypoint using the first order approximation [62], which is reliable only around the neighborhood of the keypoint. After obtaining all K warping flows, we apply each of them to warp the source feature volume. The K warped features are aggregated to estimate a flow composition mask m using the motion field estimation network M. This mask indicates which of the K flows to use at each spatial 3D location. We use this mask to combine the K flows to produce the final flow w. Details of the operation are given in Appendix A.1. (For all appendices, please refer to our full technical report [78].) 3.4. Training: We train our model using a dataset of talking-head videos where each video contains a single person. For each video, we sample two frames: one as the source image s and the other as the driving image d. We train the networks F, Δ, H, L, M, and G by minimizing the following loss:
\begin{align*} & {\mathcal{L}_P}\left( {d,y} \right) + {\mathcal{L}_G}\left( {d,y} \right) + {\mathcal{L}_E}\left( {\left\{ {{x_{d,k}}} \right\}} \right) + \\ & {\mathcal{L}_L}\left( {\left\{ {{x_{d,k}}} \right\}} \right) + {\mathcal{L}_H}\left( {{R_d},{{\bar R}_d}} \right) + \mathcal{L}\Delta \left( {\left\{ {{\delta _{d,k}}} \right\}} \right) \tag{3}\end{align*}View Source\begin{align*} & {\mathcal{L}_P}\left( {d,y} \right) + {\mathcal{L}_G}\left( {d,y} \right) + {\mathcal{L}_E}\left( {\left\{ {{x_{d,k}}} \right\}} \right) + \\ & {\mathcal{L}_L}\left( {\left\{ {{x_{d,k}}} \right\}} \right) + {\mathcal{L}_H}\left( {{R_d},{{\bar R}_d}} \right) + \mathcal{L}\Delta \left( {\left\{ {{\delta _{d,k}}} \right\}} \right) \tag{3}\end{align*} In short, the first two terms ensure the output image looks similar to the ground truth. The next two terms enforce the predicted keypoints to be consistent and satisfy some prior knowledge about the keypoints. The last two terms constrain the estimated head pose and keypoint perturbations. We briefly discuss these losses below and leave the implementation details in Appendix A.2. Perceptual loss ℒP. We minimize the perceptual loss [30], [77] between the output and the driving image, which is helpful in producing sharp-looking outputs. GAN loss ℒG. We use a multi-resolution patch GAN where the discriminator predicts at the patch-level. We also minimize the discriminator feature matching loss [75],[77]. Equivariance loss ℒE. This loss ensures the consistency of image-specific keypoints xd,k. For a valid keypoint, when applying a 2D transformation to the image, the predicted keypoints should change according to the applied transformation [62],[92]. Since we predict 3D instead of 2D keypoints, We use an orthographic projection to project the keypoints to the image plane before computing the loss. Keypoint prior loss ℒL. We use a keypoint coverage loss to encourage the estimated image-specific keypoints xd,k’s to spread out across the face region, instead of crowding around a small neighborhood. We compute the distance between each pair of the keypoints and penalize the model if the distance falls below a preset threshold. We also use a keypoint depth prior loss that encourages the mean depth of the keypoints to be around a preset value. Head pose loss ℒH. We penalize the prediction error of the head rotation Rd compared to the ground truth {\bar R_d}. Since acquiring the ground truth head pose for a large-scale video dataset is expensive, we use a pre-trained pose estimation network [60] to approximate {\bar R_d}. Deformation prior loss ℒΔ. The loss penalizes the magnitude of the deformations δd,k’s. As the deformations model the deviation from the canonical keypoints due to expression changes, their magnitudes should be small. 

SECTION 4. Experiments: Implementation details. The network architecture and training hyper-parameters are available in Appendix A.3. Datasets. Our evaluation is based on VoxCeleb2 [13] and TalkingHead-1KH, a newly collected large-scale talkinghead video dataset. It contains 180K videos, which are often with higher quality and larger resolution than those in VoxCeleb2. Details are available in Appendix B.1. 4.1. Talking-head image synthesis: Baselines. We compare our neural talking-head model with three state-of-the-art methods: FOMM [62], few-shot vid2vid (fs-vid2vid) [75], and bi-layer neural avatars (bilayer) [86]. We use the released pre-trained model on VoxCeleb2 for bi-layer [86], and retrain from scratch for others on the corresponding datasets. Since bi-layer does not predict the background, we subtract the background when doing quantitative analyses. Metrics. We evaluate a synthesis model on 1) reconstruction faithfulness using L1, PSNR, SSIM/MS-SSIM, 2) output visual quality using FID, and 3) semantic consistency using average keypoint distance (AKD). Please consult Appendix B.2 for details of the performance metrics. Same-identity reconstruction. We first compare the face synthesis results where the source and driving images are of the same person. The quantitative evaluation is shown in Table 1. It can be seen that our method outperforms other competing methods on all metrics for both datasets. To verify that our superior performance does not come from more parameters, we train another large FOMM model with doubled filter size (FOMM-L), which is larger than our model. We can see that enlarging the model actually hurts the performance, proving that simply making the model larger does not help. Figures 6 and 7 show the qualitative comparisons. Our method can more faithfully reproduce the driving motions. Table 1: 
Comparisons with state-of-the-art methods on face reconstruction. ↑ larger is better. ↓ smaller is better.
Figure 6: 
Qualitative comparisons on the Voxceleb2 dataset [13]. Our method better captures the driving motions. 
Figure 7: 
Qualitative comparisons on the TalkingHead-1KH dataset. Our method produces more faithful and sharper results. 
Cross-identity motion transfer. Next, we compare results where the source and driving images are from different persons (cross-identity). Table 2 shows that our method achieves the best results compared to other methods. Figure 8 compares results from different approaches. It can be seen that our method generates more realistic images while still preserving the original identity. For cross-identity motion transfer, it is sometimes useful to use relative motion [62], where only motion differences between two neighboring frames in the driving video are transferred. We report comparisons using relative motion in Appendix B.3. Table 2: 
Quantitative results on cross-identity motion transfer. Our method achieves lowest FIDs and highest identity-preserving scores (CSIM [87]).
Table 3: 
Face frontalization quantitative comparisons. We compute the identity loss and angle difference for each method and report the percentage where the losses are within a threshold (0.05 and 15 degrees, respectively).
Ablation study. We benchmark the performance gains from the proposed keypoint decomposition scheme, the mask estimation network, and pose supervision in Appendix B.4. Failure cases. Our model fails when large occlusions and image degradation occur, as visualized in Appendix B.5. Face recognition. Since the canonical keypoints are independent of poses and expressions, they can also be applied to face recognition. In Appendix B.6, we show that this achieves 5x accuracy than using facial landmarks. 4.2. Face redirection.: Baselines. We benchmark our talking-head model’s face redirection capability using latest face frontalization methods: pixel2style2pixel (pSp) [58] and Rotate-and-Render (RaR) [93]. pSp projects the original image into a latent code and then uses a pre-trained StyleGAN [1] to synthesize the frontalized image. RaR adopts a 3D face model to rotate the input image and re-renders it in a different pose. Metrics. The results are evaluated by two metrics: identity preservation and head pose angles. We use a pre-trained face recognition network [53] to extract high-level features, and compute the distance between the rotated face and the original one. We use a pre-trained head pose estimator [60] to obtain head angles of the rotated face. For a rotated image, if its identity distance to the original image is within some threshold, and/or its head angle is within some tolerance to the desired angle, we consider it as a "good" image. We report the ratio of "good" images using our metric for each method in Table 3. Example comparisons can be found in Fig. 9. It can be seen that while pSp can always frontalize the face, the identity is usually lost. RaR generates more visually appealing results since it adopts 3D face models, but has problems outside the inner face regions. Besides, both methods have issues regarding the temporal stability. Only our method can realistically frontalize the inputs. Figure 8: 
Qualitative results for cross-subject motion transfer. Ours captures the motion and preserves the identity better. 
Figure 9: 
Qualitative results for face frontalization. Our method more realistically frontalizes the faces. 


SECTION 5. Neural Talking-Head Video Conferencing: Our talking-head synthesis model distills motions in a driving image using a compact representation, as discussed in Sec. 3. Due to this advantage, our model can help reduce the bandwidth consumed by video conferencing applications. We can view the process of video conferencing as the receiver watching an animated version of the sender’s face. Figure 10 shows a video conferencing system built using our neural talking-head model. For a driving image d, we use the driving image encoder, consisting of Δ and H, to extract the expression deformations δd,k and the head pose Rd,td. By representing a rotation matrix using Euler angles, we have a compact representation of d using 3K +6 numbers: 3 for the rotation, 3 for the translation, and 3K for the deformations. We further compress these values using an entropy encoder [14]. Details are in Appendix C.1. The receiver receives the entropy-encoded representation and uses the entropy decoder to recover δd,k and Rd,td. They are then fed into our talking-head synthesis framework to reconstruct the original image d. We assume that the source image s is sent to the receiver at the beginning of the video conferencing session or re-used from a previous session. Hence, it does not consume additional bandwidth. We note that the source image is different from the I-frame in traditional video codecs. While I-frames are sent frequently in video conferencing, our source image only needs to be sent once at the beginning. Moreover, the source image can be an image of the same person captured on a different day, a different person, or even a face portrait painting. Figure 10: 
Our video compression framework. On the sender’s side, the driving image encoder extracts keypoint perturbations δd,k and head poses Rd and td. They are then compressed using an entropy encoder and sent to the receiver. The receiver decompresses the message and uses them along with the source image s to generate y, a reconstruction of the input d. Our framework can also change the head pose on the receiver’s side by using the pose offset Ru and tu. 
Figure 11: 
Automatic and human evaluations for video compression. Ours requires much lower bandwidth due to our keypoint decomposition and adaptive scheme. 
Adaptive number of keypoints. Our basic model uses a fixed number of keypoints during training and inference. However, since the transmitted bits are proportional to the number of keypoints, it is advantageous to change this number to accommodate varying bandwidth requirements dynamically. Using keypoint dropouts at training time, we derive a model that can dynamically use a smaller number of keypoints for reconstruction. This allows us to transmit even fewer bits without compromising visual quality. On average, with the adaptive scheme, the number of sent keypoints is reduced from K = 20 to 11.52 (Appendix C.2). Benchmark dataset. We manually select a dataset of 222 high-quality talking-head videos. Each video’s resolution is 512×512 and the length is up to 1024 frames for evaluation. Baselines. We compare our video streaming method with the popular H.264 codec. In order to conform to real-time video streaming cases, we disable the use of bidirectional B-frames, as this uses information from the future. By varying the constant rate factor (CRF) while encoding the ground truth input videos, we can obtain a set of videos of varying qualities and sizes suitable for a range of bandwidth availability. We also compare with FOMM [62] and fs-vid2vid [75], which also use keypoints or facial landmarks. For a fair comparison, we also compress their keypoints and Jacobians using our entropy coding scheme. Metrics. We compare the compression effectiveness using the average number of bits required per pixel (bpp) for each output frame. We measure the compression quality using both automatic and human evaluations. Unlike traditional compression methods, our method does not reproduce the input image in a pixel-aligned manner but can faithfully reproduce facial motions and gestures. Metrics based on exact pixel alignments are ill-suited for measuring the quality of our output videos. We hence use the LPIPS perceptual similarity metric [90] for measuring compression quality [7]. As shown on the left side of Fig. 11, compared to the other neural talking-head synthesis methods, ours obtains better quality while requiring much lower bandwidth. This is because other methods send the full keypoints [62], [75] and Jacobians [62], while ours only sends the head pose and keypoint deformations. Compared with H.264 videos of the same quality, ours requires significantly lower bandwidth. For human evaluation, we show MTurk workers two videos side-by-side, one produced by H.264 and the other produced by our method’s adaptive version. We then ask the workers to choose the video that they feel is of better quality. The preference scores are visualized on the right side of Fig. 11. Based on these scores, our compression method is comparable to the H.264 codec at a CRF value of 36, which means our adaptive and 20 keypoint scheme obtains 10.37× and 6.5× reduction in bandwidth compared to the H.264 codec, respectively. To handle challenging corner cases for our video conferencing system and out-of-distribution videos, we further develop a binary latent encoding network that can efficiently encode the residual at the expense of additional bandwidth, the details of which are in Appendix C.3. 

SECTION 6. Conclusion: In this work, we present a novel framework for neural talking-head video synthesis and compression. We show that by using our unsupervised 3D keypoints, we are able to decompose the representation into person-specific canonical keypoints and motion-related transformations. This decomposition has several benefits: By modifying the keypoint transformation only, we are able to generate free-view videos. By transmitting just the keypoint transformations, we can achieve much better compression ratios than existing methods. These features provide users a great tool for streaming live videos. By dramatically reducing the bandwidth and ensuring a more immersive experience, we believe this is an important step towards the future of video conferencing.