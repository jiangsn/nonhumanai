Figure 3. A comparison of our model's architecture with mip-nerf's. Mip-nerf uses one multi-scale mlp that is repeatedly queried (only two repetitions shown here) for weights that are re-sampled into intervals for the next stage, and supervises the ren-derings produced at all scales. We use a “proposal mlp” that emits weights (but not color) that are resampled, and in the final stage we use a “nerf mlp” to produce weights and colors that result in the rendered image, which we supervise. The proposal mlp is trained to produce proposal weights \hat{\mathbf{w}} that are consistent with the nerf mlp's w output. By using a small proposal mlp and a large nerf mlp we obtain a combined model with a high capacity that is still tractable to train.