Neural Radiance Fields (NeRF) synthesize highly realis-tic renderings of scenes by encoding the volumetric density and color of a scene within the weights of a coordinate-based multi-layer perceptron (MLP). This approach has enabled significant progress towards photorealistic view synthesis [30]. However, NeRF models the input to the MLP using infinitesimally small 3D points along a ray, which causes aliasing when rendering views of varying res-olutions. Mip-NeRF rectified this problem by extending NeRF to instead reason about volumetric frustums along a cone [3]. Though this improves quality, both NeRF and mip-NeRF struggle when dealing with unbounded scenes, where the camera may face any direction and scene content may exist at any distance. In this work, we present an extension to mip-NeRF we call “mip-NeRF 360” that is ca-pable of producing realistic renderings of these unbounded scenes, as shown in Figure 1. 
Applying NeRF-like models to large unbounded scenes raises three critical issues:
Parameterization. Unbounded 360 degree scenes can occupy an arbitrarily large region of Euclidean space, but mip-NeRF requires that 3D scene coordinates lie in a bounded domain.
Figure 1. (a) Though mip-nerf is able to produce accurate ren-derings of objects, for unbounded scenes it often generates blurry backgrounds and low-detail foregrounds. (b) Our model produces detailed realistic renderings of these unbounded scenes, as evi-denced by the renderings (top) and depth maps (bottom) from both models. See the supplemental video for additional results. 
Efficiency. Large and detailed scenes require more net-work capacity, but densely querying a large MLP along each ray during training is expensive. Ambiguity. The content of unbounded scenes may lie at any distance and will be observed by only a small number of rays, exacerbating the inherent ambiguity of reconstructing 3D content from 2D images.  
Parameterization: Due to perspective projection, an object placed far from the camera will occupy a small portion of the image plane, but will occupy more of the image and be visible in detail if placed nearby. Therefore, an ideal pa-rameterization of a 3D scene should allocate more capacity to nearby content and less capacity to distant content. Out-side of NeRF, traditional view-synthesis methods address this by parameterizing the scene in projective panoramic space [2], [4], [8], [14], [21], [2] [4], [33], [4] [2], [49] or by embedding scene content within some proxy geometry [15], [23], [38] that has been recovered using multi-view stereo. One aspect of NeRF's success is its pairing of specific scene types with their appropriate 3D parameterizations. The original NeRF paper [30] focused on 360 degree captures of objects with masked backgrounds and on front-facing scenes where all images face roughly the same di-rection. For masked objects NeRF directly parameterized the scene in 3D Euclidean space, but for front-facing scenes NeRF used coordinates defined in projective space (normal-ized device coordinates, or “NDC” [5]). By warping an infinitely deep camera frustum into a bounded cube where distance along the z-axis corresponds to disparity (inverse distance), NDC effectively reallocates the NeRF MLP's ca-pacity in a way that is consistent with the geometry of per-spective projection. However, scenes that are unbounded in all directions, not just in a single direction, require a different parame-terization. This idea was explored by NeRF++ [46], which used an additional network to model distant objects, and by DONeRF [31], which proposed a space-warping procedure to shrink distant points towards the origin. Both of these approaches behave somewhat analogously to NDC but in ev-ery direction, rather than just along the z-axis. In this work, we extend this idea to mip-NeRF and present a method for applying any smooth parameterization to volumes (rather than points), and also present our own parameterization for unbounded scenes. 
Efficiency: One fundamental challenge in dealing with unbounded scenes is that such scenes are often large and detailed. Though NeRF-like models can accurately repro-duce objects or regions of scenes using a surprisingly small number of weights, the capacity of the NeRF MLP saturates when faced with increasingly intricate scene content. Ad-ditionally, larger scenes require significantly more samples along each ray to accurately localize surfaces. For exam-ple, when scaling NeRF from objects to buildings, Martin-Brualla et al. [27] doubled the number of MLP hidden units and increased the number of MLP evaluations by 8 ×. This increase in model capacity is expensive - a NeRF already takes multiple hours to train, and multiplying this time by an additional ~40 × is prohibitively slow for most uses. This training cost is exacerbated by the coarse-to-fine re-sampling strategy used by NeRF and mip-NeRF: MLPs are evaluated multiple times using “coarse” and “fine” ray inter-vals, and are supervised using an image reconstruction loss on both passes. This approach is wasteful, as the “coarse” rendering of the scene does not contribute to the final im-age. Instead of training a single NeRF MLP that is super-vised at multiple scales, we will instead train two MLPs: a “proposal MLP” and a “NeRF MLP”. The proposal MLP predicts volumetric density (but not color) and those densi-ties are used to resample new intervals that are provided to the NeRF MLP, which then renders the image. Crucially, the weights produced by the proposal MLP are not super-vised using the input image, but are instead supervised with the histogram weights generated by the NeRF MLP. This al-lows us to use a large NeRF MLP that is evaluated relatively few times, alongside a small proposal MLP that is evaluated many more times. As a result, our whole model's total ca-pacity is significantly larger than mip-NeRF's (~15 ×), re-sulting in greatly improved rendering quality, but our training time only increases modestly (~2 ×). We can think of this approach as a kind of “online distillation”: while “distillation” commonly refers to training a small network to match the output of an already-trained large network [17], here we distill the structure of the outputs predicted by the NeRF MLP into the proposal MLP “online” by training both networks simultaneously. NeRV [43] performs a similar kind of online distillation for an entirely different task: training MLPs to approximate rendering integrals for the purpose of modeling visibility and indirect illumination. Our online distillation approach is similar in spirit to the “sampling oracle networks” used in DONeRF, though that approach uses ground-truth depth for supervision [31]. A related idea was used in TermiN-eRF [36], though that approach only accelerates inference and actually slows training (a NeRF is trained to conver-gence, and an additional model is trained afterwards). A learned “proposer” network was explored in NeRF in De-tail [1] but only achieves a speedup of 25%, while our approach accelerates training by 300%. Several works have attempted to distill or “bake” a trained NeRF into a format that can be rendered quickly [16], [37], [45], but these techniques do not accelerate training. The idea of accelerating ray-tracing through a hierarchical data structure such as octrees [40] or bounding volume hi-erarchies [39] is well-explored in the rendering literature, though these approaches assume a-priori knowledge of the geometry of the scene and therefore do not naturally gener-alize to an inverse rendering context in which the geometry of the scene is unknown and must be recovered. Indeed, despite building an octree acceleration structure while opti-mizing a NeRF-like model, the Neural Sparse Voxel Fields approach does not significantly reduce training time [25]. 
Ambiguity: Though NeRFs are traditionally optimized using many input images of a scene, the problem of re-covering a NeRF that produces realistic synthesized views from novel camera angles is still fundamentally underconstrained - an infinite family of NeRFs can explain away the input images, but only a small subset produces acceptable results for novel views. For example, a NeRF could recreate all input images by simply reconstructing each image as a textured plane immediately in front of its respective camera. The original NeRF paper regularized ambigu-ous scenes by injecting Gaussian noise into the density head of the NeRF MLP before the rectifier [30], which encour-ages densities to gravitate towards either zero or infinity. Though this reduces some “floaters” by discouraging semi-transparent densities, we will show that it is insufficient for our more challenging task. Other regularizers for NeRF have been proposed, such as a robust loss on density [16] or smoothness penalties on surfaces [32], [48], but these so-lutions address different problems than ours (slow rendering and non-smooth surfaces, respectively). Additionally, these regularizers are designed for the point samples used by NeRF, while our approach is designed to work with the continuous weights defined along each mip-NeRF ray. These three issues will be addressed in Sections 2, 3, and 4 respectively, after a review of mip-NeRF. We will demon-strate our improvement over prior work using a new dataset consisting of challenging indoor and outdoor scenes. We urge the reader to view our supplemental video, as our re-sults are best appreciated when animated. 

SECTION 1. Preliminaries: Mip-Nerf: Let us first describe how a fully-trained mip-NeRF [3] renders the color of a single ray cast into the scene \mathbf{r}(t)= \mathbf{o}+t\mathbf{d}r(t)=o+td, where \mathbf{o}o and d are the origin and direction of the ray respectively, and tt denotes distance along the ray. In mip-N eRF, a sorted vector of distances t is defined and the ray is split into a set of intervals T_{i}=[t_{i}, t_{i+1})Ti=[ti,ti+1). For each interval ii we compute the mean and covariance ({\mu},{\Sigma})=\mathbf{r}(T_{i})(μ,Σ)=r(Ti) of the conical frustum corresponding to the interval (the radii of which are determined by the ray's focal length and pixel size on the image plane), and featurize those values using an integrated positional encoding:
\begin{equation*}\gamma({\mu}, {\Sigma})=\left\{\left[\begin{matrix}\sin (2^{\ell} {\mu}) \exp (-2^{2 \ell-1} \text{diag}({\Sigma})) \\\cos (2^{\ell} {\mu}) \exp (-2^{2 \ell-1} \text{diag}({\Sigma}))\end{matrix}\right]\right\}_{\ell=0}^{L-1}\tag{1}\end{equation*}γ(μ,Σ)={[sin(2ℓμ)exp(−22ℓ−1diag(Σ))cos(2ℓμ)exp(−22ℓ−1diag(Σ))]}L−1ℓ=0(1)View Source\begin{equation*}\gamma({\mu}, {\Sigma})=\left\{\left[\begin{matrix}\sin (2^{\ell} {\mu}) \exp (-2^{2 \ell-1} \text{diag}({\Sigma})) \\\cos (2^{\ell} {\mu}) \exp (-2^{2 \ell-1} \text{diag}({\Sigma}))\end{matrix}\right]\right\}_{\ell=0}^{L-1}\tag{1}\end{equation*}
This is the expectation of the encodings used by NeRF with respect to a Gaussian approximating the conical frustum. These features are used as input to an MLP parameterized by weights \Theta_{\text{NeRF}}ΘNeRF that outputs a density \tauτ and color \mathbf{c}c:
\begin{equation*}\forall T_{i}\in \mathbf{t},\quad (\tau_{i}, \mathbf{c}_{i})=\text{ MLP}(\gamma(\mathbf{r}(T_{i}));\Theta_{\text{NeRF}}).\tag{2}\end{equation*}∀Ti∈t,(τi,ci)= MLP(γ(r(Ti));ΘNeRF).(2)View Source\begin{equation*}\forall T_{i}\in \mathbf{t},\quad (\tau_{i}, \mathbf{c}_{i})=\text{ MLP}(\gamma(\mathbf{r}(T_{i}));\Theta_{\text{NeRF}}).\tag{2}\end{equation*}
The view direction d is also provided as input to the MLP, but we omit this for simplicity. With these densities and colors we approximate the volume rendering integral using numerical quadrature [28]:
\begin{gather*}\mathbf{C}(\mathbf{r}, \mathbf{t})=\sum_{i} w_{i} \mathbf{c}_{i}\tag{3} \\
w_{i}=\left(1-e^{-\tau_{i}(t_{i+1}-t_{i})}\right) e^{-\sum_{i}^{\prime}< i \tau_{i^{\prime}}(t_{i^{\prime}+1}-t_{i}^{\prime})}\tag{4}\end{gather*}C(r,t)=∑iwiciwi=(1−e−τi(ti+1−ti))e−∑′i<iτi′(ti′+1−t′i)(3)(4)View Source\begin{gather*}\mathbf{C}(\mathbf{r}, \mathbf{t})=\sum_{i} w_{i} \mathbf{c}_{i}\tag{3} \\
w_{i}=\left(1-e^{-\tau_{i}(t_{i+1}-t_{i})}\right) e^{-\sum_{i}^{\prime}< i \tau_{i^{\prime}}(t_{i^{\prime}+1}-t_{i}^{\prime})}\tag{4}\end{gather*}
where \mathbf{C}(\mathbf{r}, \mathbf{t})C(r,t) is the final rendered pixel color. By construction, the alpha compositing weights w are guaranteed to sum to less than or equal to 1. The ray is first rendered using evenly-spaced “coarse” distances \mathbf{t}^{c}tc, which are sorted samples from a uniform distribution spanning [t_{n}, t_{f}][tn,tf], the camera's near and far planes:
\begin{equation*}
t^{c}\sim \mathcal{U}[t_{n}, t_{f}],\quad \mathrm{t}^{c}=\text{sort}(\{t^{c}\}).\tag{5}\end{equation*}tc∼U[tn,tf],tc=sort({tc}).(5)View Source\begin{equation*}
t^{c}\sim \mathcal{U}[t_{n}, t_{f}],\quad \mathrm{t}^{c}=\text{sort}(\{t^{c}\}).\tag{5}\end{equation*}
During training this sampling is stochastic, but during eval-uation samples are evenly spaced from t_{n}tn to t_{f}tf. After the MLP generates a vector of “coarse” weights \mathbf{w}^{c}wc, “fine” distances \mathbf{t}^{f}tf are sampled from the histogram defined by \mathbf{t}^{c}tc and \mathbf{w}^{c}wc using inverse transform sampling:
\begin{equation*}
t^{f}\sim\text{hist }(\mathbf{t}^{c}, \mathbf{w}^{c}),\quad \mathbf{t}^{f}= \text{sort}(\{t^{f}\}).\tag{6}\end{equation*}tf∼hist (tc,wc),tf=sort({tf}).(6)View Source\begin{equation*}
t^{f}\sim\text{hist }(\mathbf{t}^{c}, \mathbf{w}^{c}),\quad \mathbf{t}^{f}= \text{sort}(\{t^{f}\}).\tag{6}\end{equation*}
Because the coarse weights \mathbf{w}^{c}wc tend to concentrate around scene content, this strategy improves sampling efficiency. A mip-NeRF is recovered by optimizing MLP param-eters \Theta_{\text{NeRF}}ΘNeRF via gradient descent to minimize a weighted combination of coarse and fine reconstruction losses:
\begin{equation*}
\sum_{\mathrm{r}\in \mathcal{R}}\dfrac{1}{10}\mathcal{L}_{\text{recon}}(\mathbf{C}(\mathbf{r},\mathbf{t}^{c}), \mathbf{C}^{\ast}(\mathbf{r}))+\mathcal{L}_{\text{recon}}(\mathbf{C}(\mathbf{r},\mathbf{t}^{f}), \mathbf{C}^{\ast}(\mathbf{r}))\tag{7}\end{equation*}∑r∈R110Lrecon(C(r,tc),C∗(r))+Lrecon(C(r,tf),C∗(r))(7)View Source\begin{equation*}
\sum_{\mathrm{r}\in \mathcal{R}}\dfrac{1}{10}\mathcal{L}_{\text{recon}}(\mathbf{C}(\mathbf{r},\mathbf{t}^{c}), \mathbf{C}^{\ast}(\mathbf{r}))+\mathcal{L}_{\text{recon}}(\mathbf{C}(\mathbf{r},\mathbf{t}^{f}), \mathbf{C}^{\ast}(\mathbf{r}))\tag{7}\end{equation*}
where \mathcal{R}R is the set of rays in our training data, \mathbf{C}^{\ast}(\mathbb{r})C∗(r) is the ground truth color corresponding to ray \mathbf{r}r taken from an input image, and \mathcal{L}_{\text{recon}}Lrecon is mean squared error. 

SECTION 2. Scene and Ray Parameterization: Though there exists prior work on the parameterization of points for unbounded scenes, this does not provide a solution for the mip-NeRF context, in which we must re-parameterize Gaussians. To do this, first let us define f(\mathbf{x})f(x) as some smooth coordinate transformation that maps from \mathbb{R}^{n}\rightarrow \mathbb{R}^{n}Rn→Rn (in our case, n=3n=3). We can compute the linear approximation of this function:
\begin{equation*}
f(\mathbf{x})\approx f({\mu})+\mathbf{J}_{f}({\mu})(\mathbf{x}-{\mu})\tag{8}\end{equation*}f(x)≈f(μ)+Jf(μ)(x−μ)(8)View Source\begin{equation*}
f(\mathbf{x})\approx f({\mu})+\mathbf{J}_{f}({\mu})(\mathbf{x}-{\mu})\tag{8}\end{equation*}
Where \mathbf{J}_{f}({\mu})Jf(μ) is the Jacobian of ff at {\mu}μ. With this, we can apply ff to ({\mu},{\Sigma})(μ,Σ) as follows:
\begin{equation*}
f({\mu}, {\Sigma})=(f({\mu}), \mathbf{J}_{f}({\mu}){\Sigma}\mathbf{J}_{f}({\mu})^{\mathrm{T}})\tag{9}\end{equation*}f(μ,Σ)=(f(μ),Jf(μ)ΣJf(μ)T)(9)View Source\begin{equation*}
f({\mu}, {\Sigma})=(f({\mu}), \mathbf{J}_{f}({\mu}){\Sigma}\mathbf{J}_{f}({\mu})^{\mathrm{T}})\tag{9}\end{equation*} This is functionally equivalent to the classic Extended Kalman filter [19], where ff is the state transition model. Our choice for ff is the following contraction:
\begin{equation*}\text{contract}(\mathbf{x})= \begin{cases}\mathbf{x} & \Vert\mathbf{x}\Vert\leq 1 \\
\left(2-\frac{1}{\Vert\mathbf{x}\Vert}\right)\left(\frac{\mathbf{x}}{\Vert\mathbf{x}\Vert}\right) & \Vert\mathbf{x}\Vert>1\end{cases}\tag{10}\end{equation*}View Source\begin{equation*}\text{contract}(\mathbf{x})= \begin{cases}\mathbf{x} & \Vert\mathbf{x}\Vert\leq 1 \\
\left(2-\frac{1}{\Vert\mathbf{x}\Vert}\right)\left(\frac{\mathbf{x}}{\Vert\mathbf{x}\Vert}\right) & \Vert\mathbf{x}\Vert>1\end{cases}\tag{10}\end{equation*} This design shares the same motivation as NDC: distant points should be distributed proportionally to disparity (in-verse distance) rather than distance. In our model, instead of using mip-NeRF's IPE features in Euclidean space as per Equation 1 we use similar features (see supplement) in this contracted space: \gamma(\text{contract}({\mu}, {\Sigma})). See Figure 2 for a visualization of this parameterization.
Figure 2. A 2d visualization of our scene parameterization. We define a contract (\cdot) operator (equation 10, shown as arrows) that maps coordinates onto a ball of radius 2 (orange), where points within a radius of 1 (blue) are unaffected. We apply this contraction to mip-nerf gaussians in euclidean 3d space (gray ellipses) similarly to a kalman filter to produce our contracted gaussians (red ellipses), whose centers are guaranteed to lie within a ball of radius 2. The design of contract (\cdot) combined with our choice to space ray intervals linearly according to disparity means that rays cast from a camera located at the origin of the scene will have equidistant intervals in the orange region, as demonstrated here. 
In addition to the question of how 3D coordinates should be parameterized, there is the question of how ray distances \mathbf{t} should be selected. In NeRF this is usually done by sam-pling uniformly from the near and far plane as per Equation 5. However, if an NDC parameterization is used, this uniformly-spaced series of samples is actually uniformly spaced in inverse depth (disparity). This design decision is well-suited to unbounded scenes when the camera faces in only one direction, but is not applicable to scenes that are unbounded in all directions. We will therefore explicitly sample our distances \mathbf{t} linearly in disparity (see [29] for a detailed motivation of this spacing). To parameterize a ray in terms of disparity we define an invertible mapping between Euclidean ray distance t and a “normalized” ray distance s:
\begin{equation*}
s \triangleq\dfrac{g(t)-g(t_{n})}{g(t_{f})-g(t_{n})}, t\triangleq g^{-1}(s\cdot g(t_{f})+(1-s)\cdot g(t_{n})),\tag{11}\end{equation*}View Source\begin{equation*}
s \triangleq\dfrac{g(t)-g(t_{n})}{g(t_{f})-g(t_{n})}, t\triangleq g^{-1}(s\cdot g(t_{f})+(1-s)\cdot g(t_{n})),\tag{11}\end{equation*}
where g(\cdot) is some invertible scalar function. This gives us “normalized” ray distances s\in[0, 1] that map to [t_{n}, t_{f}]. Throughout this paper we will refer to distances along a ray in either t-space or s-space, depending on which is more convenient or intuitive. By setting g(x)=1/x and constructing ray samples that are uniformly distributed in s-space, we produce ray samples whose t-distances are distributed linearly in disparity (additionally, setting g(x)= \log(x) yields DONeRF's logarithmic spacing [31]). In our model, instead of performing the sampling in Equations 5 and 6 using t distances, we do so with s distances. This means that, not only are our initial samples spaced lin-early in disparity, but subsequent resamplings from indi-vidual intervals of the weights \mathbf{w} will also be distributed similarly. As can be seen from the camera in the cen-ter of Figure 2, this linear-in-disparity spacing of ray sam-ples counter-balances contract (\cdot). Effectively, we have co-designed our scene coordinate space with our inverse-depth spacing, which gives us a parameterization of unbounded scenes that closely resembles the highly-effective setting of the original NeRF paper: evenly-spaced ray intervals within a bounded space. 

SECTION 3. Coarse-To-Fine Online Distillation: As discussed, mip-NeRF uses a coarse-to-fine resam-pling strategy (Figure 3) in which the MLP is evaluated once using “coarse” ray intervals and again using “fine” ray intervals, and is supervised using an image reconstruction loss at both levels. We instead train two MLPs, a “NeRF MLP” \Theta_{\text{NeRF}} (which behaves similarly to the MLPs used by NeRF and mip-NeRF) and a “proposal MLP” \Theta_{\text{pror}}. The proposal MLP predicts volumetric density, which is converted into a proposal weight vector \hat{\mathbf{w}} according to Equation 4, but does not predict color. These proposal weights \hat{\mathbf{w}} are used to sample s-intervals that are then pro-vided to the NeRF MLP, which predicts its own weight vec-tor \mathbf{w} (and color estimates, for use in rendering an image). Critically, the proposal MLP is not trained to reproduce the input image, but is instead trained to bound the weights \mathbf{w} produced by the NeRF MLP. Both MLPs are initialized ran-domly and trained jointly, so this supervision can be thought of as a kind of “online distillation” of the NeRF MLP's knowledge into the proposal MLP. We use a large NeRF MLP and a small proposal MLP, and repeatedly evaluate and resample from the proposal MLP with many samples (some figures and discussion illustrate only a single resam-pIing for clarity) but evaluate the NeRF MLP only once with a smaller set of samples. This gives us a model that behaves as though it has a much higher capacity than mip-NeRF but is only moderately more expensive to train. As we will show, using a small MLP to model the proposal distribution does not reduce accuracy, which suggests that distilling the NeRF MLP is an easier task than view synthesis.
Figure 3. A comparison of our model's architecture with mip-nerf's. Mip-nerf uses one multi-scale mlp that is repeatedly queried (only two repetitions shown here) for weights that are re-sampled into intervals for the next stage, and supervises the ren-derings produced at all scales. We use a “proposal mlp” that emits weights (but not color) that are resampled, and in the final stage we use a “nerf mlp” to produce weights and colors that result in the rendered image, which we supervise. The proposal mlp is trained to produce proposal weights \hat{\mathbf{w}} that are consistent with the nerf mlp's w output. By using a small proposal mlp and a large nerf mlp we obtain a combined model with a high capacity that is still tractable to train. 
This online distillation requires a loss function that en-courages the histograms emitted by the proposal MLP (\hat{\mathbf{t}},\hat{\mathbf{w}}) and the NeRF MLP (\mathbf{t}, \mathbf{w}) to be consistent. At first this problem may seem trivial, as minimizing the dissim-ilarity between two histograms is a well-established task, but recall that the “bins” of those histograms \mathbf{t} and \hat{\mathbf{t}} need not be similar - indeed, if the proposal MLP successfully culls the set of distances where scene content exists, \hat{\mathbf{t}} and \mathbf{t} will be highly dissimilar. Though the literature contains numerous approaches for measuring the difference between two histograms with identical bins [11], [26], [35], our case is relatively underexplored. This problem is challenging be-cause we cannot assume anything about the distribution of contents within one histogram bin: an interval with non-zero weight may indicate a uniform distribution of weight over that entire interval, a delta function located anywhere in that interval, or myriad other distributions. We therefore construct our loss under the following assumption: If it is in any way possible that both histograms can be explained using any single distribution of mass, then the loss must be zero. A non-zero loss can only be incurred if it is impossi-ble that both histograms are reflections of the same “true” continuous underlying distribution of mass. See the supple-ment for visualizations of this concept. To do this, we first define a function that computes the sum of all proposal weights that overlap with interval T:
\begin{equation*}\text{bound}(\hat{\mathbf{t}},\hat{\mathbf{w}}, T)= \sum_{j:T\cap\hat{T}_{j}\neq\varnothing}\hat{w}_{j}.\tag{12}\end{equation*}View Source\begin{equation*}\text{bound}(\hat{\mathbf{t}},\hat{\mathbf{w}}, T)= \sum_{j:T\cap\hat{T}_{j}\neq\varnothing}\hat{w}_{j}.\tag{12}\end{equation*}
If the two histograms are consistent with each other, then it must hold that w_{i}\leq bound (\hat{\mathbf{t}},\hat{\mathbf{w}},T_{i}) for all intervals (T_{i}, w_{i}) in (\mathbf{t}, \mathbf{w}). This property is similar to the additivity property of an outer measure in measure theory [13]. Our loss penalizes any surplus histogram mass that violates this inequality and exceeds this bound:
\begin{equation*}\mathcal{L}_{\text{prop}}(\mathbf{t}, \mathbf{w},\hat{\mathbf{t}},\hat{\mathbf{w}})=\sum_{i}\frac{1}{w_{i}}\max(0, w_{i} -\text{bound}(\hat{\mathbf{t}},\hat{\mathbf{w}}, T_{i}))^{2}\tag{13}\end{equation*}View Source\begin{equation*}\mathcal{L}_{\text{prop}}(\mathbf{t}, \mathbf{w},\hat{\mathbf{t}},\hat{\mathbf{w}})=\sum_{i}\frac{1}{w_{i}}\max(0, w_{i} -\text{bound}(\hat{\mathbf{t}},\hat{\mathbf{w}}, T_{i}))^{2}\tag{13}\end{equation*}
This loss resembles a half-quadratic version of the chi-squared histogram distance that is often used in statistics and computer vision [35]. This loss is asymmetric because we only want to penalize the proposal weights for under-estimating the distribution implied by the NeRF MLP - overestimates are to be expected, as the proposal weights will likely be more coarse than the NeRF weights, and will therefore form an upper envelope over it. The division by w_{i} guarantees that the gradient of this loss with respect to the bound is a constant value when the bound is zero, which leads to well-behaved optimization. Because \mathbf{t} and \hat{\mathbf{t}} are sorted, Equation 13 can be computed efficiently through the use of summed-area tables [10]. Note that this loss is in-variant to monotonic transformations of distance t (assuming that \mathbf{w} and \hat{\mathbf{w}} have already been computed in t-space) so it behaves identically whether applied to Euclidean ray t-distances or to normalized ray s-distances. We impose this loss between the NeRF histogram (\mathbf{t},\ \mathbf{w}) and all proposal histograms (\hat{\mathbf{t}}^{k},\hat{\mathbf{w}}^{k}). The NeRF MLP is supervised using a reconstruction loss with the input im-age \mathcal{L}_{\text{recon}}, as in mip-NeRF. We place a stop-gradient on the NeRF MLP's outputs \mathbf{t} and \mathbf{w} when computing \mathcal{L}_{\text{prop}} so that the NeRF MLP “leads” and the proposal MLP “fol-lows” - otherwise the NeRF may be encouraged to pro-duce a worse reconstruction of the scene so as to make the proposal MLP's job less difficult. The effect of this pro-posal supervision can be seen in Figure 4, where the NeRF MLP gradually localizes its weights \mathbf{w} around a surface in the scene, while the proposal MLP “catches up” and pre-dicts coarse proposal histograms that envelope the NeRF weights.
Figure 4. A visualization of the histograms (\mathbf{t}, \mathbf{w}) emitted from the NeRF MLP (black) and the two sets of histograms (\hat{\mathbf{t}},\hat{\mathbf{w}}) emit-ted by the proposal MLP (yellow and orange) for a single ray from our dataset's bicycle scene over the course of training. Below we visualize the entire ray with fixed x and y axes, but above we crop both axes to better visualize details near scene content. Histogram weights are plotted as distributions that integrate to 1. (a) When training begins, all weights are uniformly distributed with respect to ray distance t. (b, c) As training progresses, the NeRF weights begin to concentrate around a surface and the proposal weights form a kind of envelope around those NeRF weights. 


SECTION 4. Regularization for Interval-Based Models: Due to ill-posedness, trained NeRFs often exhibit two characteristic artifacts we will call “floaters” and “back-ground collapse”, both shown in Figure 5(a). By “floaters” we refer to small disconnected regions of volumetrically dense space which serve to explain away some aspect of a subset of the input views, but when viewed from another angle look like blurry clouds. By “background collapse” we mean a phenomenon in which distant surfaces are incor-rectly modeled as semi-transparent clouds of dense content close to the camera. Here we presents a regularizer that, as shown in Figure 5, prevents floaters and background col-lapse more effectively than the approach used by NeRF of injecting noise into volumetric density [30]. Our regularizer has a straightforward definition in terms of the step function defined by the set of (normalized) ray distances \mathbf{s} and weights \mathbf{w} that parameterize each ray:
\begin{equation*}\mathcal{L}_{\text{dist}}(\mathbf{s}, \mathbf{w})=-\int\limits^{\infty} \int\limits_{-\infty}\mathbf{w}_{\mathbf{s}}(u)\mathbf{w}_{\mathbf{s}}(v)\vert u-v\vert d_{u}d_{v},\tag{14}\end{equation*}View Source\begin{equation*}\mathcal{L}_{\text{dist}}(\mathbf{s}, \mathbf{w})=-\int\limits^{\infty} \int\limits_{-\infty}\mathbf{w}_{\mathbf{s}}(u)\mathbf{w}_{\mathbf{s}}(v)\vert u-v\vert d_{u}d_{v},\tag{14}\end{equation*}
where \mathbf{w}_{\mathrm{s}}(u) is interpolation into the step function defined by (\mathbf{s}, \mathbf{w}) at u:\mathbf{w}_{\mathrm{s}}(u)=\sum_{i}w_{i} 1 [s_{i},s_{i+1})(u). We use nor-malized ray distances \mathbf{s} because using \mathbf{t} significantly up-weights distant intervals and causes nearby intervals to be effectively ignored. This loss is the integral of the distances between all pairs of points along this 1D step function, scaled by the weight w assigned to each point by the NeRF MLP. We refer to this as “distortion”, as it resem-bles a continuous version of the distortion minimized by k-means (though it could also be thought of as maximizing a kind of autocorrelation). This loss is minimized by setting \mathbf{w}=0 (recall that \mathbf{w} sums to no more than 1, not exactly 1). If that is not possible (i.e., if the ray is non-empty), it is minimized by consolidating weights into as small a region as possible. Figure 6 illustrates this behavior by showing the gradient of this loss on a toy histogram.
Figure 5. Our regularizer suppresses “floaters” (pieces of semi-transparent material floating in space, which are easy to identify in the depth map) and prevents a phenomenon in which surfaces in the background “collapse” towards the camera (shown in the bottom left of (a)). The noise-injection approach of mildenhall et al. [30] only partially eliminates these artifacts, and reduces reconstruction quality (note the lack of detail in the depths of the distant trees). See the supplemental video for more visualizations. 
Figure 6. A visualization of \nabla\mathcal{L}_{\text{dist}}, the gradient of our regular-izer, as a function of \mathbf{s} and \mathbf{w} on a toy step function. Our loss encourages each ray to be as compact as possible by 1) minimizing the width of each interval, 2) pulling distant intervals towards each other, 3) consolidating weight into a single interval or a small number of nearby intervals, and 4) driving all weights towards zero when possible (such as when the entire ray is unoccupied). 
Though Equation 14 is straightforward to define, it is non-trivial to compute. But because \mathbf{w}_{\mathrm{s}}(\cdot) has a constant value within each interval we can rewrite Equation 14 as:
\begin{align*}\mathcal{L}_{\text{dist}}(\mathbf{s}, \mathbf{w})&=\sum_{i,j}w_{i}w_{j}\left\vert \dfrac{s_{i}+s_{i+1}}{2}-\frac{s_{j}+s_{j+1}}{2}\right\vert \\
&+\frac{1}{3}\sum_{i}w_{i}^{2}(s_{i+1}-s_{i})\tag{15}\end{align*}View Source\begin{align*}\mathcal{L}_{\text{dist}}(\mathbf{s}, \mathbf{w})&=\sum_{i,j}w_{i}w_{j}\left\vert \dfrac{s_{i}+s_{i+1}}{2}-\frac{s_{j}+s_{j+1}}{2}\right\vert \\
&+\frac{1}{3}\sum_{i}w_{i}^{2}(s_{i+1}-s_{i})\tag{15}\end{align*} In this form, our distortion loss is trivial to compute. This reformulation also provides some intuition for how this loss behaves: the first term minimizes the weighted distances between all pairs of interval midpoints, and the second term minimizes the weighted size of each individual interval. 

SECTION 5. Optimization: Now that we have described our model components in general terms, we can detail the specific model used in all experiments. We use a proposal MLP with 4 layers and 256 hidden units and a NeRF MLP with 8 layers and 1024 hidden units, both of which use ReLU internal activations and a softplus activation for density \tau. We do two stages of evaluation and resampling of the proposal MLP each using 64 samples to produce (\hat{\mathbf{s}}^{0},\hat{\mathbf{w}}^{0}) and (\hat{\mathbf{s}}^{1},\hat{\mathbf{w}}^{1}), and then one stage of evaluation of the NeRF MLP using 32 samples to produce (\mathbf{s},\mathbf{w}). We minimize the following loss:
\begin{equation*}
\mathcal{L}_{\text{recon}}(\mathbf{C}(\mathbf{t}), \mathbf{C}^{\ast})+\lambda \mathcal{L}_{\text{dist}}(\mathbf{s},\mathbf{w})+\sum_{k=0}^{1}\mathcal{L}_{\text{prop}}(\mathbf{s},\mathbf{w},\hat{\mathbf{s}}^{k},\hat{\mathbf{w}}^{k}),\tag{16}\end{equation*}View Source\begin{equation*}
\mathcal{L}_{\text{recon}}(\mathbf{C}(\mathbf{t}), \mathbf{C}^{\ast})+\lambda \mathcal{L}_{\text{dist}}(\mathbf{s},\mathbf{w})+\sum_{k=0}^{1}\mathcal{L}_{\text{prop}}(\mathbf{s},\mathbf{w},\hat{\mathbf{s}}^{k},\hat{\mathbf{w}}^{k}),\tag{16}\end{equation*}
averaged over all rays in each batch (rays are not included in our notation). The \lambda hyperparameter balances our data term \mathcal{L}_{\text{recon}} and our regularizer \mathcal{L} dist; we set \lambda=0.01 in all experiments. The stop-gradient used in \mathcal{L}_{\text{prop}} makes the optimization of \Theta_{\text{pror}} independent from the optimization of \Theta_{\text{NeRF}}, and as such there is no need for a hyperparam-eter to scale the effect of \mathcal{L}_{\text{prop}}. For \mathcal{L}_{\text{recon}} we use Char-bonnier loss [9]: \sqrt{(x-x^{\ast})^{2}+\epsilon^{2}} with \epsilon=0.001, which achieves slightly more stable optimization than the mean squared error used in mip-NeRF. We train our model (and all reported NeRF-like baselines) using a slightly modified version of mip-NeRF's learning schedule: 250k iterations of optimization with a batch size of 2{14}, using Adam [22] with hyperparameters \beta_{1}=0.9, \beta_{2}=0.999, \epsilon=10^{-6}, a learning rate that is annealed log-linearly from 2 × 10–3 to 2 × 10–5 with a warm-up phase of 512 iterations, and gradient clipping to a norm of 10–3. 

SECTION 6. Results: We evaluate our model on a novel dataset: 9 scenes (5 outdoors and 4 indoors) each containing a complex cen-tral object or area and a detailed background. During capture we attempted to prevent photometric variation by fixing camera exposure settings, minimizing lighting variation, and avoiding moving objects - we do not intend to probe all challenges presented by “in the wild” photo col-lections [27], only scale. Camera poses are estimated using COLMAP [41], as in NeRF. See the supplement for details. Compared Methods: We compare our model with NeRF [30] and mip-NeRF [3], both using additional po-sitional encoding frequencies so as to bound the entire scene inside the coordinate space used by both models. We evaluate against NeRF++ [46], which uses two MLPs to separately encode the “inside” and “outside” of each scene. We also evaluate against a version of NeRF that uses DONeRF's [31] scene parameterization, which uses logarithmically-spaced samples and a different contraction from our own. We also evaluate against mip-NeRF and NeRF++ variants in which the MLP(s) underlying each model have been scaled up to roughly match our own model in terms of number of parameter count (1024 hid- den units for mip-NeRF, 512 hidden units for both MLPs in NeRF++). We evaluate against Stable View Synthesis [38], a non-NeRF model that represents the state-of-the-art of a different view-synthesis paradigm in which neural networks are trained on external scenes and combined with a proxy geometry produced by structure-from-motion [41]. We ad-ditionally compare with the publicly available SIBR imple-mentations [7] of Deep Blending [15] and Point-Based Neu-ral Rendering [23], two real-time IBR-based view synthesis approaches that also depend on an external proxy geometry. We also present a variant of our own model in which we use the latent appearance embedding (4 dimensions) presented in NeRF-W [6], [27] which ameliorates artifacts caused by inconsistent lighting conditions during scene capture (be-cause our scenes do not contain transient objects, we do not benefit from NeRF-W's other components).
Table 1. A quantitative comparison of our model with several prior works using the dataset presented in this paper.
Comparative Evaluation: In Table 1 we report mean PSNR, SSIM [44], and LPIPS [47] across the test images in our dataset. For all NeRF-like models, we report train times from a TPU v2 with 32 cores [18], as well as model size (the train times and model sizes of SVS, Deep Blending, and Point-Based Neural Rendering are not presented, as this comparison would not be particularly meaningful). Our model outperforms all prior NeRF-like models by a signif-icant margin, and we see a 57% reduction in mean squared error relative to mip-NeRF with a 2.17 × increase in train time. The mip-NeRF and NeRF++ baselines that use larger MLPs are more competitive, but are ~3 × slower to train than our model and still achieve significantly lower accu-racies. Our model outperforms Deep Blending and Point-Based Neural Rendering across all error metrics. It also outperforms SVS for PSNR and SSIM, but not LPIPS. This may be due to SVS being supervised to directly minimize an LPIPS-like perceptual loss, while we minimize a per-pixel reconstruction loss. See the supplement for renderings from SVS that achieve lower LPIPS scores than our model de-spite having reduced image quality [20]. Our model has several advantages over SVS and Deep Blending in addition to image quality: those models require external training data while our model does not, those models require the proxy geometry produced by a MVS package (and may fail when that geometry is incorrect) while we do not, and our model produces extremely detailed depth maps while SVS and Deep Blending do not (the “SVS depths” we show were produced by COLMAP [41] and are used as input to the model). Figure 7 shows model outputs, though we en-courage the reader to view our supplemental video. Ablation Study: In Table 2 we present an ablation study of our model on the bicycle scene in our dataset, the findings of which we summarize here. A) Removing \mathcal{L}_{\text{prop}} signif-icantly reduces performance, as the proposal MLP is not supervised during training. B) Removing \mathcal{L}_{\text{dist}} does not substantially affect our metrics but results in “floater” ar-tifacts in scene geometry, as shown in Figure 5. C) the reg-ularization proposed by Mildenhall et al. [30] of injecting Gaussian noise (\sigma=1) into density degrades performance (and as shown in Figure 5 is less effective at eliminating floaters). D) Removing the proposal MLP and using a single MLP to model both the scene and the proposal weights does not degrade performance but increases training time by ~3 ×, hence our small proposal MLP. E) Removing the proposal MLP and training our model using mip-NeRF's approach (applying \mathcal{L}_{\text{recon}} at all coarse scales instead of using our \mathcal{L}_{\text{prop}}) worsens both speed and accuracy, jus-tifying our supervision strategy. F) Using a small NeRF MLP (256 hidden units instead of our 1024 hidden units) accelerates training but reduces quality, demonstrating the value of a high-capacity model when dealing with detailed scenes. G) Removing IPE completely and using NeRF's positional encoding [30] reduces performance, showing the value in building upon mip-NeRF instead of NeRF. H) Ablating the contraction and instead adding positional encoding frequencies to bound the scene decreases accuracy and speed. I) Using the parameterization and logarithmic ray-spacing presented in DONeRF [31] reduces accuracy.
Figure 7. (a) A test-set image from our dataset's stump scene, with (b) our model's rendered image and depth map (median ray termination distance [34]). Cropped patches are shown to highlight details. Compared to prior work (c-e) our renderings more closely resemble the ground-truth and our depths look more plausible (though no ground-truth depth is available). See the supplement for more results. 
Table 2. An ablation study in which we remove or replace model components to measure their effect. See the text for details.
Limitations: Though mip-NeRF 360 significantly outperforms mip-NeRF and other prior work, it is not perfect. Some thin structures and fine details may be missed, such as the tire spokes in the bicycle scene (Figure 5), or the veins on the leaves in the stump scene (Figure 7). View synthesis quality will likely degrade if the camera is moved far from the center of the scene. And, like most NeRF-like models, recovering a scene requires several hours of training on an accelerator, precluding on-device training. 

SECTION 7. Conclusion: We have presented mip-NeRF 360, a mip-NeRF ex-tension designed for real-world scenes with unconstrained camera orientations. Using a novel Kalman-like scene pa-rameterization, an efficient proposal-based coarse-to-fine distillation framework, and a regularizer designed for mip-N eRF ray intervals, we are able to synthesize realistic novel views and complex depth maps for challenging unbounded real-world scenes, with a 57% reduction in mean-squared error compared to mip-NeRF. 
ACKNOWLEDGEMENTS: Our sincere thanks to David Salesin and Ricardo Martin-Brualla for their help in reviewing this paper before submission, and to George Drettakis and Geor-gios Kopanas for their help in evaluating baselines on our 360 dataset.