Table 1: 
Given a Minecraft world, our goal is to train a neural renderer that can convert any camera trajectory in the Minecraft world to a sequence of view-consistent images in the real world, at test time. The training needs to be achieved without paired Minecraft–real data as it does not exist. Among prior work, only unsupervised image-to-image translation methods such as CycleGAN [71] and MUNIT [21] can work in this setting. However, they do not generate 3D view-consistent outputs. Neural radiance field-based methods like NeRF [39] and NSVF [31] are suited for novel view synthesis. They cannot handle the Minecraft–real domain gap. All other prior works require paired training data unavailable in our setting. GANcraft, our proposed method, can generate 3D view-consistent Minecraft-to-real synthesis results without paired Minecraft–real training data.
