SECTION 1. Introduction: Synthesizing novel views of a scene from a sparse set of captured images is a long-standing problem in computer vision, and a prerequisite to many AR and VR applications. Though classic techniques have addressed this problem using structure-from-motion [11] or image-based rendering [30], this field has recently seen significant progress due to neural rendering techniques — learning-based modules embedded within a 3D geometric context, and trained to reconstruct observed images. The Neural Radiance Fields (NeRF) approach [25] models the radiance field and density of a scene with the weights of a neural network. Volume rendering is then used to synthesize new views, demonstrating a hereto-fore unprecedented level of fidelity on a range of challenging scenes. However, NeRF has only been demonstrated to work well in controlled settings: the scene is captured within a short time frame during which lighting effects remain constant, and all content in the scene is static. As we will demonstrate, NeRF’s performance degrades significantly when presented with moving objects or variable illumination. This limitation prohibits direct application of NeRF to large-scale in-the-wild scenarios, where input images may be taken hours or years apart, and may contain pedestrians and vehicles moving through them. Figure 1: 
Given only an internet photo collection (a), our method is able to render novel views with variable illumination (b). Photos by Flickr users dbowie78, vasnic64, punch /\color{Magenta}{\text{CC BY}}CC BY. 
The central limitation of NeRF that we address here is its assumption that the world is geometrically, materially, and photometrically static — that the density and radiance of the world is constant. NeRF therefore requires that any two photographs taken at the same position and orientation must be identical. This assumption is severely violated in many real-world datasets, such as large-scale internet photo collections of tourist landmarks. Two photographers may stand in the same location and photograph the same landmark, but in the time between those two photographs the world can change significantly: cars and people may move, construction may begin or end, seasons and weather may change, the sun may move through the sky, etc. Even two photos taken at the same time and location can exhibit considerable variation: exposure, color correction, and tone-mapping all may vary depending on the camera and post-processing. We will demonstrate that naively applying NeRF to in-the-wild photo collections results in inaccurate reconstructions that exhibit severe ghosting, oversmoothing, and further artifacts. To handle these demanding scenarios, we present NeRF-W, an extension of NeRF that relaxes its strict consistency assumptions. First, we model per-image appearance variations such as exposure, lighting, weather, and post-processing in a learned low-dimensional latent space. Following the framework of Generative Latent Optimization [3], we optimize an appearance embedding for each input image, thereby granting NeRF-W the flexibility to explain away photometric and environmental variations between images by learning a shared appearance representation across the entire photo collection. The learned latent space provides control of the appearance of output renderings as illustrated in Figure 1, (b). Second, we model the scene as the union of shared and image-dependent elements, thereby enabling the unsupervised decomposition of scene content into "static" and "transient" components. Our approach models transient elements using a secondary volumetric radiance field combined with a data-dependent uncertainty field, where the latter captures variable observation noise and further reduces the effect of transient objects on the static scene representation. Because optimization is able to identify and discount transient image content, we can synthesize realistic renderings of novel views by rendering only the static component. We apply NeRF-W to several challenging in-the-wild photo collections of cultural landmarks and show that it can produce detailed, high-fidelity renderings from novel view-points, surpassing the prior state of the art by a large margin on PSNR and MS-SSIM. Unlike prior work, renderings from our model exhibit smooth appearance interpolation and temporal consistency, even for wide camera trajectories. We find that NeRF-W significantly improves quality over NeRF in the presence of appearance variation and transient occluders while achieving similar quality in controlled settings. 

SECTION 2. Related Work: The last decade has seen the integration of physics-based multi-view geometry techniques into deep learning-based approaches for the task of 3D scene reconstruction. Here we review recent progress on novel view synthesis and neural rendering, and highlight the main differences between existing approaches and our proposed method. Novel View Synthesis: Constructing novel views of a scene captured by multiple images is a long standing problem in computer vision. Structure-from-Motion [11] and bundle adjustment [39] can be used to reconstruct a sparse point cloud representation and recover camera parameters. Photo Tourism [33] showed how these reconstruction techniques could be scaled to unconstrained photo collections and used to perform view synthesis [1], [10]. Other approaches to view synthesis include light-field photography [17] and image-based rendering [5] but these generally require a dense capture of the scene. Recent works explicitly infer the light and reflectance properties of the objects in the scene from a set of unconstrained photo collections [16], [29], [19] using them to manipulate scene appearance and geometry. Whereas other methods utilize semantic knowledge to reconstruct transient objects [27]. Figure 2: 
Example in-the-wild photographs from the Phototourism dataset [13] used to train NeRF-W. Due to variable illumination and post-processing (top), the same object’s color may vary from image to image. In-the-wild photos may also contain transient occluding subjects (bottom). Photos by Flickr users paradasos, itia4u, jblesa, joshheumann, ojotes, chyauchentravelworld /\color{Magenta}{\text{CC BY}}CC BY. 
Neural Rendering: More recently, neural rendering techniques [36] have been applied to scene reconstruction. Several approaches employ image translation networks [12] to re-render content more realistically using as input traditional reconstruction results [21], learned latent textures [37], point clouds [2], voxels [31], or plane sweep volumes [8], [9]. Most similar in application to our work is Neural Rerendering in the Wild (NRW) [23] which synthesizes realistic novel views of tourist sites from point cloud renderings by learning a neural re-rendering network conditioned on a learned latent appearance embedding module. Common drawbacks of these approaches are the checkerboard and temporal artifacts visible under camera motion caused by the employed 2D image translation network. Another recent approach represents the scene as camera-centric multiplane images to reconstruct captured scenes [24], [43], and internet photo collections [18]. These methods produce photorealistic renderings of novel viewpoints but the views they can interpolate are restricted to a small volume surrounding the ground truth camera poses. In contrast, volume rendering approaches [20], [25], [32] allow for accurate and consistent reconstructions even with large camera motions, as does NeRF-W. Neural Radiance Fields (NeRF) [25] use a multi-layer perceptron (MLP) to model a radiance field at an unprecedented level of fidelity, in part thanks to the use of positional encoding within the MLP [35]. Our work focuses on extending NeRF to unconstrained scenarios, like internet photo collections. Figure 3: 
NeRF-W model architecture. Given a 3D position, viewing direction, and learned appearance and transient embeddings, NeRF-W produces static and transient colors and densities as well as a measure of uncertainty. Note that the static opacity is generated before the model is conditioned on the appearance embedding, ensuring that static geometry is shared across all images. 


SECTION 3. Background: Our goal is to produce a system that takes as input a photo collection and then learns a 3D representation that is capable of generating the photos of that collection. Such a scene representation should encode the 3D structure of the scene together with appearance information so as to enable the synthesis of novel, unseen views. In the following we describe Neural Radiance Fields [25] (NeRF), the method for 3D scene reconstruction that NeRF-W extends. NeRF represents a scene using a learned, continuous volumetric radiance field Fθ defined over a bounded 3D volume. Fθ is modeled using a multilayer perceptron (MLP) that takes as input a 3D position x = (x, y, z) and unit-norm viewing direction d = (dx, dy, dz), and produces as output a density σ and color c = (r, g, b). To compute the color of a single pixel, NeRF approximates the volume rendering integral using numerical quadrature [22]. Let r(t) = o + td be the camera ray emitted from the center of projection of a camera o through a given pixel on the image plane. NeRF’s approximation of the expected color Ĉ(r) of that pixel is:
\begin{align*} & \widehat {\mathbf{C}}({\mathbf{r}}) = {\mathcal{R}}({\mathbf{r}},{\mathbf{c}},\sigma ) = \sum\limits_{k = 1}^K T \left( {{t_k}} \right)\alpha \left( {\sigma \left( {{t_k}} \right){\delta _k}} \right){\mathbf{c}}\left( {{t_k}} \right),\tag{1} \\ & {\text{where }}T\left( {{t_k}} \right) = \exp \left( { - \sum\limits_{{k^\prime } = 1}^{k - 1} \sigma \left( {{t_{{k^\prime }}}} \right){\delta _{{k^\prime }}}} \right){\text{,}}\tag{2}\end{align*}Cˆ(r)=R(r,c,σ)=∑k=1KT(tk)α(σ(tk)δk)c(tk),where T(tk)=exp(−∑k′=1k−1σ(tk′)δk′),(1)(2)View Source\begin{align*} & \widehat {\mathbf{C}}({\mathbf{r}}) = {\mathcal{R}}({\mathbf{r}},{\mathbf{c}},\sigma ) = \sum\limits_{k = 1}^K T \left( {{t_k}} \right)\alpha \left( {\sigma \left( {{t_k}} \right){\delta _k}} \right){\mathbf{c}}\left( {{t_k}} \right),\tag{1} \\ & {\text{where }}T\left( {{t_k}} \right) = \exp \left( { - \sum\limits_{{k^\prime } = 1}^{k - 1} \sigma \left( {{t_{{k^\prime }}}} \right){\delta _{{k^\prime }}}} \right){\text{,}}\tag{2}\end{align*}
where {\mathcal{R}}({\mathbf{r}},{\mathbf{c}},\sigma )R(r,c,σ) is the volumetric rendering of color c with density σ, c(t) and σ(t) are the color and density at point r(t), α(x) = 1 − exp(−x), and δk = tk+1 −tk is the distance between two quadrature points. Stratified sampling is used to select quadrature points \left\{ {{t_k}} \right\}_{k = 1}^K{tk}Kk=1 between tn and tf, the near and far planes of the camera. NeRF represents the volumetric density σ(t) and color c(t) using ReLU MLPs of the following form:
\begin{align*} & [\sigma (t),{\mathbf{z}}(t)] = {\operatorname{MLP} _{{\theta _1}}}\left( {{\gamma _{\mathbf{x}}}({\mathbf{r}}(t))} \right),\tag{3} \\ & {\mathbf{c}}(t) = {\operatorname{MLP} _{{\theta _2}}}\left( {{\mathbf{z}}(t),{\gamma _{\mathbf{d}}}({\mathbf{d}})} \right),\tag{4}\end{align*}[σ(t),z(t)]=MLPθ1(γx(r(t))),c(t)=MLPθ2(z(t),γd(d)),(3)(4)View Source\begin{align*} & [\sigma (t),{\mathbf{z}}(t)] = {\operatorname{MLP} _{{\theta _1}}}\left( {{\gamma _{\mathbf{x}}}({\mathbf{r}}(t))} \right),\tag{3} \\ & {\mathbf{c}}(t) = {\operatorname{MLP} _{{\theta _2}}}\left( {{\mathbf{z}}(t),{\gamma _{\mathbf{d}}}({\mathbf{d}})} \right),\tag{4}\end{align*}
with parameters θ = [θ1, θ2] and fixed encoding functions γx (for position) and γd (for viewing direction). The final activations in generating σ(t) and c(t) are a ReLU and a sigmoid respectively, as density must be non-negative and color must be in [0, 1]. Unlike [25], we describe the neural network as two MLPs where the latter depends on one output of the former, z(t), to highlight the fact that volume density σ(t) is independent of viewing direction d. To fit parameters θ, NeRF minimizes the sum of squared reconstruction errors with respect to an RGB image collection . Each image {{\mathcal{I}}_i}Ii is paired with its corresponding intrinsic and extrinsic camera parameters which can be estimated using structure-from-motion [28]. We precompute the set of camera rays \left\{{{{\mathbf{r}}_{ij}}} \right\}_{j = 1}^{H \times W \times 3}{rij}H×W×3j=1 corresponding to pixel j from image i with each ray passing through the 3D location oi with direction dij, where rij(t) = oi + tdij. To improve sample efficiency, NeRF simultaneously optimizes two MLPs: one coarse and one fine, where the density predicted by the coarse model determines the sampling of quadrature points for the fine model. The parameters of both models are optimized by minimizing the following loss:
\begin{equation*}\sum\limits_{ij} {\left\| {{\mathbf{C}}\left( {{{\mathbf{r}}_{ij}}} \right) - {{\widehat {\mathbf{C}}}^c}\left( {{{\mathbf{r}}_{ij}}} \right)} \right\|_2^2} + \left\| {{\mathbf{C}}\left( {{{\mathbf{r}}_{ij}}} \right) - {{\widehat {\mathbf{C}}}^f}\left( {{{\mathbf{r}}_{ij}}} \right)} \right\|_2^2,\tag{5}\end{equation*}∑ij∥∥C(rij)−Cˆc(rij)∥∥22+∥∥∥C(rij)−Cˆf(rij)∥∥∥22,(5)View Source\begin{equation*}\sum\limits_{ij} {\left\| {{\mathbf{C}}\left( {{{\mathbf{r}}_{ij}}} \right) - {{\widehat {\mathbf{C}}}^c}\left( {{{\mathbf{r}}_{ij}}} \right)} \right\|_2^2} + \left\| {{\mathbf{C}}\left( {{{\mathbf{r}}_{ij}}} \right) - {{\widehat {\mathbf{C}}}^f}\left( {{{\mathbf{r}}_{ij}}} \right)} \right\|_2^2,\tag{5}\end{equation*}
where C(rij) is the observed color of ray j in image , and Ĉc and Ĉf are the coarse and fine models respectively. 

SECTION 4. NeRF in the Wild: We now present NeRF-W, a system for reconstructing 3D scenes from in-the-wild photo collections. We build on NeRF [25] and introduce two enhancements explicitly designed to handle the challenges of unconstrained imagery. Figure 4: 
NeRF-W separately renders the static (a) and transient (b) elements of the scene, and then composites them (c). Training minimizes the difference between the composite and the true image (d) weighted by uncertainty (e), which is simultaneously optimized to identify and discount anomalous image regions. Photo by Flickr user vasnic64 /\color{Magenta}{\text{CC BY}}CC BY. 
Similar to NeRF, we learn a volumetric density representation Fθ from an unstructured photo collection for which camera parameters are known. NeRF assumes consistency in its input views: that a point in 3D space observed from the same position and viewing direction in two different images has the same intensity. But this assumption is violated by internet photos (such as those shown in Figure 2) due to two distinct phenomena:
Photometric variation: In outdoor photography, time of day and atmospheric conditions directly impact the illumination (and consequently, the emitted radiance) of objects in the scene. This issue is exacerbated by photographic imaging pipelines, as variation in auto-exposure settings, white balance, and tone-mapping across photographs may result in additional photometric inconsistencies [4]. Transient objects: Real-world landmarks are rarely captured in isolation, without moving objects or occluders around them. Tourist photos of landmarks are particularly challenging, as they often contain posing human subjects and other pedestrians.  We propose two model components to address these issues. In Section 4.1 we extend NeRF to allow for image-dependent appearance and illumination variations such that photometric discrepancies between images can be modeled explicitly. In Section 4.2 we further extend this model by allowing transient objects to be jointly estimated and disentangled from a static representation of the 3D world. Figure 3 shows an overview of the proposed model architecture. 4.1. Latent Appearance Modeling: To adapt NeRF to variable lighting and photometric post-processing, we adopt the approach of Generative Latent Optimization (GLO) [3] in which each image {{\mathcal{I}}_i}Ii is assigned a corresponding real-valued appearance embedding vector \ell _i^{(a)} of length n(a). We replace the image-independent radiance c(t) in Equation (1) with an image-dependent radiance ci(t), which also introduces a dependency on image index i to the approximated pixel color Ĉi:
\begin{align*} & {\widehat {\mathbf{C}}_i}({\mathbf{r}}) = {\mathcal{R}}\left( {{\mathbf{r}},{{\mathbf{c}}_i},\sigma } \right),\tag{6} \\ & {{\mathbf{c}}_i}(t) = {\operatorname{MLP} _{{\theta _2}}}\left( {{\mathbf{z}}(t),{\gamma _{\mathbf{d}}}({\mathbf{d}}),\ell _i^{(a)}} \right).\tag{7}\end{align*}View Source\begin{align*} & {\widehat {\mathbf{C}}_i}({\mathbf{r}}) = {\mathcal{R}}\left( {{\mathbf{r}},{{\mathbf{c}}_i},\sigma } \right),\tag{6} \\ & {{\mathbf{c}}_i}(t) = {\operatorname{MLP} _{{\theta _2}}}\left( {{\mathbf{z}}(t),{\gamma _{\mathbf{d}}}({\mathbf{d}}),\ell _i^{(a)}} \right).\tag{7}\end{align*} The \left\{ {\ell _i^{(a)}} \right\}_{i = 1}^N embeddings are optimized alongside θ. Using these appearance embeddings as input to only the branch of the network that emits color grants our model the freedom to vary the emitted radiance of the scene in a particular image while still guaranteeing that the 3D geometry (predicted earlier by {\text{ML}}{{\text{P}}_\theta }_{_1}) is static and shared across all images. By setting n(a) to a small value, we encourage optimization to identify a continuous space in which illumination conditions can be embedded, thereby enabling smooth interpolations between conditions as demonstrated in Figure 8. 4.2. Transient Objects: We address transient phenomena using two distinct design decisions: First, we designate the color-emitting MLP (Equation (4)) used in NeRF as the "static" head of our model, and we add an additional "transient" head that emits its own color and density, where that density is allowed to vary across training images. This enables NeRF-W to reconstruct images containing occluders without introducing artifacts into the static scene representation. Second, instead of assuming that all observed pixel colors are equally reliable, we allow our transient head to emit a field of uncertainty (much like our existing fields of color and density), which allows our model to adapt its reconstruction loss to ignore unreliable pixels and 3D locations that are likely to contain occluders. We model each pixel’s color as an isotropic normal distribution whose likelihood we will maximize, and we "render" the variance of that distribution using the same volume rendering approach used by NeRF. These two model components allow NeRF-W to disentangle static and transient phenomena without explicit supervision. To construct our transient head, we build on the volume rendering formulation of Equation (6) and augment the static density σ(t) and radiance ci(t) with transient counterparts and {\mathbf{c}}_i^{(\tau )}(t),
\begin{align*} & {\widehat {\mathbf{C}}_i}({\mathbf{r}}) = \sum\limits_{k = 1}^K {{T_i}} \left( {{t_k}} \right)\left( {\alpha \left( {\sigma \left( {{t_k}} \right){\delta _k}} \right){{\mathbf{c}}_i}\left( {{t_k}} \right) + \alpha \left( {\sigma _i^{(\tau )}\left( {{t_k}} \right){\delta _k}} \right){\mathbf{c}}_i^{(\tau )}\left( {{t_k}} \right)} \right),\tag{8} \\ & {\text{where }}{T_i}\left( {{t_k}} \right) = \exp \left( { - \sum\limits_{{k^\prime } = 1}^{k - 1} {\left( {\sigma \left( {{t_{{k^\prime }}}} \right) + \sigma _i^{(\tau )}\left( {{t_{{k^\prime }}}} \right)} \right)} {\delta _{{k^\prime }}}} \right).\tag{9}\end{align*}View Source\begin{align*} & {\widehat {\mathbf{C}}_i}({\mathbf{r}}) = \sum\limits_{k = 1}^K {{T_i}} \left( {{t_k}} \right)\left( {\alpha \left( {\sigma \left( {{t_k}} \right){\delta _k}} \right){{\mathbf{c}}_i}\left( {{t_k}} \right) + \alpha \left( {\sigma _i^{(\tau )}\left( {{t_k}} \right){\delta _k}} \right){\mathbf{c}}_i^{(\tau )}\left( {{t_k}} \right)} \right),\tag{8} \\ & {\text{where }}{T_i}\left( {{t_k}} \right) = \exp \left( { - \sum\limits_{{k^\prime } = 1}^{k - 1} {\left( {\sigma \left( {{t_{{k^\prime }}}} \right) + \sigma _i^{(\tau )}\left( {{t_{{k^\prime }}}} \right)} \right)} {\delta _{{k^\prime }}}} \right).\tag{9}\end{align*} The expected color of r(t) then becomes the alpha composite of both the static and the transient components. We employ the Bayesian learning framework of Kendall et al. [15] to model the uncertainty of the observed color. We assume that observed pixel intensities are inherently noisy (aleatoric) and further that this noise is input-dependent (heteroscedastic). We model the observed color Ci(r) with an isotropic normal distribution with image- and ray-dependent variance β (r)2 i and mean Ĉi(r). Variance βi(r) is "rendered" analogously to color via alpha-compositing according to the transient density :
\begin{equation*}{\hat \beta _i}({\mathbf{r}}) = {\mathcal{R}}\left( {{\mathbf{r}},{\beta _i},\sigma _i^{(\tau )}} \right).\tag{10}\end{equation*}View Source\begin{equation*}{\hat \beta _i}({\mathbf{r}}) = {\mathcal{R}}\left( {{\mathbf{r}},{\beta _i},\sigma _i^{(\tau )}} \right).\tag{10}\end{equation*} To allow the transient component of the scene to vary across images, we assign each training image {{\mathcal{I}}_i} a second embedding \ell _i^{(\tau )} \in {{\mathbb{R}}^{{n^{(\tau )}}}} that is given as input to the transient MLP,
\begin{align*} & \left[ {\sigma _i^{(\tau )}(t),{\mathbf{c}}_i^{(\tau )}(t),{{\tilde \beta }_i}(t)} \right] = {\text{ML}}{{\text{P}}_{{\theta _3}}}\left( {{\mathbf{z}}(t),\ell _i^{(\tau )}} \right),\tag{11} \\ & {\beta _i}(t) = {\beta _{\min }} + \log \left( {1 + \exp \left( {{{\tilde \beta }_i}(t)} \right)} \right),\tag{12}\end{align*}View Source\begin{align*} & \left[ {\sigma _i^{(\tau )}(t),{\mathbf{c}}_i^{(\tau )}(t),{{\tilde \beta }_i}(t)} \right] = {\text{ML}}{{\text{P}}_{{\theta _3}}}\left( {{\mathbf{z}}(t),\ell _i^{(\tau )}} \right),\tag{11} \\ & {\beta _i}(t) = {\beta _{\min }} + \log \left( {1 + \exp \left( {{{\tilde \beta }_i}(t)} \right)} \right),\tag{12}\end{align*} ReLU and sigmoid activations are used for and {\mathbf{c}}_i^{(\tau )}(t), and a softplus is used as the activation for βi(t) (shifted by βmin > 0, a hyperparameter that ensures a minimum importance is assigned to each ray). See Figure 3 for an illustration of our complete model architecture. The loss for ray r in image i with true color Ci(r) is
\begin{equation*}{L_i}({\mathbf{r}}) = \frac{{\left\| {{{\mathbf{C}}_i}({\mathbf{r}}) - {{\widehat {\mathbf{C}}}_i}({\mathbf{r}})} \right\|_2^2}}{{2{\beta _i}{{({\mathbf{r}})}^2}}} + \frac{{\log {\beta _i}{{({\mathbf{r}})}^2}}}{2} + \frac{{{\lambda _u}}}{K}\sum\limits_{k = 1}^K {\sigma _i^{(\tau )}} \left( {{t_k}} \right).\tag{13}\end{equation*}View Source\begin{equation*}{L_i}({\mathbf{r}}) = \frac{{\left\| {{{\mathbf{C}}_i}({\mathbf{r}}) - {{\widehat {\mathbf{C}}}_i}({\mathbf{r}})} \right\|_2^2}}{{2{\beta _i}{{({\mathbf{r}})}^2}}} + \frac{{\log {\beta _i}{{({\mathbf{r}})}^2}}}{2} + \frac{{{\lambda _u}}}{K}\sum\limits_{k = 1}^K {\sigma _i^{(\tau )}} \left( {{t_k}} \right).\tag{13}\end{equation*} The first two terms are the (shifted) negative log likelihood of Ci(r) according to a normal distribution with mean Ĉi(r) and variance βi (r)2. Larger values of βi(r) attenuate the importance assigned to a pixel, under the assumption that it belongs to some transient object. The first term is balanced by the second, which corresponds to the log-partition function of the normal distribution and precludes the trivial minimum at βi(r) = ∞. The third term is an L1 regularizer with a multiplier λu on (non-negative) transient density , and this discourages the model from using transient density to explain away static phenomena. At test time we omit the transient and uncertainty fields, and render only σ(t) and c(t). See Figure 4 for an illustration of static, transient, and uncertainty components. 4.3. Optimization: Like NeRF, we simultaneously optimize two copies of Fθ: A fine model that uses the model and losses described above, and a coarse model that uses only the latent appearance modeling component. Alongside parameters θ we optimize per-image appearance embeddings \left\{ {\ell _i^{(a)}} \right\}_{i = 1}^N and transient embeddings \left\{ {\ell _i^{(\tau )}} \right\}_{i = 1}^N. NeRF-W’s loss function is then,
\begin{equation*}\sum\limits_{ij} {{L_i}} \left( {{{\mathbf{r}}_{ij}}} \right) + \frac{1}{2}\left\| {{\mathbf{C}}\left( {{{\mathbf{r}}_{ij}}} \right) - \widehat {\mathbf{C}}_i^c\left( {{{\mathbf{r}}_{ij}}} \right)} \right\|_2^2,\tag{14}\end{equation*}View Source\begin{equation*}\sum\limits_{ij} {{L_i}} \left( {{{\mathbf{r}}_{ij}}} \right) + \frac{1}{2}\left\| {{\mathbf{C}}\left( {{{\mathbf{r}}_{ij}}} \right) - \widehat {\mathbf{C}}_i^c\left( {{{\mathbf{r}}_{ij}}} \right)} \right\|_2^2,\tag{14}\end{equation*} Figure 5: 
Because optimization only yields appearance embeddings ℓ(a) for images in the training set, when evaluating error metrics on test-set images we optimize ℓ(a) to match the appearance of the true image using only the left half of each image. Error metrics are evaluated on only the right half of each image, so as to avoid information leakage. Photo by Flickr user eadaoinflynn /\color{Magenta}{\text{CC BY}}. 
λu, βmin, and embedding dimensionalities n(a) and n(τ) form the set of additional hyperparameters for NeRF-W. As optimization only produces appearance embeddings \left\{ {\ell _i^{(a)}} \right\} for images in the training set, the embeddings of test-set images are unspecified. For test-set visualizations, we choose ℓ(a) to best fit a target image (e.g. Figure 8) or set it to an arbitrary value. 

SECTION 5. Experiments: Here we provide an evaluation of NeRF-W on unconstrained (e.g. "in-the-wild") internet photo collections of cultural landmarks. We select six landmarks from the Phototourism dataset [13]. Inspired by prior work [23], we reconstruct the Trevi Fountain and Sacre Coeur as well as four novel scenes, the Brandenburg Gate, Taj Mahal, Prague Old Town Square, and Hagia Sophia. Empirical performance for these scenes can be found in Table 1, but we urge the reader to visually inspect the video results in the supplement. Baselines: We evaluate our proposed method against Neural Rerendering in the Wild (NRW) [23], NeRF [25], and two ablations of NeRF-W: NeRF-A (appearance), wherein the "transient" head is eliminated; and NeRF-U (uncertainty), wherein the appearance embedding \ell _i^{(a)} is eliminated. NeRF-W is the composition of NeRF-A and NeRF-U. While other recent work such as [18] is employed on a similar domain, we restrict baselines to those capable of extrapolating significantly beyond the views represented in the dataset. Optimization: Building on NeRF1, we implement all experiments in TensorFlow 2 using Keras. For each scene, we use COLMAP [28] with two radial and two tangential distortion parameters enabled to estimate each image’s camera parameters. As in NeRF, for each scene we train a model initialized to random weights. We optimize all NeRF variants for 300,000 steps with a batch size of 2048 on 8 Nvidia V100 GPUs using Adam [7] (with hyperparameters β1 = 0.9, β2 = 0.999, ϵ = 10−7), which takes approximately 2 days. Hyperparameters shared by all NeRF variants are chosen to maximize PSNR on the Brandenburg Gate dataset and are fixed to those values in all other scenes. Additional hyperparameters for variants of NeRF-W are chosen via grid search to maximize PSNR on a held-out validation set on the Brandenburg Gate scene and are fixed to those values for all other scenes. See the supplement for additional details on hyperparameters. Figure 6: 
Depth maps from NeRF and NeRF-W, rendered by computing the expected termination depth of each ray. NeRF’s geometry is corrupted by appearance variation and occluders, while NeRF-W is robust to such phenomena and produces accurate 3D reconstructions. Photos by Flickr users burkeandhare, photogreuhphies /\color{Magenta}{\text{CC BY}}. 
Evaluation: We evaluate on the task of novel view synthesis: given a held-out image with accompanying camera parameters, we render an image from the same pose and compare it to the ground truth. As measuring perceptual image similarity is challenging [26], [38], [40], [42], we present rendered images for visual inspection and report quantitative results based on PSNR, MS-SSIM [41], and LPIPS [42]. Because optimization only produces appearance embeddings for training-set images, when computing error metrics on test-set images we optimize an appearance embedding ℓ(a) on the left half of each image and report metrics on the right half (Figure 5). See the supplement for additional discussion of error metrics. Results: Figure 7 shows qualitative results for all models and baselines on a subset of scenes. NRW produces renderings with checkerboard artifacts characteristic of 2D re-rendering methods [14]. NRW is also sensitive to upstream errors in 3D geometry such as incomplete point clouds, as can be seen in the smaller towers of the church in the Prague Old Town. NeRF produces a consistent 3D geometry, but large parts of the scene have ghosting artifacts and occlusions, which are particularly noticeable on Sacre Coeur and Prague Old Town. Renderings from NeRF also tend to exhibit strong global color shifts when compared to the ground truth. These artifacts are the direct consequence of NeRF’s static-world assumption — NeRF attempts to explain away all photometric variation and transient occlusion using a single scene representation. This static assumption impairs not only NeRF’s renderings but also its underlying geometry, while NeRF-W produces accurate 3D reconstructions (Figure 6). The NeRF-A ablation produces less "foggy" renderings than NeRF, as shown in Figure 7. However, NeRF-A is unable to reconstruct high-frequency details such as the brickwork on Sacre Coeur’s dome. In contrast, the NeRF-U ablation is better able to capture fine detail, but is unable to model varying photometric effects. NeRF-W has the benefits of both ablations, and thereby produces sharper and more accurate renderings. Quantitative results are summarized in Table 1. Optimizing NeRF on in-the-wild photo collections leads to particularly poor results that are unable to compete with NRW. In contrast, NeRF-W outperforms the baselines on PSNR and MS-SSIM across all datasets. In particular, NeRF-W improves over the previous state of the art NRW by an average margin of 4.4dB in PSNR, and with up to 40% improvements in MS-SSIM. In spite of minimizing only a per-pixel squared error during training, NeRF-W improves upon the prior state of the art on LPIPS in 3 of 6 scenes and remains competitive in the remainder. Lacking a perceptual loss, NeRF-W is not incentivized to produce the high-frequency textures favored by perceptual metrics such as LPIPS. However, NRW exhibits temporal instability — as the camera moves, renderings appear to flicker and wobble unrealistically, and this is not captured by the single-image metrics or figures used in this paper. We strongly encourage the reader to inspect the supplemental video to observe the temporal instability of NRW compared to NeRF and NeRF-W. Controllable Appearance: One consequence of modeling appearance with a latent embedding space {\ell ^{(a)}} \in {{\mathbb{R}}^{{n^{(a)}}}} is that it enables the modification of lighting and appearance of a rendering without altering the underlying 3D geometry. In Figure 1 (right), we see slices of four rendered images produced by NeRF-W using appearance embeddings associated with four training set images. In addition to the embeddings associated with images in the training set, one may also apply NeRF-W to arbitrary vectors in the same space. In Figure 8, we present five images rendered from a fixed camera position, where we interpolate between the appearance embeddings associated with the left and right training images. Note that the appearance of the rendered images smoothly transitions between the two end points without introducing artifacts to the 3D geometry. We encourage readers to view the supplementary video to better appreciate the naturalness of such interpolations. Figure 7: 
Qualitative results from experiments on the Phototourism dataset. NeRF-W is simultaneously able to model appearance variation (top), remove transient occluders (flag, middle), and reconstruct fine details in the scene (bottom). Further datasets are shown in Figure 14 (supplementary). Photos by Flickr users firewave, clintonjeff, leoglenn_g /\color{Magenta}{\text{CC BY}}. 
Table 1: 
Quantitative results on the Phototourism dataset [13] for NRW [23], NeRF [25], and two ablations of the proposed model. Best results are \color{blue}{\text{highlighted}}. NeRF-W outperforms the previous state of the art across all datasets on PSNR and MS-SSIM and achieves competitive results in LPIPS. Note that LPIPS generally favours methods such as NRW trained with an adversarial or perceptual loss and it is less sensitive to typical GAN artifacts, see Figures 7 and 14 (supplementary).
Figure 8: 
Interpolations between the appearance embeddings ℓ(a) of two training images (left, right), which results in renderings (middle) where color and illumination are interpolated but geometry is fixed. Note that the training images contain people (left) and lights (right) that do not appear in the renderings. Photos by Flickr users mightyohm, blatez / {\text{CC}}\;{\text{BY}}. 
Figure 9: 
Epipolar plane images (EPI) synthesized from videos rendered by different models for the Brandenburg Gate scene. The camera is translated from left to right along a straight path, and the horizontal line at the same position (red line, reference) is taken across all video frames and stacked vertically, producing the EPIs shown above. A temporally consistent video results in a clean and smooth EPI, while noise in an EPI indicates temporal flickering artifacts. NRW’s video contains heavy flickering with transient objects popping in and out of the frame while NeRF produces severe ghosting artifacts in front of the landmarks. NeRF-W produces highly temporally consistent videos. We strongly encourage the readers to watch the video in the supplementary material. 
View-consistency: Figure 9 shows "flatland" light field renderings for NRW, NeRF, and NeRF-W with the camera panning along a straight path. Renderings from NeRF-W are more view-consistent (the Lambertian scene content is correctly reconstructed as being constant across viewing directions) and exhibits significantly less flickering than NRW or NeRF. NRW is unable to model temporal consistency between frames for transient objects, while NeRF is forced to embed view-dependent effects as colored fog into its scene representation. Limitations: While NeRF-W is able to produce photorealistic and temporally consistent renderings from unstructured photographs, rendering quality degrades in areas of the scene that are rarely observed in the training images, or only observed at very oblique angles, like the ground, as shown in Figure 10. Similar to NeRF, NeRF-W is also sensitive to camera calibration errors, which can lead to blurry reconstructions on the parts of the scene that have been imaged by incorrectly-calibrated cameras. Figure 10: 
Limitations of NeRF-W on the Phototourism dataset. Rarely-seen parts of the scene (ground, left) and incorrect camera poses (lamp post, right) can result in blur. 
Synthetic Experiments: The components of NeRF-W were designed to deal with specific forms of photometric inconsistency, such as color shifts and occluders. Unfortunately, the uncontrolled nature of the Phototourism dataset means that it is challenging to demonstrate that each model component does indeed address the confounding factor that it was designed to address. For this reason, in the supplement we present a controlled ablation study in which we construct variations of a synthetic dataset used in [25] wherein we manually introduce the phenomena we expect to find in in-the-wild imagery. As can be seen in the supplement, the results of this ablation study are consistent with our expectations. 

SECTION 6. Conclusion: We have presented NeRF-W, a novel approach for 3D scene reconstruction of complex environments from unstructured internet photo collections that builds upon NeRF. We learn a per-image latent embedding capturing photometric appearance variations often present in in-the-wild data, and we decompose the scene into image-dependent and shared components to allow our model to disentangle transient elements from the static scene. Experimental evaluation on real-world (and synthetic) data demonstrates significant qualitative and quantitative improvement over previous state-of-the-art approaches.