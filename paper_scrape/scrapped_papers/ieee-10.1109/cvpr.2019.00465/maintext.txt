Figure 1. 
Our model predicts dense depth when both an ordinary camera and people in the scene are freely moving (right). We train our model on our new MannequinChallenge dataset—a collection of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a camera tours the scene (left). Because people are stationary, geometric constraints hold; this allows us to use multi-view stereo to estimate depth which serves as supervision during training.2 


SECTION 1. Introduction: A hand-held camera viewing a dynamic scene is a common scenario in modern photography. Recovering dense geometry in this case is a challenging task: moving objects violate the epipolar constraint used in 3D vision, and are often treated as noise or outliers in existing Structure-from-Motion (SfM) and Multi-view Stereo (MVS) methods. Our human depth perception, however, is not easily fooled by object motion–rather, we maintain a feasible interpretation of the objects’ geometry and depth ordering even if both objects and the observer are moving, and even when the scene is observed with just one eye [11]. In this work, we take a step towards achieving this ability computationally. We focus on the task of predicting accurate, dense depth from ordinary videos where both the camera and people in the scene are naturally moving. We focus on humans for two reasons: i) in many applications (e.g., augmented reality), humans constitute the salient objects in the scene, and ii) human motion is articulated and difficult to model. By taking a data-driven approach, we avoid the need to explicitly impose assumptions on the shape or deformation of people, but rather learn these priors from data. Where do we get data to train such a methodƒ Generating high-quality synthetic data in which both the camera and the people in the scene are naturally moving is very challenging. Depth sensors (e.g., Kinect) can provide useful data, however such data is typically limited to indoor environments and requires significant manual work in capture and process. Furthermore, it is difficult to gather people of different ages and genders with diverse poses at scale. Instead, we derive data from a surprising source: YouTube videos in which people imitate mannequins, i.e., freeze in elaborate, natural poses, while a hand-held camera tours the scene (Fig. 2). These videos comprise our new MannequinChallenge (MC) dataset, which we plan to release for the research community. Since the entire scene, including the people, is stationary, we estimate the camera poses and depth using SfM and MVS, which serves as supervision for training. Using this data, we design and train a deep neural network that takes an input RGB image, a mask of human regions, and an initial depth of the environment (i.e., non-human regions), and outputs a dense depth map over the entire image, both the environment and the people (see Fig. 1). Note that the initial depth of the environment is computed using motion parallax between two frames of the video, providing the network with information not available from a single frame. Once trained, our model can handle natural videos with arbitrary camera and human motion. We demonstrate the applicability of our method on a variety of real-world Internet videos, shot with a hand-held camera, depicting complex human actions such as walking, running, and dancing. Our model predicts depth to higher accuracy than state-of-the-art monocular depth prediction and motion stereo methods. We further show how our depth maps can be used to produce various 3D effects such as synthetic depth-of-field, depth-aware inpainting, and inserting virtual objects into the 3D scene with correct occlusion. In summary, our contributions are: i) a new source of data for depth prediction consisting of a large number of Internet videos in which the camera moves around people "frozen" in natural poses, along with a methodology for generating accurate depth maps and camera poses; ii) a deep-network-based model designed and trained to predict dense depth maps in the challenging case of simultaneous camera motion and complex human motion. 

SECTION 2. Related Work: Learning-based depth prediction. Numerous algorithms, based on both supervised and unsupervised learning, have recently been proposed for predicting dense depth from a single RGB image [46], [17], [7], [6], [3], [19], [33], [8], [52], [49], [21], [41]. Some recent learning based methods also consider multiple images, either assuming known camera poses [12], [47] or simultaneously predicting camera poses along with depth [39], [51]. However, none of them is designed to predict the depth of dynamic objects, which is the focus of our work. Depth estimation for dynamic scenes. RGBD data has been widely used for 3D modeling of dynamic scenes [25], [55], [48], [5], [14], but only a few methods attempt to estimate depth from a monocular camera. Several methods have been proposed to reconstruct sparse geometry of a dynamic scene [27], [50], [36], [40]. Russell et al. [31] and Ran-ftl et al. [29] suggest motion/object segmentation based algorithms to decompose a dynamic scene into piecewise rigid parts. However, these methods impose strong assumptions of the object’s motion that are violated by articulated human motion. Konstantinos et al. [30] predict depth of moving soccer players using synthetic training data from FIFA video games. However, their method is limited to soccer players, and cannot handle general people in the wild. RGBD data for learning depth. There are a number of RGBD datasets of indoor scenes, captured using depth sensors [35], [2], [4], [45] or synthetically rendered [37]. However, none of these datasets provide depth supervision for moving people in natural environments. Several action recognition methods use depth sensors to capture human actions [54], [34], [22], [26], however most of them are captured by a static camera and provide only a limited number of indoor scenes. REFRESH [20] is a recent semi-synthetic scene flow dataset created by overlaying animated people on NYUv2 images. Here too, the dataset is limited to indoor scenes and consists of synthetic humans placed in an unrealistic configuration with their surrounding. Human shape and pose prediction. Recovery of a posed 3D human mesh from a single RGB image has attracted significant attention [18], [9], [16], [1], [28], [23]. Recent methods achieve impressive results on natural images spanning a variety of poses. However, such methods only model the human body, disregarding hair, clothing, and the non-human parts of the scenes. Finally, many of these methods rely on correctly detecting human keypoints, requiring most of the body to be within the frame. 

SECTION 3. MannequinChallenge Dataset: The Mannequin Challenge [42] is a popular video trend in which people freeze in place—often in an interesting pose—while the camera operator moves around the scene filming them (e.g., Fig. 2). Thousands of such videos have been created and uploaded to YouTube since late 2016. To the extent that people succeed in staying still during the videos, we can assume the scenes are static and obtain accurate camera poses and depth information by processing them with SfM and MVS algorithms. We found around 2,000 candidate videos for which this processing is possible. These videos comprise our new MannequinChallenge Dataset, which spans a wide range of scenes with people of different ages, naturally posing in different group configurations. We next describe in detail how we process the videos and derive our training data. Estimating camera poses. Following a similar approach to Zhou et al. [53], we use ORB-SLAM2 [24] to identify trackable sequences in each video and to estimate an initial camera pose for each frame. At this stage, we process a lower-resolution version of the video for efficiency, and set the field of view to 60 degrees (typical value for modern cell-phone cameras). We then reprocess each sequence at a higher resolution using a visual SfM system [32], which refines the initial camera poses and intrinsic parameters. This method extracts and matches features across frames, then performs a global bundle adjustment optimization. Finally, sequences with non-smooth camera motion are removed using the technique of Zhou et al. [53]. Computing dense depth with MVS. With the estimated camera poses for each clip in hand, we turn to reconstructing the scene’s dense geometry. Specifically, we recover per-frame dense depth maps using the state-of-the-art MVS system COLMAP [33]. Figure 2. 
Sample images from Mannequin Challenge videos. Each image is a frame from a video sequence in which the camera is moving but humans are all static. The videos span a variety of natural scenes, poses, and configuration of people. 
Because our data consists of challenging Internet videos (i.e., often involve camera motion blur, shadows and reflections), the raw depth maps estimated by MVS are often too noisy for training purposes. We address this issue by a careful depth filtering mechanism. We first refine and filter depth outliers using the depth refinement method of [19]. Additionally, we remove additional erroneous depth values by considering the consistency of the MVS depth and the depth obtained from motion parallax between two frames. Specifically, for each frame, we compute a normalized error Δ(p) for every valid pixel p:
Δ(p)=|DMVS(p)−Dpp(p)|DMVS(p)+Dpp(p)(1)View Source\begin{equation*}\Delta ({\mathbf{p}}) = \frac{{\left| {{D_{{\text{MVS}}}}({\mathbf{p}}) - {D_{{\text{pp}}}}({\mathbf{p}})} \right|}}{{{D_{{\text{MVS}}}}({\mathbf{p}}) + {D_{{\text{pp}}}}({\mathbf{p}})}}\tag{1}\end{equation*}
where DMVS is the depth map obtained by MVS and Dpp is the depth map computed from two-frame motion parallax (see Sec. 4.1). Depth values for which Δ(p) > δ are removed, where we empirically set δ = 0.2. Fig. 3 shows sample frames from our processed sequences with corresponding estimated MVS depths after filtering. See Supplemental Material (SM) for examples showing the effect of the proposed cleaning approach. Filtering clips. Several factors can make a video clip unsuitable for training. For example, people may "unfreeze" (start moving) at some point in the video, or the video may contain synthetic graphical elements in the background. Dynamic objects and synthetic backgrounds do not obey multi-view geometric constraints and hence are treated as outliers and filtered out by MVS, potentially leaving few valid pixels. Therefore, we remove frames where < 20% of pixels have valid MVS depth after our two-pass cleaning stage. Further, we remove frames where the estimated radial distortion coefficient |k1| > 0.1 (indicative of a fisheye camera) or where the estimated focal length is ≤ 0.6 or ≥ 1.2 (camera parameters are likely inaccurate). We keep sequences that are at least 30 frames long, have an aspect ratio of 16:9, and have a width of ≥1600 pixels. Finally, we manually inspect the trajectories and point clouds of the remaining sequences and remove obviously incorrect reconstructions. Examples of removed images are shown in SM. After processing, we obtain 4,690 sequences with a total of more then 170K valid image-depth pairs. We split our MC dataset into training, validation and testing sets with a 80:3:17 split over clips. 

SECTION 4. Depth Prediction Model: We train the depth prediction model on the Mannequin-Challenge dataset in a supervised manner, i.e., by regressing to the depth generated by the MVS pipeline. A key question is how to structure the input to the network to allow training on frozen people but inference on freely moving people. One option is to regress from a single RGB image to depth, but this approach disregards geometric information about the static regions of the scene that is available by considering more than a single view. To benefit from such information, we input to the network a depth map for the static, non-human regions, estimated from motion parallax w.r.t. another view of the scene. The full input to our network, illustrated in Fig. 3, includes a reference image Ir, a binary mask of human regions M, a depth map estimated from motion parallax (with human regions removed) Dpp, confidence map C, and an optional human keypoint map K. We assume known, accurate camera poses from SfM during both training and inference stages. In an online inference setting, camera poses can be obtained by visual-inertial odometry. Given these inputs, the network predicts a full depth map for the entire scene. To match the MVS depth values, the network must inpaint the depth in human regions, refine the depth in non-human regions from the estimated Dpp, and finally make the depth of entire scene consistent. Our network architecture is a variant of the hourglass network of [3], with the nearest-neighbor upsampling layers replaced by bilinear upsampling layers. The following sections describe each of the inputs to our model and our training losses in detail. Please refer to the SM for additional implementation details and full derivation. Figure 3. 
System inputs and training data. The input to our network consists of: (a) RGB image, (b) human mask, (c) masked depth computed from motion parallax w.r.t. a selected source image, and (d) masked confidence map. Low confidence regions (dark circles) in the first two rows indicate the vicinity of the camera epipole, where depth from parallax is unreliable and is removed. The network is trained to regress to MVS depth (e). 
4.1. Depth from motion parallax: Motion parallax between two frames in a video provides our initial depth estimate for the static regions of the scene (assuming humans are dynamic while the rest of the scene is static). Given a reference image Ir and source image Is pair, we estimate an optical flow field from Ir to Is using FlowNet2.0 [13]. Using the relative camera poses between the two views, we compute an initial depth map Dpp from the estimated flow field, using the Plane-Plus-Parallax (P+P) representation [15], [43]. In some cases, such as forward/backward relative camera motion between the frames, the estimated depth may be ill-defined in some image regions (i.e., the epipole may be located within the image). We detect and filter out such depth values as described in Sec. 4.2. Key-frame selection. Depth from motion parallax may not be meaningful if the 2D displacement between the two views is small or if it can be well-approximated by a homography (e.g., in the case of pure camera rotation). To avoid such cases, we apply a baseline criterion when selecting a reference frame Ir and a corresponding source key-frame Is. We want the two views to have significant overlap, while having a large enough baseline. Formally, for each Ir, we find the index s of Is as
s=argmaxjdrjorj(2)View Source\begin{equation*}s = \mathop {\arg \max }\limits_j {d^{rj}}{o^{rj}}\tag{2}\end{equation*}
where drj is the L2 distance between the camera centers of Ir and its neighbor frame Ij. The term orj is the fraction of co-visible SfM features in Ir and orj=2∣∣Vr∩Vj∣∣|Vr|+∣∣Vj∣∣, where V j is the set of features visible in Ij. We discard pairs of frames for which orj < τo, i.e., the fraction of co-visible features should be larger than a threshold τo (we set τo = 0.6), and limit the maximum frame interval to 10. We found these view selection criteria work well in all our experiments. 4.2. Confidence: Our data consists of challenging Internet video clips with camera motion blur, shadows, low lighting, and reflections. In such cases, optical flow is often noisy [44], compounding uncertainty in the input depth map, Dpp. We thus estimate, and input to the network, a confidence map, C. This allows the network to rely more on the input depth in high-confidence regions, and potentially use it to improve its prediction in low-confidence regions. The confidence value at each pixel p in the non-human regions is defined as:
C(p)=Clr(p)Cep(p)Cpa(p).(3)View Source\begin{equation*}C({\mathbf{p}}) = {C_{lr}}({\mathbf{p}}){C_{ep}}({\mathbf{p}}){C_{pa}}({\mathbf{p}}).\tag{3}\end{equation*} The term Clr measures "left-right" consistency between the forward and backward flow fields. That is, Clr(p) = max 0, 1 − r(p)2 , where r(p) is the forward-backward warping error. For perfectly consistent forward and backward flows Clr =1, while Clr =0 when the error is greater than 1px. The term Cep measures how well the flow field complies with the epipolar constraint between the views [10]. Specifically, Cep(p)=max(0,1−(γ(p)/γ¯)2), where γ(p) is the distance between the warped pixel position of p based on its optical flow and its corresponding epipolar line; γ¯ controls the epipolar distance tolerance (we set γ¯=2px in our experiments). Finally, Cpa assigns low confidence to pixels for which the parallax between the views is small [33]. This is measured by the angle β(p) between the camera rays meeting at the pixel p. That is, Cpa(p)=1−(min(β¯,β(p))−β¯β), where β¯ is the angle tolerance (we use β¯=1∘ in our experiments). Figure 4. 
Qualitative results on MC test set. From top to bottom: reference images and their corresponding MVS depth (pseudo ground truth); our depth predictions using: our single view model (third row) and our two-frame model (forth row). The additional network inputs give improved performance in both human and non-human regions. 
Fig. 3(d) shows examples of computed confidence maps. Note that human regions as well as regions for which the confidence C(p) < 0.25 are masked out. 4.3. Losses: We train our network to regress to depth maps computed by our data pipeline. Because the computed depth values have arbitrary scale, we use a scale-invariant depth regression loss. That is, our loss is computed on log-space depth values and consists of three terms:
Lsi=LMSE+α1Lgrad+α2Lsm.(4)View Source\begin{equation*}{\mathcal{L}_{{\text{si}}}} = {\mathcal{L}_{{\text{MSE}}}} + {\alpha _1}{\mathcal{L}_{{\text{grad}}}} + {\alpha _2}{\mathcal{L}_{{\text{sm}}}}.\tag{4}\end{equation*} Scale-invariant MSE. ℒMSE denotes the scale-invariant mean square error (MSE) [6]. This term computes the squared, log-space difference in depth between two pixels in the prediction and the same two pixels in the ground-truth, averaged over all pairs of valid pixels. Intuitively, we look at all pairs of points, and penalize the difference in their ratio of depth values w.r.t. ground truth. Multi-scale gradient term. We use a multi-scale gradient term, ℒgrad, which is the L1 difference between the predicted log depth derivatives (in x and y directions) and the ground truth log depth derivatives, at multiple scales [19]. This term allows the network to recover sharp depth discontinuities and smooth gradient changes in the predicted depth images. Multi-scale, edge-aware smoothness terms. To encourage smooth interpolation of depth in texture-less regions where MVS fails to recover depth, we use a simple smoothness term, ℒsm, which penalizes L1 norm of log depth derivatives based on the first- and second-order derivatives of images and is applied at multiple scales [41]. This term encourages piecewise smoothness in depth regions where there is no image intensity change. Table 1. 
Quantitative comparisons on MC test set. Different input configurations of our model: (I.) single image; (II.) optical flow masked in the human region (F ), confidence and human mask; (III.) masked input depth, human mask, and additional confidence for IV.; in V, we also input human keypoints. Lower is better for all metrics.


SECTION 5. Results: We tested our method quantitatively and qualitatively and compare it with several state-of-the-art single-view and motion-based depth prediction algorithms. We show additional qualitative results on challenging Internet videos with complex human motion and natural camera motion, and demonstrate how our predicted depth maps can be used for several visual effects. Error metrics. We measure error using the scale-invariant RMSE (si-RMSE), equivalent to LMSE−−−−√, described in Sec. 4.3. We evaluate si-RMSE on 5 different regions: si-full measures the error between all pairs of pixels, giving the overall accuracy across the entire image; si-env measures pairs of pixels in non-human regions E, providing depth accuracy of the environment; and si-hum measures pairs where at least one pixel lies in the human region ℋ, providing depth accuracy for people. si-hum can further be divided into two error measures: si-intra measures si-RMSE within ℋ, or human accuracy independent of the environment; si-inter measures si-RMSE between pixels in ℋ and in E, or human accuracy w.r.t. the environment. We include derivations in SM. Figure 5. 
Qualitative comparisons on TUM RGBD dataset. (a) Reference images, (b) source images (used to compute our initial depth input), (c) ground truth sensor depth, (d) single view depth prediction method DORN [7], (e) two-frame motion stereo DeMoN [39], (f-g) depth predictions from our single view and two-frame models, respectively. 
5.1. Evaluation on MC test set: We evaluated our method on our MC test set, which consists of more than 29K images taken from 756 video clips. Processed MVS depth values DMVS obtained by our pipeline (see Sec. 3) are considered as ground truth. To quantify the importance of our designed model’s input, we compare the performance of several models, each trained on our MC dataset with a different input configuration. The two main configurations are: (i) a single-view model (input is RGB image) and (ii) our full two-frame model, where the input includes a reference image, an initial masked depth map Dpp, a confidence map C, and a human mask M. We also perform ablation studies by replacing the input depth with optical flow F , removing C from the input, and adding a human keypoint map K. Quantitative evaluations are shown in Table 1. By comparing rows (I.), (III.) and (IV.), it is clear that adding the initial depth of environment as well as a confidence map significantly improves the performance for both human and non-human regions. Adding human keypoint locations to the network input further improves performance. Note that if we input an optical flow field to the network instead of depth (II.), the performance is only on a par with the single view method. The mapping from 2D optical flow to depth depends on the relative camera poses, which are not given to the network. This result indicates that the network is not able to implicitly learn the relative poses and extract the depth information. Fig. 4 shows qualitative comparisons between our single-view model (I) and our full model (IDppCMK). Our full model results are more accurate in both human regions (e.g., first column) and non-human regions (e.g., second column). In addition, the depth relations between people and their surroundings are improved in all examples. 5.2. Evaluation on TUM RGBD dataset: We used a subset of the TUM RGBD dataset [38], which contains indoor scenes of people performing complex actions, captured from different camera poses. Sample images from this dataset are shown in Fig. 5(a-b). To run our model, we first estimate camera poses using ORB-SLAM2 3. In some cases, due to severe low image quality, motion blur and rolling shutter effects, the estimated camera poses may be incorrect. We manually filter such failures by inspecting the camera trajectory and point cloud. In total, we obtain 11 valid image sequences with 1815 images in total for evaluations. Table 2. 
Results on TUM RGBD datasets. Different si-RMSE metrics as well as standard RMSE and relative error (Rel) are reported. We evaluate our models (light gray background) under different input configurations, as described in Table 1. w/o d. cleaning indicates the model is trained using raw MVS depth predictions as supervision, without our depth cleaning method. Dataset ‘-’ indicates the method is not learning based. Lower is better for all error metrics.
Figure 6. 
Comparisons on Internet video clips with moving cameras and people. From left to right: (a) reference image, (b) source image, (c) DORN [7], (d) Chen et al. [3], (e) DeMoN [39], (f) our full method. 
We compare our depth predictions (using our MC trained models) with several state-of-the-art monocular depth prediction methods trained on indoor NYUv2 [17], [46], [7] and Depth in the Wild (DIW) datasets [3], and the recent two-frame stereo model DeMoN [39], which assumes a static scene. We also compare with Video-Popup [31], which deals with dynamic scenes. We use the same image pairs for computing Dpp as inputs to DeMoN and Video-Popup. Quantitative comparisons are show in Table 2, where we report 5 different scale-invariance error measures as well as standard RMSE and relative error; the last two are computed by applying a single scaling factor that aligns the predicted and ground-truth depth in the least-squares sense. Our single-view model already outperforms the other single-view models,demonstrating the benefit of the MC dataset for training. Note that VideoPopup [31] failed to produce meaningful results due to the challenging camera and object motion. Our full model, by making use of the initial (masked) depth map, significantly improves performance for all the error measures. Consistent with our MC test set results, when we use optical flow as input (instead of initial depth map) the performance is only slightly better than the single-view network. Finally, we show the importance of our proposed "depth cleaning" method, applied to the training data (see Eq. 1). Compared to the same model, only trained using the raw MVS depth predictions as supervision ("w/o d. cleaning"), we see a drop of about 15% in performance. Figure 7. 
Depth-based visual effects. We use our predicted depth maps to apply depth-aware visual effects on (a, e) input images; we show (b) defocus, (c) object insertion, and (d, f) people removal with inpainting results. 
Fig. 5 shows qualitative comparison between the different methods. Our models’ depth predictions (Fig. 5(f-g)) strongly resemble the ground truth and show high level of details and sharp depth discontinuities. This result is a notable improvement over competing methods, which often produce significant errors in both human regions (e.g., legs in the second row of Fig. 5), and non-human regions (e.g., table and ceiling in the last two rows). 5.3. Internet videos of dynamic scenes: We tested our method on challenging Internet videos (downloaded from YouTube and Shutterstock), involving simultaneous natural camera motion and human motion. Our SLAM/SfM pipeline was used to generate sequences ranging from 5 seconds to 15 seconds with smooth and accurate camera trajectories, after which we apply our method to obtain the required network input buffers. We qualitatively compare our full model (IDppCMK) with several recent learning based depth prediction models: DORN [7], Chen et al. [3], and DeMoN [39]. For fair comparisons, we use DORN with a model trained on NYUv2 for indoor videos and a model trained on KITTI for outdoor videos; For [3], we use the models trained on both NYUv2 and DIW. For all of our predictions, we use a single model trained from scratch on our MC dataset. As illustrated in Fig. 6, our depth predictions are significantly better than the baseline methods. In particular, DORN [7] has very limited generalization to Internet videos, and Chen et al. [3], which is mainly trained on Internet photos, is not able to capture accurate depth. DeMoN often produces incorrect depth, especially in human regions, as it designed for static scenes. Our predicted depth maps depict accurate depth ordering both between people and other objects in the scene (e.g., between people and buildings, fourth row of Fig. 6), and within human regions (such as the arms and legs of people in the first three rows of Fig. 6). Figure 8. 
Failure cases. Moving, non-human objects such as cars and shadows can cause bad estimates (left and middle, boxed); fine structures such as limbs may be blurred for distant people in challenging poses (right, boxed). 
Depth-Based Visual Effects. Our depth can be used to apply a range of depth-based visual effects. Fig. 7 shows depth-based defocus, insertion of synthetic 3D graphics, and removal of nearby humans with inpainting. See SM for more examples including mono-to-stereo conversion. The depth estimates are sufficiently stable over time to allow inpainting from frames elsewhere in the video. To use a frame for inpainting, we construct a triangle heightfield from the depth map, texture the heightfield with the video frame, and render the heightfield from the target frame using the relative camera transformation. Fig. 7 (d, f) show the results of inpainting two street scenes. Humans near the camera are removed using the human mask M, and holes are filled with colors from up to 200 frames later in the video. Some artifacts are visible in areas the human mask misses, such as shadows on the ground. 

SECTION 6. Discussion and Conclusion: We demonstrated the power of a learning-based approach for predicting dense depth of dynamic scenes where a monocular camera and people are freely moving. We make a new source of data available for training: a large corpus of Mannequin Challenge videos from YouTube, in which the camera moves around and people "frozen" in natural poses. We showed how to obtain reliable depth supervision from such noisy data, and demonstrated that our models significantly improve over state-of-the-art methods. Our approach still has limitations. We assume known camera poses, which may difficult to infer if moving objects cover most of the scene. In addition, the predicted depth may be inaccurate for non-human, moving regions such as cars and shadows (Fig. 8). Our approach also only uses two views, sometimes leading to temporally inconsistent depth estimates. However, we hope this work can guide and trigger further progress in monocular dense reconstruction of dynamic scenes.