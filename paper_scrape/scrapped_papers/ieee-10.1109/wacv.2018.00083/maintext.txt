SECTION 1. Introduction: Modern computer vision tasks rely heavily on carefully designed convolutional neural networks architectures [21], [27], [30], [14]. These CNNs usually consist of multiple convolutional layers with a large amount of parameters. They are computationally expensive and are often over-parametrized [5]. Recently, network pruning has become an important topic that aims at simplifying and accelerating large CNN networks. In [12], [11], Han et al. proposed compressing CNNs by removing weights with small magnitudes and achieved promising compression results. Pruning individual weights increases the sparsity in CNNs. In order to get real compression and speedup, it requires specially designed software (like sparse BLAS library) or hardware [10] to handle the sparsity. Pruning filters is another means to simplify CNNs. An advantage of pruning filters is that it provides both compression and speedup benefits without requiring specially designed software or hardware. Moreover, pruning filters can be used in addition to other sparsity or low- rank-approximation based methods to further reduce computations. In [23], Hao et al. pruned filters according to a hand-crafted criteria, the Ll norm. They showed that pruning filters with small L1 norms gives better pruning performance than random pruning or pruning the largest filters. However, it remains unknown that if pruning filters in a data-driven way offers better pruning performances. Moreover, in many practical scenarios, it is desirable to have an easy control of the tradeoff between network performance and scale during pruning. However, to the best of our knowledge, this is not available in existing works. For example, in some situations, we are willing to sacrifice certain level of performances. Unfortunately, it usually takes tremendous human efforts to test different tradeoffs between the network performance and scale during pruning in order to achieve the best pruning performance under the desired performance drop budget. Our work focuses on addressing aforementioned problems. Firstly, our method learns to prune redundant filters in a data-driven way. We show that pruning in a data-driven way gives better performances than the hand-crafted pruning criteria [23]. Secondly, our method provides an easy control of the tradeoff between network performance and scale during pruning. After specifying the desired network performance, our method automatically outputs a compact model with filters aggressively pruned without involving humans in the loop. In order to achieve these goals, we formulate the filter pruning problem as a “try-and-learn” learning task. Specifically, we train a pruning agent, modeled by a neural network, to take the filter weights as input and output binary decisions to remove or keep filters. The agent is trained with a novel reward function which encourages high pruning ratios and guarantees the pruned network performance remains above a specified level. In another word, this reward function provides an easy control of the tradeoff between network performance and scale. Since the reward function is non-differentiable w.r.t the parameters of pruning agents, we use the policy gradient method [29], [33] to update the parameters in training. Intuitively, our algorithm functions as a “try-and-learn” process. The pruning agent starts by guessing which filters to prune. Every time it takes a pruning action, the action is evaluated by our reward function. The reward is then fed back to the agent which supervises the agent to output actions with higher rewards. Often the search space is extremely large. For example, for a layer with 64 filters, there are 264 different options of which filters to remove. However, our algorithm is highly efficient and experiments show that it converges after only a relatively small number of trials. Our method is totally automatic and data-driven. Once started, it automatically discovers redundant filters and removes them while keeping the performance of the model within a specified tolerance. Our method also makes it possible to control the tradeoff between network performance and its scale during pruning without involving humans in the loop. We demonstrate the performance of our algorithm through extensive experiments with various CNN networks in section 4. 

SECTION 2. Related Works: Early works on model compression are based on the saliency guidance [22] or the Hessian of the loss function [13]. Recently, there are many pruning methods proposed for modern large scale CNNs. We review related works in the following categories. Low-rank approximation. LRA methods [6], [17] are based on one key observation that most of CNN filters or features are of low rank and can be decomposed into to lightweight layers by matrix factorization. [6] made one of the early attempts at applying LRA methods such as Single Value Decomposition (SVD) for network simplification. [17] decomposed k x k filters into k x 1 and 1 x k filters, which effectively saved the computations. [17] investigated two different optimization schemes, one for filter-based approximation and one for feature-based approximation. Similarly, [7] used low-cost collaborative kernels for acceleration. [34] used Generalized SVD for the non-linearity in networks and achieved promising results in very deep CNNs. Recently, [15] combined the low-rank approximation with channel pruning and [32] proposed to use Force Regularization to train neural networks towards low-rank spaces. Increasing Sparsity. Methods in this category mainly focus on increasing the sparsity in CNNs. [12], [11] introduced a three-step training pipeline to convert a dense network into a sparse one. They achieved this by removing connections with small weights. Their method showed promising compression rates on various CNNs. However, it required an additional mask to mask out pruned parameters and handle the sparsity, which actually does not save computations. [9] proposed a dynamic network surgery algorithm to compress CNNs by making them more sparse. [1], [35], [28] also proposed to remove the neurons in networks to increase the sparsity. Usually, these methods require specially designed software or hardware [10] to handle the sparsity in CNNs in order to gain real speedup. Quantization and binarization. On top of the method in [12], [11] used additional quantization and Huffman encoding to further compress the storage requirement. These methods can also be applied on top of our filter pruning algorithm. Another group of methods tried to use bit-wise operations in CNNs to reduce the computations. [16] introduced an algorithm to train networks with binary weights and activations. In [25], the authors suggested to also bi-narize the inputs to save more computations and memories. However, experiments showed that the performance of these binarized networks are worse than their full prevision counterparts [25]. Pruning filters. Pruning filters has some nice properties as mentioned above. In [23], the authors proposed a magnitude-based pruning method. Our method differs from their work in 1). our algorithm is able to prune filters in a data-driven way which gives better pruning performances as shown in section 4; 2). our algorithm supports the control of the tradeoff between network performance and scale without involving humans in the loop. Reinforcement learning. There are some works applying reinforcement learning methods to neural network architecture design. In [36], the authors proposed to generate neural network architecture descriptions using a recurrent neural network (RNN). They trained the RNN via policy gradient method. [3] explored with more possible architectures using Q-learning [31], [26]. These methods are all used to generate neural network architectures from scratch and require tremendous computing resources for training. In contrast, our work focuses on pruning a given baseline network and consumes much less computing resources. 

SECTION 3. Method: We introduce the proposed method for pruning a CNN in this section. Assume there is a baseline CNN f with L convolutional layers. Firstly, we describe how to prune an individual layer. Then, we present details about how to prune all layers in a CNN. 3.1. Learning to Prune Filters in One Individual Layer: Let Nl denote the number of filters in the lthlayer in the baseline CNN f. Suppose the filter matrix is represented by Wl={wl1, wl2, …, wli, …, wlNl}, where wli∈Rml×h×w with ml,h, and w being the number of input feature maps, the height and width of the lthlayer filter. Our goal is to simplify the network by pruning redundant filters in Wl. We achieve this by training a pruning agent πlwhich takes Wl as the input and makes a set of binary actions A1={al1, al2, …, ali, …, alNl}. Here all∈{0,1}. ali=0 means the agent treats the ith filter as unnecessary and decides to remove it and ali=1 means the opposite. The agent is parametrized by θl. Formally, the agent models the following conditional probability, πl(A1|Wl, θl). We use a neural network to model the pruning agent in our experiments.
Algorithm 1 Prune filters of one layer in CNN:   For the given task, there is a validation set Xval={xval, yval}.xval and yval denote the set of validation images and the corresponding ground truth, respectively. The learning goal is to maximize the objective L=R(A1, Xval). Here R is a reward function defined as the multiplication of two terms, the accuracy term and efficiency term.
R(A1, Xval)=ψ(A1, Xval, b,p∗)∗φ(A1)(1)View Source\begin{equation*}
R(\mathbf{A}^{1},\ \mathbf{X}_{val})=\psi(\mathbf{A}^{1},\ \mathbf{X}_{val},\ b, p^{\ast})\ast\varphi(\mathbf{A}^{1})
\tag{1}
\end{equation*} The accuracy term ψ(A1, Xval)b,p∗) is calculated by equation (2). It guarantees the performance drop evaluated on Xval under metric M is bounded by b. The performance drop bound b is a hyper-parameter used to control the trade- off between network performance and scale. p^ and p∗ are the performance of new model f^A1 and baseline model f. The new model f^A1 is generated by surgeryingf according to the action set A1. Before evaluation, fA1 is first fine-tuned by a training set Xtrain={xtrain, ytrain} for some epochs to adjust to the pruning actions. In evaluation, the metric M is set as the classification accuracy in recognition tasks and the global accuracy in segmentation tasks in this paper. The larger p^ is, the more ψ contributes to the final reward. However, when the performance drop (p∗−p^) is larger than the bound b,ψ contributes negatively to the final reward. This forces the pruning agent to keep the performance of pruned network above a specified level.
ψ(A1,Xval,b,p∗)=b−(p∗−pA)bpA=M(f^A1,Xval), p∗=M(f,Xval)(2)(3)View Source\begin{align*}
&\quad\psi(\mathbf{A}^{1}, \mathbf{X}_{val}, b, p^{\ast})=\displaystyle \frac{b-(p^{\ast}-p^{\mathbf{A}})}{b}
\tag{2}\\
&p^{\mathrm{A}}=\mathcal{M}(\hat{f}_{\mathbf{A}^{1}}, \mathbf{X}_{val}),\ p^{\ast}=\mathcal{M}(f, \mathbf{X}_{val})
\tag{3}
\end{align*} The efficiency term φ(A1) is calculated by equation (4). It encourages the agent πl to prune more filters away. C(A1) denotes the number of 1-actions in A1 which is also the number of kept filters. A small C(A1) means only a few filters are kept and most of the filters are removed. The smaller C(A1) is, the more efficient the pruned model is, and more φ contributes to the final reward. The log operator guarantees two terms in equation (1) are of the same order of magnitude.
φ(A1)=log(NlC(A1))(4)View Source\begin{equation*}
\displaystyle \varphi(\mathbf{A}^{1})=\log(\frac{N^{l}}{\mathcal{C}(\mathbf{A}^{1})})
\tag{4}
\end{equation*} Since \mathcal{L}L is non-differentiable w.r.t \mathrm{t}\theta^{l}tθl, we use the policy gradient estimation method [29], specifically the RE-INFORCE [33], to estimate the gradients \nabla_{\theta^{l}}\mathcal{L}∇θlL as equation (5). Furthermore, we can sample MM times from the output distribution to approximate the gradients. This gives the final gradient estimation formula in equation (6). In order to get an unbiased estimation, the rewards of MM samples are normalized to zero mean and unit standard deviation. Algorithm 1 summarizes the whole training process.
\begin{align*}
&\nabla_{\theta^{l}}\mathcal{L}=\mathbb{E}[R(\mathbf{A}^{1})\ast\nabla_{\theta^{l}}\log(\pi^{l}(\mathbf{A}^{1}\vert W^{l},\ \theta^{l}))]
\tag{5}\\
&\displaystyle \nabla_{\theta^{l}}\mathcal{L}=\sum_{=i1}^{j\psi}[R(\mathbf{A}_{\mathrm{i}}^{1})\ast\nabla_{\theta^{l}}\log(\mathbf{A}_{\mathbf{i}}^{1}\vert \pi^{l}(W^{l},\ \theta^{l}))]
\tag{6}
\end{align*}∇θlL=E[R(A1)∗∇θllog(πl(A1|Wl, θl))]∇θlL=∑=i1jψ[R(A1i)∗∇θllog(A1i|πl(Wl, θl))](5)(6)View Source\begin{align*}
&\nabla_{\theta^{l}}\mathcal{L}=\mathbb{E}[R(\mathbf{A}^{1})\ast\nabla_{\theta^{l}}\log(\pi^{l}(\mathbf{A}^{1}\vert W^{l},\ \theta^{l}))]
\tag{5}\\
&\displaystyle \nabla_{\theta^{l}}\mathcal{L}=\sum_{=i1}^{j\psi}[R(\mathbf{A}_{\mathrm{i}}^{1})\ast\nabla_{\theta^{l}}\log(\mathbf{A}_{\mathbf{i}}^{1}\vert \pi^{l}(W^{l},\ \theta^{l}))]
\tag{6}
\end{align*} 3.2. Learning to Prune Filters in all Layers in a Network: In a baseline with LL convolutional layers, our algorithm prunes all of them by training a set of pruning agents \pi=\{\pi^{1},\ \pi^{2},\ \ldots,\ \pi^{l},\ \ldots,\ \pi^{L}\}π={π1, π2, …, πl, …, πL} where \pi^{l}πl prunes the filters in the lthlayer. It starts from lower layers and proceeds to higher layers one by one. Algorithm 2 summarizes the overall training process. After finishing the pruning of one layer, it fine-tunes the entire network for some epochs again using \mathbf{X}_{train}Xtrain to compensate the performance decrease. We find this layer-by-layer and low-to-high pruning strategy works better than other alternatives such as pruning all layers by their order of pruning-sensitivity.
Figure 1: Pruning results of VGG-16 on CIFAR 10. Numbers on top of bars are the pruning ratios. 
Table 1: Statistics of baseline models
Algorithm 2 Prune filters in the entire CNN:  


SECTION 4. Experiments: We experiment with the prunin \mathrm{g}g algorithm on two tasks, visual recognition and semantic segmentation. For the vi- sual recognition task, we test the VGG-16 [27] and ResNet-18 [14] network on the CIFAR 10 dataset [20]. For the segmentation task, we test the FCN-32s network [24] on Pascal VOC dataset [8] and the SegNet network [2] on the CamVid dataset [4]. Pruning agents in all experiments are modeled by neural networks built by following the same protocol. For VGG-16 and ResNet-18 network, we use the official PyTorch implementation 1. For the FCN-32s and SegNet network, we use their official caffe [18] implementation. The performances of baseline models are reported in Table1. For all datasets, we select a hold-out set from training set as our validation set.
Figure 2: Result comparison of pruning the VGG-16 network on CIFAR 10 using different drop bounds. 
Figure 3: Training curves of the pruning agent for the 2^{nd}2nd layer in VGG-16 on CIFAR 10. Originally there are 64 filters. Red line is the reward. Green line denotes the number of kept filters. Light lines are the raw data and bold lines are the smoothed data for better visualization. 
Figure 4: Result comparison of pruning the ResNet-18 network on CIFAR 10 using different drop bounds. 
Table 2: Various pruning results of VGG-16 network on CIFAR 10
Pruning agent design protocol. All pruning agents are modeled by neural networks which takes the filter matrix W^{l}(Wl( of size N_{l}\times m^{l}\times h\times w)Nl×ml×h×w) as input and outputs N_{l}Nl binary decisions. In order to apply same operations to all filter weights, W^{l}Wl is first rearranged into a 2D matrix of size N_{l}\times M^{l}Nl×Ml where M^{l}=m^{l}\times h\times wMl=ml×h×w and then fed into the pruning agent. If M^{l}Ml is larger than 24, the pruning agent will be composed of 4 alternating convolutional layers with 7 \times 7×7 kernels and pooling layers followed by two fully connected layers. Otherwise, the pruning agent will only consist of two fully connected layers. In training, all pruning agents are updated with the Adam optimizer [19]. We use a fixed learning rate 0.01 for all recognition experiments and 0.001 for all segmentation experiements. We roll out the output distributions for 5 times (M=5)(M=5) for all experiments. Distributed training. The output distribution sampling and evaluation process in Algorithm 1 is the most time-consuming part in our algorithm. However, it is highly parallelizable and we use distributed computation in experiments for speed up. All training are conducted on Nvidia K40 GPUs. GPU speeds are measured on one single K40 while CPU speeds are measured on one core of a Xeon(R) CPU E5-2640 v4 CPU. All measurements are averaged over 50 runs. The measurements use a batch of 512 32\times 3232×32 images for recognition networks, a batch of one 256 \times 256×256 image for the FCN-32s and a batch of one 360 \times 480×480 image for the SegNet. The training process takes \sim 5∼5 days for pruning on CIFAR 10 networks and SegNet and \sim 10∼10 days for FCN-32s network using 6 GPUs. 4.1. Pruning VGG-16 on CIFAR 10: We first prune single layer in VGG-16 on CIFAR 10 using Algorithm 1. The accuracy drop bound bb is set as 2. The results are reported in Fig. 1(a). As a byproduct, our algorithm can analyze the redundancy in each layer of a given CNN. Pruning results show that in VGG-16 on CIFAR-10, higher layers contain more unnecessary filters than lower layers as removing more than 95% of the filters in some higher layers (layer 9, 10, 11, 13) has relatively less impact on performance. One set of typical training curves is presented in Fig. 3. As the number of training epochs increase, the reward keeps increasing and more and more filters are removed. Originally, there are 64 filters in this layer, which means there are 264 different decision options. However, our agent converges after less than 200 epochs in this case. This shows the efficiency of our algorithm. We now prune all layers in the VGG-16 network on CI-FAR 10 using the same drop bound b=2b=2. The pruning results are presented in Fig. 1(b). Generally, the pruning ratio of each layer is slightly smaller than the single-layer pruning. But our algorithm still aggressively prunes lots of redundant filters. Several quantitative evaluations are reported in Table 2. We also experiment with various drop bound bb to show how to control the tradeoff between network performance and scale using our algorithm. All the results are reported in Table 2. Note that the final accuracy drops are usually not exactly same as the drop bound. That is because there is a generalization gap between validation and final test set.
Figure 5: Visualization of filters. Filters are ranked by its L1L1 norm in a descending order from top left to bottom right. Filters with red bounding boxes are the ones kept by our algorithm (with b=2b=2). 
Table 3: Various pruning results of ResNet-18 network on CIFAR 10
For further comparisons, we also apply the magnitude-based filter pruning method in [23] to the same baseline network and prune the network using the same pruning ratios. The accuracy drops of these pruned networks are reported in the parenthesis in Table 2. Two key observations include: 1 )). A larger drop bound generally gives a higher prune ratio, more saved FLOPs, a higher speedup ratio, and a larger accuracy drop on test set; 2). With the same pruning ratios, our data-driven algorithm discovers better pruning solutions (which results in less accuracy drops) for VGG-16 on CIFAR 10 than the method in [23]. For further comparison with [23], we also visualize the filters in the 1st layer in VGG-16 on CIFAR 10 in Fig. 5. They show that our algorithm does not prune filters based on their magnitude. 4.2. Pruning ResNet-18 on CIFAR 10: With promising results on VGG-16 on CIFAR 10, we also experiment with a larger and more complex network, ResNet −18 [14], on the same CIFAR 10 dataset. Different drop bounds are tested as well. Results are reported and compared with [23] 2 in the same way as section 4.1 in Table. 3. Compared with VGG-16, ResNet-18 is a more lightweight and efficient network architecture. Thus, the overall pruning ratios are smaller. However, our algorithm is still capable to prune a lot of redundant filters. In Fig. 4, we show the detailed pruning ratios of each layer with different drop bounds. In total, there are 20 convolutional layers in the ResNet-18 network including the shortcut convolutional layers in residual blocks. Generally, larger drop bounds offer larger pruning ratios. Moreover, results show that in a residual block, the first convolution layer is easier to prune than the second one. For example, there are more filters are removed in the 7^{th}/9^{th}/12^{th}/14^{th}7th/9th/12th/14th layer than the 8^{th}/10^{th}13^{th}/15^{th}8th/10th13th/15th layer, regardless of the drop bound. 4.3. Pruning FCN-32s on Pascal VOC: CNNs designed for semantic segmentation tasks are much more challenging to prune as the pixel-level labeling process requires more weights and more representation capacities. In order to show the performance of our pruning algorithm on segmentation tasks, we apply our algorithm to a pre-trained FCN-32s network [24] on the Pascal VOC dataset [8]. We use the global pixel accuracy as the evaluation metric \mathcal{M}M in our reward function. Note that depending on the scenario, other metrics, such as per-class accuracy and mean IU, can also be used as metric \mathcal{M}M. The drop bound bb is set as 2. Following Algorithm 2, our algorithm removes near 63.7% redundant filters in FCN-32s and the inference process is accelerated by 37.0% on GPU and 49.1 % on CPU as reported in Table. 4. In the FCN-32s network, the last two convolutional layers are converted from fully connected layers (one is of size 512\times 4096\times 7\times 7512×4096×7×7 and the other one is of size 4096\times 4096\times 1\times14096×4096×1×1). These two layers contribute 87.8% of the parameters in the entire network and are of high redundancy. In our algorithm, 51.6% and 68.7% of the filters in these two layers are removed, respectively. Detailed pruning ratios of each layer in the FCN-32s network is presented in Fig. 6a.
Table 4: Pruning results of segmentation networks
We also apply the magnitude based method in [23] to the same baseline using the same pruning ratio. Compared to their method, our algorithm can find a better combination of filters to remove which results in a lower accuracy drop (1.5 vs 3.4). Moreover, some sample segmentation results after pruning are presented and compared with original results in Fig. 7. 4.4. Pruning SegNet on CamVid: We also experiment with a network with different architecture, SegNet [2], on a different dataset, the CamVid dataset [4]. The drop bound is set as bb == 2. Pruning results are reported in Table. 4. Also, the method in [23] is also applied to the same baseline with same pruning ratio for comparison. Our algorithm removes near 56.9% of parameters in the baseline SegNet and speeds it up by 42.4% on GPU and 53.0% on CPU. The global accuracy of the pruned network produced by our method is increased 2.1% while the network produced by the magnitude-based decreases 3.0%. This is because our reward function not only guarantees the accuracy not to drop below a specified level but also encourages higher accuracies. The SegNet network is twice larger than its building block, the VGG-16 network. However, CamVid is a small scale dataset. In this case, our pruning algorithm prevents the network from over-fitting which results in a higher global accuracy after pruning. Detailed pruning ratios of each layer in the FCN-32s network is presented in Fig. 6b. In the SegNet network, the first half and second half part of the network are symmetrical. However, the pruning results show that the second half contains more unnecessary filters than the first half. Only 26.9% of the filters are removed in the first half. In contrast, 49.2% of the filters are removed in the second half. Some segmentation visualizations of the SegNet network on CamVid are presented in Fig. 8. 

SECTION 5. Conclusion: This paper introduces a “try-and-learn” learning algorithm for pruning filters in convolutional neural networks. Our work focuses on the following questions: 1). how to prune redundant CNN filters in a data-driven way; 2). how to enable the control of the tradeoff between network performance and its scale in pruning. Our training algorithm is based the policy gradient method. By using a novel reward function, our method aggressively prunes the filters in baseline network while maintaining the performance in a desired level. We benchmark our algorithm on several widely used visual recognition and semantic segmentation CNN networks. Experimental results demonstrate the effectiveness of our algorithm. Potential future directions of our work include 1). extending our method to a more efficient learning algorithm to reduce the training time; 2). formulating the pruning of the entire network as one learning task for higher automation.
Figure 6: Pruning results of CNNs on segmentation tasks. Numbers on top of the bars are the pruning ratios. 
Figure 7: Segmentation visualization of the FCN-32s network on pascal VOC. The number in each column represents the change of global accuracy. The first two, middle two, and last two columns are samples with global accuracies increased, unchanged, and decreased, respectively. 
Figure 8: Segmentation visualization of the segnet network on CamVid. The number in each column represents the change of global accuracy. The first two, middle two, and last two columns are samples with global accuracies increased, unchanged, and decreased, respectively.