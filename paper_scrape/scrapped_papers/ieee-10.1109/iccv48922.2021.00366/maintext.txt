SECTION 1. Introduction: When faced with a machine learning problem, the hardest challenge often isn’t choosing the right machine learning model, it’s finding the right data. This is especially difficult in the realm of human-related computer vision, where concerns about the fairness of models and the ethics of deployment are paramount [31]. Instead of collecting and labelling real data, which is slow, expensive, and subject to bias, it can be preferable to synthesize training data using computer graphics [68]. With synthetic data, you can guarantee perfect labels without annotation noise, generate rich labels that are otherwise impossible to label by hand, and have full control over variation and diversity in a dataset. Rendering convincing humans is one of the hardest problems in computer graphics. Movies and video games have shown that realistic digital humans are possible, but with significant artist effort per individual [22], [26]. While it’s possible to generate endless novel face images with recent self-supervised approaches [27], corresponding labels for supervised learning are not available. As a result, previous work has resorted to synthesizing facial training data with simplifications, with results that are far from realistic. We have seen progress in efforts that attempt to cross the domain gap using domain adaptation [60] by refining synthetic images to look more real, and domain-adversarial training [13] where machine learning models are encouraged to ignore differences between the synthetic and real domains, but less work has attempted to improve the quality of synthetic data itself. Synthesizing realistic face data has been considered so hard that we encounter the assumption that synthetic data cannot fully replace real data for problems in the wild [60]. Figure 1. 
We render training images of faces with unprecedented realism and diversity. The first example above is shown along with 3D geometry and accompanying labels for machine learning. 
In this paper we demonstrate that the opportunities for synthetic data are much wider than previously realised, and are achievable today. We present a new method of acquiring training data for faces – rendering 3D face models with an unprecedented level of realism and diversity (see Figure 1). With a sufficiently good synthetic framework, it is possible to create training data that can be used to solve real world problems in the wild, without using any real data at all. Figure 2. 
We procedurally construct synthetic faces that are realistic and expressive. Starting with our template face, we randomize the identity, choose a random expression, apply a random texture, attach random hair and clothing, and render the face in a random environment. 
It requires considerable expertise and investment to develop a synthetics framework with minimal domain gap. However, once implemented, it becomes possible to generate a wide variety of training data with minimal incremental effort. Let’s consider some examples; say you have spent time labelling face images with landmarks. However, you suddenly require additional landmarks in each image. Re-labelling and verifying will take a long time, but with synthetics, you can regenerate clean and consistent labels at a moment’s notice. Or, say you are developing computer vision algorithms for a new camera, e.g. an infrared face-recognition camera in a mobile phone. Few, if any, hardware prototypes may exist, making it hard to collect a dataset. Synthetics lets you render faces from a simulated device to develop algorithms and even guide hardware design itself. We synthesize face images by procedurally combining a parametric face model with a large library of high-quality artist-created assets, including textures, hair, and clothing (see Figure 2). With this data we train models for common face-related tasks: face parsing and landmark localization. Our experiments show that models trained with a single generic synthetic dataset can be just as accurate as those trained with task-specific real datasets, achieving results in line with the state of the art. This opens the door to other face-related tasks that can be confidently addressed with synthetic data instead of real. Our contributions are as follows. First, we describe how to synthesize realistic and diverse training data for face analysis in the wild, achieving results in line with the state of the art. Second, we present ablation studies that validate the steps taken to achieve photorealism. Third is the synthetic dataset itself, which is available from our project webpage: https://microsoft.github.io/FaceSynthetics. 

SECTION 2. Related work: Diverse face datasets are very difficult to collect and annotate. Collection techniques such as web crawling pose significant privacy and copyright concerns. Manual annotation is error-prone and can often result in inconsistent labels. Hence, the research community is increasingly looking at augmenting or replacing real data with synthetic. 2.1. Synthetic face data: The computer vision community has used synthetic data for many tasks, including object recognition [23], [44], [51], [73], scene understanding [12], [25], [47], [50], eye tracking [63], [68], hand tracking [40], [61], and full-body analysis [41], [59], [65]. However, relatively little previous work has attempted to generate full-face synthetics using computer graphics, due to the complexity of modeling the human head. A common approach is to use a 3D Morphable Model (3DMM) [5], since these can provide consistent labels for different faces. Previous work has focused on parts of the face such as the eye region [62] or the hockey mask [45], [76]. Zeng et al. [76], Richardson et al. [46], and Sela et al. [58] used 3DMMs to render training data for reconstructing detailed facial geometry. Similarly, Wood et al. [69] rendered an eye region 3DMM for gaze estimation. However, since these approaches only render part of the face, the resulting data has limited use for tasks that consider the whole face. Building parametric models is challenging, so an alternative is to render 3D scans directly [4], [55], [62], [68]. Jeni et al. [24] rendered the BU-4DFE dataset [74] for dense 3D face alignment, and Kuhnke and Ostermann [30] rendered commercially-available 3D head scans for head pose estimation. While often realistic, these approaches are limited by the diversity expressed in the scans themselves, and cannot provide rich semantic labels for machine learning. Manipulating 2D images can be an alternative to using a 3D graphics pipeline. Zhu et al. [79] fit a 3DMM to face images, and warped them to augment the head pose. Nojavanasghari et al. [42] composited hand images onto faces to improve face detection. These approaches can only make minor adjustments to existing images, limiting their use. 2.2. Training with synthetic data: Although it is common to rely on synthetic data alone for full-body tasks [54], [59], synthetic data is rarely used on its own for face-related machine learning. Instead it is either first adapted to make it look more like some target domain, or used alongside real data for pre-training [76] or regularizing models [16], [29]. The reason for this is the domain gap – a difference in distributions between real and synthetic data which makes generalization difficult [25]. Learned domain adaptation modifies synthetic images to better match the appearance of real images. Shrivastava et al. [60] use an adversarial refiner network to adapt synthetic eye images with regularization to preserve annotations. Similarly, Bak et al. [3] adapt synthetic data using a CycleGAN [77] with a regularization term for preserving identities. A limitation of learned domain adaptation is the tendency for image semantics to change during adaptation [15], hence the need for regularization [3], [40], [60]. These techniques are therefore unsuitable for fine-grained annotations, such as per-pixel labels or precise landmark coordinates. Instead of adapting data, it is possible to learn features that are resistant to the differences between domains [13], [57]. Wu et al. [71] mix real and synthetic data through a domain classifier to learn domain-invariant features for text detection, and Saleh et al. [56] exploit the observation that shape is less affected by the domain gap than appearance for scene semantic segmentation. In our work, we do not perform any of these techniques and instead minimize the domain gap at the source, by generating highly realistic synthetic data. 

SECTION 3. Synthesizing face images: The Visual Effects (VFX) industry has developed many techniques for convincing audiences that 3D faces are real, and we build upon these in our approach. However, a key difference is scale: while VFX might be used for a handful of actors, we require diverse training data of thousands of synthetic individuals. To address this, we use procedural generation to randomly create and render novel 3D faces without any manual intervention. We start by sampling a generative 3D face model that captures the diversity of the human population. We then randomly ‘dress up’ each face with samples from large collections of hair, clothing, and accessory assets. All collections are sampled independently to create synthetic individuals who are as diverse as possible from one another. This section describes the technical components we built in order to enable asset collections that can be mixed-and-matched atop 3D faces in a random, yet plausible manner. 3.1. 3D face model: Our generative 3D face model captures how face shape varies across the human population, and changes during facial expressions. It is a blendshape-based face rig similar to previous work [17], [34], and comprises a mesh of N =7, 667 vertices and 7, 414 polygons, and a minimal skeleton of K =4 joints: the head, neck, and two eyes. Figure 3. 
3D faces sampled from our generative model, demonstrating how our model captures the diversity of the human population. 
Figure 4. 
Histograms of self-reported age and ethnicity in our scan collection, which was used to build our face model and texture library. Our collection covers a range of age and ethnicity. 
The face mesh vertex positions are defined by mesh generating function M(β⃗ ,ψ⃗ ,θ⃗ ):R∣∣β⃗ ∣∣×∣∣ψ⃗ ∣∣×∣∣θ⃗ ∣∣→RN×3 which takes parameters β⃗ ∈R∣∣β⃗ ∣∣ for identity, ψ⃗ ∈R∣∣ψ⃗ ∣∣ for expression, and θ⃗ ∈RK×3 for skeletal pose. The pose parameters θ⃗  are per-joint local rotations represented as Euler angles. M is defined as
M(β⃗ ,ψ⃗ ,θ⃗ )=L(T(β⃗ ,ψ⃗ ),θ⃗ ,J(β⃗ );W)View Source\begin{equation*}\mathcal{M}(\vec \beta ,\vec \psi ,\vec \theta ) = \mathcal{L}(\mathcal{T}(\vec \beta ,\vec \psi ),\vec \theta ,\mathcal{J}(\vec \beta );{\mathbf{W}})\end{equation*}
where L(X,θ⃗ ,J;W) is a standard linear blend skinning (LBS) function [33] that rotates vertex positions X∈RN×3 about joint locations J∈RK×3 by local joint rotations θ⃗ , with per-vertex weights W∈RK×N determining how rotations are interpolated across the mesh. T(β⃗ ,ψ⃗ ):R∣∣β⃗ ∣∣×∣∣ψ⃗ ∣∣→RN×3 constructs a face mesh in the bind pose by adding displacements to the template mesh T¯¯¯¯∈RN×3, which represents the average face with neutral expression:
T(β⃗ ,ψ⃗ )jk=T¯jk+βiSijk+ψiEijkView Source\begin{equation*}\mathcal{T}(\vec \beta ,\vec \psi )_k^j = \bar T_k^j + {\beta _i}S_k^{ij} + {\psi _i}E_k^{ij}\end{equation*}
given linear identity basis S∈R∣∣β⃗ ∣∣×N×3 and expression basis E∈R∣∣ψ⃗ ∣∣×N×3. Note the use of Einstein summation notation in this definition and below. Finally, J(β⃗ ):R∣∣β⃗ ∣∣→RK×3 moves the template joint locations J¯¯¯∈RK×3 to account for changes in identity:
J(β⃗ )jk=J¯jk+WjlβiSilk.View Source\begin{equation*}\mathcal{J}(\vec \beta )_k^j = \bar J_k^j + W_l^j{\beta _i}S_k^{il}.\end{equation*} We learn the identity basis S from high quality 3D scans of M =511 individuals with neutral expression. Each scan was cleaned (see Figure 5), and registered to the topology of T¯¯¯¯ using commercial software [52], resulting in training dataset V∈RM×3N. We then jointly fit identity basis S and parameters [β⃗ 1,…,β⃗ M] to V. In order to generate novel face shapes, we fit a multivariate normal distribution to the fitted identity parameters, and sample from it (see Figure 3). As is common in computer animation, both expression basis E and skinning weights W were authored by an artist, and are kept fixed while learning S. Figure 5. 
We manually "clean" raw high-resolution 3D head scans to remove noise and hair. We use the resulting clean scans to build our generative geometry model and texture library. 
Figure 6. 
Examples from our data-driven expression library and manually animated sequence, visualized on our template face. 
3.2. Expression: We apply random expressions to each face so that our downstream machine learning models are robust to facial motion. We use two sources of facial expression. Our primary source is a library of 27,000 expression parameters {ψ⃗ i} built by fitting a 3D face model to a corpus of 2D images with annotated face landmarks. However, since the annotated landmarks are sparse, it is not possible to recover all types of expression from these landmarks alone, e.g. cheek puffs. Therefore, we additionally sample expressions from a manually animated sequence that was designed to fill the gaps in our expression library by exercising the face in realistic, but extreme ways. Figure 6 shows samples from our expression collection. In addition to facial expression, we layer random eye gaze directions on top of sampled expressions, and use procedural logic to pose the eyelids accordingly. 3.3. Texture: Synthetic faces should look realistic even when viewed at extremely close range, for example by an eye-tracking camera in a head-mounted device. To achieve this, we collected 200 sets of high resolution (8192×8192 px) textures from our cleaned face scans. For each scan, we extract one albedo texture for skin color, and two displacement maps (see Figure 7). The coarse displacement map encodes scan geometry that is not captured by the sparse nature of our vertex-level identity model. The meso-displacement map approximates skin-pore level detail and is built by high-pass filtering the albedo texture, assuming that dark pixels correspond to slightly recessed parts of the skin. Figure 7. 
We apply coarse and meso-displacement to our 3D face model to ensure faces look realistic even when viewed close-up. 
Figure 8. 
Our hair library contains a diverse range of scalp hair, eyebrows, and beards. When assembling a 3D face, we choose hair style and appearance at random. 
Unlike previous work [45], [76], we do not build a generative model of texture, as such models struggle to faithfully produce high-frequency details like wrinkles and pores. Instead, we simply pick a corresponding set of albedo and displacement textures from each scan. The textures are combined in a physically-based skin material featuring subsurface scattering [9]. Finally, we optionally apply makeup effects to simulate eyeshadow, eyeliner and mascara. 3.4. Hair: In contrast to other work which approximates hair with textures or coarse geometry [17], [55], we represent hair as individual 3D strands, with a full head of hair comprising over 100,000 strands. Modelling hair at the strand level allows us to capture realistic multi-path illumination effects. Shown in Figure 8, our hair library includes 512 scalp hair styles, 162 eyebrows, 142 beards, and 42 sets of eyelashes. Each asset was authored by a groom artist who specializes in creating digital hair. At render time, we randomly combine scalp, eyebrow, beard, and eyelash grooms. We use a physically-based procedural hair shader to accurately model the complex material properties of hair [8]. This shader allows us to control the color of the hair with parameters for melanin [38] and grayness, and even lets us dye or bleach the hair for less common hair styles. Figure 9. 
Each face is dressed in a random outfit assembled from our digital wardrobe – a collection of diverse 3D clothing and accessory assets that can be fit around our 3D head model. 
Figure 10. 
We use HDRIs to illuminate the face. The same face can look very different under different illumination. 
3.5. Clothing: Images of faces often include what someone is wearing, so we dress our faces in 3D clothing. Our digital wardrobe contains 30 upper-body outfits which were manually created using clothing design and simulation software [10]. As shown in Figure 9, these outfits include formal, casual, and athletic clothing. In addition to upper-body garments, we dress our faces in headwear (36 items), facewear (7 items) and eyewear (11 items) including helmets, head scarves, face masks, and eyeglasses. All clothing items were authored on an unclothed body mesh with either the average male or female body proportions [37] in a relaxed stance. We deform garments with a non-rigid cage-based deformation technique [2] so they fit snugly around different shaped faces. Eyeglasses are rigged with a skeleton, and posed using inverse kinematics so the temples and nose-bridge rest on the corresponding parts of the face. 3.6. Rendering: We render face images with Cycles, a photorealistic ray-tracing renderer [6]. We randomly position a camera around the head, and point it towards the face. The focal length and depth of field are varied to simulate different cameras and lenses. We employ image-based lighting [11] with high dynamic range images (HDRI) to illuminate the face and provide a background (see Figure 10). For each image, we randomly pick from a collection of 448 HDRIs that include a range of different environments [75]. See Figure 11 for examples of faces rendered with our framework. Figure 11. 
Examples of synthetic faces that we randomly generated and rendered for use as training data. 
Figure 12. 
We also synthesize labels for machine learning. Above are additional label types beyond those shown in Figure 1. 
In addition to rendering color images, we generate ground truth labels (see Figure 12). While our experiments in section 4 focus on landmark and segmentation annotations, synthetics lets us easily create a variety of rich and accurate labels that enable new face-related tasks (see subsection 4.5). 

SECTION 4. Face analysis: We evaluate our synthetic data on two common face analysis tasks: face parsing and landmark localization. We show that models trained on our synthetic data demonstrate competitive performance to the state of the art. Note that all evaluations using our models are cross-dataset – we train purely on synthetic data and test on real data, while the state of the art evaluates within-dataset, allowing the models to learn potential biases in the data. 4.1. Training methodology: We render a single training dataset for both landmark localization and face parsing, comprising 100,000 images at 512×512 resolution. It took 48 hours to render using 150 NVIDIA M60 GPUs. During training, we perform data augmentation including rotations, perspective warps, blurs, modulations to brightness and contrast, addition of noise, and conversion to grayscale. Such augmentations are especially important for synthetic images which are otherwise free of imperfection (see subsection 4.4). While some of these could be done at render time, we perform them at training time in order to randomly apply different augmentations to the same training image. We implemented neural networks with PyTorch [43], and trained them with the Adam optimizer [28]. Figure 13. 
We train a face parsing network (using synthetic data only) followed by a label adaptation network to address systematic differences between synthetic and human-annotated labels. 
4.2. Face parsing: Face parsing assigns a class label to each pixel in an image, e.g. skin, eyes, mouth, or nose. We evaluate our synthetic training data on two face parsing datasets: Helen [32] is the best-known benchmark in the literature. It contains 2,000 training images, 230 validation images, and 100 testing images, each with 11 classes. Due to labelling errors in the original dataset, we use Helen* [35], a popular rectified version of the dataset which features corrected training labels, but leaves testing labels unmodified for a fair comparison. LaPa [36] is a recently-released dataset which uses the same labels as Helen, but has more images, and exhibits more challenging expressions, poses, and occlusions. It contains 18,176 training images, 2,000 validation images and 2,000 testing images. As is common [35], [36], we use the provided 2D landmarks to align faces before processing. We scale and crop each image so the landmarks are centered in a 512 × 512px region of interest. Following prediction, we undo this transform to compute results against the original label annotation, without any resizing or cropping. Method We treat face parsing as image-to-image translation. Given an input color image x containing C classes, we wish to predict a C-channel label image y^ of the same spatial dimensions that matches the ground truth label image y. Pixels in y are one-hot encoded with the index of the true class. For this, we use a UNet [49] with ResNet-18 encoder [21], [72]. We train this network with synthetic data only, minimizing a binary cross-entropy (BCE) loss between predicted and ground truth label images. Note that there is nothing novel about our choice of architecture or loss function, this is a well-understood approach for this task. Label adaptation. There are bound to be minor systematic differences between synthetic labels and human-annotated labels. For example, where exactly is the boundary between the nose and the rest of the face? To evaluate our synthetic data without needing to carefully tweak our synthetic label generation process for a specific real dataset, we use label adaptation. Label adaptation transforms labels predicted by our face parsing network (trained with synthetic data alone) into labels that are closer to the distribution in the real dataset (see Figure 13). We treat label adaptation as another image-to-image translation task, and use a UNet with ResNet18 encoder [72]. To ensure this stage is not able to ‘cheat’, it is trained only on pairs of predicted labels y^ and ground truth labels y. It is trained entirely separately from the face parsing network, and never sees any real images. Figure 14. 
Face parsing results by networks trained with synthetic data (with and without label adaptation) and real data. Label adaptation addresses systematic differences between synthetic and real labels, e.g. the shape of the nose class, or granularity of hair. 
Results See Tables 1 and 2 for comparisons against the state of the art, and Figure 14 for some example predictions. Although networks trained with our generic synthetic data do not outperform the state of the art, it is notable that they achieve similar results to previous work trained within-dataset on task-specific data. Comparison to real data. We also trained a network on the training portion of each real dataset to separate our training methodology from our synthetic data, presented as "Ours (real)" in Tables 1 and 2. It can be seen that training with synthetic data alone produces comparable results to training with real data. 4.3. Landmark localization: Landmark localization finds the position of facial points of interest in 2D. We evaluate our approach on the 300W [53] dataset, which is split into common (554 images), challenging (135 images) and private (600 images) subsets. Method We train a ResNet34 [21] with mean squared error loss to directly predict 68 2D landmark coordinates per-image. We use the provided bounding boxes to extract a 256×256 pixel region-of-interest from each image. The private set has no bounding boxes, so we use a tight crop around landmarks. Table 1. 
A comparison with the state of the art on the Helen dataset, using F1 score. As is common, scores for hair and other fine-grained categories are omitted to aid comparison to previous work. The overall score is computed by merging the nose, brows, eyes, and mouth categories. Training with our synthetic data achieves results in line with the state of the art, trained with real data.
Table 2. 
A comparison with the state of the art on LaPa, using F1 score. For eyes and brows, L and R are left and right. For lips, U, I, and L are upper, inner, and lower. Training with our synthetic data achieves results in line with the state of the art, trained with real data.
Figure 15. 
Predictions before (top) and after (bottom) label adaptation. The main difference is changing the jawline from a 3D-to-2D projection to instead follow the facial outline in the image. 
Figure 16. 
Predictions by networks trained with real (top) and synthetic data (bottom). Note how the synthetic data network generalizes better across expression, illumination, pose, and occlusion. 
Label adaptation is performed using a two-layer perceptron to address systematic differences between synthetic and real landmark labels (Figure 15). This network is never exposed to any real images during training. Results As evaluation metrics we use: Normalized Mean Error (NME) [53] – normalized by inter-ocular outer eye distance; and Failure Rate below a 10% error threshold (FR10%). See Table 3 for comparisons against state of the art on 300W dataset. It is clear that the network trained with our synthetic data can detect landmarks with accuracy comparable to recent methods trained with real data. Table 3. 
Landmark localization results on the common, challenging, and private subsets of 300W. Lower is better in all cases. Note that 0.5 FR rate translates to 3 images, while 0.17 corresponds to 1.
Comparison to real data We apply our training methodology (including data augmentations and label adaptation) to the the training and validation portions of the 300W dataset, to more directly compare real and synthetic data. Table 3 clearly shows that training with synthetic data leads to better results, even when comparing to a model trained on real data and evaluated within-dataset. 4.4. Ablation studies: We investigate the effect of synthetic dataset size on landmark accuracy. Figure 17 shows that landmark localization improves as we increase the number of training images, before starting to plateau at 100,000 images. Figure 17. 
Landmark localization accuracy improves as we use more and more synthetic training data. 
Figure 18. 
It is easy to generate synthetic training data for eye tracking (left) which generalizes well to real-world images (right). 
We study the importance of data augmentation when training models on synthetic data. We train models with: 1) no augmentation; 2) appearance augmentation only (e.g. colour shifts, brightness and contrast); 3) full augmentation, varying both appearance and geometry (e.g. rotation and warping). Table 3 shows the importance of augmentation, without which synthetic data does not outperform real. Table 3 also shows the importance of label adaptation when evaluating models trained on synthetic data – using label adaptation to improve label consistency reduces error. Adding label adaptation to a model trained on real data results in little change in performance, showing that it does not benefit already-consistent within-dataset labels. If we remove clothing and hair, landmark accuracy suffers (Table 3). This verifies the importance of our hair library and digital wardrobe, which improve the realism of our data. Additional ablation studies analyzing the impact of render quality, and variation in pose, expression, and identity can be found in the supplementary material. 4.5. Other examples: In addition to the quantitative results above, this section qualitatively demonstrates how we can solve additional problems using our synthetic face framework. Eye tracking can be a key feature for virtual or augmented reality devices, but real training data can be difficult to acquire [14]. Since our faces look realistic close-up, it is easy for us to set up a synthetic eye tracking camera and render diverse training images, along with ground truth. Figure 18 shows example synthetic training data for such a camera, along with results for semantic segmentation. Dense landmarks. In subsection 4.3, we presented results for localizing 68 facial landmarks. What if we wanted to predict ten times as many landmarks? It would be impossible for a human to annotate this many landmarks consistently and correctly. However, our approach lets us easily generate accurate dense landmark labels. Figure 19 shows the results of modifying our landmark network to regress 679 coordinates instead of 68, and training it with synthetic data. Figure 19. 
With synthetic data, we can easily train models that accurately predict ten times as many landmarks as usual. Here are some example dense landmark predictions on the 300W dataset. 
4.6. Discussion: We have shown that it is possible to achieve results comparable with the state of the art for two well-trodden tasks: face parsing and landmark localization, without using a single real image during training. This is important since it opens the door to many other face-related tasks that can be addressed using synthetic data in the place of real data. Limitations remain. As our parametric face model includes the head and neck only, we cannot simulate clothing with low necklines. We do not include expression-dependent wrinkling effects, so realism suffers during certain expressions. Since we sample parts of our model independently, we sometimes get unusual (but not impossible) combinations, such as feminine faces that have a beard. We plan to address these limitations with future work. Photorealistic rendering is computationally expensive, so we must consider the environmental cost. In order to generate the dataset used in this paper, our GPU cluster used approximately 3,000kWh of electricity, equivalent to roughly 1.37 metric tonnes of CO2, 100% of which was offset by our cloud computing provider. This impact is mitigated by the ongoing progress of cloud computing providers to become carbon negative and use renewable energy sources [1], [18], [39]. There is also the financial cost to consider. Assuming 1perhourforanM60GPU(averagepriceacrosscloudproviders),itwouldcost7,200 to render 100,000 images. Though this seems expensive, real data collection costs can run much higher, especially if we take annotation into consideration. 
ACKNOWLEDGEMENTS: We thank Pedro Urbina, Jon Hanzelka, Rodney Brunet, and Panagiotis Giannakopoulos for their artistic contributions; and Virginia Estellers and Matthew Johnson for contributions to the face model.