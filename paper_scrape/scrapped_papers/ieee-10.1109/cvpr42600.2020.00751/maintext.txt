Figure 1: Daytime translation results. Left - original images, right - translated and enhanced images (one style per column). 


SECTION 1. Introduction: In this work, we consider the task of generating daytime timelapse videos and pose it as an image-to-image translation problem. Recent image-to-image translation methods have successfully handled the task of conversion between two predefined paired domains [8], [30], [16], [7] as well as between multiple domains [2], [14], [13], [17]. Given the success of these methods, using image-to-image translation methods to generate daytime changes is a natural idea. Image-to-image translation approaches require domain labels at training as well as at inference time. The recent FUNIT model [17] relaxes this constraint partially. Thus, to extract the style at inference time, it uses several images from the target domain as guidance for translation (known as the few-shot setting). The domain annotations are however still needed during training. In our task, domains correspond to different times of the day and different lighting, and therefore domain labels are hard to define and hard to solicit from users. Furthermore, while timelapse videos might have provided us with weakly supervised data, we have found that collecting high-resolution diverse daytime timelapse videos is hard. Therefore, in our work, we aim to develop an image-to-image translation problem suitable for the setting when domain labels are unavailable. Thus, as our first contribution, we show how to train a multi-domain image-to-image translation model on a large dataset of unaligned images without domain labels. We demonstrate that the internal bias of the collected dataset, the inductive bias caused by the network architecture, and a specially developed training procedure make it possible to learn style transformations even in this setting. The only external (weak) supervision used by our approach are coarse segmentation maps estimated using an off-the-shelf semantic segmentation network. As the second contribution, to ensure fine detail preservation, we propose an architecture for image-to-image translation that combines the two well-known ideas: skip connections [22] and adaptive instance normalizations (AdaIN) [6]. We show that such a combination is feasible and leads to an architecture that preserves details much better than currently dominant AdaIN architectures without skip connections. We evaluate our system against several state-of-the-art baselines through objective measures as well as a user study. While our main focus is the task of photorealistic daytime alteration for landscape images, we also show that such architecture system can be used to handle other multi-domain image stylization/recoloring tasks. Finally, as the third contribution, we address the task of image-to-image translation at high resolution. In our case, as well as in many other settings, training a high-capacity image-to-image translation network directly at high resolution is computationally infeasible. We therefore propose a new enhancement scheme that allows to apply the image-to-image translation network trained at medium resolution for high-resolution images. The rest of the paper is organized as follows. Section 2 reviews related work. The main Section 3 presents the High-Resolution Daytime Translation (HiDT) model and the resolution-increasing enhancement model. Section 4 presents the results of a comprehensive experimental study, and Section 5 concludes the paper. Representative time-lapse videos generated by our system are provided at the project webpage. 

SECTION 2. Related Work: Unpaired Image-To-Image Translation: The task of image translation aims to transfer images from one domain to another (e.g. from summer to winter) or add/remove some image attributes (e.g. adding eyeglasses to a portrait). Many image translation models exploit generative adversarial networks (GAN) with conditional generators to inject information about the target attribute or domain [2]. Others [7], [13] split input images into content and style representations and subsequently edit the style to obtain the desired effect. In both cases, most works target the two-domain setting [30] or a setting with several discrete domains [2]. More closely related to our work, several recent approaches [7], [13], [11] split input images into content and style representations and subsequently edit the style to obtain the desired effect. The most common choice for generators uses adaptive instance normalization (AdaIN) in the encoder-decoder architecture [6]. Providing explicit domain labels is still mandatory for most multi-domain algorithms. The recently proposed FUNIT model [17] is designed for the case when those labels are used only by the conditional discriminator, while the generator is extracting some style code from given samples in the target domain. In this work, we take the next logical step in the evolution of GAN-based style transfer and do not use domain labels at all. Timelapse Generation: The generation of timelapses has attracted some attention from researchers, but most previous approaches use a dataset of timelapse videos for training. In particular, the work [24] used a bank of timelapse videos to find the scene most similar to a given image and then exploited the retrieved video as guidance for editing. Following them, the work [12] used a database of labeled images to create a library of transformations and apply them to image regions similar to input segments. Both methods rely on global affine transforms in the color space, which are often insufficient to model daytime appearance changes. Unlike them, a recent paper [20] has introduced a neural generation approach. The authors leveraged two timelapses datasets: one with timestamp labels and another without them, both of different image quality and resolution. Finally, a very recent and parallel research [3] uses a dataset of diverse videos to solve the daytime appearance change modeling problems. Note that the method [3] also considers the problem of modeling short-term changes and rapid object motion, which we do not tackle in our pipeline. Our approach is different from all previous works for timelapse generation, as it needs neither timestamps nor spatial alignment (such as, e.g. timelapse frames). High-Resolution Translation: Modern generative models are often hard to scale to high-resolution input images due to memory constraints; most models are trained on either cropped parts or downscaled versions of images. Therefore, to generate a plausible image in high resolution one needs an additional enhancement step to upscale the translation output and remove artifacts. Such enhancement is closely related to the superresolution problem. The work [15] compared photorealistic smoothing and image-guided filtering [4], and noted that the latter slightly degraded the performance as compared to the former, but led to a significant performance gain. Another way, proposed in [20], is to apply a different kind of guided upsampling via local color transfer [5]. However, unlike image-guided filtering, this method does not have a closed-form solution and requires an optimization procedure at inference time. In [3], the model predicts the parameters of a pixel-wise affine transformation of the downscaled image and then applies bilinear upsampling with these parameters to the full-resolution image. Unfortunately, both approaches often produce halo-type artifacts near image edges.
Figure 2: Diagram of the adaptive U-net architecture: an encoder-decoder network with dense skip-connections and content-style decomposition (c, s). 
The work most similar to ours in this regard, the pix2pixHD model [8], developed a separate refinement network. Our enhancement model is similar to their approach, as we also use the refinement procedure as a postprocessing step. But instead of training on the features, we use the output of low-resolution translation directly in a way inspired by classical multi-frame superresolution approaches [27]. 

SECTION 3. Methods: The main part of HiDT is an encoder-decoder architecture. The encoder performs decomposition into style (vector) and content (tensor). The decoder is then able to generate a new image x^ by taking content from the content input image x and style from the style input image x′. The two components (the content and the style) are combined together using the AdaIN connection [6], [17]. The overall architecture has the following structure: the content encoder Ec maps the initial image to a 3D tensor c using several convolutional downsampling layers and residual blocks. The style encoder Es is a fully convolutional network that ends with global pooling and a compressing 1 × 1 convolutional layer. The generator G processes c with several residual blocks with AdaIN modules inside and then upsamples it.
Figure 3: Hidt learning data flow. We show half of the (symmetric) architecture; s′=Es(x′) is the style extracted from the other image x’, and ŝ’ is obtained similarly to ŝ with x and x′ swapped. Light blue nodes denote data elements; light green, loss functions; others, functions (subnetworks). Functions with identical labels have shared weights. Adversarial losses are omitted for clarity. 
To create a plausible daytime landscape image, the model should preserve fine details from the original image. To satisfy this requirement, we enhance the encoder-decoder architecture with skip connections between the downsampling part of tje encoder Ecand the upsampling part of the generator G. Regular skip connections would also “leak” the style of the initial input into the output. Therefore, we introduce an additional convolutional block with AdaIN [6] and apply it to the skip connections (see Fig. 2). 3.1. Learning: Overall, the architecture is trained using a reconstruction loss as well as a number of additional losses (Fig. 3). During training, the decoder predicts not only the input image x but also its semantic segmentation mask m (produced by a pre-trained network [26]). While we do not aim to achieve state-of-the-art segmentation as a by-product, having the segmentation loss helps to control the style transfer and to preserve the semantic layout. Importantly, segmentation masks are not given as input to the networks, and are thus not needed at inference time. NotationDenote the space of input images by X, their segmentation masks by M, and individual images with segmentation masks by x,m∈X×M. Denote the space of latent content codes c is c∈C, and the space of latent style codes s is s∈S (as we will see below, S=R3 while C has a more complex structure). To extract c and s from an image x, HiDT employs two encoders: Ec:X→C extracts the content representation c of the input image x, and Es:X→S extracts the style representation s of the input image x. Given a content code c∈C and a style code s∈S, the decoder (generator) G:C×S→X×M produces a new image x^ and the corresponding segmentation mask m^. In particular, one can combine content from x and style from a different image x′ as (x^,m^)=G(Ec(x),Es(x′)). We call the result of the combination the translated image (and the translated mask). Also, during training we consider random style codes sr sampled from a prior distribution p∗ on S. Then we get a random style image (and a random style mask) by applying the decoder to the content code c and the random style sr respectively. During learning for each batch, we take the reconstructed images/masks, the translated images/masks (where the images are paired, and the styles are swapped) and the random style images/masks. Image Reconstruction LossThe image reconstruction loss Lrec is defined as the L1 -norm of the difference between original and reconstructed images. It is applied in three different ways: (1) to the reconstruction x~ of the original image x,Lrec=∥x~−x∥1, (2) to the reconstruction x~r of the random style image xr,Lrrec=∥x~r−xr∥1, and (3) to the reconstruction x^~ of the image x obtained from the content of the stylized image x^ and the style of the stylized image x^′ (cross cycle consistency): Lcyc=∥x^−x∥1, where x^~=G(c^,s^′) (see Fig. 3). Segmentation LossThe segmentation loss Lseg is used together with the image reconstruction loss and is defined as the cross entropy CE(m,m^)=−∑(i,j)mi,jlogm^i,j between the original m and reconstructed m^ segmentation masks. It is applied in two ways: first, to the translated mask m^,Lseg=CE(m,m^), and then to the random style mask mr:Lrseg=CE(m, mr). Adversarial LossWe use two discriminators, namely, the unconditional discriminator and the conditional discriminator, where the style vector is used as conditioning. Both discriminators consider translated and random style images as fakes. Both discriminators are trained with the least squares GAN approach [18]. We utilize the projection conditioning scheme [19] and detach the styles from the computational graph when feeding them to the conditional discriminator during the generator update step. Latent Reconstruction LossesWe enforce cycle consistency with respect to the style and the content codes. We pass the translated and the random style images into the encoders, and compute the losses between the resulting style (content) and the style (content) that the respective translated or the random style image was obtained from. We apply the L1 loss to content codes as well as to the style codes. Style Distribution LossTo enforce the structure of the space of extracted style codes, the style distribution loss inspired by the CORAL approach [25], is applied to a pool of styles collected from a number of previous training iterations. Namely, for a given pool size T we collect the styles {s(1),…,s(T)} from past minibatches with the stop gradient operation applied. We then add styles s and s′ (which are part of the current computational graph) to this pool, and calculate the mean vector μ^s and covariance matrix Σ^s using the updated pool. Then the style distribution loss matches empirical moments of the resulting distribution to the moments of the prior distribution N(0,I): Ldist=∥μ^T∥1+∥Σ∧T−I∥1+∥diag(Σ^T)−1∥1. Since the space S=R3 is low-dimensional, and our target is the unit normal distribution N(0,I), this simplified approach suffices to enforce the structure in the space of latent codes. After computing the loss value, the oldest styles are removed from the pool to keep its size at T. Total Loss FunctionThus, overall HiDT is jointly training the style encoder, content encoder, generator, and discriminator with the following objective:minEc,Es,GmaxDL(Ec,Es,G,D)=λ1(Ladv+Lradv)++λ2(Lrec+Lrrec+Lcyc)+λ3(Lseg+Lrseg)++λ4(Lc+Lrc)+λ5Ls+λ6Lrs+λ7Ldist.View Source\begin{align*}
& \min_{E_{c},E_{s},G} \max_{D} \mathcal{L}(E_{c}, E_{s}, G, D)=\lambda_{1}(\mathcal{L}_{\mathrm{adv}} +\mathcal{L}_{\mathrm{adv}} ^{r})+\\
& \quad +\lambda_{2}(\mathcal{L}_{\mathrm{rec}}+\mathcal{L}_{\mathrm{rec}}^{r}+\mathcal{L}_{\mathrm{cyc}})+\lambda_{3}(\mathcal{L}_{\mathrm{seg}}+\mathcal{L}_{\mathrm{seg}}^{r})+\\
& \qquad \qquad +\lambda_{4}(\mathcal{L}_{\mathrm{c}}+\mathcal{L}_{\mathrm{c}}^{r})+\lambda_{5}\mathcal{L}_{\mathbf{s}}+\lambda_{6}\mathcal{L}_{\mathrm{s}}^{r}+\lambda_{7}\mathcal{L}_{\mathrm{dist}}.
\end{align*} Hyperparameters λ1,…,λ7 define the relative importance of the components in the overall loss function; they have been chosen by hand and will be shown below. During our experiments, we have observed that the projection discriminator significantly improves the results, while removing the segmentation loss function sometimes leads to undesirable hallucinations in the generator (see Fig. 5 for an example). However, the model is still well trained without segmentation loss function and gets a comparable user preference score. We provide a further ablation study in the supplementary material. 3.2. Enhancement Postprocessing: Training image-to-image translation on high resolution images is infeasible due to both memory and computation time constraints. In principle, our architecture can be trained at medium resolution and applied to high resolution images in a fully convolutional way. Alternatively, guided filtering [4] can be used to upsample results of processing at medium resolution. Although both of these techniques show good results in most cases, they have limitations. A fully convolutional application might yield scene corruption due to limited receptive field, which is the case with sunsets where multiple suns might be drawn, or water reflections where the border between sky and water surface might be confused. Guided filtering, on the other hand, works great with water or sun but fails if small details like twigs were changed by the style transfer procedure. It also often generates halo artefacts near the horizon and other high-contrast borders. Finally, we have found that a superresolution architecture [29] does not generalize well even to well-looking translated images, effectively amplifying translation artefacts.
Figure 4: Enhancement scheme: the input is split into subimages (color-coded) that are translated individually by hidt at medium resolution. The outputs are then merged using the merging network Genh. For illustration purposes, we show upsampling by a factor of two, but in the experiments we use a factor of four. We also apply bilinear downsampling (with shifts - see text for detail) rather than strided subsampling when decomposing the input into medium resolution images. 
Inspired by existing multiframe image restoration methods [27], we propose to apply translation multiple times at medium resolution and then use a separate merging network Genh to combine the results into a high-resolution translated image. More specifically, we consider a high resolution image xhi (in our experiments, 1024×1024). We then consider sixteen shifted versions of xhi denoted as {x(i)hi}i, each having the same size as xhi and obtained with integer displacement spanning the range [0; 4] in x and y (missing pixels are filled with zeros). The shifted images are then downsampled bilinearly resulting in sixteen medium-resolution images {x(i)med}i, from which the original image xhi can be easily recovered. We then apply HiDT to each of the medium-resolution images separately, getting translated medium-resolution images {x^(i)med}i,x^(i)med=G(Ec(x(i)med), Es(x(i)med)). These frames are stacked into a single tensor in a fixed order and are fed to the merging network Genh that outputs the translated high-resolution image. The process is illustrated in Fig. 4.
Table 1: User preference study of hidt against the baselines. N is the number of styles averaged in the few-shot setting. The user score is the share of users that choose hidt in the pairwise comparison. Our results show that all methods are competitive. The increase of N leads to the better quality of FUNIT-t.
The merging network Genh is trained in a semi-supervised mode on two datasets: paired and unpaired. To obtain a paired dataset, we use HiDT in an “autoencoder mode” (i.e. without changing the style). To obtain each training pair, we take a high-res image, decompose it into sixteen medium-resolution images, and pass them through the HiDT architecture without changing the style. For the unpaired dataset collection we use the same procedure, but the style is being sampled from normal distribution (since we used it as a prior during training). The merging network is thus shown stacks of resulting images and is tasked with restoring the original image. At test time, we can use a new style s', when translating each of the medium-resolution images. The output of the merging network will then correspond to the high-resolution input image xhi translated to the style s′. We note the similarity of our approach to [28], with the difference being that we use several RGB images as input instead of feature maps. During training, we use the same losses as pix2pixHD [28], namely perceptual, feature matching, and adversarial loss functions. We apply only adversarial loss for the unpaired data. 

SECTION 4. Experiments: 4.1. Daytime Translation: Training DetailsIn our experiments, the content encoder has two downsampling and four residual blocks; after each downsampling, only five channels are used for skip connections in order to limit the information flow through them. The style encoder contains four downsampling blocks. The output of the style encoder is a three-channel tensor, which is averaged-pooled into a three- dimensional vector. The decoder has five residual blocks with AdaIN layers and two upsampling blocks. AdaIN parameters are computed from the style vector via three-layer fully-connected network. Both discriminators are multiscale, with three downsampling levels. We trained the translation model for 450 thousand iterations with batch size four on a single NVIDIA Tesla P40. For training, the images were downscaled to the resolution of 256 × 256. The loss weights were set to λ1=5,λ2=2,λ3=3,λ4=1,λ5= 0.1, λ6=4,λ7=1. We used the Adam optimizer [10] with β1=0.5,β2=0.999, and initial learning rate 0.0001 for both generators and discriminators, halving the learning rate every 200000 iterations.
Figure 5: Training without segmentation losses is prone to failures of semantic consistency. Left: original images. Right: transferred images. (a) Our ablated model, trained without auxiliary segmentation task, turns grass into water; (b) FUNIT hallucinates grass on the building. 
Table 2: Performance comparison of three models using a hold-out dataset. FUNIT is not applicable in the random setting. According to the selected metrics, none of the models shows complete superiority over the others.
Dataset and Daytime ClassifierFollowing previous works, we collected a dataset of 20,000 landscape photos from the Internet. A small part of these images were manually labeled into four classes (night, sunset/sunrise, morning/evening, noon) using a crowdsourcing platform. A ResNet-based classifier was trained on those labels and applied to the rest of the dataset. We used predicted labels in two ways: (1) to balance the training set for image translation models with respect to daytime classes; (2) to provide domain labels for baseline models. Segmentation masks were produced by an external state of the art model [26] and reduced to nine classes: sky, grass, ground, mountains, water, buildings, trees, roads, and humans. BaselinesWe used two recent image-to-image translation models as baselines: FUNIT [17] and Multi-domain DRIT ++ [13] (refered to as DRIT for brevity). Both of them use domain labels. We trained the models on our dataset: DRIT with original hyperparameters, and FUNIT with both original (FUNIT-O) and properly tuned (FUNIT-T) hyperparameters. At inference time, FUNIT transfers the original image using styles extracted from other images, while DRIT in addition can transfer to randomly sampled styles. As another weak baseline, we train our model with only the autoencoding loss Lrec (HiDT-AE). The trained HiDT-AE still produces some color shifting when the styles are swapped; the result does not resemble the target daytime well enough, although it preserves the content (details) well. Evaluation MetricsTo compare our model with the baselines, we use several metrics, also commonly employed in previous works. The domain-invariant perceptual distance (DIPD) [7], [17] is the L2 distance between normalized Conv5 features of the original image and its translated version. It is used to measure content preservation. The Inception score (IS) [23] assesses the photorealism of generated images. We use the classifier described above to predict the domain label of the translated image. Styles for the translation may be either sampled from the prior distribution p∗(s) (IS-random) or extracted from other images (IS-swapped). The conditional inception score (CIS) [7] measures the diversity of translation results, which is suitable for our multidomain setting. We calculate CIS for style swapping translation. To estimate the visual plausibility and photorealism of translation results, we use human evaluation with the following protocol. The assessors on a crowd-sourcing platform 1 were shown triplets containing 1. the original image, 2. the image translated with our method, and 3. the image translated using one of the baseline models. We also show assessors the target label (time of day) and ask to choose the image that looks better with respect to both details preserved from the original image and the correct time of day. As both our model and FUNIT support the few-shot setting, styles for translation were obtained by averaging N styles extracted from images with the corresponding labels (N=1,5,10). Assessment time was limited to two minutes per task, and original images were independently collected from the Internet. For each compared pair of methods, we generated 500 triplets, and each triplet was assessed by five different workers. ResultsSample results of our image translation model are shown in the teaser figure on the first page. Fig. 6 shows style swapping between different images, while image translation with styles randomly sampled from the prior distribution is shown in Fig. 7. In these experiments, we applied the truncation trick known for improving the average output quality [1], [9] at inference time. Random styles are sampled with reduced variance, and the styles extracted from other images are interpolated with the style extracted from the original image. One important application of our model is daytime timelapse generation using some video as a guidance; we showcase frames from such a timelapse in Fig. 9.
Figure 6: Swapping styles between two images. Original images are shown on the main diagonal. The examples show that hidt is capable to swap the styles between two real images while preserving details. 
Figure 7: The original content image (top left), transferred to randomly sampled styles from prior distribution. The results demonstrate the diversity of possible outputs. 
A qualitative comparison of our model with baselines is shown in Fig. 8. Results of different models are hard to distinguish, which is supported by our human evaluation study (Table 1). We report user preference of our model over the baselines and evaluate its statistical significance, applying the one-tailed binomial test to the hypothesis “User score equals 0.5” against “User score is less than 0.5”. Due to multiple hypothesis testing, we also apply the Holm-Sidak adjustment and show adjusted p-values. Table 1 suggests that unlabeled training is sufficient for time-of-day translation. Traditional image-to-image translation metrics are summarized in Table 2. Again, all models are basically on par with each other, with different winners according to different metrics.
Figure 8: Comparison with baselines. Columns, left to right: the original image, FUNIT-T, FUNIT-O, DRIT, HIDT (ours). Our model, trained and applied without knowledge about domain labels, has translation quality similar to the models that require such supervision. 
Figure 9: Timelapse generation using styles extracted from a real video. Top: frames from a guidance video. Bottom: timelapse generated from a single image using extracted styles. 
4.2. High-Resolution Translation: Training DetailsFor the merging network, we used the RRBDNet architecture from ESRGAN [29] with five residual blocks for Genh and a multiscale discriminator with three scales and five layers. We used multiplier coefficients of 10 for perceptual and feature matching losses and the unit weight for adversarial loss. We set learning rate of 0.0001 for both the merging network and the discriminator.
Figure 10: Enhancement of our translation network outputs with different methods. Columns, left to right: original image; result of our translation network applied directly to the hi-res input; low-res translation output upsampled with lanczos' method; the result of our enhancement scheme. In this example, direct fully-convolutional application to hi-res turns water into sky with stars, while the enhancement network preserves the semantics of the scene. 
Figure 11: A flower image (left) translated to several randomly sampled styles by hidt trained on oxford flowers dataset. 
BaselinesWe compare the proposed enhancement scheme with the following baselines: (1) fully convolutional application of the translation network to a high-resolution image, (2) Lanczos upsampling. The pix2pixHD [28] enhancement scheme requires supervision for translated images. Therefore, we do not use pix2pixHD as a baseline. ResultsThe resulting downsampled images produced with the enhancement procedure are presented in Fig. 1, and a detailed example is shown in Fig. 10. The latter figure contains image patch produced by different models and shows that our model is more plausible than the result of direct Lanczos upsampling: the rightmost patch contains more details from the original. 4.3. Additional Task: To show the generality of the proposed HiDT approach, we additionally trained the image translation model on the Flowers dataset [21] for 60,000 iterations. Segmentation masks and associated losses were not used in this experiment. The results of translation to random styles (with no enhancement) are presented in Fig. 11. We have also applied HiDT to the WikiArt dataset of paintings (for which we have increased the dimensionality of the style space to 12). The result of style swapping in this case is shown in Fig. 12.
Figure 12: Style swapping for the hidt system trained on a paintings dataset. The main diagonal contains original paintings and off-diagonal entries correspond to translated results. Plausbile translations obtained by hidt in this case, suggests its generality. 


SECTION 5. Conclusion: We have presented an image-to-image translation model that does not rely on domain labels during either training or inference. The new enhancement scheme shows promising results for increasing the resolution of translation outputs. We have shown that our model is able to learn daytime translation for high-resolution landscape images and provided qualitative evidence that our approach can be generalized to other domains. The results show that our method is on par with state of the art baselines that require labels at least at training time. Our model can generate images using styles extracted from images, as well as sampled from the prior distribution. An appealing straightforward application of our model is the generation of timelapses from a single image (the task currently mainly tackled with paired datasets). One direction for further work would be to unite the translation and enhancement networks into a single model trained end-to-end.