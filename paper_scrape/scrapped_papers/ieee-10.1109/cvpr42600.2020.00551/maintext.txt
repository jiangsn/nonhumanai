SECTION 1. Introduction: Semantic image synthesis, namely translating semantic labels to natural images, has many real-world applications and draws much attention from the community. It is innately a one-to-many mapping problem. Countless possible natural images correspond to one single semantic label. Previous works utilized different strategies for the task: adopting the idea of variational auto-encoder [36], [56], [2], [11], introducing noise while training [19], building multiple subnetworks [10] and including instance-level feature embeddings [41], etc. While these methods made exceptional achievements in improving image quality and extending more applications, we take a step further to particularly focus on a specific multi-modal image synthesis task that adds more flexibility to control the generated results. Just imagine a content creation scenario from a human parsing map. With the help of semantics-to-image translation models, the parsing map can be converted to a real person image. It looks good in general, but the upper clothes do not suit your taste. Then comes the problem-either these models do not support multi-modal synthesis, or when these models change the upper clothes, other parts vary accordingly. Neither of these fulfills your intention. To conclude, this user controllable content creation scenario can be interpreted as performing a task that produces multi-modal results at the semantic level with other semantic parts untouched. We summarize this task as: Semantically Multimodal Image Synthesis (SMIS). As exemplified in Fig. 1, for each semantics, we have its specific controller. By adjusting the controller of a specific class, only the corresponding areas are changed accordingly.
Figure 1: Demonstration of the semantically multi-modal image synthesis (SMIS) task. Text of each column above the images indicates the semantic area that is changing across the whole column. The first row represents the input labels and the rest rows are generated images by our method. 
An intuitive solution for the task is to build different generative networks for different semantics and then produce the final image by fusing the outputs of different networks. It is quite similar to the overall scheme of [10], which focused on portrait editing. However, this type of methods soon face degradation in performance, a linear increase of training time and computational resource consumption under a growing number of classes. To make the network more elegant, we unify the generation process in only one model by creatively replacing all regular convolutions in the generator with group convolutions. Our strategy is mathematically and functionally equivalent to [10] when the group numbers of the convolutions are equal to that of the classes. Another strategy we adopt, however, set our paths differently-we decrease the number of groups in the decoder convolutions in the forwarding progress. We observe that different classes have internal correlations among each other, e.g., the color of the grass and the tree leaves should be highly similar. In this case, progressively merging the groups gives the model enough capacity to build inter-correlations among different classes, and consequently improves the overall image quality. Besides, this strategy also considerably mitigates the computation consumption problem when the class number of the dataset is substantial (e.g., ADE20K [53]). We call the generator equipped with these two strategies as Group Decreasing Network (GroupDNet). To evaluate GroupDNet's performance on the SMIS task, we propose two new metrics called mean Class-Specific Diversity (mCSD) and mean Other-Class Diversity (mOCD). The mCSD metric tends to hold high value and the mOCD tends to be low when some semantic parts vary drastically but other parts stay unchanged. We conduct experiments on several challenging datasets: DeepFashion [30], Cityscapes [5], and ADE20K [53]. The results show that our GroupDNet introduces more controllability over the generation process, and thus produces semantically multi-modal images. Moreover, GroupDNet maintains to be competitive with previous state-of-the-art methods in terms of image quality, exhibiting the superiority of GroupDNet. Furthermore, GroupDNet introduces much controllability over the generation process and has a variety of interesting applications such as appearance mixture, semantic manipulation, and style morphing. 

SECTION 2. Related Work: Generative Models: Generative Adversarial Networks (GANs) [9], comprised of a generator and a discriminator, have the amazing ability to generate sharp images even for very challenging datasets. [9], [21], [3], [22]. Variational auto-encoder [24] contains an encoder and a decoder, and requires the latent code yield by the encoder to conform to Gaussian distribution. Its results usually exhibit large diversity. Some methods [2] combine VAE and GAN in their models, producing realistic while diverse images. Conditional Image Synthesis: Conditional Generative Adversarial Networks [34] inspire a wide range of conditional image synthesis applications, such as image-to-image translation [19], [17], [41], [55], [27], [29], [15], super resolution [26], [20], domain adaption [14], [52], single model image synthesis [54], [37], [33], [38], style transfer [16], [7], [20], person image generation [31], [57], [11] and image synthesis from text [48], [49] etc. We focus on transforming conditional semantic labels to natural images while adding more diversity and controllability to this task at the semantic level. Multi-Modal Label-to-Image Synthesis: There has been a number of works [1], [28], [40] in multi-modal label-to-image synthesis task. Chen et. al. [4] avoided using GAN and leveraged cascaded refinement network to generate high-resolution images. Wang et. al. [41] added additional instance-level feature channels to the output of the encoder that allows object-level control on the generated results. Wang et. al. [40] used another source of images as stylish examples to guide the generation process. Park et. al. [36] incorporated VAE into their network that enables the generator to yield multi-modal images. Li et. al. [28] adopted an implicit maximum likelihood estimation framework to alleviate the mode collapse issue of GAN, thus encouraged diverse outputs. Bansal et. al. [1] used classic tools to match the shape, context and parts from a gallery with the semantic label input in exponential ways, producing diverse results. Different from these works, we focus on semantically multi-modal image synthesis, which requires fine-grained controllability at the semantic level instead of the global level. Gu et. al. [10] built several auto-encoders for each face component to extract different component representations which are then merged into the next foreground generator in the task of portrait editing. Our work is highly related to this work since both methods devise to cope with the SMIS task by treating different classes with different parameters. However, our unique design of progressively decreasing the group numbers in the decoder enables our network to deal with datasets of many classes where their method is possibly incapable of. Group Convolution: Previous works [25], [44], [51], [32], [43] indicate that group convolutions are advantageous for reducing the computational complexity and model parameters, thus they have been widely used in light-weight networks. Ma et. al. [32] mentioned that excessive use of group convolutions results in large Memory Access Cost (MAC). Although it is ideal to use group convolutions with small groups or even no group convolutions in the network, we show in our experiments that completely avoiding group convolutions in the decoder is problematic for the performance on the SMIS task. Moreover, our decreasing group numbers strategy considerately alleviates the huge MAC problem so that it is applicable to real-world applications. 

SECTION 3. Semantically Multi-Modal Image Synthesis: 3.1. Problem Definition: Let MM denote a semantic segmentation mask. Suppose there are \mathcal{C}C semantic classes in the dataset. HH and WW represent the image height and width, respectively. As a very straightforward manner of conducting label-to-image translation, the generator GG requires MM as conditional input to generate images. However, in order to support multi-modal generation, we need another input source to control generation diversity. Normally, we employ an encoder to extract a latent code ZZ as the controller inspired by VAE [24]. Upon receiving these two inputs, the image output OO can be yield through O=G(Z, M)O=G(Z,M). However, in the semantically multi-modal image synthesis (SMIS) task, we aim to produce semantically diverse images by perturbing the class-specific latent code which independently controls the diversity of its corresponding class. 3.2. Challenge: For the SMIS task, the key is to divide the latent code into a series of class-specific latent codes each of which controls only a specific semantic class generation. The traditional convolutional encoder is not an optimal choice because the feature representations of all classes are internally entangled inside the latent code. Even if we have class-specific latent code, it is still problematic on how to utilize the code. As we will illustrate in the experiment part, simply replacing the original latent code in SPADE [36] with class-specific codes has limited capability to deal with the SMIS task. This phenomenon inspires us that we need to make some architecture modifications in both the encoder and decoder to accomplish the task more effectively. 3.3. GroupDNet: Based on the above analysis, we now give more details about our solution for this task—Group Decreasing Network (GroupDNet). The main architecture of GroupDNet takes design inspirations from SPADE [36], considering its superior performance in the label-to-image generation task. A major modification of GroupDNet is the replacement of typical convolutions to group convolutions [25] to achieve class-specific controllability. In the following, we will first present a brief overview of the architecture of our network and then describe the modifications that we made in different components of the network. OverviewAs can be seen from Fig. 2, GroupDNet contains one encoder and one decoder. Inspired by the idea of VAE [24] and SPADE [36], the encoder EE produces a latent code ZZ that is supposed to follow a Gaussian distribution \mathcal{N}(0,1)N(0,1) during training. While testing, the encoder EE is discarded. A randomly sampled code from the Gaussian distribution substitutes for ZZ. To fulfill this, we use the re-parameterization trick [24] to enable a differentiable loss function during training. Specifically, the encoder predicts a mean vector and a variance vector through two fully connected layers to represent the encoded distribution. The gap between the encoded distribution and Gaussian distribution can be minimized by imposing a KL-divergence loss: \begin{equation*}
\mathcal{L}_{\mathrm{KL}}=\mathcal{D}_{\mathrm{KL}}(E(I)\Vert \mathcal{N}(0,1)), \tag{1}
\end{equation*}LKL=DKL(E(I)∥N(0,1)),(1)View Source\begin{equation*}
\mathcal{L}_{\mathrm{KL}}=\mathcal{D}_{\mathrm{KL}}(E(I)\Vert \mathcal{N}(0,1)), \tag{1}
\end{equation*} where \mathcal{D}_{\mathrm{KL}}DKL represents the KL divergence. EncoderLet M_{c}Mc denote the binary mask for class cc and X\in \mathbb{R}^{H\times W}X∈RH×W be the input image. By splitting XX to different images of different semantic classes, we have \begin{equation*}
X_{c}=M_{c}\cdot X. \tag{2}
\end{equation*}Xc=Mc⋅X.(2)View Source\begin{equation*}
X_{c}=M_{c}\cdot X. \tag{2}
\end{equation*} This operation reduces the dependence on EE to process feature disentanglement, saving more capacity to precisely encode the feature. The input to the encoder is the concatenation of these images: S=\underset{c}{\mathrm{cat}}X_{c}S=catcXc. All convolutions inside EE have the same number of groups, that is, the total number of classes \mathcal{C}C. From both the input and the architecture side, we decouple different classes to be independent on each other. As a result, the encoded latent code ZZ is comprised of the class-specific latent code Z_{c}Zc (a discrete part of ZZ) of all classes. And Z_{c}Zc serves as the controller of class cc in the forthcoming decoding phase. Different from the general scheme of producing two vectors as the mean and variance prediction of the Gaussian distribution, our encoder produces a mean map and a variance map through convolutional layers to massively retain structural information in the latent code ZZ. DecoderUpon receiving the latent code ZZ, the decoder transforms it to natural images with the guidance of semantic labels. The question is how to leverage the semantic labels to guide the decoding phase properly. Several ways can serve this purpose, such as concatenating the semantic labels to the input or conditioning on every stage of the decoder. The former one is not suitable for our case because the decoder input has a very limited spatial size that will acutely lose many structural information of the semantic labels. We opt to the latter one and choose a typical advanced model-SPADE generator [36] as the backbone of our network. As mentioned in [36], SPADE is a more general form of some conditional normalization layers [6], [16], and shows superior ability to produce pixel-wise guidance in semantic image synthesis. Following the general idea of using all group convolutions in the generator, we replace the convolutional layers in SPADE module with group convolutions and call this new conditional module as Conditional Group Normalization (CG-Norm), as depicted in Fig 2. We then compose a network block called Conditional Group Block (CG-Block) by dynamically merging CG-Norm and group convolutions. The architecture of CG-Block is also demonstrated in Fig 2. Likewise, let \mathbf{F}^{i}\in \mathbb{R}^{H^{i}\times W^{i}}Fi∈RHi×Wi denote the feature maps of the ii-th layer of the decoder network and \mathcal{G}^{i}Gi represent the number of groups of the ii-th layer. Moreover, N,\ D^{i},\ H^{i}N, Di, Hi and W^{i}Wi are the batch size, number of channels, height and width of the feature map, respectively. As demonstrated in Fig. 2, the group convolutions inside CG-Norm will transform the semantic label input to pixel-wise modulation parameters \gamma\in \mathbb{R}^{D^{i}\times H^{i}\times W^{i}}γ∈RDi×Hi×Wi and \beta\in \mathbb{R}^{D^{i}\times H^{i}\times W^{i}}β∈RDi×Hi×Wi. The feature input \mathbf{F}^{i}Fi will first go through a batch normalization layer [18] that normalizes \mathbf{F}^{i}Fi: \begin{equation*}
\mathrm{BN}(\mathbf{F}^{i})=\gamma_{\mathrm{BN}}\left(\frac{\mathbf{F}^{i}-\mu(\mathbf{F}^{i})}{\sigma(\mathbf{F}^{i})}\right)+\beta_{\mathrm{BN}}, \tag{3}
\end{equation*}BN(Fi)=γBN(Fi−μ(Fi)σ(Fi))+βBN,(3)View Source\begin{equation*}
\mathrm{BN}(\mathbf{F}^{i})=\gamma_{\mathrm{BN}}\left(\frac{\mathbf{F}^{i}-\mu(\mathbf{F}^{i})}{\sigma(\mathbf{F}^{i})}\right)+\beta_{\mathrm{BN}}, \tag{3}
\end{equation*} where here \gamma_{\mathrm{BN}}, \beta_{\mathrm{BN}}\in \mathbb{R}^{D} are affine parameters learned from data. \mu_{d} and \sigma_{d} are computed across batch size and spatial dimensions for each feature channel: \begin{align*}
\mu_{d}(\mathbf{F}^{i}) &= \frac{1}{NH^{i}W^{i}} \sum\limits_{n=1}^{N} \sum\limits_{h=1}^{H^{i}} \sum\limits_{w=1}^{W^{i}}\mathbf{F}_{ndhw}^{i}\\
\sigma_{d}(\mathbf{F}^{i}) &=\sqrt{\frac{1}{NH^{i}W^{i}} \sum\limits_{n=1}^{N} \sum\limits_{h=1}^{H^{i}} \sum\limits_{w=1}^{W^{l}}(\mathbf{F}_{ndhw}^{i})^{2}-(\mu_{d}(\mathbf{F}^{i}))^{2}} \tag{4}
\end{align*}View Source\begin{align*}
\mu_{d}(\mathbf{F}^{i}) &= \frac{1}{NH^{i}W^{i}} \sum\limits_{n=1}^{N} \sum\limits_{h=1}^{H^{i}} \sum\limits_{w=1}^{W^{i}}\mathbf{F}_{ndhw}^{i}\\
\sigma_{d}(\mathbf{F}^{i}) &=\sqrt{\frac{1}{NH^{i}W^{i}} \sum\limits_{n=1}^{N} \sum\limits_{h=1}^{H^{i}} \sum\limits_{w=1}^{W^{l}}(\mathbf{F}_{ndhw}^{i})^{2}-(\mu_{d}(\mathbf{F}^{i}))^{2}} \tag{4}
\end{align*}
Figure 2: Architecture of our generator (GroupDNet). “GConv” means group convolution and “Sync BN” represents synchronized batch normalization. \mathcal{G}^{i} is the group number of i-th layer. Note normally \mathcal{G}^{i}\geq \mathcal{G}^{i+1} for i\geq 1 for GroupDNet. 
Afterwards, the output \mathrm{BN}(\mathbf{F}^{i}) interacts with previously predicted pixel-wise \gamma and \beta, yielding a new feature map \mathbf{F}^{o} with semantic information inserted. Taking Eq. 3 into account, \begin{align*}
\mathbf{F}^{o} &=\gamma\cdot \mathrm{BN}(\mathbf{F}^{i})+\beta\\
&= \gamma\cdot \gamma_{\mathrm{BN}}\left(\frac{\mathbf{F}^{i}-\mu(\mathbf{F}^{i})}{\sigma(\mathbf{F}^{i})}\right)+(\gamma\cdot\beta_{\mathrm{BN}}+\beta). \tag{5}
\end{align*}View Source\begin{align*}
\mathbf{F}^{o} &=\gamma\cdot \mathrm{BN}(\mathbf{F}^{i})+\beta\\
&= \gamma\cdot \gamma_{\mathrm{BN}}\left(\frac{\mathbf{F}^{i}-\mu(\mathbf{F}^{i})}{\sigma(\mathbf{F}^{i})}\right)+(\gamma\cdot\beta_{\mathrm{BN}}+\beta). \tag{5}
\end{align*} When i becomes larger, the group number is finally reduced to 1. After a regular convolution, the feature is mapped to a three-channel RGB image O. 3.4. Other Solutions: Aside from GroupDNet, a simple solution to perform the SMIS task is to build a set of encoders and decoders, each of which focus on a specific semantic class, as demonstrated in Fig. 3(a). The underlying idea is to treat each class independently and then fuse the results of different subnetworks. For simplicity, we call such networks as Multiple Networks (MulNet). Another alternative with a similar idea is to use group convolution [25] throughout the network. As depicted in Fig. 3(b), replacing all the convolutions in the encoder and the decoder with group convolutions [25] and setting the group number equal to the class number present the Group Network (GroupNet). It is theoretically equivalent to MulNet if the channel number in every group is equal to those of the corresponding layer in a single network of MulNet. Fig. 3(c) illustrates our GroupDNet. The primary difference between GroupDNet and GroupNet is the monotonically decreasing number of groups in the decoder. Although this modification seems to be simple, it brings several noticeable benefits, mainly in the following three aspects: Class BalanceIt is worth noticing that different classes have a different number of instances [30], [5], [53] and require different network capacity to model these classes. It is difficult for MulNet and GroupNet to find a suitable network design to balance all the classes. More importantly, not all the classes appear in one image. In this case, MulNet and GroupNet inevitably waste a lot of computational resources because they have to activate all the sub-networks or subgroups for all the classes during training or testing. However, in GroupDNet, unbalanced classes share parameters with their neighbor classes, hugely alleviating the class imbalance problem. Class CorrelationIn natural worlds, a semantic class usually has relationships with other classes, e.g., the color of grass and the color of tree leaves are similar, and buildings influence the sunshine on the roads in their vicinity, etc. To generate plausible results, both of MulNet and GroupNet have a fusion module (several regular convolutions in our case) at the end of the decoder to merge features of different classes into one image output. In general, the fusion module roughly considers the correlations of different classes. However, we argue it is not sufficient because the correlation of different classes is too complex to be fully explored by using such a simple component with restricted receptive fields. An alternative is to use some network modules like self-attention block to capture long-range dependencies of the image, but its prohibitive computation hinders its usage in such scenarios [47]. GroupDNet, however, carves these relationships throughout the decoder; hence, it exploits the correlations more accurately and thoroughly. As a result, the generated images of GroupDNet are better and more realistic than those generated by the other two methods.
Figure 3: An illustration of MulNet (a), GroupNet (b) and GroupDNet (c). Note the last layers of MulNet and GroupNet are fusing modules that are comprised of several normal convolutional layers to fuse results of different classes. 
GPU MemoryIn order to guarantee that every single network of MulNet, or the grouped parameters for each class in GroupNet have sufficient capacity, the channel numbers in total will increase significantly with the increase of class number. Up to a limit, the maximum GPU memory of a graphics card would no longer be able to hold even one sample. As we roughly estimate on the ADE20K dataset [53], one Tesla V100 graphics card cannot hold the model with sufficient capacity even when batch size is set to 1. However, the problem is less severe in GroupDNet because different classes share parameters, thus it is unnecessary to set so many channels for each class. 3.5. Loss Function: We adopt the same loss function as SPADE [36]: \begin{equation*}
\mathcal{L}_{\mathrm{full}}=\arg\min\limits_{G}\max\limits_{D}\mathcal{L}_{\mathrm{GAN}}+\lambda_{1}\mathcal{L}_{\mathrm{FM}}+\lambda_{2}\mathcal{L}_{\mathrm{P}}+\lambda_{2}\mathcal{L}_{\mathrm{KL}}.\tag{6}
\end{equation*}View Source\begin{equation*}
\mathcal{L}_{\mathrm{full}}=\arg\min\limits_{G}\max\limits_{D}\mathcal{L}_{\mathrm{GAN}}+\lambda_{1}\mathcal{L}_{\mathrm{FM}}+\lambda_{2}\mathcal{L}_{\mathrm{P}}+\lambda_{2}\mathcal{L}_{\mathrm{KL}}.\tag{6}
\end{equation*} The \mathcal{L}_{\mathrm{GAN}} is the hinge version of GAN loss, and \mathcal{L}_{\mathrm{FM}} is the feature matching loss between the real and synthesized images. Specifically, we use a multiple-layer discriminator to extract features from real and synthesized images. Then, we calculate the L_{1} distance between these paired features. Likewise, \mathcal{L}_{\mathrm{P}} is the perceptual loss proposed for style transfer [20]. A pre-trained VGG network [39] is used to get paired intermediate feature maps, and then we calculate the L_{1} distance between these paired maps. \mathcal{L}_{\mathrm{KL}} is the KL-divergence loss term as Eq. 1. We set \lambda_{1}=10, \lambda_{2}= 10, \lambda_{3}=0.05, the same as SPADE [36]. 

SECTION 4. Experiments: 4.1. Implementation Details: We apply Spectral Normalization [35] to all the layers in both the generator and discriminator. The learning rates for the generator and discriminator are set to 0.0001 and 0.0004, respectively, following the two time-scale update rule [12]. We use the Adam optimizer [23] and set \beta_{1}=0, \beta_{2}=0.9. All the experiments are conducted on at least 4 P40 GPUs. Besides, we use synchronized batch normalization to synchronize the mean and variance statistics across multiple GPUs. More details, such as the detailed network design and more hyper-parameters, are given in the supplementary materials. 4.2. Datasets: We conduct experiments on three very challenging datasets, including DeepFashion [30], Cityscapes [5], and ADE20K [53]. We choose DeepFashion because this dataset shows lots of diversities among all semantic classes, which is naturally suitable for assessing the model's ability to conduct multi-modal synthesis. Consequently, we compare with several baseline models on this dataset to evaluate the superior power of our model on the SMIS task. The size of the images in Cityscapes are quite large, so it is proper to test the model's ability to produce high-resolution images on this dataset. ADE20K is extremely challenging for its massive number of classes, and we find it hard to train MulNet and GroupNet on ADE20K with our limited GPUs. More details can be found in the supplementary materials. 4.3. Metrics: Mean SMIS DiversityIn order to evaluate the performance of a model designed for the SMIS task, we introduce two new metrics named: mean Class-Specific Diversity (mCSD) and mean Other-Classes Diversity (mOCD). We design the new metrics based on the LPIPS metric [50], which is used to assess the generation diversity of a model by computing the weighted \mathcal{L}_{2} distance between deep features of image pairs. For the same semantic label input, we generate n images for each semantic class by only modulating the latent code Z_{c} for the semantic class c. Therefore, we have a set of images \mathcal{S}=\{I_{1}^{1},\ldots, I_{1}^{n},\ldots, I_{\mathcal{C}}^{1},\ldots, I_{\mathcal{C}}^{n}\}. Finally, mCSD and mOCD are calculated by \begin{equation*}
\mathrm{mCSD}=\frac{1}{\mathcal{C}}\sum\limits_{c=1}^{\mathcal{C}}L_{c},\ \mathrm{mOCD}=\frac{1}{\mathcal{C}}\sum\limits_{c=1}^{\mathcal{C}}L_{\neq c}. \tag{7}
\end{equation*}View Source\begin{equation*}
\mathrm{mCSD}=\frac{1}{\mathcal{C}}\sum\limits_{c=1}^{\mathcal{C}}L_{c},\ \mathrm{mOCD}=\frac{1}{\mathcal{C}}\sum\limits_{c=1}^{\mathcal{C}}L_{\neq c}. \tag{7}
\end{equation*} where L_{c} is the average LPIPS distance [50] of the semantic area of class c between sampled m pairs and L_{\neq c} represents the average LPIPS distance [50] in the areas of all other classes between the same pairs. In our settings, we set n=100, m=19 following [56], [17]. ImageNet pre-trained AlexNet [25] is served as the deep feature extractor. Higher performance on the SMIS task demands a high diversity of the specific semantic areas (high mCSD) as well as a low diversity of all other areas (low mOCD). Besides, we also report the overall LPIPS distance by producing globally diverse results for the same semantic labels. Human Evaluation MetricsWe further introduce human evaluation to evaluate whether the generative model performs well in the SMIS task. We recruit 20 volunteers that have research experience in generation tasks. We show them an input mask along with two images generated from only one model. The two images are multi-modal results which only vary in the areas of one random semantic class. The volunteers judge whether the given two images only vary in one semantic class. The percentage of pairs that are judged to be semantically different in only one semantic class represents the human evaluation of a model's performance on the SMIS task. We abbreviate this metric as SHE (SMIS Human Evaluation). For each phase, the volunteers are given 50 questions of unlimited answering time. Fréchet Inception DistanceWe use Fréchet Inception Distance (FID) [13] to calculate the distance between the distributions of synthesized results and the distribution of real images. Lower FID generally hints better fidelity of the generated images. Segmentation PerformanceIt is reasonable that the predicted labels of the generated images are highly similar to those of the original images if they look realistic. Therefore, we adopt the evaluation protocol from previous work [4], [41], [36] to measure the segmentation accuracy of the generated images. We report results on the mean Intersection-over-Union (mIoU) and pixel accuracy (Acc) metrics without considering the classes that can be ignored. Images are evaluated using well-trained segmentation models Uper-Net101 [42] for ADE20K, DRN-D-105 [46] for Cityscapes, off-the-shelf human parser CIHP [8] for DeepFashion.
Figure 4: Qualitative comparison between GroupDNet and other baseline models. The first two rows represent the results of different models by changing their upper-clothes latent code while the last two rows represent their results of changing the pants latent code. Note, for those models which have no class-specific controller such as VSPADE, we alter their overall latent codes to generate different images. 
4.4. Results: Besides the following sections, we have more justification of our model design in the supplementary materials for the reference of interested readers. 4.4.1 Comparison on SMISA basic requirement for models that potentially could be modified for the SMIS task is that they should possess the ability to conduct multi-modal image synthesis. We compare with several methods that support multi-modal image synthesis to demonstrate the superiority of GroupDNet:
Variational SPADE [36] (VSPADE) has an image encoder processing a real image to a mean and a variance vector where a KL divergence loss is applied to support multi-modal image synthesis. Detailed description can be found in their paper; BicycleGAN [56] maps the given image input into a latent code, which is later combined with a label input to produce outputs. Since the latent code is constrained by a KL divergence loss, it could be substituted by a random sample from the Gaussian distribution; DSCGAN [45] is complementary to BicycleGAN by introducing an explicit regularization upon the generator, trying to alleviate the mode collapse issue of previous models.  Aside from MulNet and GroupNet described in Sec. 3.4, we also conduct two further experiments by replacing the convolutions in the encoder/decoder of the VSPADE model to group convolutions with group numbers set equal to the dataset class number, denoted as GroupEnc/GroupDec, respectively. Note MulNet, GroupNet, GroupEnc, GroupDec and VSPADE are trained with the same kind of multi-scale discriminator [41] and training settings as GroupDNet. To fairly compare the performances, we balance the number of parameters of these models to mitigate the suspicion that the performance improvements are brought by using more parameters. For BicycleGAN and DSCGAN, we adopt their original training and testing protocols. Quantitative and qualitative results are given in Tab. 1 and Fig. 4, respectively. The quantitative results demonstrate the overall superiority of GroupDNet. Generally, GroupDNet exhibits the best image quality (lowest FID) and overall diversity (highest LPIPS). In terms of the performance on the SMIS task, MulNet and GroupNet are slightly better than GroupDNet, given evidence that they have either larger mCSD or lower mOCD. However, the image quality of MulNet and GroupNet is not satisfactory (high FID) and MulNet shows much lower FPS than GroupDNet. In terms of the SHE metric, GroupDNet is also very competitive to MulNet and GroupNet. Although VSPADE, has rather large mCSD, its mOCD is also very large, indicating that it performs unsatisfactorily on the SMIS task. The same phenomenon is also observed in BicycleGAN and DSCGAN and their FIDs are relatively much higher than VSPADE, showing the advantage of the SPADE architecture. From the high mOCD values of VSPADE and GroupDec, whose encoders are composed from regular convolutions, we conclude that group encoder serves as a key to the high performance of the SMIS task. However, the exceptional performance of GroupDNet suggests that the group decreasing modification in the decoder is also effective and brings further performance boost when compared to GroupEnc. Gathering these information, GroupDNet is a good tradeoff model considering the speed, visual quality and the performance on the SMIS task. According to the qualitative results, it is obvious that MulNet, GroupNet, GroupEnc and GroupDNet are able to generate semantically multi-modal images while others cannot. However, the image quality of MulNet, GroupNet, BicycleGAN and DSCGAN is far from satisfaction because their images are visually implausible. GroupEnc is better in image quality but it degrades in the SMIS task. It can be seen from the first two rows in Fig. 4 that, when the upper clothes are changed to another style, GroupEnc slightly changes the color of the short jeans pants as well. 4.4.2 Comparison on Label-to-Image TranslationIn this section, we mainly assess the generated image quality of our method by comparing with some label-to-image methods on the FID, mIoU and Accuracy metrics. We choose four very recent state-of-the-art methods: BicycleGAN [56], DSCGAN [45], pix2pixHD [41] and SPADE [36], as the comparison methods. Comparisons are performed across the DeepFashion, Cityscapes and ADE20K datasets. We evaluate the performance of their well-trained models downloaded from their official GitHub repositories if they have. For those experiments not included in their original papers, we follow their codes and run the experiments with similar settings to GroupDNet. Quantitative results are shown in Tab. 2. In general, as our network is built based on SPADE, it maintains nearly the same performance as SPADE on the DeepFashion and Cityscapes datasets. While on the ADE20K dataset, our method is inferior to SPADE but still outperform other methods. This phenomenon on the one hand shows the superiority of the SPADE architecture and on the other hand also exposes even GroupDNet still struggles to handle datasets with a huge number of semantic classes.
Table 1: Quantitative comparison results with baseline models. “SHE” means human evaluation of a model's performance on SMIS task. We use frame per second (FPS) to represent the “speed” of the model. “# Param” means the number of parameters, whose unit is “M”, denoting million. For mCSD, the higher, the better. For mOCD, the lower, the better.
Figure 5: Qualitative comparison with the SOTA label-to-image methods. From top to bottom, the images represent the experiments on DeepFashion, cityscapes and ADE20K, respectively. 
Qualitative comparisons on DeepFashion, Cityscapes and ADE20K are shown in Fig. 5. In general, the images generated by GroupDNet are more realistic and plausible than others. These visual results consistently show the high image quality of GroupDNet's generated images, verifying its efficacy on various datasets.
Figure 6: Exemplar applications of the proposed method. (a) Demonstration of the semantically multi-modal image synthesis (SMIS) task. (b) Application of our SMIS model in appearance mixture. Our model extracts styles of different semantic classes from different sources and generates a mixed image by combining these semantic styles with the given semantic mask. (c) Application of our SMIS model in semantic manipulation. (d) Application of our SMIS model in image extrapolation. Zoom in for better details. 
Table 2: Quantitative comparison with label-to-image models. The numbers of pix2pixHD and SPADE are collected by running the evaluation on our machine instead of their papers.
4.4.3 ApplicationsSince GroupDNet contributes more user controllability to the generation process, it can also be applied to lots of exciting applications in addition to the SMIS task, which are demonstrated as follows. More results are available in the supplementary materials. Appearance MixtureBy utilizing the encoder in GroupDNet during inference, we can gather the distinct styles of a person's different body parts. Every combination of these styles presents a distinct person image, given a human parsing mask. In this way, we can create thousands of diverse and realistic person images given a person image gallery. This application is demonstrated in Fig. 6(b). A demo video can be found in our code repository. Semantic ManipulationSimilar to most label-to-image methods [41], [36], [28], our network also supports semantic manipulation. As exemplified in Fig. 6(c), we can insert a bed in the room or replace the building with trees, etc. Style MorphingFeeding two real images to the encoder generates two style codes of these images. By extrapolating between these two codes, we can generate a sequence of images that progressively vary from image a to image b, depicted in Fig. 6(d). 

SECTION 5. Conclusion and Future Work: In this paper, we propose a novel network for semantically multi-modal synthesis task, called GroupDNet. Our network unconventionally adopts all group convolutions and modifies the group numbers of the convolutions to decrease in the decoder, considerably improving the training efficiency over other possible solutions like multiple generators. Although GroupDNet performs well on semantically multi-modal synthesis task and generates results with relatively high quality, there are still some problems remained to be solved. First, it requires more computational resources to train compared to pix2pixHD and SPADE though it is nearly 2 times faster than multiple generators networks. Second, GroupDNet still has difficulty in modeling different layouts of a specific semantic class for datasets with limited diversity, even though it demonstrates some low-level variations like illumination, color, and texture, etc. 
ACKNOWLEDGMENT: We thank Taesung Park for his kind help to this project. This work was supported by NSFC 61573160, to Dr. Xiang Bai by the National Program for Support of Top-notch Young Professionals and the Program for HUST Academic Frontier Youth Team.