Figure 1: Given any two unordered image collections X and Y, our algorithm learns to automatically “translate” an image from one into the other and vice versa. Example application (bottom): Using a collection of paintings of a famous artist, learn to render a user's photograph into their style. 

SECTION 1. Introduction: What did Claude Monet see as he placed his easel by the bank of the Seine near Argenteuil on a lovely spring day in 1873 (Figure 1, top-left)? A color photograph, had it been invented, may have documented a crisp blue sky and a glassy river reflecting it. Monet conveyed his impression of this same scene through wispy brush strokes and a bright palette. What if Monet had happened upon the little harbor in Cassis on a cool summer evening (Figure 1, bottom-left)? A brief stroll through a gallery of Monet paintings makes it easy to imagine how he would have rendered the scene: perhaps in pastel shades, with abrupt dabs of paint, and a somewhat flattened dynamic range. We can imagine all this despite never having seen a side by side example of a Monet painting next to a photo of the scene he painted. Instead we have knowledge of the set of Monet paintings and of the set of landscape photographs. We can reason about the stylistic differences between these two sets, and thereby imagine what a scene might look like if we were to “translate” it from one set into the other.
Figure 2: Paired training data (left) consists of training examples {xi, yi}Ni=1, where the yi that corresponds to each xi is given [20]. We instead consider unpaired training data (right), consisting of a source set {xi}Ni=1∈X and a target set {yj}Mj=1∈Y, with no information provided as to which xi matches which yj. 
In this paper, we present a system that can learn to do the same: capturing special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples. This problem can be more broadly described as image-to-image translation [20], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale to color, image to semantic labels, edge-map to photograph. Years of research in computer vision, image processing, and graphics have produced powerful translation systems in the supervised setting, where example image pairs {x, y} are available (Figure 2, left), e.g., [9], [17], [20], [21], [24], [29], [41], [52], [54], [57]. However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g., [4]), and they are relatively small. Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. For many tasks, like object transfiguration (e.g., zebra→horse, Figure 1 top-middle), the desired output is not even well-defined. We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains — for example, that they are two different renderings of the same underlying world — and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y. We may train a mapping G:X→Y such that the output y^=G(x), x∈X, is indistinguishable from images y∈Y by an adversary trained to classify y^ apart from y. In theory, this objective can induce an output distribution over y^ that matches the empirical distribution pY(y) (in general, this requires that G be stochastic) [14]. The optimal G thereby translates the domain X to a domain Y^ distributed identically to Y. However, such a translation does not guarantee that the individual inputs and outputs x and y are paired up in a meaningful way — there are infinitely many mappings G that will induce the same distribution over y^. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well-known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress [13]. These issues call for adding more structure to our objective. Therefore, we exploit the property that translation should be “cycle consistent”, in the sense that if we translate, e.g., a sentence from English to French, and then translate it back from French to English, we should arrive back at the original sentence [3]. Mathematically, if we have a translator G: X→Y and another translator F:Y→X, then G and F should be inverses of each other, and both mappings should be bijections. We apply this structural assumption by training both the mapping G and F simultaneously, and adding a cycle consistency loss [60] that encourages F(G(x))≈x and G(F(y))≈y. Combining this loss with adversarial losses on domains X and Y yields our full objective for unpaired image-to-image translation. We apply our method to a wide range of applications, including style transfer, object transfiguration, attribute transfer and photo enhancement. We also compare against previous approaches that rely either on hand-defined factorizations of style and content, or on shared embedding functions, and show that our method outperforms these baselines. Our code is available at https://github.com/junyanz/CycleGAN. Check out the full version of the paper at https://arxiv-org.proxy.lib.ohio-state.edu/abs/1703.10593. 

SECTION 2. Related Work: Generative Adversarial Networks (GANs): [14], [58] have achieved impressive results in image generation [5], [35], image editing [61], and representation learning [35], [39], [33]. Recent methods adopt the same idea for conditional image generation applications, such as text2image [36], image inpainting [34], and future prediction [32], as well as to other domains like videos [50] and 3D models [53]. The key to GANs' success is the idea of an adversarial loss that forces the generated images to be, in principle, indistinguishable from real images. This is particularly powerful for image generation tasks, as this is exactly the objective that much of computer graphics aims to optimize. We adopt an adversarial loss to learn the mapping such that the translated image cannot be distinguished from images in the target domain. Image-to-Image Translation: The idea of image-to-image translation goes back at least to Hertzmann et al.'s Image Analogies [17], who employ a nonparametric texture model [8] on a single input-output training image pair. More recent approaches use a dataset of input-output examples to learn a parametric translation function using CNNs, e.g. [29]. Our approach builds on the “pix2pix” framework of Isola et al. [20], which uses a conditional generative adversarial network [14] to learn a mapping from input to output images. Similar ideas have been applied to various tasks such as generating photographs from sketches [40] or from attribute and semantic layouts [22]. However, unlike these prior works, we learn the mapping without paired training examples.
Figure 3: (a) Our model contains two mapping functions g:X→y and f:Y→x, and associated adversarial discriminators dy and dx.Dy encourages g to translate x into outputs indistinguishable from domain y, and vice versa for dx, f, and x. To further regularize the mappings, we introduce two “cycle consistency losses” that capture the intuition that if we translate from one domain to the other and back again we should arrive where we started: (b) Forward cycle-consistency loss: x→G(x)→F(G(x))≈x, and (c) Backward cycle-consistency loss: y→F(y)→G(F(y))≈y 
Unpaired Image-to-Image Translation: Several other methods also tackle the unpaired setting, where the goal is to relate two data domains, X and Y. Rosales et al. [37] propose a Bayesian framework that includes a prior based on a patch-based Markov random field computed from a source image, and a likelihood term obtained from multiple style images. More recently, CoupledGANs [28] and cross-modal scene networks [1] use a weight-sharing strategy to learn a common representation across domains. Concurrent to our method, Liu et al. [27] extends this framework with a combination of variational autoencoders [23] and generative adversarial networks. Another line of concurrent work [42], [45], [2] encourages the input and output to share certain “content” features even though they may differ in “style”. They also use adversarial networks, with additional terms to enforce the output to be close to the input in a predefined metric space, such as class label space [2], image pixel space [42], and image feature space [45]. Unlike the above approaches, our formulation does not rely on any task-specific, predefined similarity function between the input and output, nor do we assume that the input and output have to lie in the same low-dimensional embedding space. This makes our method a general-purpose solution for many vision and graphics tasks. We directly compare against several prior approaches in Section 5.1. Concurrent with our work, in these same proceedings, Yi et al. [55] independently introduce a similar objective for unpaired image-to-image translation, inspired by dual learning in machine translation [15]. Cycle Consistency: The idea of using transitivity as a way to regularize structured data has a long history. In visual tracking, enforcing simple forward-backward consistency has been a standard trick for decades [44]. In the language domain, verifying and improving translations via “back translation and reconsiliation” is a technique used by human translators [3] (including, humorously, by Mark Twain [47]), as well as by machines [15]. More recently, higher-order cycle consistency has been used in structure from motion [56], 3D shape matching [19], co-segmentation [51], dense semantic alignment [59], [60], and depth estimation [12]. Of these, Zhou et al. [60] and Godard et al. [12] are most similar to our work, as they use a cycle consistency loss as a way of using transitivity to supervise CNN training. In this work, we are introducing a similar loss to push G and F to be consistent with each other. Neural Style Transfer: [11], [21], [48], [10] is another way to perform image-to-image translation, which synthesizes a novel image by combining the content of one image with the style of another image (typically a painting) by matching the Gram matrix statistics of pre-trained deep features. Our main focus, on the other hand, is learning the mapping between two domains, rather than between two specific images, by trying to capture correspondences between higher-level appearance structures. Therefore, our method can be applied to other tasks, such as painting→ photo, object transfiguration, etc. where single sample transfer methods do not perform well. We compare these two methods in Section 5.2. 

SECTION 3. Formulation: Our goal is to learn mapping functions between two domains X and Y given training samples {xi}Ni=1∈X and {yj}Mj=1∈Y. As illustrated in Figure 3 (a), our model includes two mappings G:X→Y and F:Y→X. In addition, we introduce two adversarial discriminators DX and DY, where DX aims to distinguish between images {x} and translated images {F(y)}; in the same way, DY aims to discriminate between {y} and {G(x)}. Our objective contains kinds of two terms: adversarial losses [14] for matching the distribution of generated images to the data distribution in the target domain; and a cycle consistency loss to prevent the learned mappings G and F from contradicting each other. 3.1. Adversarial Loss: We apply adversarial losses [14] to both mapping functions. For the mapping function G:X→Y and its discriminator DY, we express the objective as:
LGAN(G, DY, X, Y)=Ey∼pdata(y)[logDY(y)]+Ex∼pdata(x)[log(1−DY(G(x))],(1)View Source\begin{align*}
\mathcal{L}_{\text{GAN}}(G,\ D_{Y},\ X,\ Y)& = \mathbb{E}_{y\sim p_{\text{data}}(y)}[\log D_{Y}(y)]\\
&+\mathbb{E}_{x\sim p_{\text{data}}(x)}[\log(1- D_{Y}(G(x))],\tag{1}
\end{align*} where G tries to generate images G(x) that look similar to images from domain Y, while DY aims to distinguish between translated samples G(x) and real samples y. We introduce a similar adversarial loss for the mapping function F:Y→X and its discriminator DX as well: i.e. LGAN(F, DX, Y, X). 3.2. Cycle Consistency Loss: Adversarial training can, in theory, learn mappings G and F that produce outputs identically distributed as target domains Y and X respectively (strictly speaking, this requires G and F to be stochastic functions) [13]. However, with large enough capacity, a network can map the same set of input images to any random permutation of images in the target domain, where any of the learned mappings can induce an output distribution that matches the target distribution. To further reduce the space of possible mapping functions, we argue that the learned mapping functions should be cycle-consistent: as shown in Figure 3 (b), for each image x from domain X, the image translation cycle should be able to bring x back to the original image, i.e. x→G(x)→F(G(x))≈x. We call this forward cycle consistency. Similarly, as illustrated in Figure 3 (c), for each image y from domain Y, G and F should also satisfy backward cycle consistency: y→F(y)→G(F(y))≈y. We can incentivize this behavior using a cycle consistency loss:
Lcyc(G, F)=Ex∼pdata(x)[∥F(G(x))−x∥1]+Ey∼pdata((y)[∥G(F(y))−y∥1].(2)View Source\begin{align*}
\mathcal{L}_{\text{cyc}}(G,\ F)&=\mathbb{E}_{x\sim p_{\text{data}}(x)}[\Vert F(G(x))-x \Vert_{1}]\\
&+\mathbb{E}_{y\sim p_{\text{data}}((y)}[\Vert G(F(y))-y \Vert_{1}].\tag{2}
\end{align*} In preliminary experiments, we also tried replacing the L1 norm in this loss with an adversarial loss between F(G(x)) and x, and between G(F(y)) and y, but did not observe improved performance. The behavior induced by the cycle consistency loss can be observed in the arXiv version. 3.3. Full Objective: Our full objective is:
L(G, F, DX, DY)=LGAN(G, DY, X, Y)+LGAN(F, DX, Y, X)+λLcyc(G, F),(3)View Source\begin{align*}
\mathcal{L}(G,\ F,\ D_{X},\ D_{Y})=&\mathcal{L}_{\text{GAN}}(G,\ D_{Y},\ X,\ Y)\\
&+\mathcal{L}_{\text{GAN}}(F,\ D_{X},\ Y,\ X)\\
&+\lambda \mathcal{L}_{\text{cyc}}(G,\ F),\tag{3}
\end{align*} where λ controls the relative importance of the two objectives. We aim to solve:
G∗, F∗=argminG, F maxDx, DYL(G, F, DX, DY).(4)View Source\begin{equation*}
G^{\ast},\ F^{\ast}= \arg\min_{G,\ F}\ \max_{D_{x},\ D_{Y}}\mathcal{L}(G,\ F,\ D_{X},\ D_{Y}).\tag{4}
\end{equation*} Notice that our model can be viewed as training two “autoencoders” [18]: we learn one autoencoder F∘G:X→X jointly with another G∘F:Y→Y. However, these autoencoders each have special internal structure: they map an image to itself via an intermediate representation that is a translation of the image into another domain. Such a setup can also be seen as a special case of “adversarial autoencoders” [30], which use an adversarial loss to train the bottleneck layer of an autoencoder to match an arbitrary target distribution. In our case, the target distribution for the X→X autoencoder is that of domain Y. In Section 5.1.3, we compare our method against ablations of the full objective, and empirically show that both objectives play critical roles in arriving at high-quality results. 

SECTION 4. Implementation: Network Architecture: We adapt the architecture for our generative networks from Johnson et al. [21] who have shown impressive results for neural style transfer and super-resolution. This network contains two stride-2 convolutions, several residual blocks [16], and two 12-strided convolutions. Similar to Johnson et al. [21], we use instance normalization [49]. For the discriminator networks we use 70×70 Patch-GANs [20], [26], [25], which aim to classify whether 70×70 overlapping image patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator, and can be applied to arbitrarily-sized images in a fully convolutional fashion [20]. Training Details: We apply two techniques from recent works to stabilize our model training procedure. First, for LGAN (Equation 1), we replace the negative log likelihood objective by a least square loss [31]. This loss performs more stably during training and generates higher quality results. Equation 1 then becomes:
LLSGAN(G, DY, X, Y)=Ey∼pdata(y)[(DY(y)−1)2]+Ex∼pdata(x)[DY(G(x))2],(5)View Source\begin{align*}
\mathcal{L}_{\text{LSGAN}}(G,\ D_{Y},\ X,\ Y)&=\mathbb{E}_{y\sim p_{\text{data}}(y)}[(D_{Y}(y)-1)^{2}]\\
&+\mathbb{E}_{x\sim p_{\text{data}}(x)}[D_{Y}(G(x))^{2}],\tag{5}
\end{align*} Second, to reduce model oscillation [13], we follow Shrivastava et al's strategy [42] and update the discriminators DX and DY using a history of generated images rather than the ones produced by the latest generative networks. We keep an image buffer that stores the 50 previously generated images. Please refer to our arXiv paper for more details about the datasets, architectures and training procedures.
Figure 4: Different methods for mapping labels→photos trained on cityscapes. From left to right: Input, BiGAN/ALI [6], [7], CoGAN [28], CycleGAN (ours), pix2pix [20] trained on paired data, and ground truth. 
Figure 5: Different methods for mapping aerial photos↔maps on Google maps. From left to right: Input, BiGAN/ALI [6], [7], CoGAN [28], CycleGAN (ours), pix2pix [20] trained on paired data, and ground truth. 


SECTION 5. Results: We first compare our approach against recent methods for unpaired image-to-image translation on paired datasets where ground truth input-output pairs are available for evaluation. We then study the importance of both the adversarial loss and the cycle consistency loss, and compare our full method against several variants. Finally, we demonstrate the generality of our algorithm on a wide range of applications where paired data does not exist. For brevity, we refer to our method as CycleGAN. 5.1. Evaluation: Using the same evaluation datasets and metrics as “pix2pix” [20], we compare our method against several baselines both qualitatively and quantitatively. We also perform ablation study on the full loss function. 5.1.1 BaselinesCoGAN [28]This method learns one GAN generator for domain X and one for domain Y, with shared weights on the first few layers for shared latent representation. Translation from X to Y can be achieved by finding a latent representation that generates image X and then rendering this latent representation into style Y. Pixel Loss+GAN [42]Like our method, Shrivastava et al. [42] uses an adversarial loss to train a translation from X to Y. The regularization term ∥X−Y^∥1 was used to penalize making large changes at pixel level. Feature Loss+GANWe also test a variant of [42] where the L1 loss is computed over deep image features using a pretrained network (VGG-16 relu4_2 [43]), rather than over RGB pixel values.
Table 1: AMT “real vs fake” test on maps↔aerial photos.
Table 2: FCN-scores for different methods, evaluated on cityscapes labels→photos.
Table 3: Classification performance of photo→labels for different methods on cityscapes.
BiGAN/ALI [7], [6]Unconditional GANs [14] learn a generator G:Z\rightarrow X, that maps random noise Z to images X. The BiGAN [7] and ALI [6] propose to also learn the inverse mapping function F:X\rightarrow Z. Though they were originally designed for mapping a latent vector z to an image x, we implemented the same objective for mapping a source image x to a target image y. Pix2pix [20]We also compare against pix2pix [20], which is trained on paired data, to see how close we can get to this “upper bound” without using paired data. For fair comparison, we implement all the baselines using the same architecture and details as our method except for CoGAN [28]. We use the public implementation of CoGAN due to fundametal differences in architecture 1. 5.1.2 Comparison Against BaselinesAs can be seen in Figure 4 and Figure 5, we were unable to achieve compelling results with any of the baselines. Our method, on the other hand, is able to produce translations that are often of similar quality to the fully supervised pix2pix. We exclude pixel loss + GAN and feature loss + GAN in the figures, as both of the methods fail to produce results at all close to the target domain (full results can be viewed at https://junyanz.github.io/CycleGAN/). In addition, our method and the baselines are quantitatively compared in three ways. First, we run “real vs fake” study on Amazon Mechanical Turk (AMT) workers to assess perceptual realism [20]. Second, we train photo→label task on the Cityscapes dataset, and compare the output label images with the ground truth using the standard metrics on the Cityscapes benchmark [4]. Lastly, we train label→photo task on the same dataset and evaluate the output photos using an off-the-shelf fully-convolutional semantic segmentation network [29]. We find that our method significantly outperforms the baselines in all three experiments. Table 1 reports performance on the AMT perceptual realism task. Here, we see that our method can fool participants on around a quarter of trials, in both the map→photo direction and the photo→map direction. All baselines almost never fooled participants. Table 2 and Table 3 assess the performance of the label→photo task on the Cityscapes. In both cases, our method again outperforms the baselines. Detailed procedures and results of each experiment can be found in our arXiv version.
Figure 6: Different variants of our method for mapping labels↔photos trained on cityscapes. From left to right: Input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss (F(G(x))\approx x), GAN + backward cycle-consistency loss (G(F(y))\approx y), cyclegan (our full method), and ground truth. Both cycle alone and GAN + backward fail to produce images similar to the target domain. GAN alone and GAN + forward suffer from mode collapse, producing identical label maps regardless of the input photo. 
5.1.3 Ablation StudyWe compare against ablations of our full loss. Figure 6 shows several qualitative examples. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cycle loss in only one direction: GAN+forward cycle loss \mathbb{E}_{x\sim p_{\text{data}}(x)}[\Vert F(G(x))-x\Vert_{1}], or GAN+backward cycle loss \mathbb{E}_{y\sim p_{\text{data}}(y)}[\Vert G(F(y))-y\Vert_{1}] (Equation 2) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. We also quantitatively measured the ablations on Cityscapes photos→label, whose results can be found in our arXiv version. 5.2. Applications: We demonstrate our method on several applications where paired training data does not exist. We observe that translations on training data are often more appealing than those on test data, and full results of all applications on both training and test data can be viewed on our project website. Object Transfiguration (Figure 7)The model is trained to translate one object class from Imagenet [38] to another (each class contains around 1000 training images). Turmukham-betov et al.[46] proposes a subspace model to translate one object into another object of the same category, while our method focuses on object transfiguration between two visually similar categories. Season Transfer (Figure 7)The model is trained on the winter and summer photos of Yosemite on Flickr. Collection Style Transfer (Figure 8)We train the model on landscape photographs downloaded from Flickr and WikiArt. Note that unlike recent work on “neural style transfer” [11], our method learns to mimic the style of an entire set of artworks (e.g. Van Gogh), rather than transferring the style of a single selected piece of art (e.g. Starry Night). In Figure 5.2, we compare our results with [11]. Photo Generation from Paintings (Figure 9)For painting→photo, we find that it is helpful to introduce an additional loss to encourage the mapping to preserve color composition between the input and output. In particular, we adopt the technique of Taigman et al. [45] and regularize the generator to be near an identity mapping when real samples of the target domain are provided as the input to the generator: i.e. \mathcal{L}_{\text{identity}}(G,\ F)=\mathbb{E}_{y\sim p_{\text{data}}(y)}[\Vert G(y)-y\Vert_{1}]+ \mathbb{E}_{x\sim p_{\text{data}}(x)}[\Vert F(x)-x\Vert_{1}]. Without \mathcal{L}_{\text{identity}}, the generator G and F are free to change the tint of input images when there is no need to. For example, when learning the mapping between Monet's paintings and Flickr photographs, the generator often maps paintings of daytime to photographs taken during sunset, because such a mapping may be equally valid under the adversarial loss and cycle consistency loss. The effect of this identity mapping loss can be found in our arXiv paper. In Figure 9, we show additional results translating Monet paintings to photographs. This figure shows results on paintings that were included in the training set, whereas for all other experiments in the paper, we only evaluate and show test set results. Because the training set does not include paired data, coming up with a plausible translation for a training set painting is a nontrivial task. Indeed, since Monet is no longer able to create new paintings, generalization to unseen, “test set”, paintings is not a pressing problem.
Figure 7: Results on several translation problems. These images are relatively successful results — please see our website for more comprehensive results. 
Figure 8: We transfer input images into different artistic styles. Please see our website for additional examples. 
Photo Enhancement (Figure 7)We show that our method can be used to generate photos with shallower depth of field. We train the model on flower photos downloaded from Flickr. The source domain consists of photos of flower taken by smartphones, which usually have deep depth of field due to a small aperture. The target photos were taken with DSLRs with a larger aperture. Our model successfully generates photos with shallower depth of field from the photos taken by smartphones. 

SECTION 6. Limitations and Discussion: Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Several typical failure cases are shown in Figure 12. On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog→cat transfiguration, the learned translation degenerates to making minimal changes to the input (Figure 12). Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work.
Figure 9: Results on mapping monet paintings to photographs. Please see our website for additional examples. 
Figure 10: Photo enhancement: Mapping from a set of iPhone snaps to professional DSLR photographs, the system often learns to produce shallow focus. Here we show some of the most successful results in our test set-average performance is considerably worse. Please see our website for more comprehensive and random examples. 
Some failure cases are caused by the distribution characteristic of the training datasets. For example, the horse → zebra task of Figure 12 has completely failed, because our model was trained on the wild horse, zebra synsets of ImageNet, which does not contain images of a person riding horse or zebra. We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard — or even impossible — to close: for example, our method sometimes permutes the labels for tree and building in the output of the photos→labels task. To resolve this ambiguity may require some form of weak semantic supervision. Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems. Nonetheless, in many cases completely unpaired data is plentifully available and should be made use of. This paper pushes the boundaries of what is possible in this “unsupervised” setting.
Figure 11: We compare our method with neural style transfer [11]. Left to right: Input images, results from [11] using single representative image as a style image, results from [11] using all the images from the target domain, and CycleGAN (ours) 
Figure 12: Some failure cases of our method. 

ACKNOWLEDGMENTS: We thank Aaron Hertzmann, Shiry Ginosar, Deepak Pathak, Bryan Russell, Eli Shechtman, Richard Zhang, and Tinghui Zhou for many helpful comments. This work was supported in part by NSF SMA-1514512, NSF IIS-1633310, a Google Research Award, Intel Corp, and hardware donations from NVIDIA. JYZ is supported by the Facebook Graduate Fellowship and TP is supported by the Samsung Scholarship. The photographs used in style transfer were taken by AE, mostly in France.