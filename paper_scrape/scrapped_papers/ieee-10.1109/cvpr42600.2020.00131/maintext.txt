SECTION 1. Introduction: Before deployment to the real world, an artificial agent needs to undergo extensive testing in challenging simulated environments. Designing good simulators is thus extremely important. This is traditionally done by writing procedural models to generate valid and diverse scenes, and complex behavior trees that specify how each actor in the scene behaves and reacts to actions made by other actors, including the ego agent. However, writing simulators that encompass a large number of diverse scenarios is extremely time consuming and requires highly skilled graphics experts. Learning to simulate by simply observing the dynamics of the real world is the most scaleable way going forward.
Figure 1. If you look at the person on the left picture, you might think she is playing Pacman of Toru Iwatani, but she is not! She is actually playing with a GAN generated version of Pacman. In this paper, we introduce GameGAN that learns to reproduce games by just observing lots of playing rounds. Moreover, our model can disentangle background from dynamic objects, allowing us to create new games by swapping components as shown in the center and right images. 
A plethora of existing work aims at learning behavior models [2], [28], [16], [3]. However, these typically assume a significant amount of supervision such as access to agents' ground-truth trajectories. We aim to learn a simulator by simply watching an agent interact with an environment. To simplify the problem, we frame this as a 2D image generation problem. Given sequences of observed image frames and the corresponding actions the agent took, we wish to emulate image creation as if “rendered” from a real dynamic environment that is reacting to the agent's actions. We focus on graphics games as a proxy of the real environment. Our goal is to replace the graphics engine at test time, by visually imitating the game using a learned model. This is a challenging problem: different games have different number of components as well as different physical dynamics. Furthermore, many games require long-term consistency in the environment. For example, imagine a game where an agent navigates through a maze. When the agent moves away and later returns to a location, it expects the scene to look consistent with what it has encountered before. In visual SLAM, detecting loop closure (returning to a previous location) is already known to be challenging, let alone generating one. Last but not least, both deterministic and stochastic behaviors typically exist in a game, and modeling the latter is known to be particularly hard. In this paper, we introduce GameGAN, a generative model that learns to imitate a desired game. GameGAN ingests screenplay and keyboard actions during training and aims to predict the next frame by conditioning on the action, i.e. a key pressed by the agent. It learns from rollouts of image and action pairs directly without having access to the underlying game logic or engine. We make several advancements over the recently introduced World Model [11] that aims to solve a similar problem. While [11] employs a straightforward conditional decoder, GameGAN features a carefully designed architecture. In particular, we propose a new memory module that encourages the model to build an internal map of the environment, allowing the agent to return to previously visited locations with high visual consistency. Furthermore, we introduce a purposely designed decoder that learns to disentangle static and dynamic components within the image. This makes the behavior of the model more interpretable, and it further allows us to modify existing games by swapping out different components. We test GameGAN on a modified version of Pacman and the VizDoom environment [17], and propose several synthetic tasks for both quantitative and qualitative evaluation. We further introduce a come-back-home task to test the long-term consistency of learned simulators. Note that GameGAN supports several applications such as transferring a given game from one operating system to the other, without requiring to re-write code. Our GameGAN will be made available to human players, enabling them to play games with a GAN and modify them in creative ways. 

SECTION 2. Related Work: Generative Adversarial Networks: In GANs [9], a generator and a discriminator play an adverserial game that encourages the generator to produce realistic outputs. To obtain a desired control over the generated outputs, categorical labels [23], images [15], [21], captions [29], or masks [27] are provided as input to the generator. Works such as [33] synthesize new videos by transferring the style of the source to the target video using the cycle consistency loss [35], [18]. Note that this is a simpler problem than the problem considered in our work, as the dynamic content of the target video is provided and only the visual style needs to be modified. In this paper, we consider generating the dynamic content itself. We adopt the GAN framework and use the user-provided action as a condition for generating future frames. To the best of our knowledge, ours is the first work on using action-conditioned GANs for emulating game simulators. Video Prediction: Our work shares similarity to the task of video prediction which aims at predicting future frames given a sequence of previous frames. Several works [31], [5], [26] train a recurrent encoder to decode future frames. Most approaches are trained with a reconstruction loss, resulting in a deterministic process that generates blurry frames and often does not handle stochastic behaviors well. The errors typically accumulate over time and result in low quality predictions. Action-LSTM models [5], [26] achieved success in scaling the generated images to higher resolution but do not handle complex stochasticity present in environments like Pacman. Recently, [11], [7] proposed VAE-based frameworks to capture the stochasticity of the task. However, the resulting videos are blurry and the generated frames tend to omit certain details. GAN loss has been previously used in several works [8], [20], [32], [6]. [8] uses an adversarial loss to disentangle pose from content across different videos. In [20], VAE-GAN [19] formulation is used for generating the next frame of the video. Our model differs from these works in that in addition to generating the next frame, GameGAN also learns the intrinsic dynamics of the environment.
Figure 2. Overview of GameGAN: The dynamics engine takes at,zt,mt−1, and xt as input to update the hidden state at time t. Optionally, it can write to and read from the external memory module M (in the dashed box). Finally, the rendering engine is used to decode the output image xt+1. All modules are neural networks and trained end-to-end. 
World Models: In model-based reinforcement learning, one uses interaction with the environment to learn a dynamics model. World Models [11] exploit a learned simulated environment to train an RL agent instead. Recently, World Models have been used to generate Atari games in a concurrent work [1]. The key differences with respect to these models are in the design of the architecture: we introduce a memory module to better capture long-term consistency, and a carefully designed decoder that disentangles static and dynamic components of the game. 

SECTION 3. GameGAN: We are interested in training a game simulator that can model both deterministic and stochastic nature of the environment. In particular, we focus on an action-conditioned simulator in the image space where there is an egocentric agent that moves according to the given action at∼A at time t and generates a new observation xt+1. We assume there is also a stochastic variable zt∼N(0;I) that corresponds to randomness in the environment. Given the history of images x1:t along with at and zt, GameGAN predicts the next image xt+1. GameGAN is composed of three main modules. The dynamics engine (Sec 3.1), which maintains an internal state variable, takes at and zt as inputs and updates the current state. For environments that require longterm consistency, we can optionally use an external memory module (Sec 3.2). Finally, the rendering engine (Sec 3.3) produces the output image given the state of the dynamics engine. It can be implemented as a simple convolutional decoder or can be coupled with the memory module to disentangle static and dynamic elements while ensuring longterm consistency. We use adversarial losses along with a proposed temporal cycle loss (Sec 3.4) to train GameGAN. Unlike some works [11] that use sequential training for stability, GameGAN is trained end-to-end. We provide more details of each module in the supplementary materials.
Figure 3. Screenshots of a human playing with GameGAN trained on the official version of Pac-Man2. GameGAN learns to produce a visually consistent simulation as well as learning the dynamics of the game well. On the bottom row, the player consumes a capsule, turning the ghosts purple. Note that ghosts approach Pacman before consuming the capsule, and run away after. 
3.1. Dynamics Engine: GameGAN has to learn how various aspects of an environment change with respect to the given user action. For instance, it needs to learn that certain actions are not possible (e.g. walking through a wall), and how other objects behave as a consequence of the action. We call the primary component that learns such transitions the dynamics engine (see illustration in Figure 2). It needs to have access to the past history to produce a consistent simulation. Therefore, we choose to implement it as an action-conditioned LSTM [13], motivated by the design of Chiappa et al. [5]:
vt=ht−i⊙H(at,zt,mt−1),st=C(xt)it=σ(Wivvt+Wisst),ft=σ(Wfvvt+Wfsst),ot=σ(Wovvt+Wosst)ct=ft⊙ct−1+it⊙tanh(Wcvvt+Wcsst)ht=ot⊙tanh(ct)(1)(2)(3)(4)View Source\begin{gather*}
v_{t}= h_{t-i}\odot\mathcal{H}(a_{t}, z_{t}, m_{t-1}), s_{t}=\mathcal{C}(x_{t}) \tag{1}\\
i_{t}=\sigma(W^{iv} v_{t}+W^{is} s_{t}), f_{t}=\sigma(W^{fv} v_{t}+W^{fs} s_{t}),\tag{2}\\
o_{t}=\sigma(W^{ov} v_{t}+W^{os} s_{t})\\
c_{t}= f_{t}\odot c_{t-1}+ i_{t}\odot\tanh(W^{cv} v_{t}+W^{cs} s_{t}) \tag{3}\\
h_{t}= o_{t}\odot\tanh(c_{t}) \tag{4}
\end{gather*} where ht,at,zt,ct,xt are the hidden state, action, stochastic variable, cell state, image at time step t.mt−1 is the retrieved memory vector in the previous step (if the memory module is used), and it,ft,ot are the input, forget, and output gates. at,zt,mt−1 and ht are fused into vt, and st is the encoding of the image xt.H is a MLP, C is a convolutional encoder, and W are weight matrices. ⊙ denotes the hadamard product. The engine maintains the standard state variables for LSTM, ht and ct, which contain information about every aspect of the current environment at time t. It computes the state variables given at,zt,mt−1, and xt.
Figure 4. Visualizing attended memory location α: Red dots marking the center are placed to aid visualization. Note that we learn the memory shift, so the user action does not always align with how the memory is shifted. In this case, Right shifts α to the left, and Left shifts α to the right. It also learns not to shift when an invalid action is given. 
3.2. Memory Module: Suppose we are interested in simulating an environment in which there is an agent navigating through it. This requires long-term consistency in which the simulated scene (e.g. buildings, streets) should not change when the agent comes back to the same location a few moments later. This is a challenging task for typical models such as RNNs because 1) the model needs to remember every scene it generates in the hidden state, and 2) it is non-trivial to design a loss that enforces such long-term consistency. We propose to use an external memory module, motivated by the Neural Turing Machine (NTM) [10]. The memory module has a memory block M  ∈RN×N×D, and the attended location αt∈RN×N at time t.M contains N×ND-dimensional vectors where N is the spatial width and height of the block. Intuitively, αt is the current location that the egocentric agent is located at. M is initialized with random noise ∼N(0,I) and α0 is initialized with Os except for the center location (N/2,N/2) that is set to 1. At each time step, the memory module computes:
w=softmax(K(at))∈R3×3g=G(ht)∈Rαt=g⋅Conv2D(αt−1,w)+(1−g)⋅αt−1M=write(αt,E(ht),M)mt=read(αt,M)(5)(6)(7)(8)(9)View Source\begin{gather*}
w= \text{softmax} (\mathcal{K}(a_{t}))\in \mathbb{R}^{3\times 3} \tag{5}\\
g=\mathcal{G}(h_{t})\in \mathbb{R} \tag{6}\\
\alpha_{t}=g\cdot \text{Conv}2\mathrm{D}(\alpha_{t-1}, w)+(1-g)\cdot \alpha_{t-1} \tag{7}\\
\mathcal{M}= \text{write}(\alpha_{t}, \mathcal{E}(h_{t}), \mathcal{M}) \tag{8}\\
m_{t}= \text{read} (\alpha_{t}, \mathcal{M}) \tag{9}
\end{gather*} where K,G and E are small MLPs. w is a learned shift kernel that depends on the current action, and the kernel is used to shift αt−1. In some cases, the shift should not happen (e.g. cannot go forward at a dead end). With the help from ht, we also learn a gating variable g\in[0,1]g∈[0,1] that determines if \alphaα should be shifted or not. \mathcal{E}E is learned to extract information to be written from the hidden state. Finally, write and read operations softly access the memory location specified by \alphaα similar to other neural memory modules [10]. Using this shift-based memory module allows the model to not be bounded by the block \mathcal{M}M's size while enforcing local movements. Therefore, we can use any arbitrarily sized block at test time. Figure 4 demonstrates the learned memory shift. Since the model is free to assign actions to different kernels, the learned shift does not always correspond to how humans would do. We can see that Right is assigned as a left-shift, and hence Left is assigned as a right-shift. Using the gating variable gg, it also learns not to shift when an invalid action, such as going through a wall, is given.
Figure 5. Example showing how static and dynamic elements are disentangled in VizDoom and Pacman games with GameGAN. Static components usually include environmental objects such as walls. Dynamic elements typically are objects that can change as the game progresses such as food and other non-player characters. 
Enforcing long-term consistency in our case refers to remembering generated static elements (e.g. background) and retrieving them appropriately when needed. Accordingly, the benefit of using the memory module would come from storing static information inside it. Along with a novel cycle loss (Section 3.4.2), we introduce inductive bias in the architecture of the rendering engine (Section 3.3) to encourage the disentanglement of static and dynamic elements. 3.3. Rendering Engine: The (neural) rendering engine is responsible for rendering the simulated image x_{t+1}xt+1 given the state h_{t}ht. It can be simply implemented with standard transposed convolution layers. However, we also introduce a specialized rendering engine architecture (Figure 6) for ensuring long-term consistency by learning to produce disentangled scenes. In Section 4, we compare the benefits of each architecture.
Figure 6. Rendering engine for disentangling static and dynamic components. See sec 3.3 for details. 
The specialized rendering engine takes a list of vectors \mathbf{c}=\{c^{1},\ldots, c^{K}\}c={c1,…,cK} as input. In this work, we let K=2K=2, and \mathbf{c}=\{m_{t}, h_{t}\}c={mt,ht}. Each vector c^{k}ck corresponds to one type of entity and goes through the following three stages (see Fig 6). First, c^{k}ck is fed into convolutional networks to produce an attribute map A^{k}\in \mathbb{R}^{H_{1}\times H_{1}\times D_{1}}Ak∈RH1×H1×D1 and object map O^{k}\in \mathbb{R}^{H_{1}\times H_{1}\times 1}Ok∈RH1×H1×1. It is also fed into a linear layer to get the type vector v^{k}\in \mathbb{R}^{D_{1}}vk∈RD1 for the kk-th component. OO for all components are concatenated together and fed through either a sigmoid to ensure 0\leq O^{k}[x][y]\leq 10≤Ok[x][y]≤1 or a spatial softmax function so that \sum\nolimits_{k=1}^{K}O^{k}[x][y]=1∑Kk=1Ok[x][y]=1 for all x, yx,y. The resulting object map is multiplied by the type vector v_{k}vk in every location and fed into a convnet to produce R^{k}\in \mathbb{R}^{H_{2}\times H_{2}\times D_{2}}Rk∈RH2×H2×D2. This is a rough sketch of the locations where k-th type objects are placed. However, each object could have different attributes such as different style or color. Hence, it goes through the attribute stage where the tensor is transformed by a SPADE layer [27], [14] with the masked attribute map O^{k}\odot A^{k}Ok⊙Ak given as the contextual information. It is further fed through a few transposed convolution layers, and finally goes through an attention process similar to the rough sketch stage where concatenated components goes through a spatial softmax to get fine masks. The intuition is that after drawing individual objects, it needs to decide the “depth” ordering of the objects to be drawn in order to account for occlusions. Let us denote the fine mask as \eta^{k}ηk and the final tensor as X^{k}Xk. After this process, the final image is obtained by summing up all components, x= \sum\nolimits_{k=1}^{K}\eta^{k}\odot X^{k}x=∑Kk=1ηk⊙Xk. Therefore, the architecture of our neural rendering engine encourages it to extract different information from the memory vector and the hidden state with the help of temporal cycle loss (Section 3.4.2). We also introduce a version with more capacity that can produce higher quality images in Section A.5 of the supplementary materials. 3.4. Training GameGAN: Adversarial training has been successfully employed for image and video synthesis tasks. GameGAN leverages adversarial training to learn environment dynamics and to produce realistic temporally coherent simulations. For certain cases where long-term consistency is required, we propose temporal cycle loss that disentangles static and dynamic components to learn to remember what it has generated. 3.4.1 Adversarial LossesThere are three main components: single image discriminator, action discriminator, and temporal discriminator. Single Image DiscriminatorTo ensure each generated frame is realistic, the single image discriminator and GameGAN simulator play an adversarial game. Action-Conditioned DiscriminatorGameGAN has to reflect the actions taken by the agent faithfully. We give three pairs to the action-conditioned discriminator: (x_{t}, x_{t+1}, a_{t}), (x_{t}, x_{t+1},\bar{a}_{t})(xt,xt+1,at),(xt,xt+1,a¯t) and (\hat{x}_{t},\hat{x}_{t+1}, a_{t}). x_{t}(x^t,x^t+1,at).xt denotes the real image, \hat{x}_{t}x^t the generated image, and \bar{a}_{t}\in \mathcal{A}a¯t∈A a sampled negative action \bar{a}_{t}\neq a_{t}a¯t≠at. The job of the discriminator is to judge if two frames are consistent with respect to the action. Therefore, to fool the discriminator, GameGAN has to produce realistic future frame that reflects the action. Temporal DiscriminatorDifferent entities in an environment can exhibit different behaviors, and also appear or disappear in partially observed states. To simulate a temporally consistent environment, one has to take past information into account when generating the next states. Therefore, we employ a temporal discriminator that is implemented as 3D convolution networks. It takes several frames as input and decides if they are a real or generated sequence. Since conditional GAN architectures [22] are known for learning simplified distributions ignoring the latent code [34], [30], we add information regularization [4] that maximizes the mutual information I(z_{t},\phi(x_{t}, x_{t+1}))I(zt,ϕ(xt,xt+1)) between the latent code z_{t}zt and the pair (x_{t}, x_{t+1}xt,xt+1). To help the action-conditioned discriminator, we add a term that minimizes the cross entropy loss between a_{t}at and a_{t}^{pred}=\psi(x_{t+1}, x_{t})apredt=ψ(xt+1,xt). Both \phiϕ and \psiψ are MLP that share layers with the action-conditioned discriminator except for the last layer. Lastly, we found adding a small reconstruction loss in image and feature spaces helps stabilize the training (for feature space, we reduce the distance between the generated and real frame's single image discriminator features). A detailed descriptions are provided in the supplementary material. 3.4.2 Cycle LossRNN based generators are capable of keeping track of the recent past to generate coherent frames. However, it quickly forgets what happened in the distant past since it is encouraged simply to produce realistic next observation. To ensure long-term consistency of static elements, we leverage the memory module and the rendering engine to disentangle static elements from dynamic elements.
Figure 7. Samples from datasets studied in this work. For Pacman and Pacman-Maze, training data consists of partially observed states, shown in the red box. Left: Pacman, center: Pacman-Maze, right: VizDoom 
After running through some time steps TT, the memory block \mathcal{M}M is populated with information from the dynamics engine. Using the memory location history \alpha_{t}αt, we can retrieve the memory vector \hat{m}_{t}m^t which could be different from m_{t}mt if the content at the location \alpha_{t}αt has been modified. Now, c=\{\hat{m}_{t}, 0\}c={m^t,0} is passed to the rendering engine to produce X^{\hat{m}_{t}}Xm^t where 0 is the zero vector and X^{\hat{m}_{t}}Xm^t is the output component corresponding to \hat{m}_{t}m^t. We use the following loss:
\begin{equation*}
L_{cycle}= \sum\limits_{t}^{T}\Vert X^{m_{t}}-X^{\hat{m}_{t}}\Vert \tag{10}
\end{equation*}Lcycle=∑tT∥Xmt−Xm^t∥(10)View Source\begin{equation*}
L_{cycle}= \sum\limits_{t}^{T}\Vert X^{m_{t}}-X^{\hat{m}_{t}}\Vert \tag{10}
\end{equation*} As dynamic elements (e.g. moving ghosts in Pacman) do not stay the same across time, the engine is encouraged to put static elements in the memory vector to reduce L_{cycle}Lcycle. Therefore, long-term consistency is achieved. To prevent the trivial solution where the model tries to ignore the memory component, we use a regularizer that minimizes the sum of all locations in the fine mask \min\sum\eta^{k} from the hidden state vector so that X^{m_{t}} has to contain content. Another trivial solution is if shift kernels for all actions are learned to never be in the opposite direction of each other. In this case, \hat{m}_{t} and m_{t} would always be the same because the same memory location will never be revisited. Therefore, we put a constraint that for actions a with a negative counterpart \hat{a} (e.g. Up and Down), \hat{a}'s shift kernel \mathcal{K}(\hat{a}) is equal to horizontally and vertically flipped \mathcal{K}(a). Since most simulators that require long-term consistency involves navigation tasks, it is trivial to find such counterparts. 3.4.3 Training SchemeGameGAN is trained end-to-end. We employ a warm-up phase where real frames are fed into the dynamics engine for the first few epochs, and slowly reduce the number of real frames to 1 (the initial frame x_{0} is always given). We use 18 and 32 frames for training GameGAN on Pacman and Vizddom environments, respectively. 

SECTION 4. Experiments: We present both qualitative and quantitative experiments. We mainly consider four models: 1) Action-LSTM: model trained only with reconstruction loss which is in essence similar to [5], 2) World Model [11], 3) GameGAN-M: our model without the memory module and with the simple rendering engine, and 4) GameGAN: the full model with the memory module and the rendering engine for disentanglement. Experiments are conducted on the following three datasets (Figure 7):
Figure 8. Rollout of models from the same initial screen. Action-LSTM trained with reconstruction loss produces frames without refined details (e.g. Foods). World model has difficulty keeping temporal consistency, resulting in occasional significant discontinuities. GameGAN can produce consistent simulation. 
Pacman: We use a modified version of the Pacman game3 in which the Pacman agent observes an egocentric 7\times 7 grid from the full 14\times 14 environment. The environment is randomly generated for each episode. This is an ideal environment to test the quality of a simulator since it has both deterministic (e.g., game rules & viewpoint shift) and highly stochastic components (e.g., game layout of foods and walls; game dynamics with moving ghosts). Images in the episodes are 84\times 84 and the action space is \mathcal{A}=\{left, right, up, down, stay\}. 45K episodes of length greater than or equal to 18 are extracted and 40K are used for training. Training data is generated by using a trained DQN [25] agent that observes the full environment with high entropy to allow exploring diverse action sequences. Each episode consists of a sequence of 7\times 7 Pacman-centered grids along with actions. Pacman-Maze: This game is similar to Pacman except that it does not have ghosts, and its walls are randomly generated from a maze-generation algorithm, thus are structured better. The same number of data is used as Pacman. Vizdoom We follow the experiment set-up of Ha and Schmidhuber [11] that uses takecover mode of the VizDoom platform [17]. Training data consists of 10k episodes extracted with random policy. Images in the episodes are 64\times 64 and the action space is \mathcal{A}=\{left, right, stay\} 4.1. Qualitative Evaluation: Figure 8 shows rollouts of different models on the Pacman dataset. Action-LSTM, which is trained only with reconstruction loss, produces blurry images as it fails to capture the multi-modal future distribution, and the errors accumulate quickly. World model [11] generates realistic images for VizDoom, but it has trouble simulating the highly stochastic Pacman environment. In particular, it sometimes suffers from large unexpected discontinuities (e.g. t=0 to t=1). On the other hand, GameGAN produces temporally consistent and realistic sharp images. GameGAN consists of only a few convolution layers to roughly match the number of parameters of World Model. We also provide a version of GameGAN that can produce higher quality images in the supplementary materials Section A.5. Disentangling Static & Dynamic ElementsOur GameGAN with the memory module is trained to disentangle static elements from dynamic elements. Figure 5 shows how walls from the Pacman environment and the room from the VizDoom environment are separated from dynamic objects such as ghosts and fireballs. With this, we can make interesting environments in which each element is swapped with other objects. Instead of the depressing room of VizDoom, enemies can be placed in the user's favorite place, or alternatively have Mario run around the room (Figure 9). We can swap the background without having to modify the code of the original games. Our approach treats games as a black box and learns to reproduce the game, allowing us to easily modify it. Disentangled models also open up many promising future directions that are not possible with existing models. One interesting direction would be learning multiple disentangled models and swapping certain components. As the dynamics engine learns the rules of an environment and the rendering engine learns to render images, simply learning a linear transformation from the hidden state of one model to make use of the rendering engine of the other could work. Pacman-Maze GenerationGameGAN on the Pacman-Maze produces a partial grid at each time step which can be connected to generate the full maze. It can generate realistic walls, and as the environment is sufficiently small, GameGAN also learns the rough size of the map and correctly draws the rectangular boundary in most cases. One failure case is shown in the bottom right corner of Figure 10, that fails to close the loop.
Figure 9. GameGAN on Pacman and VizDoom with swapping background/foreground with random images. 
Figure 10. Generated mazes by traversing with a Pacman agent on GameGAN model. Most mazes are realistic. Right shows a failure case that does not close the loop correctly. 
4.2. Task 1: Training an RL Agent: Quantitatively measuring environment quality is challenging as the future is multi-modal, and the ground truth future does not exist. One way of measuring it is through learning a reinforcement learning agent inside the simulated environment and testing the trained agent in the real environment. The simulated environment should be sufficiently close to the real one to do well in the real environment. It has to learn the dynamics, rules, and stochasticity present in the real environment. The agent from the better simulator that closely resembles the real environment should score higher. We note that this is closely related to model-based RL. Since GameGAN do not internally have a mechanism for denoting the game score, we train an external classifier. The classifier is given N previous image frames and the current action to produce the output (e.g. Win/Lose). PacmanFor this task, the Pacman agent has to achieve a high score by eating foods (+0.5 reward) and capturing the flag (+1 reward). It is given −1 reward when eaten by a ghost, or the maximum number of steps (40) are used. Note that this is a challenging partially-observed reinforcement learning task where the agent observes 7\times 7 grids. The agents are trained with A3C [24] with an LSTM component. VizDoomWe use the Covariance Matrix Adaptation Evolution Strategy [12] to train RL agents. Following [11], we use the same setting with corresponding simulators.
Table 1. Numbers are reported as mean scores ± standard deviation. Higher is better. For Pacman, an agent trained in real environment achieves 3.02 \pm 2.64 which can be regarded as the upper bound. VizDoom is considered solved when a score of 750 is achieved.
Table 1 shows the results. For all experiments, scores are calculated over 100 test environments, and we report the mean scores along with standard deviation. Agents trained in Action-LSTM simulator performs similar to the agents with random policy, indicating the simulations are far from the real ones. On Pacman, GameGAN-M shows the best performance while GameGAN and WorldModel have similar scores. VizDoom is considered solved when a score of 750 is achieved, and GameGAN solves the game. Note that World Model achieves a higher score, but GameGAN is the first work trained with a GAN framework that solves the game. Moreover, GameGAN can be trained end-to-end, unlike World Model that employs sequential training for stability. One interesting observation is that GameGAN shows lower performance than GameGAN-M on the Pacman environment. This is due to having additional complexity in training the model where the environments do not need long-term consistency for higher scores. We found that optimizing the GAN objective while training the memory module was harder, and this attributes to RL agents exploiting the imperfections of the environments to find a way to cheat. In this case, we found that GameGAN sometimes failed to prevent agents from walking through the walls while GameGAN-M was nearly perfect. This led to RL agents discovering a policy that liked to hit the walls, and in the real environment, this often leads to premature death. In the next section, we show how having long-term consistency can help in certain scenarios.
Figure 11. Come-back-home task rollouts. The forward rows show the path going from the initial position to the goal position. The backward rows show the path coming back to the initial position. Only the full GameGAN can successfully recover the initial position. 
Figure 12. Box plot for come-back-home metric. Lower is better. As a reference, a pair of randomly selected frames from the same episode gives a score of 1.17\pm 0.56 
4.3. Task 2: Come-Back-Home: This task evaluates the long-term consistency of simulators in the Pacman-Maze environment. The Pacman starts at a random initial position (x_{A}, y_{A}) with state s. It is given K random actions (a_{1}, \ldots, a_{K}), ending up in position (x_{B}, y_{B}). Using the reverse actions (\hat{a}_{K}, \ldots,\hat{a}_{1}) (e.g. a_{k}= Down, \hat{a}_{k}=Up), it comes back to the initial position (x_{A}, y_{A}), resulting in state \hat{s}. Now, we can measure the distance d between \hat{s} and s to evaluate long-term consistency (d=0 for the real environment). As elements other than the wall (e.g. food) could change, we only compare the walls of \hat{s} and s. Hence, s is an 84\times 84 binary image whose pixel is 1 if the pixel is blue. We define the metric d as
\begin{equation*}
d= \frac{\text{sum}(\text{abs}(s-\hat{s}))}{\text{sum}(s)+1} \tag{11}
\end{equation*}View Source\begin{equation*}
d= \frac{\text{sum}(\text{abs}(s-\hat{s}))}{\text{sum}(s)+1} \tag{11}
\end{equation*} where sum() counts the number of 1s. Therefore, d measures the ratio of the number of pixels changed to the initial number of pixels. Figure 12 shows the results. We again observe occasional large discontinuities in World Model that hurts the performance a lot. When K is small, the differences in performance are relatively small. This is because other models also have short-term consistency realized through RNNs. However, as K becomes larger, GameGAN with memory module steadily outperforms other models, and the gaps become larger, indicating GameGAN can make efficient use of the memory module. Figure 11 shows the rollouts of different models in the Pacman-Maze environment. As it can be seen, models without the memory module do not remember what it has generated before. This shows GameGAN opens up promising directions for not only game simulators, but as a general environment simulator that could mimic the real world. 

SECTION 5. Conclusion: We propose GameGAN which leverages adversarial training to learn to simulate games. GameGAN is trained by observing screenplay along with user's actions and does not require access to the game's logic or engine. GameGAN features a new memory module to ensure long-term consistency and is trained to separate static and dynamic elements. Thorough ablation studies showcase the modeling power of GameGAN. In future works, we aim to extend our model to capture more complex real-world environments.