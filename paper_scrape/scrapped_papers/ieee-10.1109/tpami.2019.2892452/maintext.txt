SECTION 1 Introduction: Recently, estimating 3D full-body human poses from monocular RGB imagery has attracted substantial academic interests for its vast potential on human-centric applications, including human-computer interactions [1], surveillance [2], and virtual reality [3]. In fact, estimating human pose from images is quite challenging with respect to large variances in human appearances, arbitrary viewpoints, invisibilities of body parts. Besides, the 3D articulated pose recovery from monocular imagery is considerably more difficult since 3D poses are inherently ambiguous from a geometric perspective [4], as shown in Fig. 1. Fig. 1. 
Some visual results of our approach on the Human3.6M benchmark [5]. (a) illustrates the intermediate 3D poses estimated by the 2D-to-3D pose transformer module, (b) denotes the final 3D poses refined by the 3D-to-2D pose projector module, and (c) denotes the ground-truth. The estimated 3D joints are reprojected into the images and shown by themselves from the side view (next to the images). As shown, the predicted 3D poses in (b) have been significantly corrected, compared with (a). Best viewed in color. Note that, red and green indicate left and right, respectively. 
Recently, notable successes have been achieved for 2D pose estimation based on 2D part models coupled with 2D deformation priors [6], [7], and the deep learning techniques  [8], [9], [10], [11]. Driven by these successes, some 3D pose estimation works [12], [13], [14], [15], [16], [17] attempt to leverage the state-of-the-art 2D pose network architectures (e.g., Convolutional Pose Machines (CPM) [10] and Stacked Hourglass Networks [18]) by combing the image-based 2D part detectors, 3D geometric pose priors and temporal models. These attempts mainly follow three types of pipelines. The first type [19], [20], [21] focuses on directly recovering 3D human poses from 2D input images by utilizing the state-of-the-art 2D pose network architecture to extract 2D pose-aware features with separate techniques and prior knowledge. In this way, these methods can employ sufficient 2D pose annotations to improve the shared feature representation of the 3D pose and 2D pose estimation tasks. The second type [16], [22], [23] concentrates on learning a 2D-to-3D pose mapping function. Specifically, the methodsbelonging to this kind first extract 2D poses from 2D input images and further perform 3D pose reconstruction/regression based on these 2D pose predictions. The third type [24], [25], [26] aims at integrating the Skinned Multi-Person Linear (SMPL) model [27] within a deep network to reconstruct 3D human pose and shape in a full 3D mesh of human bodies. Although having achieved a promising performance, all of these kinds suffer from the heavy computational cost by using the time-consuming network architecture (e.g., ResNet-50 [28]) and limited scalability for all scenarios due to the insufficient 3D pose data. To address the above-mentioned issues and utilize the sufficient 2D pose data for training, we propose an effective yet efficient 3D human pose estimation framework, which implicitly learns to integrate the 2D spatial relationship, temporal coherency and 3D geometry knowledge by utilizing the advantages afforded by Convolutional Neural Networks (CNNs) [10] (i.e., the ability to learn feature representations for both image and spatial context directly from data), recurrent neural networks (RNNs) [29] (i.e., the ability to model the temporal dependency and prediction smoothness) and the self-supervised correction (i.e., the ability to implicitly retain 3D geometric consistency between the 2D projections of 3D poses and the predicted 2D poses). Concretely, our model employs a sequential training to capture long-range temporal coherency among multiple human body parts, and it is further enhanced via a novel self-supervised correction mechanism, which involves two dual learning tasks, i.e., 2D-to-3D pose transformation and 3D-to-2D pose projection, to generate geometrically consistent 3D pose predictions under a self-supervised correction mechanism, i.e., forcing the 2D projections of the generated 3D poses to be identical to the estimated 2D poses. As illustrated in Fig. 1, our model enables the gradual refinement of the 3D pose prediction for each frame according to the coherency of sequentially predicted 2D poses and 3D poses, contributing to seamlessly learning the pose-dependent constraints among multiple body parts and sequence-dependent context from the previous frames. Specifically, taking each frame as input, our model first extracts the 2D pose representations and predicts the 2D poses. Then, the 2D-to-3D pose transformer module is injected to transform the learned pose representations from the 2D domain to the 3D domain, and it further regresses the intermediate 3D poses via two stacked long short-term memory (LSTM) layers by combining the following two lines of information, i.e., the transformed 2D pose representations and the learned states from past frames. Intuitively, the 2D pose representations are conditioned on the monocular image, which captures the spatial appearance and context information. Then, temporal contextual dependency is captured by the hidden states of LSTM units, which effectively improves the robustness of the 3D pose estimations over time. Finally, the 3D joint prediction implicitly encodes the 3D geometric structural information by the 3D-to-2D pose projector module under the introduced self-supervised correction mechanism. In specific, considering that the 2D projections of 3D poses and the predicted 2D poses should be identical, the minimization of their dissimilarities is regarded as a learning objective for the 3D-to-2D pose projector module to bidirectionally correct (or refine) the intermediate 3D pose predictions. Through this self-supervised correction mechanism, our model is capable of effectively achieving geometrically coherent 3D human pose predictions without requesting additional 3D pose annotations. Therefore, our introduced correction mechanism is self-supervised, and can enhance our model by adding the external large-scale 2D human pose data into the training process to cost-effectively increase the 3D pose estimation performance. The main contributions of this work are three-fold. i) We present a novel model that learns to integrate rich spatial and temporal long-range dependencies as well as 3D geometric constraints, rather than relying on specific manually defined body smoothness or kinematic constraints; ii) Developing a simple yet effective self-supervised correction mechanism to incorporate 3D pose geometric structural information is innovative in literature, and may also inspire other 3D vision tasks; iii) The proposed self-supervised correction mechanism enables our model to significantly improve 3D human pose estimation via sufficient 2D human pose data. Extensive evaluations on the public challenging Human3.6M [5] and HumanEva-I [30] benchmarks demonstrate the superiority of our framework over all the compared competing methods. The remainder of this paper is organized as follows. Section 2 briefly reviews the existing 3D human pose estimation approaches that motivate this work. Section 3 presents the details of the proposed model, with a thorough analysis of every component. Section 4 presents the experimental results on two public benchmarks with comprehensive evaluation protocols, as well as comparisons with competing alternatives. Finally, Section 5 concludes this paper. 

SECTION 2 Related Work: Considerable research has addressed the challenge of 3D human pose estimation. Early research on 3D monocular pose estimation from videos involved frame-to-frame pose tracking and dynamic models that rely on Markov dependencies among previous frames, e.g., [31], [32]. The main drawbacks of these approaches are the requirement of the initialization pose and the inability to recover from track- ing failure. To overcome these drawbacks, more recent approaches [12], [33] focus on detecting candidate poses in each individual frame, and a post-processing step attempts to establish temporally consistent poses. Yasin et al. [22] proposed a dual-source approach for 3D pose estimation from a single image. They combined the 3D pose data from a motion capture system with an image source annotated with 2D poses. They transformed the estimation into a 3D pose retrieval problem. One major limitation of this approach is its time efficiency. Processing an image requires more than 20 seconds. Sanzari et al. [34] proposed a hierarchical Bayesian non-parametric model, which relies on a representation of the idiosyncratic motion of human skeleton joint groups, and the consistency of the connected group poses is considered during the pose reconstruction. Deep learning has recently demonstrated its capabilities in many computer vision tasks, such as 3D human pose estimation. Li and Chan [35] first used CNNs to regress the 3D human pose from monocular images and proposed two training strategies to optimize the network. Li et al. [36] proposed integrating the structure learning into a deep learning framework, which consists of a convolutional neural network to extract image features and two following subnetworks to transform the image features and poses into a joint embedding. Tekin et al. [15] proposed exploiting motion information from consecutive frames and applied a deep learning network to regress the 3D pose. Zhou et al. [14] proposed a 3D pose estimation framework from videos that consists of a novel synthesis among a deep-learning-based 2D part detector, a sparsity-driven 3D reconstruction approach and a 3D temporal smoothness prior. Zhou et al. [4] proposed directly embedding a kinematic object model into the deep learning network. Du et al. [37] introduced additional built-in knowledge for reconstructing the 2D pose and formulated a new objective function to estimate the 3D pose from the detected 2D pose. More recently, Zhou et al. [19] presented a coarse-to-fine prediction scheme to cast 3D human pose estimation as a 3D keypoint localization problem in a voxel space in an end-to-end manner. Moreno-Noguer et al. [38] formulated the 3D human pose estimation problem as a regression between matrices encoding 2D and 3D joint distances. Chen et al. [16] proposed a simple approach to 3D human pose estimation by performing 2D pose estimation followed by 3D exemplar matching. Tome et al. [20] proposed a multi-task framework to jointly integrate 2D joint estimation and 3D pose reconstruction to improve both tasks. To leverage the well-annotated large-scale 2D pose datasets, Zhou et al. [23] proposed a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a unified deep two-stage cascaded structure network. However, these methods oversimplify the 3D geometric knowledge. In contrast to all these aforementioned methods, our model can leverage a lightweight network architecture to implicitly learn to integrate the 2D spatial relationship, temporal coherency and 3D geometry knowledge in a fully differential manner. Instead of directly computing 2D and 3D joint locations, several works concentrate on producing a 3D mesh body representation by using a CNN to predict Skinned Multi-Person Linear model [27]. For instance, Omran et al. [25] proposed to integrate a statistical body model within a CNN, leveraging reliable bottom-up semantic body part segmentation and robust top-down body model constraints. Kanazawa et al. [26] presented an end-to-end adversarial learning framework for recovering a full 3D mesh model of a human body by parameterizing the mesh in terms of 3D joint angles and a low dimensional linear shape space. Furthermore, this method employs the weak-perspective camera model to project the 3D joints onto the annotated 2D joints via an iterative error feedback loop [39]. Similar to our proposed method, these approaches also regard the in-the-wild images with 2D ground-truth as the supervision to improve the model performance. The main difference is that our self-supervised learning method is more flexible and robust without relying on the assumption of the weak-perspective camera model. Our approach is close to [20], which also used the projection from the 3D space to the 2D space to improve the 3D pose estimation performance. However, there are two main differences between [20] and our model: i) The definition of the 3D-to-2D projection function and the optimization strategy. Rather than explicitly defining a concrete model, our 3D-to-2D projection is implicitly learned in a completely data-driven manner. However, the projection of 3D poses in [20] is explicitly modeled by using a weak perspective model, which consists of the orthographic projection matrix, a known external camera calibration matrix and an unknown rotation matrix. As claimed in [20], this explicit model is prone to sticking in local minima during the training. Thus, the authors have to quantize over the space of possible rotations. Through this approximation, their model performance may suffer from the fixed choices of rotations; ii) The way of utilizing the projected 2D pose. In contrast to [20] which learns to weightily fuse the projected 2D and the estimated 2D poses for further regressing the final 3D pose, our model exploits the 3D geometric consistency between the projected 2D and the estimated 2D poses to bidirectionally refine the intermediate 3D pose predictions. Self-Supervised Learning. Aiming at training the feature representation without relying on manual data annotation, self-supervised learning (SSL) has first been introduced in [40] for vowel class recognition, and further extended for object extraction in [41]. Recently, plenty of SSL methods (e.g., [42], [43]) have been proposed. For instance, [42] investigated multiple self-supervised methods to encourage the network to factorize the information in its representation. In contrast to these methods that focus on learning an optimal visual representation, our work considers the self-supervision as an optimization guidance for 3D pose estimation. Note that a preliminary version of this work was published in [21], which uses multiple stages to gradually refine the predicted 3D poses. The network parameters in the multiple stages are recurrently trained in a fully end-to-end manner. However, the multi-stage mechanism results in a heavy computational cost, and the stage-by-stage improvement is less significant as the number of stages increases. In this paper, we inherit its idea of integrating the 2D spatial relationship, temporal coherency as well as 3D geometry knowledge, and we further impose a novel self-supervised correction mechanism to further enhance our model by bridging the domain gap between the 3D and 2D human poses. Specifically, we develop a 3D-to-2D pose projector module to replace the multi-stage refinement to correct the intermediate 3D pose predictions by retaining the 3D geometric consistency between their 2D projections and the predicted 2D poses. Therefore, the imposed correction mechanism enables us to leverage the external large-scale 2D human pose data to boost 3D human pose estimation. Moreover, more comparisons with competing approaches and more detailed analyses of the proposed modules are included to further verify our statements. 

SECTION 3 3D Human Pose Machine: We propose a 3D human pose machine to resolve 3D pose sequence generation for monocular frames, and we introduce a concise self-supervised correction mechanism to enhance our model by retaining the 3D geometric consistency. After extracting the 2D pose representation and estimating the 2D poses for each frame via a common 2D pose sub-network, our model employs two consecutive modules. The first module is the 2D-to-3D pose transformer module for transforming the 2D pose-aware features from the 2D domain to the 3D domain. This module is designed to estimate intermediate 3D poses for each frame by incorporating temporal dependency in the image sequence. The second module is the 3D-to-2D pose projector module for bidirectionally refining the intermediate 3D pose prediction via our introduced self-supervised correction mechanism. These two modules are combined in a unified framework to be optimized in a fully end-to-end manner. As illustrated in Fig. 2, our model performs the sequential refinement with self-supervised correction to generate the 3D pose sequence. Specifically, the tth frame It is passed into the 2D pose sub-network ΨR, the 2D-to-3D pose transformer module ΨT, and the 3D-to-2D projector module {ΨC,ΨP} to predict the final 3D poses. The 2D pose sub-network is stacked by convolutional and fully connected layers, and the 2D-to-3D pose transformer module contains two LSTM layers to capture the temporal dependency over frames. Specifically, given the input image sequence with N frames, the 2D pose sub-network ΨR is first employed to extract the 2D pose-aware features f2dt and predict the 2D pose p2dt for the tth frame of the input sequence. Then, the extracted 2D pose-aware features f2dt are further fed into the 2D-to-3D pose transformer module ΨT to obtain the intermediate 3D pose p3dt, where ΨT is composed of the hidden states Ht−1 learned from the past frames. Finally, the predicted 2D poses p2dt and intermediate 3D pose p3dt are fed into the 3D-to-2D projector module with two functions, i.e., ΨC and ΨP, to obtain the final 3D poses p^∗3dt. Considering that most existing 2D human pose data are still images without temporal orders, we additionally introduce a simple yet effective regression function ΨC to transform the intermediate 3D pose vector p3dt into a changeable prediction p^3dt. The projection function ΨP implies projecting the 3D coordinate p^3dt into the image plane to obtain the projected 2D pose p^2dt. Formally, f2dt, p3dt, p^3dt, and p^2dt are formulated as follows:

{f2dt,p2dt}p3dtp^3dtp^2dt=ΨR(It;ωR),=ΨT(f2dt;ωT,Ht−1),=ΨC(p3dt;ωC),=ΨP(p^3dt;ωP),(1)
View Source
\begin{align*}
\lbrace f^{2d}_{t}, p_{t}^{2d} \rbrace &= \Psi _R(I_t; \omega _R),\\
p_{t}^{3d} &= \Psi _T(f^{2d}_{t}; \omega _T, H_{t-1}),\\
\hat{p}_{t}^{3d} &= \Psi _C(p_{t}^{3d}; \omega _C),\\
\hat{p}_{t}^{2d} &= \Psi _P(\hat{p}_{t}^{3d}; \omega _P),
\tag{1}\end{align*}

where ωR, ωT, ωC and ωP are parameters of ΨR,ΨT,ΨC and ΨP, respectively. Note that, H0 is initially set to be a vector of zeros. After obtaining the predicted 2D pose p2dt via ΨR, and the projected 2D pose p^2dt via ΨP in Eq. (1), we consider minimizing the dissimilarity between p2dt and p^2dt as an optimization objective to obtain the optimal p^∗3dt for the tth frame. Fig. 2. 
An overview of the proposed 3D human pose machine framework. Our model predicts the 3D human poses for the given monocular image frames, and it progressively refines its predictions with the proposed self-supervised correction. Specifically, the estimated 2D pose p2dt with the corresponding pose representation f2dt for each frame of the input sequence is first obtained and further passed into two neural network modules: i) a 2D-to-3D pose transformer module for transforming the pose representations from the 2D domain to the 3D domain to intermediately predict the human joints p3dt in the 3D coordinates, and ii) a 3D-to-2D pose projector module to obtain the projected 2D pose p^2dt after regressing p_t^{3d}p3dt into \hat{p}_t^{3d}. Through minimizing the difference between p_t^{2d} and \hat{p}_t^{2d}, our model is capable of bidirectionally refining the regressed 3D poses \hat{p}_t^{3d} via the proposed self-supervised correction mechanism. Note that the parameters of the 2D-to-3D pose transformer module for all frames are shared to preserve the temporal motion coherence. 3K and 2K denotes the dimension of the vector for representing the 3D and 2D human pose formed by K skeleton joints, respectively. 
In the following, we will introduce more details of our model and provide comprehensive clarifications to make the work easier to understand. The corresponding algorithm for jointly training these modules will also be discussed at the end. 3.1 2D Pose Sub-Network: The objective of the 2D pose sub-network is to encode each frame in a given monocular sequence with a compact representation of the pose information, e.g., the body shape of the human. The shallow convolution layers often extract the common low-level information, which is a very basic representation of the human image. We build our 2D pose sub-network by borrowing the architecture of the convolutional pose machines [10]. Please see Table 1 for more details. Note that other state-of-the-art architectures for 2D pose estimation can be also utilized. As illustrated in Fig. 2, the 2D pose sub-network takes the 368\times 368 image as input, and it outputs the 2D pose-aware feature maps with a size of 128\times 46\times 46 and the predicted 2D pose vectors with 2K entries being the argmax positions of these feature maps. TABLE 1 
Details of the Convolutional Layers in the 2D Pose Sub-Network
3.2 2D-to-3D Pose Transformer Module: Based on the features extracted by the 2D pose sub-network, the 3D pose transformer module is employed to adapt the 2D pose-aware features in an adapted feature space for the later 3D pose prediction. As depicted in Fig. 2a, two convolutional layers and one fully connected layer are leveraged. Each convolutional layer contains 128 different kernels with a size of 5\times 5 and a stride of 2, and a max pooling layer with a 2\times 2 kernel size and a stride of 2 is appended on the convolutional layers. Finally, the convolution features are fed to a fully connected layer with 1,024 units to produce the adapted feature vector. In this way, the 2D pose-aware features are transformed into the 1,024-dimensional adapted feature vector. Given the adapted features for all frames, we employ LSTM to sequentially predict the 3D pose sequence by incorporating rich temporal motion patterns among frames as [21]. Note that, LSTM [29] has been proven to achieve better performance in exploiting temporal correlations than a vanilla recurrent neural network in many tasks, e.g., speech recognition [44] and video description [45]. In our model, we use the LSTM layers to capture the temporal dependency in the monocular sequence for refining the 3D pose prediction for each frame. As illustrated in Fig. 2b, our model employs two LSTM layers with 1,024 hidden cells and an output layer that predicts the locations of K joint points of the human. In particular, the hidden states learned by the LSTM layers are capable of implicitly encoding the temporal dependency across different frames of the input sequence. As formulated in Eq. (1), incorporating the previous hidden states imparts our model with the ability to sequentially refine the pose predictions. 3.3 3D-to-2D Projector Module: As illustrated in Fig. 3a, this module consists of six fully connected (FC) layers containing ReLU and batch normalization operations. As one can see from left to right in Fig. 3a, the first two FC layers (denoted in blue) define the regression function \Psi _C in which the intermediate 3D pose predictions are regressed into the pose prediction \hat{p}_{t}^{3d}, and the remaining four FC layers (denoted in yellow) with 1,024 units represent the projection function \Psi _P that projects \hat{p}_{t}^{3d} into the image plane to obtain the projected 2D pose \hat{p}_{t}^{2d}. Moreover, an identical mapping as ResNet [28] is used inside \Psi _P to make the information pass through quickly to avoid overfitting. Therefore, our 3D-to-2D projector module is simple yet powerful for both regression and projection tasks. Considering the self-corrected 3D pose may need to be discarded sometimes, we regard the regression function \Psi _C as a copy to be corrected for the intermediate 3D poses. Fig. 3. 
Detailed sub-network architecture of our proposed 3D-to-2D pose projector module in the (a) training phase and (b) testing phase. The Fully Connected (FC) layers for the regression function are in blue, while those for the projection function are in yellow. The black arrows represent the forward data flow, while the dashed arrows denote the backward propagation used to update the network parameters and perform gradual pose refinement in (a) and (b), respectively. 
In the training phase, we first initialize the module parameters {\omega _C, \omega _P} for \Psi _C and \Psi _P via the supervision of the 3D and 2D ground-truth poses from 3D human pose data as illustrated in Fig. 3a, respectively. The optimization function is

\begin{equation*}
\mathop{\min} _{\lbrace \omega _C, \omega _P\rbrace } \sum _{t=1}^{N} \left\Vert \hat{p}_t^{3d} - p_t^{3d(gt)} \right\Vert _{2}^{2} + \left\Vert \Psi _P(\hat{p}_t^{3d}; \omega _P) - p_{t}^{2d(gt)} \right\Vert _{2}^{2},
\tag{2}\end{equation*}
View Source
\begin{equation*}
\mathop{\min} _{\lbrace \omega _C, \omega _P\rbrace } \sum _{t=1}^{N} \left\Vert \hat{p}_t^{3d} - p_t^{3d(gt)} \right\Vert _{2}^{2} + \left\Vert \Psi _P(\hat{p}_t^{3d}; \omega _P) - p_{t}^{2d(gt)} \right\Vert _{2}^{2},
\tag{2}\end{equation*}

where \hat{p}_t^{3d} is the regressed 3D pose via \Psi _C in Eq. (1), and its 2D projection is \hat{p}_t^{2d}=\Psi _P(\hat{p}_t^{3d}; \omega _P). Eq. (2) forces \Psi _C to regress \hat{p}_t^{3d} from intermediate 3D poses p_t^{3d} to the 3D pose ground-truth p_t^{3d(gt)}, and it further forces the output of \Psi _P, i.e., the projected 2D poses, to be similar to the 2D pose ground-truth p_{t}^{2d(gt)}. In this way, the 3D-to-2D pose projector module can learn the geometric consistency to correct intermediate 3D pose predictions. After initialization, we substitute the predicted 2D poses and 3D poses for the 2D and 3D ground-truth to optimize \Psi _C and \Psi _P in a self-supervised fashion. Considering that the predictions for certain body joints (e.g., hand_{left}, hand_{right}, foot_{left} and foot_{right} defined in the Human3.6M dataset) may not be accurate and reliable due to the challenging nature of the rich flexibilities and occlusions of body joints, we employ the dropout trick [46] in the intermediate 3D pose estimations p_{t}^{3d} and the predicted 2D pose p_{t}^{2d}, i.e., the position for each body joint has a probability \delta to be zero. This trick enables the regression function \Psi _C and the project function \Psi _P to be insensitive to the outliers inside p_{t}^{3d} and p_{t}^{2d}. As reported in [46], the dropout trick can significantly contribute to alleviating the overfitting of the fully connected layers inside \Psi _C and \Psi _P. Meanwhile, we also employ the 3D pose ground-truth to encourage the regression function \Psi _C to learn to regress the 3D pose estimation \hat{p}_{t}^{3d}. In our experiments, \delta is empirically set to be 0.3. The inference phase of this module is also self-supervised. Specifically, given the predicted 2D pose p_{t}^{2d}, we can obtain the initial \hat{p}_{t}^{3d} and the corresponding projected 2D pose \hat{p}_{t}^{2d} via forward propagation, as indicated in Fig. 3b. According to the 3D geometric consistency that the projected 2D pose \hat{p}_{t}^{2d} should be identical to the predicted 2D pose p_{t}^{2d}, we propose minimizing the dissimilarity between \hat{p}_{t}^{2d} and p_{t}^{2d} by optimizing its specific \omega _P^t and \omega _C^t as follows:

\begin{align*}
\lbrace \omega _{P}^{*t}, \omega _{C}^{*t}\rbrace &= \mathop {arg\;min}_{\lbrace \hat{p}_{t}^{3d}, \omega _P^t\rbrace } \Vert p_{t}^{2d} - \hat{p}_{t}^{2d} \Vert _2^2\\
&= \mathop {arg\;min}_{\lbrace \hat{p}_{t}^{3d}, \omega _P^t\rbrace } \Vert p_{t}^{2d} - \Psi _P(\hat{p}_{t}^{3d}; \omega _P^t) \Vert _{2}^{2}\\
&= \mathop {arg\;min}_{\lbrace \omega _{P}^t, \omega _C^t\rbrace } \Vert p_{t}^{2d} - \Psi _P(\Psi _C(p_{t}^{3d}; \omega _C^t); \omega _P^t) \Vert _{2}^{2},\\
\tag{3}\end{align*}
View Source
\begin{align*}
\lbrace \omega _{P}^{*t}, \omega _{C}^{*t}\rbrace &= \mathop {arg\;min}_{\lbrace \hat{p}_{t}^{3d}, \omega _P^t\rbrace } \Vert p_{t}^{2d} - \hat{p}_{t}^{2d} \Vert _2^2\\
&= \mathop {arg\;min}_{\lbrace \hat{p}_{t}^{3d}, \omega _P^t\rbrace } \Vert p_{t}^{2d} - \Psi _P(\hat{p}_{t}^{3d}; \omega _P^t) \Vert _{2}^{2}\\
&= \mathop {arg\;min}_{\lbrace \omega _{P}^t, \omega _C^t\rbrace } \Vert p_{t}^{2d} - \Psi _P(\Psi _C(p_{t}^{3d}; \omega _C^t); \omega _P^t) \Vert _{2}^{2},\\
\tag{3}\end{align*}

where the parameters {\omega _P^{t}, \omega _C^{t}} are initialized from the well-optimized {\omega _P, \omega _C} from the training phase. Note that, \omega _C^{*t} and \omega _P^{*t} are disposable and only valid for I_t. Since p_{t}^{2d} and p_{t}^{3d} are fixed, we first perform forward propagation to obtain the initial prediction, and further employ the standard back-propagation algorithm [47] to obtain \omega _C^{*t} and \omega _P^{*t} via Eq. (3). Thus, the output 3D pose regression \hat{p}_{t}^{3d} is bidirectionally refined to be the final 3D pose prediction during the the optimizing of \omega _C^{t} and \omega _P^{t} according to the proposed self-supervised correction mechanism. At the end, the final 3D pose \hat{p}_{t}^{*3d} is obtained according to Eq. (3) as follows:

\begin{equation*}
\hat{p}_{t}^{*3d} = \Psi _C(p_{t}^{3d}; \omega _C^{*t}).
\tag{4}\end{equation*}
View Source
\begin{equation*}
\hat{p}_{t}^{*3d} = \Psi _C(p_{t}^{3d}; \omega _C^{*t}).
\tag{4}\end{equation*}

The hyperparameters (i.e., the iteration number and learning rate) for \omega _P^{*t} and \omega _C^{*t} play a crucial role in effectively and efficiently refining the 3D pose estimation. In fact, a large iteration number and small learning rate can ensure that the model is capable of converging to a satisfactory \hat{p}_{t}^{*3d}. However, this setting results in a heavy computational cost. Therefore, a small iteration number with large learning rate is preferred to achieve a trade-off between efficiency and accuracy. Moreover, although we can achieve high accuracy on 2D pose estimation, the predicted 2D poses may contain errors due to the heavy occlusion of human body parts. Treating these inaccurate 2D poses as optimization objectives to bidirectionally refine the 3D pose prediction is prone to a decrease in performance. To address this issue, we utilize a heuristic strategy to determine the optimal hyperparameters used for each frame in our implementation. Specifically, we can check the convergence of some robust skeleton joints (i.e., Pelvis, Shoulder_{left}, Shoulder_{right}, Hip_{left} and Hip_{right} defined in the Human3.6M dataset) in each iteration. In practice, we find that the predictions of these reliable joints are generally less flexible and have lower probabilities of being occluded than other joints. If the predicted 2D pose contains small errors, then these joints of the refined 3D pose \hat{p}_{t}^{*3d} will have large and inconsistent changes within the self-supervised correction. Hence, we terminate the further refinement when the positions of these joints are converged (i.e., average changes < \epsilonmm), and discard the self-supervised correction when the average change in these joints are not within an empirical threshold \taumm. In our experiments, we empirically set \lbrace \tau, \epsilon \rbrace =\lbrace 20, 5\rbrace and employ two back-propagation operations to update \omega _P and \omega _C before outputting the final 3D pose prediction \hat{p}_{t}^{*3d}. 3.4 Model Training: In the training phase, the optimization of our proposed model occurs in a fully end-to-end manner, and we have defined several types of loss functions to fine-tune the network parameters: \omega _R, \omega _T and {\omega _C, \omega _P}, respectively. For the 2D pose sub-network, we build an extra FC layer upon the convolutional layers of the 2D pose sub-network to generate 2K joint location coordinates. We leverage the euclidean distances between the predictions for all K body joints and the corresponding ground-truth to train \omega _R. Formally, we have

\begin{equation*}
\mathop{\min} _{\omega _R} \sum _{t=1}^{N} \left\Vert p_t^{2d}(gt) - \Psi _R(I_t; \omega _R) \right\Vert _2^2,
\tag{5}\end{equation*}
View Source
\begin{equation*}
\mathop{\min} _{\omega _R} \sum _{t=1}^{N} \left\Vert p_t^{2d}(gt) - \Psi _R(I_t; \omega _R) \right\Vert _2^2,
\tag{5}\end{equation*}

where p_t^{2d}(gt) denotes the 2D pose ground-truth for the tth frame I_t. For the 2D-to-3D pose transformer module, our model enforces the 3D pose sequence prediction loss for all frames, which is also defined as follows:

\begin{align*}
\mathop{\min} _{\omega _T} & \sum _{t=1}^{N} \left\Vert p_t^{3d} - p_t^{3d(gt)} \right\Vert _{2}^{2}\\
=& \sum _{t=1}^{N} \left\Vert \Psi _T(f_t^{2d}; \omega _T, H_{t-1}) - p_t^{3d(gt)} \right\Vert _{2}^{2}, 
\tag{6}\end{align*}
View Source
\begin{align*}
\mathop{\min} _{\omega _T} & \sum _{t=1}^{N} \left\Vert p_t^{3d} - p_t^{3d(gt)} \right\Vert _{2}^{2}\\
=& \sum _{t=1}^{N} \left\Vert \Psi _T(f_t^{2d}; \omega _T, H_{t-1}) - p_t^{3d(gt)} \right\Vert _{2}^{2}, 
\tag{6}\end{align*}

where p_t^{3d(gt)} is the 3D pose ground-truth for the tth frame I_t. According to Eq. (6), we integrally fine-tune the parameters of the 2D-to-3D pose transformer module and the convolutional layers of the 2D pose sub-network in an end-to-end optimization manner. Note that, to obtain sufficient samples to train the 3D pose transformer module, we propose decomposing one long monocular image sequence into several small equal clips with N frames. In our experiments, we jointly feed our model with 2D and 3D pose data after all the network parameters are well initialized. For the 2D human pose data, this module regards their 3D pose sequence prediction loss as zero. Algorithm 1. The Proposed Training Algorithm: Input:3D human pose data \lbrace \mathbf {I}_{t}^{3d}\rbrace _{t=1}^{N} and 2D human pose data \lbrace \mathbf {I}_{i}^{2d}\rbrace _{i=1}^{M} 1:Pre-train the 2D pose sub-network with \lbrace \mathbf {I}_{i}^{2d}\rbrace _{i=1}^{M} to initialize \omega _R via Eq. (5); 2:Fixing \omega _R, initialize \omega _T with hidden variables H with \lbrace \mathbf {I}_{t}^{3d}\rbrace _{t=1}^{N} via Eq. (6); 3:Fixing \omega _R and \omega _T, initialize {\omega _C, \omega _P} with \lbrace \mathbf {I}_{t}^{3d}\rbrace _{t=1}^{N} via Eq. (2); 4:Fine-tune the whole model to further update {\omega _R, \omega _T, \omega _C, \omega _P} on \lbrace \mathbf {I}_{t}^{3d}\rbrace _{t=1}^{N} and \lbrace \mathbf {I}_{i}^{2d}\rbrace _{i=1}^{M} via Eq. (7). 5:return {\omega _R, \omega _T, \omega _C, \omega _P}. After initializing the 3D-to-2D projector module via Eq. (2), we fine-tune the whole network to jointly optimize the network parameters \lbrace \omega _R, \omega _T, \omega _C, \omega _P\rbrace in a fully end-to-end manner as follows:

\begin{equation*}
\mathop{\min} _{\lbrace \omega _R, \omega _T, \omega _C, \omega _P\rbrace } \Vert p_t^{2d} - \hat{p}_t^{2d} \Vert _2^2. 
\tag{7}\end{equation*}
View Source
\begin{equation*}
\mathop{\min} _{\lbrace \omega _R, \omega _T, \omega _C, \omega _P\rbrace } \Vert p_t^{2d} - \hat{p}_t^{2d} \Vert _2^2. 
\tag{7}\end{equation*}

Since our model consists of two cascaded modules, the training phase can be divided into the following steps: (i) Initialize the 2D pose representation via pre-training. To obtain a satisfactory feature representation, the 2D pose sub-network is first pre-trained with the MPII Human Pose dataset [48], which includes a larger variety of 2D pose data. (ii) Initialize the 2D-to-3D pose transformer module. We fix the parameters of the 2D pose sub-network and optimize the network parameter \omega _T. (iii) Initialize the 3D-to-2D pose projector module. We fix the above optimized parameters and optimize the network parameter {\omega _C, \omega _P}. (iv) Fine-tune the whole model jointly to further update the network parameters {\omega _R, \omega _T, \omega _C, \omega _P} with the 2D pose and 3D pose training data. For each of the above-mentioned steps, the ADAM [49] strategy is employed for parameter optimization. The entire algorithm can then be summarized as Algorithm 1. Obviously, this algorithm is in a good agreement with the pipeline of our model. 3.5 Model Inference: In the testing phase, every frame of the input image sequence is sequentially processed via Eq. (1). Note that each frame I_t has its own {\omega _C^t, \omega _P^t} in the 3D-to-2D projector module. {\omega _C^t, \omega _P^t} are initialized from the well trained {\omega _C, \omega _P}, and they will be updated by minimizing the difference between the predicted 2D poses p_{t}^{2d} and projected 2D poses \Psi _P(\hat{p}_{t}^{3d}; \omega _P) via Eq. (3). During the inference, the 3D pose estimation is bidirectionally refined until convergence is achieved according to the hyperparameter settings. Finally, we output the final 3D pose estimation via Eq. (4). 

SECTION 4 Experiments: 4.1 Experimental Settings: We perform extensive evaluations on two publicly available benchmarks: Human3.6M [5] and HumanEva-I [30]. Human3.6M Dataset. The Human3.6M dataset is a recently published dataset that provides 3.6 million 3D human pose images and corresponding annotations from a controlled laboratory environment. This dataset captures 11 professional actors performing in 15 scenarios under 4 different viewpoints. Moreover, there are three popular data partition protocols for this benchmark in the literature. Protocol #1: The data from five subjects (S1, S5, S6, S7, and S8) are for training, and the data from two subjects (S9 and S11) are for testing. To increase the number of training samples, the sequences from different viewpoints of the same subject are treated as distinct sequences. By downsampling the frame rate from 50 FPS to 2 FPS, 62,437 human pose images (104 images per sequence) are obtained for training and 21,911 images are obtained for testing (91 images per sequence). This is the widely used evaluation protocol on Human3.6M, and it was followed by several works [4], [15], [16], [36]. To be more general and make a fair comparison, our model is trained both on training samples from all 15 actions as previous works [4], [15], [16], [36] and by exploiting individual actions as [14], [36]. Protocol #2: This protocol only differs from Protocol #1 in that only the frontal view is considered for testing, i.e., testing is performed on every 5th frame of the sequences from the frontal camera (cam-3) from trial 1 of each activity with ground-truth cropping. The training data contain all actions and viewpoints. Protocol #3: Six subjects (S1, S5, S6, S7, S8 and S9) are used for training, and every 64th frame of S11's video clips is used for testing. The training data contain all actions and viewpoints.  HumanEva-I Dataset. The HumanEva-I dataset contains video sequences of four subjects performing six common actions (e.g., walking, jogging, boxing, etc.), and it also provides 3D pose annotation for each frame in the video sequences. We train our model on the training sequences of subjects 1, 2 and 3 and test on the ‘validation’ sequence under the same protocol as [15], [22], [31], [55], [56], [57], [58], [59]. Similar to Protocol #1 of the Human3.6M dataset, the data from different camera viewpoints are also regarded as different training samples. Note that we did not downsample the video sequences to obtain more samples for training. Implementation Details. For We follow [44] to build the LSTM memory cells, except that the peephole connections between cells and gates are omitted. Following [14], [36], the input image is cropped around the human. To maintain the human width / height ratio, we crop a square image of the subject from the image according to the bounding box provided by the dataset. Then, we resize the image region inside the bounding box to 368 × 368 before feeding it into our model. Moreover, we augment the training data by simply performing random scaling with factors in [0.9, 1.1]. To transform the absolute locations of joint points into the [0, 1] range, a \text{max-min} normalization strategy is applied. In the testing phase, the predicted 3D pose is transformed to the origin scale according to the maximum and minimum values of the pose from the training frames. During the training, the Xavier initialization method [61] is used to initialize the weights of our model. A learning rate of 1e^{-5} is employed for training. The training phase requires approximately 2 days on a single NVIDIA GeForce GTX 1,080. Evaluation Metric. Following [14], [15], [19], [21], [23], [37], we employ the popular 3D pose error metric [55], which calculates the euclidean errors on all joints and all frames up to translation. In the following section, we report the 3D pose error metric for all the experimental comparisons and analyses. 4.2 Comparisons with Existing Methods: Comparison on Human3.6M. We compare our model with the various competing methods on the Human3.6M [5] and HumanEva-I [30] datasets. For the fair comparison, we only consider the competing methods that do not need the intrinsic parameters of cameras for inference. This is reasonable for practical use under various scenarios. These methods are LinKDE [5], Tekin et al. [15], Li et al. [36], Zhou et al. [14], Zhou et al. [4], Du et al. [37], Sanzari et al. [34], Yasin et al. [17], and Bogo et al. [24]. Moreover, we compare other competing methods, i.e., Moreno-Noguer et al. [38], Tome et al. [20], Chen et al. [16], Pavlakos et al. [19], Zhou et al. [23], Bruce et al. [51], Tekin et al. [50] and our conference version, i.e., Lin et al. [21]. For those compared methods (i.e., [4], [5], [15], [16], [17], [24], [34], [36], [37], [38], [51]) whose source codes are not publicly available, we directly obtain their results from their published papers. For the other methods (i.e., [14], [19], [20], [21], [23], [50]), we directly use their official implementations for comparisons. The results under three different protocols are summarized in Table 2. Clearly, our model outperforms all the competing methods (including those trained from the individual action as in [15], [16], [36] and on all 15 actions) under Protocol #1. Specifically, under the training from individual action setting, our model achieves superior performance on all the action types, and it outperforms the best competing methods with the joint mean error reduced by approximately 8 percent compared with Tekin et al. [50] (63.79 mm versus 69.73 mm). Under the training from all 15 actions, our model still consistently performs better than the compared approaches and obtains better accuracy. Notably, our model achieves a performance gain of nearly 12 percent compared with our conference version (63.67 mm versus 73.10 mm). TABLE 2 
Quantitative Comparisons on the Human3.6M Dataset Using 3D Pose Errors (in millimeters)
The similar superior performance of our model over all the compared methods can also be observed under Protocol #2 and Protocol #3. Specifically, our model outperforms the best of the competing methods with the joint mean error reduced by approximately 19 percent (63.74 mm versus 79.6 mm) under Protocol #2 and 16 percent (54.14 mm versus 70.70 mm) under Protocol #3. In summary, our proposed model significantly outperforms all compared methods under all protocols with the mean error reduced by a clear margin. Note that some compared methods, e.g., [4], [14], [15], [19], [20], [21], [23], [36], [37], also employ deep learning techniques. In particular, Zhou et al. [4]'s method used the residual network [28]. Note that, the recently proposed methods all employ very deep network architectures (i.e., [23] and [19] proposed using Stacked Hourglass [18], while [21] and [20] employed CPM [10]) to obtain satisfactory accuracies. This makes these methods time-consuming. In contrast to these methods, our model achieves a more lightweight architecture by replacing the multi-stage refinement in [21] with the 3D-to-2D pose projector module. The superior performance achieved by our model demonstrates that our model is simple yet powerful in capturing complex contextual features within images, learning temporal dependencies within image sequences and preserving the geometric consistency within 3D pose predictions, which are critical for estimating 3D pose sequences. Some visual comparison results are presented in Fig. 4. Fig. 4. 
Qualitative comparisons on the Human3.6M dataset. The 3D poses are visualized from the side view, and the cameras are depicted. The results from Zhou et al. [14], Pavlakos et al. [19], Lin et al. [21], Zhou et al. [23], Tome et al. [20], our model and the ground truth are illustrated from left to right. Our model achieves considerably more accurate estimations than all the compared methods. Best viewed in color. Red and green indicate left and right, respectively. 
Comparison on HumanEva-I. We compare our model against competing methods, including discriminative regressions [57], [58], 2D pose detector-based methods [22], [31], [55], [56], CNN-based approaches [15], [19], [22], [38], [50] and our preliminary version Lin [21], on the HumanEva-I dataset. For a fair comparison, our model also predicts the 3D pose consisting of 14 joints, i.e., left/right shoulder, elbow, wrist, left/right hip knee, ankle, head top and neck, as [22]. Table 3 presents the performance comparisons of our model with all compared methods. Clearly, our model obtains substantially lower 3D pose errors than the compared methods on all the walking, jogging and boxing sequences. This result demonstrates the high generalization capability of our proposed model. TABLE 3 
Quantitative Comparisons on the HumanEva-I Dataset Using 3D Pose Errors (in millimeters) for the “walking”, “jogging” and “boxing” Sequences
4.3 Running Time: To compare the efficiencies of our model and of the compared methods, we have conducted all the experiments on a desktop with an Intel 3.4 GHz CPU and a single NVIDIA GeForce GTX 1,080 GPU. In terms of time efficiency, compared with [14] (880 ms per image), [19] (170 ms per image), [23] (311 ms per image), and [20] (444 ms per image), our model model only requires 51 ms per image. The detailed time analysis is presented in Table 4. Our model performs approximately 3 times faster than [19], which is the fastest of the compared methods. Moreover, although only performing slightly better than the best of the compared methods [23] under Protocol #1, our model runs nearly 6 times faster, thanks to the 3D-to-2D pose projector module enabling a lightweight architecture. This result validates the efficiency of our proposed model. TABLE 4 
Comparison of the Average Running Time (milliseconds per image) on the Human3.6M Benchmark
4.4 Ablation Study: To perform a detailed component analysis, we conducted the experiments on the Human3.6M benchmark under the Protocol #1 and our proposed model was trained on all actions for a fair comparison. 4.4.1 Self-Supervised Correction MechanismTo demonstrate the superiority of the proposed self-supervised correction (SSC) mechanism, we conduct the following experiment: disabling this module in both training and inference phase by directly regarding the intermediate 3D pose predictions as the final output (denoted as “ours w/o SSC train+test”). Moreover, we have also disabled the self-supervised correction mechanism during the inference phase and denote this version as ours w/o SSC test”. The results in Table 5 demonstrate that our w/o SSC test significantly outperforms ours w/o SSC train+test (67.63 mm versus 80.15 mm), and our model surpasses ours w/o SSC test by a clear margin (63.67 mm versus 67.63 mm). These observations justify the contribution of the proposed SSC mechanism. This result demonstrates that the self-supervised correction mechanism is highly beneficial for improving the performance both in the training and testing phase. Qualitative comparison results on the Human3.6M dataset are shown in Fig. 5. As depicted in Fig. 5, the intermediate 3D pose predictions (i.e., ours w/o self-correction) contain several inaccurate joint locations compared with the ground truth because of the self-occlusion of body parts. However, the predicted 2D poses are of high accuracy because of the powerful CNN, which is trained from a large variety of 2D pose data. Because the estimated 2D poses are more reliable, our proposed 3D-to-2D pose projector module can utilize them as optimization objectives to bidirectionally refine the 3D pose predictions. As shown in Fig. 5, the joint predictions are effectively corrected by our model. This experiment clearly demonstrates that our 3D-to-2D pose projector, utilizing these predicted 2D poses as guidance, can contribute to enhancing 3D human pose estimation by correcting the 3D pose predictions without additional 3D pose annotations. Fig. 5. 
Qualitative comparisons of ours and ours w/o self-correction on the Human3.6M dataset. The input image, estimated 2D pose, ours w/o self-correction, ours and ground truth are listed from left to right, respectively. With the ground truth as reference, one can easily observe that the inaccurately predicted human 3D joints in ours w/o self-correction are effectively corrected in ours. Best viewed in color. Red and green indicate left and right, respectively. 
TABLE 5 
Empirical Comparisons Under Different Settings for Ablation Study Using Protocol #1
To further upper bound the performance of the proposed self-supervised correction mechanism in the testing phase, we directly employ the 2D pose ground-truth to correct the intermediate predicted 3D poses. We denote this version of our model as “ours w/ 2D GT”. Table 5 demonstrates that ours w/ 2D GT achieves significantly better results than our model by reducing the mean joint error by approximately 6 percent (59.41 mm versus 63.67 mm). Moreover, we have also implemented another 2D pose prediction model from the hourglass network with two stages for self-supervised correction (denoted as “ours w/ HG features”). Our method w/ HG features performs slightly better than ours (62.85 mm versus 63.67 mm). This result evidences the effectiveness of our designed 3D-to-2D pose projector module in bidirectionally refining the intermediate 3D pose estimation. We have further compared our method with a multi-task approach that simultaneously estimates 3D and 2D pose without re-projection (denoted as “ours w/o projection”). Specifically, ours w/o projection shares the same network architecture as our full model. The only difference is that ours w/o projection directly estimates both 2D and 3D poses during the training phase. Since it is quite difficult to directly train the network into convergence with huge 2D pose data and relatively small 3D pose data, we first optimize the network parameters by using the 2D pose data only from the MPII dataset, and further perform training with the 2D pose data and 3D pose data from the Human3.6M dataset. As illustrated in Table 5, our full model performs substantially better than ours w/o projection (63.67 mm versus 86.80 mm). The reason may be that directly regressing the 2D pose may mislead the main learning task of the model, i.e., the network concentrates on improving the overall performance of both 2D and 3D pose prediction. This demonstrates the effectiveness of the proposed 3D-to-2D pose projector module. 4.4.2 Temporal DependencyThe model performance without temporal dependency in the training phase is also compared in Table 5 (denoted as “ours w/o temporal”). Note that the input of ours w/o temporal is only a single image rather than a sequence in the training and testing phases. Hence, the temporal information is ignored. Thus, the LSTM layer for the 3D pose errors is replaced by a fully connected layer with the same units as the LSTM layer. As illustrated in Table 5, ours w/o temporal has suffered considerably higher 3D pose errors than ours (90.83 mm versus 63.67 mm). Moreover, we have also analyzed the contribution of the temporal dependency in the testing phase. To discard the temporal dependency during the inference phase, we regarded a single frame as input to evaluate the performance of our model, and we denote it as “ours w/ single frame”. Table 5 demonstrates that this variant performs worse compared to ours by increasing the mean joint error by approximately 6 percent (67.70 mm versus 63.67 mm). This result validates the contribution of temporal information for 3D human pose estimation during the training and testing phases. 4.4.3 External 2D Human Pose DataTo evaluate the performance without external 2D human pose data, we have only employed 3D pose data with 3D and 2D annotations from Human3.6M for training our model. We denote this version of our model as “ours w/o external”. As shown in Table 5, the ours w/o external version performs quite worse than our model (63.67 mm versus 111.30 mm). This reason may be that the training samples from Human3.6M, compared with the MPII dataset [48], are less challenging, therein having fewer variations for our model to learn a rich and powerful 2D pose presentation. Thanks to the proposed self-supervised correction mechanism, our model can effectively leverage a large variety of 2D human pose data to improve the performance of the 3D human pose estimation. Moreover, in terms of estimating 3D human pose in the wild, our model advances the existing method [23] in leveraging more abundant 3D geometric knowledge for mining in-the-wild 2D human pose data. Instead of oversimplifying the 3D geometric constraint as the relative bone length in [23], our model introduces a self-supervised correction mechanism to retain the 3D geometric consistency between the 2D projections of 3D poses and the estimated 2D poses. Therefore, our model can bidirectionally refine the 3D pose predictions in a self-supervised manner. Fig. 6 presents qualitative comparisons for images taken from the KTH Football II [60] and MPII dataset [48], respectively. As shown in Fig. 6, our model achieves 3D pose predictions of superior accuracy compared to the competing method [23]. Fig. 6. 
Some qualitative comparisons of our model and Zhou et al. [23] on two representative datasets in the wild, i.e., KTH Football II [60] (first row) and MPII datasets [48] (the remaining rows). For each image, the original viewpoint and a better viewpoint are illustrated. Best viewed in color. Red and green indicate left and right, respectively. 


SECTION 5 Conclusion: This paper presented a 3D human pose machine that can learn to integrate rich spatio-temporal long-range dependencies and 3D geometry knowledge in an implicit and comprehensive manner. We further enhanced our model by developing a novel self-supervised correction mechanism, which involves two dual learning tasks, i.e., 2D-to-3D pose transformation and 3D-to-2D pose projection, under a self-supervised correction mechanism. This mechanism retains the geometric consistency between the 2D projections of 3D poses and the estimated 2D poses, and it enables our model to utilize the estimated 2D human pose to bidirectionally refine the intermediate 3D pose estimation. Therefore, our proposed self-supervised correction mechanism can bridge the domain gap between 3D and 2D human poses to leverage the external 2D human pose data without requiring additional 3D annotations. Extensive evaluations on two public 3D human pose datasets validate the effectiveness and superiority of our proposed model. In future work, focusing on sequence-based human centric analyses (e.g., human action and activity recognition), we will extend our proposed self-supervised correction mechanism for temporal relationship modeling, and design new self-supervision objectives to incorporating abundant 3D geometric knowledge for training models in a cost-effective manner. 
ACKNOWLEDGMENTS: This work was supported in part by the National Key Research and Development Program of China under Grant No. 2018YFC0830103, in part by National High Level Talents Special Support Plan (Ten Thousand Talents Program), in part by National Natural Science Foundation of China (NSFC) under Grant No. 61622214, and 61836012, in part by the Ministry of Public Security Science and Technology Police Foundation Project No. 2016GABJC48, and in part by Guandong “Climbing Program” special funds under Grant pdjh2018b0013.