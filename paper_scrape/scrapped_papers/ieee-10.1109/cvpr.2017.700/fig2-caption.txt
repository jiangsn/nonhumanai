Figure 2. Overview of the supervision pipeline based on view synthesis. The depth network takes only the target view as input, and outputs a per-pixel depth map D^t. The pose network takes both the target view (It) and the nearby/source views (e.g., It−1 and It+1) as input, and outputs the relative camera poses (T^t→t−1,T^t→t+1). The outputs of both networks are then used to inverse warp the source views (see sec. 3.2) to reconstruct the target view, and the photometric reconstruction loss is used for training the CNNs. By utilizing view synthesis as supervision, we are able to train the entire framework in an unsupervised manner from videos.