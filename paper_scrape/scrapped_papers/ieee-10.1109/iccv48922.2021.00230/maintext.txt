SECTION 1. Introduction: Video stabilization has become increasingly important with the rapid growth of video content on the Internet platforms, such as YouTube, Vimeo, and Instagram. Casually captured cellphone videos without a professional video stabilizer are often shaky and unpleasant to watch. These videos pose significant challenges for video stabilization algorithms. For example, videos are often noisy due to small image sensors, particularly in low-light environments. Handheld captured videos may contain large camera shake/jitter, resulting in severe motion blur and wobble artifacts from a rolling shutter camera. Figure 1: 
Full-frame video stabilization of challenging videos. Our method takes a shaky input video (top) and produces a stabilized and distortion-free video (bottom), as indicated by less fluctuation in the yᄑt epipolar plane image. Furthermore, by robustly fusing multiple neighboring frames, our results do not suffer from aggressive cropping of frame borders in the stabilized video and can even expand the field of view of the original video. Our approach significantly outperforms representative state-of-the-art video stabilization algorithms on these challenging scenarios (see Figure 1). 
Figure 2: 
Limitations of current state-of-the-art video stabilization techniques. (a) Current commercial video stabilization software (Adobe Premiere Pro 2020) fails to generate smooth videos in challenging scenarios of rapid camera shakes. (b) Yu and Ramamoorthi’s method [70] produces a temporally smooth video. However, the warped (stabilized) video contains many missing pixels at frame borders and inevitably requires applying aggressive cropping (green checkerboard areas) to generate a rectangular video. (c) The DIFRINT method [10] achieves full-frame video stabilization by iteratively applying frame interpolation to generate in-between, stabilized frames. However, interpolating between frames with large camera motion and moving occlusion is challenging. Their results are thus prone to severe artifacts. 
Existing video stabilization methods usually consist of three main components: 1) motion estimation, 2) motion smoothing and 3) stable frame generation. First, the motion estimation step involves estimating motion through 2D feature detection/tracking [28], [33], [15], [62], dense flow [69], [70], or recovering camera motion and scene structures [32], [73], [5], [52], [34]. Second, the motion smoothing step then removes the high-frequency jittering in the estimated motion and predicts the spatial transformations to stabilize each frame in the form of homography [38], mixture of homography [35], [17], or per-pixel warp fields [36], [69], [70]. Third, the stable frame generation step uses the predicted spatial transform to synthesize the stabilized video. The stabilized frames, however, often contain large missing regions at frame borders, particularly when videos with large camera motion. This forces existing methods to apply aggressive cropping for maintaining a rectangular frame and therefore leads to a significantly zoomed-in video with resolution loss (Figure 2(a) and (b)). Full-frame video stabilization methods aim to address the above-discussed limitation and produce stabilized video with the same field of view (FoV). One approach for fullframe video stabilization is to first compute the stabilized video (with missing pixels at the frame borders) and then apply flow-based video completion methods [38], [21], [12] to fill in missing contents. Such two-stage methods may suffer from the inaccuracies in flow estimation and inpainting (e.g., in poorly textured regions, fluid motion, and motion blur). A recent learning-based method, DIFRINT [10], instead uses iterative frame interpolation to stabilize the video while maintaining the original FoV. However, applying frame interpolation repeatedly leads to severe distortion and blur artifacts in challenging cases (Figure 2(c)). In this paper, we present a new algorithm that takes a shaky video and the estimated smooth motion fields for stabilization as inputs and produces a full-frame stable video. The core idea of our method lies in fusing information from multiple neighboring frames in a robust manner. Instead of using color frames directly, we use a learned CNN representation to encode rich local appearance for each frame, fuse multiple aligned feature maps, and use a neural decoder network to render the final color frame. We first explore multiple design choices for fusing and blending multiple aligned frames. We then propose a hybrid fusion mechanism that leverages both feature-level and image-level fusion to alleviate the sensitivity to flow inaccuracy. We further improve the visual quality of the synthesized results by learning to predict spatially varying blending weights, removing blurry input frames for sharp video generation, and transferring high-frequency details residual to the re-rendered, stabilized frames. To minimize regions where contents are unknown for all neighboring frames, we propose a path adjustment method for balancing the goals of smoothing camera motion and maximizing frame coverage. Our method generates stabilized video with significantly fewer artifacts and distortions while retaining (or even expanding) the original FoV (Figure 1). We evaluate the proposed algorithm with state-of-the-art methods and commercial video stabilization software (Adobe Premiere Pro 2020 warp stabilizer). Extensive experiments show that our method performs favorably against existing methods on three public benchmark datasets [35], [68], [61]. Our main contributions are: We apply a neural fusion technique in the context of full-frame video stabilization to alleviate the issues of sensitivity to flow inaccuracy. We present a hybrid fusion method for fusing information from multiple stabilized frames at both feature and image-level. We systematically validate various design choices through detailed ablation studies. We demonstrate favorable performance against representative video stabilization techniques on three public datasets. 

SECTION 2. Related work: Motion estimation and smoothing. Most video stabilization methods focus on estimating motion between frames and smoothing the motion. They often estimate 2D motion using sparse feature detection/tracking and dense optical flow. These methods differ in motion modeling, e.g., eigen-trajectories [33], epipolar geometry [15], warping grids [35], or dense flow fields [70]. For motion smoothing, prior methods use low-pass filtering [33], L1 optimization [18], and spatio-temporal optimization [62]. In contrast to estimating 2D motion, several methods recover the camera motion and proxy scene geometry by leveraging Structure from Motion (SfM) algorithms. These methods stabilize frames using 3D reconstruction and projection along with image-based rendering [25] or content-preserving warps [5], [32], [15]. However, SfM algorithms are less effective in handling complex videos with severe motion blur and highly dynamic scenes [27]. Specialized hardware such as depth cameras [34] or light field cameras [52] may be required for reliable pose estimation. Deep learning-based approaches have recently been proposed to directly predict warping fields [67], [61] or optical flows [69], [70] for video stabilization. In particular, methods with dense warp fields [36], [69], [70] offer greater flexibility for compensating motion jittering and implicitly handling rolling shutter effects than parametric warp fields [67], [61]. Our work builds upon existing 2D motion estimation/smoothing techniques for stabilization and focuses on synthesizing full-frame video outputs. Specifically, we adopt the state-of-the-art flow-based stabilization method [70] and use the estimated per-frame warped fields as inputs to our method. Note that our method is agnostic to the motion smoothing techniques. Other approaches such as parametric warps can also be applied. Image fusion and composition. With the estimated and smoothed motion, the final step of video stabilization is to render the stabilized frames. Most existing methods synthesize frames by directly warping each input frame to the stabilized location using smoothed warping grids [35], [33] or flow fields predicted by CNNs [69], [70]. However, such approaches inevitably synthesize images with missing regions around frame boundaries. To maintain a rectangle shape, existing methods often crop off the blank areas and generate output videos with a lower resolution and a smaller FOV than the input video. To address this issue, full-frame video stabilization methods aim to stabilize videos without cropping. These methods use neighboring frames to fill in the blank and produce full-frame results by 2D motion inpainting [38], [21], [14], [12]. In contrast to existing motion inpainting methods that first generate stabilized frames then filling in missing pixels, our method leverage neural rendering to encode and fuse warped appearance features and learn to decode the fused feature map to the final color frames. Several recent methods can generate full-frame stabilized videos without explicit motion estimation. For example, the method in [61] train a CNN with collected unstablestable pairs to directly synthesize stable frames. However, direct synthesis of output frames without spatial transformations remains challenging. Recently, the DIFRINT method [10] generates full-frame stable videos by iteratively applying frame interpolation. This method couples motion smoothing and frame rendering together. However, the repeated frame interpolation often introduces visible distortion and severe artifacts (see Figure 2(c)). View synthesis. View synthesis algorithms aim to render photorealistic images of novel viewpoints from a single image [41], [65], [64], [48], [26] or multiple posed images [30], [16], [7], [42], [19], [43], [45], [55], [9], [46]. These methods mainly differ in the ways to map and fuse information, e.g., view interpolation [8], [11], [47], 3D proxy geometry and mapping [6], multi-plane images [72], [53], [20], and CNN [19], [43], [45]. Our fusion network resembles the encoder-decoder network used for free view synthesis [43]. A recent line of research focuses on rendering novel views for dynamic scenes from a single video [66], [60], [31], [13] based on neural volume rendering [37], [40]. These methods can be used for full-frame video stabilization by rendering the dynamic video from a smooth camera trajectory. While promising results have been shown, these methods require per-video training and precise camera pose estimates. In contrast, our frame synthesis method can be applied to a wider variety of videos without re-training and when accurate camera poses are difficult to obtain. Neural rendering. A direct blending of multiple images in the image space may lead to glitching artifacts (visible seams). Some recent methods train neural scene representations to synthesize novel views, such as NeRF [40], scene representation networks [51], neural voxel grid [50], [37], 3D Neural Point-Based Graphics [2], and neural textures [59]. However, these methods often require time-consuming perscene training and do not handle dynamic scenes. In contrast, our method does not require per-video finetuning. Figure 3: 
Design choices for fusing multiple frames. To synthesize a full-frame stabilized video, we need to align and fuse the contents from multiple neighboring frames in the input shaky video. (a) Conventional panorama image stitching (or in general image-based rendering) methods often fuse the warped (stabilized) images in the image level. Fusing in image-level works well when the alignment is accurate, but may generate blending artifacts (e.g., visible seams) when flow estimates are not reliable. (b) One can also encode the images as abstract CNN features, perform the fusion in the feature-space, and learn a decoder to convert the fused feature to output frames. Such approaches are more robust to flow inaccuracy but often produce overly blurred images. (c) Our proposed hybrid fusion combines the advantages of both strategies. We first extract abstract image features (Eq. (1)). We then fuse the warped features from multiple frames. For each source frame, we take the fused feature map together with the individual warped features and decode it to the output frames and the associated confidence maps. Finally, we produce the final output frame by using the weighted average of the generated images as in Eq. (4). 


SECTION 3. Full-frame video stabilization: Let {I_{{k^s}}}Iks denote the source frame in the real (unstabilized) camera space and {I_{{k^t}}}Ikt the target frame in the virtual (stabilized) camera space at a timestamp k. Given an input video with T frames \left\{ {{I_{{k^s}}}} \right\}_{k = 1}^T{Iks}Tk=1, our goal is to generate a video \left\{ {{I_{{k^t}}}} \right\}_{k = 1}^T{Ikt}Tk=1 that is visually stable and maintains the same FOV as the input video without cropping. Existing video stabilization methods often apply aggressive cropping to exclude any missing pixels due to frame warping, as shown in Figure 2. In contrast, we utilize the information from neighboring frames to render stabilized frames with completed contents or even expanding the FOV of the input video. Video stabilization methods typically consist of three stages: 1) motion estimation, 2) motion smoothing, and 3) frame warping/rendering. Our method focuses on the third stage for rendering high-quality frames without any cropping. Our proposed algorithm is thus agnostic to particular motion estimation/smooth techniques. We assume that the warping field from the real camera space to the virtual camera space is available for each frame (e.g., from [70]). Given an input video, we first encode image features for each frame, warp the neighboring frames to the virtual camera space at the specific target timestamp, and then fuse the features to render a stabilized frame. We describe the technical detail of each step in the following sections. 3.1. Pre-processing: Motion estimation and smoothing. Several motion estimation and smoothing methods have been developed [33], [35], [70]. This work uses the state-of-the-art method [70] to obtain a backward dense warping field Fkt→ks for each frame, where ks indicates the source input and kt denotes the target stabilized output. These warping fields can be directly used to warp the input video. However, the stabilized video often contains irregular boundaries and a large portion of missing pixels. Therefore, the output video requires aggressive cropping and thus loses some content. Optical flow estimation. To recover the missing pixels caused by warping, we need to project the corresponding pixels from nearby frames to the target stabilized frame. For each key frame Iks at time k, we compute the optical flows {Fns→ks}n∈Ωk from neighboring frames to the key frame using RAFT [58], where n indicates a neighboring frame and Ωk denotes the set of neighboring frames for Iks. 3.2. Warping and fusion: Warping. We warp the neighboring frames {Ins}n∈Ωk to align with the target frame Ikt in the virtual camera space. Since we already have the warping field from the target frame to the keyframe Fkt→ks (estimated from [70]) and the estimated optical flow from the keyframe to neighboring frames {Fks→ns}n∈Ωk, we can then compute the warping field from the target frame to neighboring frames {Fkt→ns}n∈Ωk by chaining the flow vectors. We can thus warp a neighboring frame Ins to align with the target frame Ikt using backward warping [22]. Some pixels in the target frame are not visible in the neighboring frames due to occlusion/dis-occlusion or out-of-boundary. Therefore, we compute visibility mask {Mns}n∈Ωk for each neighboring frame to indicate whether a pixel is valid (labeled as 1) in the source frame or not. We use Sundaram et al.’s method [56] to identify occluded pixels (labeled as 0). Fusion space. With the aligned frames, we explore several fusion strategies. First, we can directly blend the warped color frames in the image space to produce the output stabilized frame, as shown in Figure 3(a). This image-space fusion approach is a commonly used technique in image stitching [1], [57], video extrapolation [29], novel view synthesis [19], and HDR reconstruction [23]. However, image-space fusion is prone to ghosting artifacts due to misalignment, or glitch artifacts due to inconsistent labeling between neighbor pixels. Alternatively, one can also fuse the aligned frames in the feature space, e.g., [9], [46], as shown in Figure 3(b). Fusing in the high-dimensional feature spaces allows the model to be more robust to flow inaccuracy. However, rendering the fused feature map using a neural image-translation decoder often leads to blurry outputs. To combine the best worlds of both image-space and feature-space fusions, we propose a hybrid-space fusion mechanism for video stabilization (Figure 3(c)). Similar to the feature-space fusion, we first extract high-dimensional features from each neighboring frame and warp the features using flow fields. We then learn a CNN to predicting the blending weights that best fuse the features. We concatenate the fused feature map and the warped feature for each neighboring frame to form the input for our image generator. The image generator learns to predict a target frame and a confidence map for each neighboring frame. Finally, we adopt an image-space fusion to merge all the predicted target frames according to the predicted weights to generate the final stabilized frame. The core difference between our hybrid-space fusion and feature-space fusion lies in the input to the image generator. The image generator in Figure 3(b) takes only the fused feature as input to predict the output frame. The fused feature map already contains mixed information from multiple frames. The image generator may thus have difficulty in synthesizing sharp image contents. In contrast, our image generator in Figure 3(c) takes the fused feature map as guidance to reconstruct the target frame from the warped feature. We empirically find that this improves the sharpness of the output frame while avoiding ghosting and glitching artifacts, as shown in the supplementary material. Fusion function. We explore a learning-based fusion method using deep CNNs. Specifically, we train a CNN to predicts a blending weight \omega _{{n^s}}^{{k^t}}ωktns for each neighboring frame using the encoded features, visibility masks, and the flow error (Figure 4):
\begin{equation*}f_{{\text{CNN}}}^{{k^t}} = \sum\limits_{n \in {\Omega _k}} {f_{{n^s}}^{{k^t}}} \underbrace {\sigma \left( {{G_\theta }\left( {f_{{n^s}}^{{k^t}},M_{{n^s}}^{{k^t}},f_{{k^s}}^{{k^t}},M_{{k^s}}^{{k^t}},e_{{n^s}}^{{k^t}}} \right)} \right)}_{\omega _{{n^s}}^{{k^t}}},\tag{1}\end{equation*}fktCNN=∑n∈Ωkfktnsσ(Gθ(fktns,Mktns,fktks,Mktks,ektns))ωktns,(1)View Source\begin{equation*}f_{{\text{CNN}}}^{{k^t}} = \sum\limits_{n \in {\Omega _k}} {f_{{n^s}}^{{k^t}}} \underbrace {\sigma \left( {{G_\theta }\left( {f_{{n^s}}^{{k^t}},M_{{n^s}}^{{k^t}},f_{{k^s}}^{{k^t}},M_{{k^s}}^{{k^t}},e_{{n^s}}^{{k^t}}} \right)} \right)}_{\omega _{{n^s}}^{{k^t}}},\tag{1}\end{equation*}
where Gθ is the CNN, σ(•) is a softmax activation, f_{{n^s}}^{{k^t}}{\text{and }}M_{{n^s}}^{{k^t}}fktnsand Mktns are the encoded feature map and warping mask of frame n, respectively. The superscript kt indicates that the encoded feature and warping mask are warped to the target stable frame k. The forward-backward flow consistency error ens is calculated by:
\begin{equation*}{e_{{n^s}}}({\mathbf{p}}) = {\left\| {{F_{{k^s} \to {n^s}}}({\mathbf{p}}) + {F_{{n^s} \to {k^s}}}\left( {{\mathbf{p}} + {F_{{k^s} \to {n^s}}}} \right)} \right\|_2},\tag{2}\end{equation*}View Source\begin{equation*}{e_{{n^s}}}({\mathbf{p}}) = {\left\| {{F_{{k^s} \to {n^s}}}({\mathbf{p}}) + {F_{{n^s} \to {k^s}}}\left( {{\mathbf{p}} + {F_{{k^s} \to {n^s}}}} \right)} \right\|_2},\tag{2}\end{equation*} Figure 4: 
Learning-based fusion. Given the warped features {\left\{ {f_{{n^s}}^{{k^t}}} \right\}_{n \in {\Omega _k}}} (or warped images for image-space fusion), warping masks {\left\{ {M_{{n^s}}^{{k^t}}} \right\}_{n \in {\Omega _k}}}, and the flow error maps {\left\{ {e_{{n^s}}^{{k^t}}} \right\}_{n \in {\Omega _k}}}, we first concatenate feature, warping mask, and flow erro maps for each frame (shown as dotted blocks). We then use a CNN to predict the blending weights \omega _{{n^s}}^{{k^t}} for each neighbor frame. Using the predicted weights, we compute the fused feature by weighted averaging the individual warped features {\left\{ {f_{{n^s}}^{{k^t}}} \right\}_{n \in {\Omega _k}}}. 
where p denotes the pixel coordinate in Fks→ns. The error e_{{n^s}}^{{k^t}} in Eq. (1) is calculated by warping the flow consistency error ens to the target frame k. All the flow consistency errors are calculated from the input unstabilized frames. After fusing the feature, we concatenate the fused feature with the warped feature and warping mask of each frame as the input to the image generator. The image generator then predicts the output color frame and confidence map for each frame:
\begin{equation*}\left\{ {I_{{n^s}}^{{k^t}},C_{{n^s}}^{{k^t}}} \right\} = {G_\phi }\left( {f_{{n^s}}^{{k^t}},M_{{n^s}}^{{k^t}},f_{{\text{CNN}}}^{{k^t}}} \right),\tag{3}\end{equation*}View Source\begin{equation*}\left\{ {I_{{n^s}}^{{k^t}},C_{{n^s}}^{{k^t}}} \right\} = {G_\phi }\left( {f_{{n^s}}^{{k^t}},M_{{n^s}}^{{k^t}},f_{{\text{CNN}}}^{{k^t}}} \right),\tag{3}\end{equation*}
where Gϕ denotes the image generator, I_{{n^s}}^{{k^t}}{\text{ and }}C_{{n^s}}^{{k^t}} represent the predicted frame and confidence map of frame n in the virtual camera space at time k, respectively. Finally, the output stabilized frame Ikt is generated by a weighted sum using these predicted frames and confidence maps:
\begin{equation*}{I_{{k^t}}} = \sum\limits_{n \in {\Omega _k}} {I_{{n^s}}^{{k^t}}} C_{{n^s}}^{{k^t}}.\tag{4}\end{equation*}View Source\begin{equation*}{I_{{k^t}}} = \sum\limits_{n \in {\Omega _k}} {I_{{n^s}}^{{k^t}}} C_{{n^s}}^{{k^t}}.\tag{4}\end{equation*} 3.3. Path adjustment: When stabilizing a long video with large motion, some pixels around the frame boundary in the target frames may not be visible in any of the neighboring frames. For such cases, the network has to "hallucinate" new contents, often resulting in blurry predictions and unwanted visual artifacts. Table 1: 
Quantitative evaluation of fusion functions and fusion spaces on the test set of [54]. We highlight {\text{the best}} and {\text{the second best}} in each column.
To mitigate this problem, we propose to adjust the flow fields by global translations in each frame to increase the coverage of valid regions in the entire video. More specifically, for each target frame k, we aim to find a global translation xkt to adjust the flow fields of its neighbor frames as {F_{{k^t} \to {n^s}}} + {{\mathbf{X}}_{{k^t}}}, where n ∈ Ωk. Let the valid pixel mask M_{{\text{valid }}}^{{k^t}}\left( {{{\mathbf{x}}_{{k^t}}}} \right) denote the union of the warping masks after the adjustment. We find the translations by optimizing the following energy function:
\begin{equation*}\mathop {\arg \min }\limits_{{{\mathbf{x}}_k}t} \sum\limits_{{k^t}} {\left( {1 - M_{{\text{valid }}}^{{k^t}}\left( {{{\mathbf{x}}_{{k^t}}}} \right)} \right)} + {\lambda _{\text{s}}}\sum\limits_{{k^t},{q^t}} {\left\| {{{\mathbf{x}}_{{k^t}}} - {{\mathbf{x}}_{{q^t}}}} \right\|_2^2} ,\tag{5}\end{equation*}View Source\begin{equation*}\mathop {\arg \min }\limits_{{{\mathbf{x}}_k}t} \sum\limits_{{k^t}} {\left( {1 - M_{{\text{valid }}}^{{k^t}}\left( {{{\mathbf{x}}_{{k^t}}}} \right)} \right)} + {\lambda _{\text{s}}}\sum\limits_{{k^t},{q^t}} {\left\| {{{\mathbf{x}}_{{k^t}}} - {{\mathbf{x}}_{{q^t}}}} \right\|_2^2} ,\tag{5}\end{equation*}
where q ∈ {k±1} indicates the neighbor frame of frame k. Here, the first term is a data term that aims to maximize the coverage of the valid mask. The second term is a smoothness term that penalizes large translation adjustments between nearby frames. The weight λs is a hyper-parameter that balances between the data and smoothness terms. We set λs = 100 in all the experiments and solve Eq. (5) using the coarse-to-fine alpha-expansion approach [3]. 3.4. Training details: Loss functions. Our loss functions include the L1 and VGG perceptual losses:
\begin{equation*}\mathcal{L} = {\left. {\left\| {\left| {{I_{{k^t}}} - {{\hat I}_{{k^t}}}\left\| {{{\left. {} \right|}_1} + \sum\limits_l {{\lambda _l}} } \right\|} \right|{\psi _l}\left( {{I_{{k^t}}}} \right) - {\psi _l}\left( {{{\hat I}_{{k^t}}}} \right)} \right\|} \right|_1},\tag{6}\end{equation*}View Source\begin{equation*}\mathcal{L} = {\left. {\left\| {\left| {{I_{{k^t}}} - {{\hat I}_{{k^t}}}\left\| {{{\left. {} \right|}_1} + \sum\limits_l {{\lambda _l}} } \right\|} \right|{\psi _l}\left( {{I_{{k^t}}}} \right) - {\psi _l}\left( {{{\hat I}_{{k^t}}}} \right)} \right\|} \right|_1},\tag{6}\end{equation*}
where {\hat I_{{k^t}}} denotes the ground-truth target frame and ψl is intermediate feature extracted from a pre-trained VGG-19 network [49]. Training data. Our model requires a pair of unstabilized and stabilized videos for training. However, it is difficult to obtain such a real video pair. Therefore, we apply random motion to a stable video sequence to synthesize the input shaky video. Specifically, we sample a short sequence of 7 frames from the training set of [54] and randomly crop the frames to generate the input unstable video. We then apply another random cropping on the center frame as the ground-truth of the target stabilized frame. 

SECTION 4. Experimental results: We start with validating various design choices of our approach (Section 4.1). Next, we present quantitative comparison against representative state-of-the-art video stabilization algorithms (Section 4.2) and visual results (Section 4.3). Finally, we evaluate our fusion method on view synthesis (Section 4.4). In the supplementary material, we further present 1) the definitions of the fusion functions, evaluation metrics, and the technical details of the network architecture, 2) the ablation studies on path adjustment, residual detail transfer, 3) comparisons with video completion method [12] and learning-based blending [19], 4) the user study, 5) the limitations of our approach, and 6) stabilized videos by the evaluated methods. 4.1. Ablation study: We analyze the contribution of each design choice, including the fusion function design and fusion mechanism. We generate 940 test videos using the same method as our training data generation, where each video contains seven input frames and one target frame. Fusion function. We explore the following differentiable functions for fusion: 1) Mean fusion, 2) Gaussian-weighted fusion, 3) Argmax fusion, 4) Flow error-weighted fusion, and 5) the proposed CNN-based fusion function. We train the proposed model using image-space fusion, featurespace fusion, and hybrid-space fusion. For image-space fusion, we also include two conventional fusion methods: multi-band blending [4] and graph-cut [1]. Table 1 shows the quantitative results of different fusion methods and fusion spaces. None of the fusion methods dominates the results for image-space fusion, where the argmax and CNN-based fusions perform slightly better than other alternatives. For both feature-space and hybrid-space fusion, the proposed CNN-based fusion shows advantages over other approaches. Fusion space. Next, we compare different fusion levels using our CNN-based fusion. The last row of Table 1 shows that the proposed hybrid-space fusion achieves the best results compared to image-space fusion and feature-space fusion. The synthesized frame from the image-space fusion looks sharp but often contains visible glitching artifacts due to the discontinuity of different frames and inaccurate motion estimation. The results of the feature-space fusion are smooth but overly blurred. Finally, our hybrid-space fusion takes advantage of both above methods, generating a sharp and artifact-free frame. Table 2: 
Quantitative evaluations with the state-of-the-art methods on the NUS dataset [35], the selfie dataset [68], and the DeepStab dataset [61]. We evaluate the following metrics: Cropping Ratio (C), Distortion Value (D), Stability Score (S), and Accumulated Optical Flow (A). \color{Red}{\text{Red}} text indicates the best and \color{Blue}{\text{blue}} text indicates the second-best performing method.
Figure 5: 
Visual comparison to state-of-the-art methods. Our proposed fusion approach does not suffer from aggressive cropping of frame borders and renders stabilized frames with significantly fewer artifacts than DIFRINT [10]. We refer the readers to our supplementary material for extensive video comparisons with prior representative methods. 
4.2. Quantitative evaluation: We evaluate the proposed method with state-of-the-art video stabilization algorithms, including L1Stabilizer [18], Bundle [35], StabNet [61], DIFRINT [10], and Yu and Ramamoorthi [70], and the warp stabilizer in Adobe Premiere Pro CC 2020. StabNet is the only online method for performance evaluation. It is included because it provides the DeepStab dataset. We obtain the results of the compared methods from the videos released by the authors or generated from the publicly available official implementation with default parameters or pre-trained models. Datasets. We evaluate the above methods on the NUS dataset [35], the selfie dataset [68], and the DeepStab dataset [61]. The NUS dataset consists of 144 video sequences, which are classified into six categories: Simple, Quick rotation, Zooming, Parallax, Crowd, and Running. The selfie dataset includes 33 video clips with frontal faces and severe jittering. The DeepStab dataset includes 61 videos with forward, pan, spin, and complex movements. Metrics. We use the following metrics to evaluate the performance of video stabilization, which are widely used in prior work [35], [61], [10], [69]: 1) Cropping ratio: measures the remaining frame area after cropping off the undefined pixels due to motion compensation. 2) Distortion value: measures the anisotropic scaling of the homography between the input and output frames. 3) Stability score: measures the stability and smoothness of the stabilized video. 4) Accumulated optical flow: accumulates the optical flow over the entire stabilized video. Note that these metrics are used to evaluate the performance of video stabilization, while in Section 4.1 we use PSNR, SSIM [63], and LPIPS [71] to evaluate the quality of synthesized frames. Results on the NUS dataset. We show the average scores on the NUS dataset [35] on the left side of Table 2. Both DIFRINT[10] and our method are full-frame methods and thus have an average cropping ratio of 1. Note that the distortion metric measures the global distortion by fitting a homography between the input and stabilized frames. Therefore, it is not suitable to measure local distortion. Although DIFRINT [10] obtains the highest distortion score, its results contain visible local or color distortion due to iterative frame interpolation, as shown in Figure 5 and the supplementary material. Adobe Premiere Pro 2020 warp stabilizer obtains the highest stability score at the cost of a low cropping ratio (0.74). Our results achieve the best distortion score and accumulated optical flow, a good stability score, and an average cropping ratio of 1 (no cropping for all the videos), demonstrating our advantages over the state-of-theart approaches in the NUS dataset. Table 3: 
Using different flow smoothing methods in our framework. Our method improves three metrics on the Selfie dataset [68] when integrating with different flow smoothing methods.
Results on the Selfie dataset. The middle of Table 2 shows the average scores on the selfie dataset [68]. Note that the videos in the dataset often contain large camera motions and are more challenging to stabilize. Similar to the NUS dataset, our method achieves the best cropping ratio and accumulated flow. Our distortion and stability scores are comparable to Yu and Ramamoorthi’s method [68], which is specially designed to stabilize selfie videos. Results on the DeepStab dataset. We show the average scores on the DeepStab dataset [61] on the right side of Table 2. Our method achieves the best distortion, second-best accumulated flow, and good stability without cropping. Integrating with different flow smoothing methods. The proposed method can be easily integrated with existing flow smoothing methods to generate a full-frame stabilized video. Table 3 shows that the proposed method not only obtains full-frame results but also improves all metrics when combining with other smoothing methods [35] or [70]. 4.3. Visual comparison: We show the stabilized frame of our method and state-of-the-art approaches from the Selfie dataset in Figure 5. Most of the methods [18], [61], [35], [70] suffer from a large amount of cropping, as indicated by the green checkerboard regions. The aspect ratios of the output frames are also changed. To keep the same aspect ratio as the input video, excessive cropping is required. DIFRINT [10] generates full-frame results. However, its results often contain visible artifacts due to frame interpolation. In contrast, our method generates full-frame stabilized videos with fewer visual artifacts. Table 4: 
Quantitative comparison of view synthesis.
Figure 6: 
Qualitative results of view synthesis. 
4.4. View Synthesis: Although we develop our fusion method for video stabilization, it is not limited to stabilization, and we show that it can be integrated into other multi-image fusion tasks such as video completion and FOV expansion in the supplementary. The proposed hybrid fusion method can also be used for view synthesis. We compare our approach to state-of-the-art novel view synthesis methods on the Tanks and Temples dataset [24]. Table 4 and Figure 6 show that our method performs favorably against other compared view synthesis methods quantitatively and qualitatively. Our method achieves the best results among all the methods in all evaluated metrics. Visually, our method generates images with fewer artifacts and more details than FVS [44]. 

SECTION 5. Conclusions: We have presented a novel method for full-frame video stabilization. Our core idea is to develop a learning-based fusion approach to aggregate warped contents from multiple neighboring frames in a robust manner. We explore several design choices, including early/late fusion, heuristic/learned fusion weights, and residual detail transfer, and provide a systematic ablation study to validate the contributions of each component. Experiments on three public benchmarks demonstrate that our method compares favorably against state-of-the-art video stabilization algorithms. 
ACKNOWLEDGMENTS.: This work is supported in part by MOST 110-2221-E-002-124-MY3, 110-2634-F-002-026, and MediaTek Inc. We thank to National Center for High-performance Computing (NCHC) for providing computational and storage resources.