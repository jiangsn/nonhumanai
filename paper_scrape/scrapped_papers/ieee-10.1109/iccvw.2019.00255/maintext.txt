SECTION 1. Introduction: Learning to understand the relationship between 3D objects and 2D images is an important topic in computer vision and computer graphics. In computer vision, it has applications in fields such as robotics, autonomous vehicles or security. In computer graphics, it benefits applications in both content generation and manipulation. This ranges from photorealistic rendering of 3D scenes or sketch-based 3D modelling, to novel-view synthesis or relighting. Recent generative image models, in particular, generative adversarial networks (GANs), have achieved impressive results in generating images of high resolution and visual quality [3], [11], [12] while their conditional versions have achieved great progress in image-to-image translation [9], [22] or image editing [5], [26]. However, GANs are still fairly limited in their applications, since they do not allow explicit control over attributes in the generated images, while conditional GANs need labels during training (Figure 1 left), which are not always available. We propose HoloGAN, an unsupervised generative image model that learns representations of 3D objects that are not only explicit in 3D but also semantically expressive. In this work, we focus on designing a novel architecture that allows unsupervised learning of 3D representations from images, enabling direct manipulation of view, shape, and appearance in generative image models. Such representations can be learnt directly from unlabelled natural images, without any supervision of poses, 3D shapes, multiple views of objects, or geometry priors such as symmetry and smoothness over the 3D representation that are common in this line of work [1], [10]. Unlike other GAN models, HoloGAN employs both 3D and 2D features for generating images. HoloGAN first learns a 3D representation, which is then transformed to a target pose, projected to 2D features, and rendered to generate the final images (Figure 1 right). Different from recent work that employs handcrafted differentiable renderers [13], [15], [17], HoloGAN learns perspective projection and rendering of 3D features from scratch using a projection unit [20]. This novel architecture enables HoloGAN to learn 3D representations directly from natural images for which there are no good hand-crafted differentiable renderers. To generate new views of the same scene, we directly apply 3D rigid-body transformations to the learnt 3D features, and visualise the results using the neural renderer that is jointly trained. This has been shown to produce sharper results than performing 3D transformations in high-dimensional latent vector space [20]. In summary, our main technical contributions are: 1) A novel architecture that combines a strong inductive bias about the 3D world with deep generative models to learn disentangled representations (pose, shape, and appearance) of 3D objects from images. The representation is explicit in 3D and expressive in semantics. 2) An unconditional GAN that, for the first time, allows native support for view manipulation 3) An unsupervised training approach that enables disentangled representation learning without using labels.
Figure 1. Comparison of generative image models. Data given to the discriminator are coloured purple. Left: In conditional GANs, the pose is observed and the discriminator is given access to this information. Right: HoloGAN does not require pose labels and the discriminator is not given access to pose information. 


SECTION 2. Method: To learn 3D representations from 2D images without labels, HoloGAN extends traditional unconditional GANs by introducing a strong inductive bias about the 3D world into the generator network. Specifically, HoloGAN generates images by learning a 3D representation of the world and to render it realistically such that it fools discriminator. View manipulation therefore can be achieved by directly applying 3D rigid-body transformations to the learnt 3D features. In other words, the images created by the generator are a view-dependent mapping from a learnt 3D representation to the 2D image space. This is different from other GANs which learn to map a noise vector z directly to 2D features to generate images. Figure 2 illustrates the generator architecture of HoloGAN: HoloGAN first learns a 3D representation (assumed to be in a canonical pose) using 3D convolutions (Section 2.1), transforms this representation to a certain pose, projects and computes visibility using the projection unit (Section 2.2), and computes shaded colour values for each pixel in the final images with 2D convolutions. HoloGAN shares many rendering insights with RenderNet [20], but works with natural images, and needs neither pre-training of the neural renderer nor paired 3D shape-2D image training data. 2.1. Learning 3D Representations: HoloGAN generates 3D representations from a learnt constant tensor (see Figure 2). The random noise vector z instead is treated as a “style” controller, and mapped to affine parameters for adaptive instance normalization (AdaIN) [8] after each convolution using a multilayer perceptron (MLP) f:z→γ(z),σ(z). Given some features Φl at layer l of an image x and the noise “style” vector z, AdaIN is defined as:
AdaIN(Φl(x),z)=σ(z)(Φl(x)−μ(Φl(x))σ(Φl(x)))+γ(z).(1)View Source\begin{equation*}
\text{AdaIN}(\mathbf{\Phi}_{l}(\mathbf{x}),\mathbf{z})=\sigma(\mathbf{z})\left(\frac{\mathbf{\Phi}_{l}(\mathbf{x})-\mu(\mathbf{\Phi}_{l}(\mathbf{x}))}{\sigma(\mathbf{\Phi}_{l}(\mathbf{x}))}\right)+\gamma(\mathbf{z}). \tag{1}
\end{equation*} This can be viewed as generating images by transforming a template (the learnt constant tensor) using AdaIN to match the mean and standard deviation of the features at different levels l (which are believed to describe the image “style”) of the training images. Empirically, we find this network architecture can disentangle pose and identity much better than those that feed the noise vector z directly to the first layer of the generator. HoloGAN inherits this style-based strategy from StyleGAN [12] but is different in two important aspects. Firstly, HoloGAN learns 3D features from a learnt 4D constant tensor (size 4×4×4×512, where the last dimension is the feature channel) before projecting them to 2D features to generate images, while StyleGAN only learns 2D features. Secondly, HoloGAN learns a disentangled representation by combining 3D features with rigid-body transformations during training, while StyleGAN injects independent random noise into each convolution. StyleGAN, as a result, learns to separate 2D features into different levels of detail, depending on the feature resolution, from coarse (e.g., pose, identity) to more fine-grained details (e.g., hair, freckles). We observe a similar separation in HoloGAN. However, HoloGAN further separates pose (controlled by the 3D transformation), shape (controlled by 3D features), and appearance (controlled by 2D features). 2.2. Learning with View-Dependent Mappings: In addition to adopting 3D convolutions to learn 3D features, during training, we introduce more bias about the 3D world by transforming these learnt features to random poses before projecting them to 2D images. This random pose transformation is crucial to guarantee that HoloGAN learns a 3D representation that is disentangled and can be rendered from all possible views as also observed by Tran et al. [23] in DR-GAN. Rigid-Body TransformationWe assume a virtual pin-hole camera that is in the canonical pose (axis-aligned and placed along the negative z-axis) relative to the 3D features being rendered. We parameterise the rigid-body transformation by 3D rotation, scaling followed by trilinear resampling. Assuming the up-vector of the object coordinate system is the global y-axis, rotation comprises rotation around the y-axis (azimuth) and the x-axis (elevation). Projection UnitIn order to learn meaningful 3D representations from just 2D images, HoloGAN learns a differentiable projection unit [20] that reasons over occlusion. The projection unit is composed of a reshaping layer that concatenates the channel dimension with the depth dimension, thus reducing the tensor dimension from 4D(W×H×D×C) to 3D (W×H×(D⋅C)), and an MLP with a non-linear activation function to learn occlusion. 2.3. Loss Functions: Identity RegulariserTo generate images at higher resolution (128×128 pixels), we find it beneficial to add an identity regulariser Lidentity that ensures a vector reconstructed from a generated image matches the latent vector z used in the generator G. We find that this encourages HoloGAN to only use z for the identity to maintain the object's identity when poses are varied, helping the model learn the full variation of poses in the dataset. We introduce an encoder network F that shares the majority of the convolution layers of the discriminator, but uses an additional fully-connected layer to predict the reconstructed latent vector. The identity loss is: Lidentity(G)=Ez∥z−F(G(z))∥2.(2)View Source\begin{equation*}
L_{\text{identity}}(\mathrm{G})=\mathbb{E}_{\mathbf{z}}\Vert \mathbf{z}-\mathrm{F}(\mathrm{G}(\mathbf{z}))\Vert^{2}. \tag{2}
\end{equation*} Style DiscriminatorOur generator is designed to match the “style” of the training images at different levels, which effectively controls image attributes at different scales. Therefore, in addition to the image discriminator that classifies images as real or fake, we propose multi-scale style discriminators that perform the same task but at the feature level. In particular, the style discriminator tries to classify the mean μ(Φl) and standard deviation σ(Φl), which describe the image “style” [8]. Empirically, the multi-scale style discriminator helps prevent mode collapse and enables longer training. Given a style discriminator Dl(x)=D˜l(μ(Φl(x)),σ(Φl(x)) for layer l, the style loss is defined as: Llstyle(G)=Ez[−logDl(G(z))].(3)View Source\begin{equation*}
L_{\text{style}}^{l}(\mathrm{G})=\mathbb{E}_{\mathbf{z}}[-\log \mathrm{D}_{l}(\mathrm{G}(\mathbf{z}))]. \tag{3}
\end{equation*} The total loss can be written as: Ltotal(G)=LGAN(G)+λi⋅Lidentity(G)+λs⋅∑lLlstyle(G).(4)View Source\begin{equation*}
L_{\text{total}}(\mathrm{G})=L_{\text{GAN}}(\mathrm{G})+\lambda_{\mathrm{i}}\cdot L_{\text{identity}}(\mathrm{G})+\lambda_{\mathrm{s}}\cdot\sum\limits_{l}L_{\text{style}}^{l}(\mathrm{G}). \tag{4}
\end{equation*} We use λi=λs=1.0 for all experiments. We use the GAN loss from DC-GAN [21] for LGAN.
Figure 2. HoloGAN's generator network: We employ 3D convolutions, 3D rigid-body transformations, the projection unit and 2D convolutions. We also remove the traditional input layer from z, and start from a learnt constant 4D tensor. The latent vector z is instead fed through MLPs to map to the affine transformation parameters for adaptive instance normalisation (AdaIN). Inputs are coloured gray. Best viewed in colour. 


SECTION 3. Results: Data: We train HoloGAN using a variety of datasets: CelebA [16], Cats [27], Chairs [4], Cars [24], and LSUN bedroom [25]. We train HoloGAN on resolutions of 64×64 pixels for Cats and Chairs, and 128×128 pixels for CelebA, Cars and Bedroom. Implementation Details: We use adaptive instance normalization [8] for the generator, and spectral normalization [19] for the discriminator. We train HoloGAN from scratch using the Adam solver [14]. During training, we sample z∼U(−1,1), and also sample random poses from a uniform distribution We use |z|=200 for Cars at 128×128, and |z|=200 for the rest. Qualitative Evaluation: Figures 3 and 4 show that HoloGAN can smoothly vary the pose along azimuth and elevation while keeping the same identities for multiple different datasets. Note that the LSUN dataset contains a variety of complex layouts of multiple objects. This makes it a very challenging dataset for learning to disentangle pose from object identity.
Figure 3. HoloGAN supports changes in both azimuth (range: 100°) and elevation (range: 35°). However, the available range depends on the dataset. For CelebA, for example, few photos in the dataset were taken from above or below. 
Figure 4. For the car dataset, HoloGAN fully captures the full 360° azimuth and elevation (range: 35°). 
Quantitative Results: To evaluate the visual fidelity of generated images, we use the Kernel Inception Distance (KID) by Bińkowski et al. [2]. The lower the KID score, the better the visual quality of generated images. We compare HoloGAN with other recent GAN models: DCGAN [21], LSGAN [18], and WGAN-GP [6], on 3 datasets in Table 1. Note that KID does not take into account feature disentanglement, which is one of the main contributions of HoloGAN. We use a publicly available implemenration1 and use the same hyper-parameters (that were tuned for CelebA) provided with this implementation for all three datasets. Similarly, for HoloGAN, we use the same network architecture and hyper-parameters 2 for all three datasets. We sample 20,000 images from each model to calculate the KID scores shown below. Table 1 shows that HoloGAN can generate images with competitive (for CelebA) or even better KID scores on more challenging datasets: Chairs, which has high intra-class variability, and Cars, which has complex backgrounds and lighting conditions. This also shows that HoloGAN architecture is more robust and can consistently produce images with high visual fidelity across different datasets with the same set of hyper-parameters (except for azimuth ranges). More importantly, HoloGAN learns a disentangled representation that allows manipulations of the generated images.
Table 1. KID [7] between real images and images generated by HoloGAN and other 2D-based GANs (lower is better). We report KID mean×100±std.×100.
Disentangling Shape and Appearance: We show that apart from pose, HoloGAN also learns to further divide identity into shape and appearance. We sample two latent codes, z1 and z2, and feed them through HoloGAN. While z1 controls the 3D features (before perspective morphing and projection), z2 controls the 2D features (after projection). Figure 5 shows the generated images with the same pose, same z1, but with a different z2 at each row. As can be seen, while the 3D features control objects' shapes, the 2D features control appearance (colour and lighting). This shows HoloGAN learns to separate shape from appearance directly from unlabelled images, allowing manipulation of these factors.
Figure 5. Combinations of different latent vectors z1 (for 3D features) and z2 (for 2D features). While z1 influences objects' shapes, z2 determines appearance (texture and lighting). Best viewed in colour. 


SECTION 4. Discussion: HoloGAN's ability to separate pose from identity depends on the variety and distribution of poses included in the training dataset. Currently, during training, we sample random poses from a uniform distribution. Future work therefore can explore learning the distribution of poses from the training data in an unsupervised manner to account for uneven pose distributions. Finally, it will be interesting to explore further disentanglement of objects' appearances, such as texture and illumination.