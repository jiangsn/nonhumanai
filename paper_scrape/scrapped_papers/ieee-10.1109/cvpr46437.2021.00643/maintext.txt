SECTION 1. Introduction: The topic of novel view synthesis has recently seen impressive progress due to the use of neural networks to learn representations that are well suited for view synthesis tasks. Most prior approaches in this domain make the assumption that the scene is static, or that it is observed from multiple synchronized input views. However, these restrictions are violated by most videos shared on the Internet today, which frequently feature scenes with diverse dynamic content (e.g., humans, animals, vehicles), recorded by a single camera. Figure 1: 
Our method can synthesize novel views in both space and time from a single monocular video of a dynamic scene. Here we show video results with various configurations of fixing and interpolating view and time (left), as well as a visualization of the recovered scene geometry (right). Please view with Adobe Acrobat or KDE Okular to see animations. 
We present a new approach for novel view and time synthesis of dynamic scenes from monocular video input with known (or derivable) camera poses. This problem is highly ill-posed since there can be multiple scene configurations that lead to the same observed image sequences. In addition, using multi-view constraints for moving objects is challenging, as doing so requires knowing the dense 3D motion of all scene points (i.e., the "scene flow"). In this work, we propose to represent a dynamic scene as a continuous function of both space and time, where its output consists of not only reflectance and density, but also 3D scene motion. Similar to prior work, we parameterize this function with a deep neural network (a multi-layer perceptron, MLP), and perform rendering using volume tracing [46]. We optimize the weights of this MLP using a scene flow fields warping loss that enforces that our scene representation is temporally consistent with the input views. Crucially, as we model dense scene flow fields in 3D, our function can represent the sharp motion discontinuities that arise when projecting the scene into image space, even with simple low-level 3D smoothness priors. Further, dense scene flow fields also enable us to interpolate along changes in both space and time. To the best our knowledge, our approach is the first to achieve novel view and time synthesis of dynamic scenes captured from a monocular camera. As the problem is very challenging, we introduce different components that improve rendering quality over a baseline solution. Specifically, we analyze scene flow ambiguity at motion disocclusions and propose a solution to it. We also show how to use data-driven priors to avoid local minima during optimization, and describe how to effectively combine a static scene representation with a dynamic one which lets us render views with higher quality by leveraging multi-view constraints in static regions. In summary, our key contributions include: (1) a neural representation for space-time view synthesis of dynamic scenes that we call Neural Scene Flow Fields, that has the capacity to model 3D scene dynamics, and (2) a method for optimizing Neural Scene Flow Fields on monocular video by leveraging multiview constraints in both rigid and non-rigid regions, allowing us to synthesize and interpolate both view and time simultaneously. 

SECTION 2. Related Work: Our approach is motivated by a large body of work in the areas of novel view synthesis, dynamic scene reconstruction, and video understanding. Novel view synthesis. Many methods propose first building an explicit 3D scene geometry such as point clouds or meshes, and rendering this geometry from novel views [11], [13], [17], [24], [33], [68]. Light field rendering methods on the other hand, synthesize novel views by using implicit soft geometry estimates derived from densely sampled images [12], [22], [35]. Numerous other works improve the rendering quality of light fields by exploiting their special structure [16], [57], [63], [75]. Yet another promising 3D representation is multiplane images (MPIs), that have been shown to model complex scene appearance [9], [10], [15], [20], [45], [69]. Recently, deep learning methods have shown promising results by learning a representation that is suited for novel view synthesis. Such methods have learned additional deep features that exist on top of reconstructed meshes [25], [60], [73] or dense depth maps [21], [79]. Alternately, pure voxel-based implicit scene representations have become popular due to their simplicity and CNN-friendly structure [14], [19], [38], [66], [67], [73]. Our method is based on a recent variation of these approaches to represent a scene as neural radiance field (NeRF) [46], which model the appearance and geometry of a scene implicitly by a continuous function, represented with an MLP. While the above methods have shown impressive view synthesis results, they all assume a static scene with fixed appearance over time, and hence cannot model temporal changes or dynamic scenes. Another class of methods synthesize novel views from a single RGB image. These methods typically work by predicting depth maps [36], [54], sometimes with additional learned features [78], or a layered scene representation [64], [74] to fill in the content in disocclusions. While such methods, if trained on appropriate data, can be used on dynamic scenes, this is only possible on a per-frame (instantaneous) basis, and they cannot leverage repeated observations across multiple views, or be used to synthesize novel times. Novel time synthesis. Most approaches for interpolating between video frames work in 2D image space, by directly predicting kernels that blend two images [51], [52], [53], or by modeling optical flow and warping frames/features [2], [31], [49], [50]. More recently, Lu et al. [39] show re-timing effect of people by using a layered representation. These approaches generate high-quality frame interpolation results, but operate in 2D and cannot be used to synthesize novel views in space. Space-time view synthesis. There are two main reasons that scenes change appearance across time. The first is due to illumination changes; prior approaches have proposed to render novel views of single object with plausible relighting [4], [5], [6], or model time-varying appearance from internet photo collections [37], [42], [44]. However, these methods operate on static scenes and treat moving objects as outliers. Second, appearance change can happen due to 3D scene motion. Most prior work in this domain [1], [3], [70], [85] require multi-view, time synchronized videos as input, and has limited ability to model complicated scene geometry. Most closely related ours, Yoon et al. [81] propose to combine single-view depth and depth from multi-view stereo to render novel views by performing explicit depth based 3D warping. However, this method has several drawbacks: it relies on human annotated foreground masks, requires cumbersome preprocessing and pretraining and tends to produce artifacts in disocclusions. Instead, we show that our model can be trained end-to-end and produces much more realistic results, and is able to represent complicated scene structure and view dependent effects along with natural degrees of motion. Dynamic scene reconstruction. Most successful nonrigid reconstruction systems either require RGBD data as input [7], [18], [29], [48], [80], [86], or can only reconstruct sparse geometry [56], [65], [77], [83]. A few prior monocular methods proposed using strong hand-crafted priors to decompose dynamic scenes into piece-wise rigid parts [34], [59], [61]. Recent work of Luo et al. [40] estimates temporally consistent depth maps of scenes with small object motion by optimizing the weights of a single image depth prediction network, but we show that this approach fails to model large and complex 3D motions. Additional work has aimed to predict per-pixel scene flows of dynamic scenes from either monocular or RGBD sequences [8], [26], [30], [41], [47], [71]. 

SECTION 3. Approach: We build upon prior work for static scenes [46], to which we add the notion of time, and estimate 3D motion by explicitly modeling forward and backward scene flow as dense 3D vector fields. In this section, we first describe this time-variant (dynamic) scene representation (Sec. 3.1) and the method for effectively optimizing this representation (Sec. 3.2) on the input views. We then discuss how to improve the rendering quality by adding an additional explicit time-invariant (static) scene representation, optimized jointly with the dynamic one by combining both during rendering (Sec. 3.3). Finally, we describe how to achieve space-time interpolation of dynamic scenes through our trained representation (Sec. 3.4). Background: static scene rendering. Neural Radiance Fields (NeRFs) [46] represent a static scene as a radiance field defined over a bounded 3D volume. This radiance field, denoted FΘ, is defined by a set of parameters Θ that are optimized to reconstruct the input views. In NeRF, FΘ is a multi-layer perceptron (MLP) that takes as input a position (x) and viewing direction (d), and produces as output a volumetric density (σ) and RGB color (c):
\begin{equation*}({\mathbf{c}},\sigma ) = {F_\Theta }({\mathbf{x}},{\mathbf{d}})\tag{1}\end{equation*}(c,σ)=FΘ(x,d)(1)View Source\begin{equation*}({\mathbf{c}},\sigma ) = {F_\Theta }({\mathbf{x}},{\mathbf{d}})\tag{1}\end{equation*} To render the color of an image pixel, NeRF approximates a volume rendering integral. Let r be the camera ray emitted from the center of projection through a pixel on the image plane. The expected color Ĉ of that pixel is then given by:
\begin{equation*}\begin{array}{c} \widehat {\mathbf{C}}({\mathbf{r}}) = \int_{{t_n}}^{{t_f}} T (t)\sigma ({\mathbf{r}}(t)){\mathbf{c}}({\mathbf{r}}(t),{\mathbf{d}})dt \\ {\text{where }}T(t) = \exp \left( { - \int_{{t_n}}^t \sigma ({\mathbf{r}}(s))ds} \right){\text{. }} \end{array} \tag{2}\end{equation*}Cˆ(r)=∫tftnT(t)σ(r(t))c(r(t),d)dtwhere T(t)=exp(−∫ttnσ(r(s))ds). (2)View Source\begin{equation*}\begin{array}{c} \widehat {\mathbf{C}}({\mathbf{r}}) = \int_{{t_n}}^{{t_f}} T (t)\sigma ({\mathbf{r}}(t)){\mathbf{c}}({\mathbf{r}}(t),{\mathbf{d}})dt \\ {\text{where }}T(t) = \exp \left( { - \int_{{t_n}}^t \sigma ({\mathbf{r}}(s))ds} \right){\text{. }} \end{array} \tag{2}\end{equation*} Intuitively, T (t) corresponds to the accumulated transparency along that ray. The loss is then the difference between the reconstructed color Ĉ, and the ground truth color C corresponding to the pixel that ray originated from r:
\begin{equation*}{\mathcal{L}_{{\text{static}}}} = \sum\limits_{\mathbf{r}} {\left\| {\widehat {\mathbf{C}}({\text{r}}) - {\mathbf{C}}({\mathbf{r}})} \right\|_2^2} .\tag{3}\end{equation*}View Source\begin{equation*}{\mathcal{L}_{{\text{static}}}} = \sum\limits_{\mathbf{r}} {\left\| {\widehat {\mathbf{C}}({\text{r}}) - {\mathbf{C}}({\mathbf{r}})} \right\|_2^2} .\tag{3}\end{equation*} 3.1. Neural scene flow fields for dynamic scenes: To capture scene dynamics, we extend the static scenario described in Eq. 1 by including time in the domain and explicitly modeling 3D motion as dense scene flow fields. For a given 3D point x and time i, the model predicts not just reflectance and opacity, but also forward and backward 3D scene flow {\mathcal{F}_i} = \left( {{{\mathbf{f}}_{i \to i + 1}},{{\mathbf{f}}_{i \to i - 1}}} \right), which denote 3D offset vectors that point to the position of x at times i + 1 and i ℒ1 respectively. Note that we make the simplifying assumption that movement that occurs between observed time instances is linear. To handle motion disocclusions in 3D space, we also predict disocclusion weights {\mathcal{W}_i} = \left( {{w_{i \to i + 1}},{w_{i \to i - 1}}} \right) (described in Sec. 3.2). Our dynamic model is thus defined as:
\begin{equation*}\left( {{{\mathbf{c}}_i},{\sigma _i},{\mathcal{F}_i},{\mathcal{W}_i}} \right) = F_\Theta ^{{\text{dy}}}({\mathbf{x}},{\mathbf{d}},i).\tag{4}\end{equation*}View Source\begin{equation*}\left( {{{\mathbf{c}}_i},{\sigma _i},{\mathcal{F}_i},{\mathcal{W}_i}} \right) = F_\Theta ^{{\text{dy}}}({\mathbf{x}},{\mathbf{d}},i).\tag{4}\end{equation*} Note that for convenience, we use the subscript i to indicate a value at a specific time i. Figure 2: 
Scene flow fields warping. To render a frame at time i, we perform volume rendering along ray ri with RGBσ at time i, giving us the pixel color Ĉi(ri) (left). To warp the scene from time j to i, we offset each step along ri using scene flow fi→j and volume render with the associated color and opacity (cj, σj) (right). 
3.2. Optimization: Temporal photometric consistency. The key new loss we introduce enforces that the scene at time i should be consistent with the scene at neighboring times j \in \mathcal{N}(i), when accounting for motion that occurs due to 3D scene flow. To do this, we volume render the scene at time i from 1) the perspective of the camera at time i and 2) with the scene warped from j to i, so as to undo any motion that occurred from i to j. As shown in Fig. 2 (right), during volume rendering, we achieve this by warping each 3D sampled point location xi along a ray ri using the predicted scene flows fields {\mathcal{F}_i} to look up the RGB color cj and opacity σj at neighboring time j. This yields a rendered image, denoted Ĉj→i, of the scene at time j with both camera and scene motion warped to time i:
\begin{equation*}\begin{array}{c} {\widehat {\mathbf{C}}_{j \to i}}\left( {{{\mathbf{r}}_i}} \right) = \int_{{t_n}}^{{t_f}} {{T_j}} (t){\sigma _j}\left( {{{\mathbf{r}}_{i \to j}}(t)} \right){{\mathbf{c}}_j}\left( {{{\mathbf{r}}_{i \to j}}(t),{{\mathbf{d}}_i}} \right)dt \\ {\text{where }}{{\mathbf{r}}_{i \to j}}(t) = {{\mathbf{r}}_i}(t) + {{\mathbf{f}}_{i \to j}}\left( {{{\mathbf{r}}_i}(t)} \right){\text{.}} \end{array} \tag{5}\end{equation*}View Source\begin{equation*}\begin{array}{c} {\widehat {\mathbf{C}}_{j \to i}}\left( {{{\mathbf{r}}_i}} \right) = \int_{{t_n}}^{{t_f}} {{T_j}} (t){\sigma _j}\left( {{{\mathbf{r}}_{i \to j}}(t)} \right){{\mathbf{c}}_j}\left( {{{\mathbf{r}}_{i \to j}}(t),{{\mathbf{d}}_i}} \right)dt \\ {\text{where }}{{\mathbf{r}}_{i \to j}}(t) = {{\mathbf{r}}_i}(t) + {{\mathbf{f}}_{i \to j}}\left( {{{\mathbf{r}}_i}(t)} \right){\text{.}} \end{array} \tag{5}\end{equation*} We minimize the mean squared error (MSE) between each warped rendered view and the ground truth view:
\begin{equation*}{\mathcal{L}_{{\text{pho}}}} = \sum\limits_{{{\mathbf{r}}_i}} {\sum\limits_{j \in \mathcal{N}(i)} {\left\| {{{\widehat {\mathbf{C}}}_{j \to i}}\left( {{{\mathbf{r}}_i}} \right) - {{\mathbf{C}}_i}\left( {{{\mathbf{r}}_i}} \right)} \right\|_2^2} } \tag{6}\end{equation*}View Source\begin{equation*}{\mathcal{L}_{{\text{pho}}}} = \sum\limits_{{{\mathbf{r}}_i}} {\sum\limits_{j \in \mathcal{N}(i)} {\left\| {{{\widehat {\mathbf{C}}}_{j \to i}}\left( {{{\mathbf{r}}_i}} \right) - {{\mathbf{C}}_i}\left( {{{\mathbf{r}}_i}} \right)} \right\|_2^2} } \tag{6}\end{equation*} An important caveat is that this loss is ambiguous at 3D disocculusion regions caused by motion. Analogous to 2D dense optical flow [43], scene flow is ambiguous when a 3D location becomes occluded or disoccluded between frames. These regions are especially important as they occur at the boundaries of moving objects (see Fig. 3 for an illustration). To mitigate errors due to this ambiguity, we predict extra continuous disocclusion weight fields wi→i+1 and wi→i−1 ∈ℒ[0, 1], corresponding to fi→i+1 and fi→i−1 respectively. These weights serve as an unsupervised confidence of where and how much strength the temporal photo-consistency loss should be applied. We apply these weights by volume rendering the weight along the ray ri with opacity from time j, and multiplying the accumulated weight at each 2D pixel:
\begin{equation*}{\hat W_{j \to i}}\left( {{{\mathbf{r}}_i}} \right) = \int_{{t_n}}^{{t_f}} {{T_j}} (t){\sigma _j}\left( {{{\mathbf{r}}_{i \to j}}(t)} \right){w_{i \to j}}\left( {{{\mathbf{r}}_i}(t)} \right)dt\tag{7}\end{equation*}View Source\begin{equation*}{\hat W_{j \to i}}\left( {{{\mathbf{r}}_i}} \right) = \int_{{t_n}}^{{t_f}} {{T_j}} (t){\sigma _j}\left( {{{\mathbf{r}}_{i \to j}}(t)} \right){w_{i \to j}}\left( {{{\mathbf{r}}_i}(t)} \right)dt\tag{7}\end{equation*} We avoid the trivial solution where all predicted weights are zero by adding ℓ1 regularization to encourage predicted weights to be close to one, giving us a new weighted loss:
\begin{equation*}\begin{array}{c} {\mathcal{L}_{{\text{pho}}}} = \sum\limits_{{{\mathbf{r}}_i}} {\sum\limits_{j \in \mathcal{N}(i)} {{{\hat W}_{j \to i}}} } \left( {{{\mathbf{r}}_i}} \right)\left\| {{{\widehat {\mathbf{C}}}_{j \to i}}\left( {{{\mathbf{r}}_i}} \right) - {{\mathbf{C}}_i}\left( {{{\mathbf{r}}_i}} \right)} \right\|_2^2 \\ + {\beta _w}\sum\limits_{{{\mathbf{x}}_i}} {{{\left\| {{w_{i \to j}}\left( {{{\mathbf{x}}_i}} \right) - 1} \right\|}_1}} , \end{array} \tag{8}\end{equation*}View Source\begin{equation*}\begin{array}{c} {\mathcal{L}_{{\text{pho}}}} = \sum\limits_{{{\mathbf{r}}_i}} {\sum\limits_{j \in \mathcal{N}(i)} {{{\hat W}_{j \to i}}} } \left( {{{\mathbf{r}}_i}} \right)\left\| {{{\widehat {\mathbf{C}}}_{j \to i}}\left( {{{\mathbf{r}}_i}} \right) - {{\mathbf{C}}_i}\left( {{{\mathbf{r}}_i}} \right)} \right\|_2^2 \\ + {\beta _w}\sum\limits_{{{\mathbf{x}}_i}} {{{\left\| {{w_{i \to j}}\left( {{{\mathbf{x}}_i}} \right) - 1} \right\|}_1}} , \end{array} \tag{8}\end{equation*}
where βw is a regularization weight which we set to 0.1 in all our experiments. We use \mathcal{N}(i) = \{ i,i \pm 1,i \pm 2\} , and chain scene flow and disocclusion weights forℒthe I ℒ2 cases. Note that when j = i, there is no scene flow warping or disocculusion weights involved (fi→j = 0, Ŵj→i( ri) = 1), meaning that Ĉi→i(ri) = Ĉi(ri), as in Fig. 2(left). Comparing Fig. 4(e) and Fig. 4(d), we can see that adding this disocclusion weight improves rendering quality near motion boundaries. Figure 3: 
Scene flow disocclusion ambiguity. In this 2D orthographic example, a single blue object translates to the right by one unit from frame i to frame j. Here, the correct scene flow at the point labeled a, e.g., fi→j (a), points one unit to the right, however, for the scene flow fi→j (c) (and similarly fj→i (a)), there can be multiple answers. If fi→j (c) = 0, then the scene flow would incorrectly point to the foreground in the next frame, and if fi→j (c) = 1, the scene flow would point to the free-space location d at j. 
Scene flow priors. To regularize the predicted scene flow fields, we add a 3D scene flow cycle consistency term to encourage that at all sampled 3D points xi, the predicted forward scene flow fi→j is consistent with the backward scene flow fj→i at the corresponding location at time j (i.e. at position xi→j = xi +fi→j). Note that this cycle consistency can also be ambiguous near motion disocclusion regions in 3D, so we use the same predicted disocclusion weights to modulate this term, giving us:
\begin{equation*}{\mathcal{L}_{{\text{cyc}}}} = \sum\limits_{{{\mathbf{x}}_i}} {\sum\limits_{j \in i \pm 1} {{w_{i \to j}}} } {\left\| {{{\mathbf{f}}_{i \to j}}\left( {{{\mathbf{x}}_i}} \right) + {{\mathbf{f}}_{j \to i}}\left( {{{\mathbf{x}}_{i \to j}}} \right)} \right\|_1}\tag{9}\end{equation*}View Source\begin{equation*}{\mathcal{L}_{{\text{cyc}}}} = \sum\limits_{{{\mathbf{x}}_i}} {\sum\limits_{j \in i \pm 1} {{w_{i \to j}}} } {\left\| {{{\mathbf{f}}_{i \to j}}\left( {{{\mathbf{x}}_i}} \right) + {{\mathbf{f}}_{j \to i}}\left( {{{\mathbf{x}}_{i \to j}}} \right)} \right\|_1}\tag{9}\end{equation*} We additionally add low-level regularizations ℒreg on the predicted scene flow fields. First, following prior work [48], [77], we enforce scene flow spatial-temporal smoothness by applying ℓ1 regularization to nearby sampled 3D points along the ray and encouraging 3D point trajectories to be piecewise linear. Second, we encourage scene flow to be small in most places [76] by applying an ℓ1 regularization term. Please see the supplementary material for complete descriptions. Figure 4: 
Qualitative ablations. Results of our full method with different loss components removed. The odd rows show zoom-in rendered color and the even rows show corresponding pseudo depth. Each component reduces the overall quality in different ways. 
Data-driven priors. Since monocular reconstruction of complex dynamic scenes is highly ill-posed, the above losses can on occasion converge to sub-optimal local minima when randomly initialized. Therefore, we introduce two data-driven losses, a geometric consistency term [29], [28] and a single-view depth term: ℒdata = ℒgeo + βzℒ z. We set β = 2 in all our experiments. The geometric consistency helps model build accurate correspondence association between adjacent frames. In particular, it minimizes the reprojection error of scene flow displaced 3D points w.r.t. the derived 2D optical flow which we compute using pretrained networks [27], [72]. Suppose pi is a 2D pixel position at time i. The corresponding 2D pixel location in the neighboring frame at time j displaced through 2D optical flow ui→j can be computed as pi→j = pi + ui→j. To estimate the expected 2D point location {\widehat {\mathbf{p}}_{i \to j}} at time j displaced by predicted scene flow fields, we first compute the expected scene flow {\widehat {\mathbf{F}}_{i \to j}}\left( {{{\mathbf{r}}_i}} \right) and the expected 3D point location {\widehat {\mathbf{X}}_i}\left( {{{\mathbf{r}}_i}} \right) of the ray ri through volume rendering. {\widehat {\mathbf{p}}_{i \to j}} is then computed by performing perspective projection of the expected 3D point location displaced by the scene flow (i.e. {\widehat {\mathbf{X}}_i}\left( {{{\mathbf{r}}_i}} \right) + {\widehat {\mathbf{F}}_{i \to j}}\left( {{{\mathbf{r}}_i}} \right)) into the viewpoint corresponding to the frame at time j. The geometric consistency is computed as the ℓ1 difference between {\widehat {\mathbf{p}}_{i \to j}}{\text{and }}{{\mathbf{p}}_{i \to j}}, \begin{equation*}{\mathcal{L}_{{\text{geo}}}} = \sum\limits_{{{\mathbf{r}}_i}} {\sum\limits_{j \in \{ i \pm 1\} } {{{\left\| {\left. {{{\widehat {\mathbf{p}}}_{i \to j}}\left( {{{\mathbf{r}}_i}} \right) - {{\text{p}}_{i \to j}}\left( {{{\mathbf{r}}_i}} \right)} \right)} \right\|}_1}} }.\tag{10}\end{equation*}View Source\begin{equation*}{\mathcal{L}_{{\text{geo}}}} = \sum\limits_{{{\mathbf{r}}_i}} {\sum\limits_{j \in \{ i \pm 1\} } {{{\left\| {\left. {{{\widehat {\mathbf{p}}}_{i \to j}}\left( {{{\mathbf{r}}_i}} \right) - {{\text{p}}_{i \to j}}\left( {{{\mathbf{r}}_i}} \right)} \right)} \right\|}_1}} }.\tag{10}\end{equation*}Figure 5: 
Dynamic and static components. Our method learns static and dynamic components in the combined representation. Note person is almost still in the bottom example. 
We also add a single view depth prior that encourages the expected termination depth {\hat Z_i} computed along each ray to be close to the depth Zi predicted from a pre-trained single-view network [58]. As single-view depth predictions are defined up to an unknown scale and shift, we utilize a robust scale-shift invariant loss [58]:
\begin{equation*}{\mathcal{L}_z} = \sum\limits_{{{\mathbf{r}}_i}} {{{\left\| {\hat Z_i^{\ast}\left( {{{\mathbf{r}}_i}} \right) - Z_i^{\ast}\left( {{{\mathbf{r}}_i}} \right)} \right\|}_1}} \tag{11}\end{equation*}View Source\begin{equation*}{\mathcal{L}_z} = \sum\limits_{{{\mathbf{r}}_i}} {{{\left\| {\hat Z_i^{\ast}\left( {{{\mathbf{r}}_i}} \right) - Z_i^{\ast}\left( {{{\mathbf{r}}_i}} \right)} \right\|}_1}} \tag{11}\end{equation*}
where ∗ is a whitening operation that normalizes the depth to have zero mean and unit scale. From Fig 4(b), we see that adding data-driven priors help the model learn correct scene geometry especially for dynamic regions. However, as both of these data-driven priors are noisy (rely on inaccurate or incorrect predictions), we use these for initialization only, and linearly decay the weight of ℒdata to zero during training. 3.3. Integrating a static scene representation: The method described so far already outperforms the state of the art, as shown in Tab. 1. However, unlike NeRF, our warping-based temporal loss can only be used in a local temporal neighborhood \mathcal{N}(i), as dynamic components typically undergo too much deformation to reliably infer correspondence over larger temporal gaps. Static regions, however, should be consistent and should leverage observations from all frames. Therefore, we propose to combine our dynamic (time-dependent) scene representation with a static (time-independent) one, and require that when combined, the resulting volume-rendered images match the input. We model each representation with its own MLP, where the dynamic scene component is represented with Eq. 4, and the static one is represented as a variant of Eq. 1:
\begin{equation*}({\mathbf{c}},\sigma ,v) = F_\Theta ^{{\text{st}}}({\mathbf{x}},{\mathbf{d}})\tag{12}\end{equation*}View Source\begin{equation*}({\mathbf{c}},\sigma ,v) = F_\Theta ^{{\text{st}}}({\mathbf{x}},{\mathbf{d}})\tag{12}\end{equation*}
where v is an unsupervised 3D blending weight field, that linearly blends the RGBσ from static and dynamic scene representations along the ray. Intuitively, v should assign a low weight to the dynamic representation at static regions with sufficient observations, as these can be rendered in higher fidelity by the static representation, while assigning a lower weight to the static representation in regions that are moving, as these can be better modeled by the dynamic representation. We found adding the extra v leads to less artifacts and more stable training than the configuration without v. The combined rendering equation is then written as:
\begin{equation*}\widehat {\mathbf{C}}_i^{{\text{cb}}}\left( {{{\mathbf{r}}_i}} \right) = \int_{{t_n}}^{{t_f}} {T_i^{{\text{cb}}}} (t)\sigma _i^{{\text{cb}}}(t){\mathbf{c}}_i^{{\text{cb}}}(t)dt,\tag{13}\end{equation*}View Source\begin{equation*}\widehat {\mathbf{C}}_i^{{\text{cb}}}\left( {{{\mathbf{r}}_i}} \right) = \int_{{t_n}}^{{t_f}} {T_i^{{\text{cb}}}} (t)\sigma _i^{{\text{cb}}}(t){\mathbf{c}}_i^{{\text{cb}}}(t)dt,\tag{13}\end{equation*}
where \sigma _i^{{\text{cb}}}(t){\mathbf{c}}_i^{{\text{cb}}}(t) is a linear combination of static and dynamic scene components, weighted by v(t):
\begin{equation*}\sigma _i^{{\text{cb}}}(t){\mathbf{c}}_i^{{\text{cb}}}(t) = v(t){\mathbf{c}}(t)\sigma (t) + (1 - v(t)){{\mathbf{c}}_i}(t){\sigma _i}(t)\tag{14}\end{equation*}View Source\begin{equation*}\sigma _i^{{\text{cb}}}(t){\mathbf{c}}_i^{{\text{cb}}}(t) = v(t){\mathbf{c}}(t)\sigma (t) + (1 - v(t)){{\mathbf{c}}_i}(t){\sigma _i}(t)\tag{14}\end{equation*} For clarity, we omit ri in each prediction. We then train the combined scene representation by minimizing MSE between \widehat {\mathbf{C}}_i^{{\text{cb}}} with the corresponding input view: \begin{equation*}{\mathcal{L}_{{\text{cb}}}} = \sum\limits_{{{\mathbf{r}}_i}} {\left\| {\widehat {\mathbf{C}}_i^{{\text{cb}}}\left( {{{\mathbf{r}}_i}} \right) - {{\mathbf{C}}_i}\left( {{{\mathbf{r}}_i}} \right)} \right\|_2^2} .\tag{15}\end{equation*}View Source\begin{equation*}{\mathcal{L}_{{\text{cb}}}} = \sum\limits_{{{\mathbf{r}}_i}} {\left\| {\widehat {\mathbf{C}}_i^{{\text{cb}}}\left( {{{\mathbf{r}}_i}} \right) - {{\mathbf{C}}_i}\left( {{{\mathbf{r}}_i}} \right)} \right\|_2^2} .\tag{15}\end{equation*}This loss is added to the previously defined losses on the dynamic representation, giving us the final combined loss: \begin{equation*}\mathcal{L} = {\mathcal{L}_{{\text{cb}}}} + {\mathcal{L}_{{\text{pho}}}} + {\beta _{{\text{cyc}}}}{\mathcal{L}_{{\text{cyc}}}} + {\beta _{{\text{data}}}}{\mathcal{L}_{{\text{data}}}} + {\beta _{{\text{reg}}}}{\mathcal{L}_{{\text{reg}}}}\tag{16}\end{equation*}View Source\begin{equation*}\mathcal{L} = {\mathcal{L}_{{\text{cb}}}} + {\mathcal{L}_{{\text{pho}}}} + {\beta _{{\text{cyc}}}}{\mathcal{L}_{{\text{cyc}}}} + {\beta _{{\text{data}}}}{\mathcal{L}_{{\text{data}}}} + {\beta _{{\text{reg}}}}{\mathcal{L}_{{\text{reg}}}}\tag{16}\end{equation*}where the β coefficients weight each term. Fig. 5 shows separately rendered static and dynamic scene components, and Fig. 6 visually compares renderings with and without integrating a static scene representation. Figure 6: 
Static scene representation ablation. Adding a static scene representation yields higher fidelity renderings, especially in static regions (a,c) when compared to the pure dynamic model (b). 
3.4. Space-time view synthesis: To render novel views at a given time, we simply volume render each pixel using Eq. 5 (dynamic) or Eq. 13 (static+dynamic). However, we observe that while this approach produces good results at times corresponding to input views, the representation does not allow us to interpolate time-variant geometry at in-between times, leading instead to rendered results that look like linearly blended combinations of existing frames (Fig. 7). Figure 7: 
Novel time synthesis. Rendering images by interpolating the time index (top) yields blending artifacts compared to our scene flow based rendering (bottom). 
Instead, we render intermediate times by warping the scene based on the predicted scene flow. For efficient rendering, we propose a splatting-based plane-sweep volume rendering approach. To render an image at intermediate time i + δi, δi ∈ℒ(0, 1) at a specified target viewpoint, we sweep over every step emitted from the novel viewpoint from front to back. At each sampled step t along the ray, we query point information through our models at both times i and i + 1, and displace the 3D points at time i by the scaled scene flow xi+δifi→i+1(xi), and similarity for time i+1. We then splat the 3D displaced points onto a (c, α) accumulation buffer at the novel viewpoint, and blend splats from time i and i + 1 with linear weights 1 ℒδi, δi. The final rendered view is obtained by volume rendering the accumulation buffer (see supplementary material for a diagram). 

SECTION 4. Experiments: Implementation details. We use COLMAP [62] to estimate camera intrinsics and extrinsics, and consider these fixed during optimization. As COLMAP assumes a static scene, we mask out features from regions associated with common classes of dynamic objects using off-the-shelf instance segmentation [23]. During training and testing, we sample 128 points along each ray and normalize the time indices i ∈ℒ[0, 1]. As with NeRF [46], we use positional encoding to transform the inputs, and parameterize scenes using normalized device coordinates. A separate model is trained for a each scene using the Adam optimizer [32] with a learning rate of 0.0005. While integrating the static scene representation, we optimize two networks simultaneously. Training a full model takes around two days per scene using two NVIDIA V100 GPUs and rendering takes roughly 6 seconds for each 512 ℒ288 frame. We refer readers to the supplemental material for our network architectures, hyper-parameter settings, and other implementation details. Table 1: 
Quantitative evaluation of novel view synthesis on the Dynamic Scenes dataset. MV indicates whether the approach makes use multi-view information or not.
Table 2: 
Quantitative evaluation of novel view and time synthesis. See Sec. 4.2 for a description of the baselines.
4.1. Baselines and error metrics: We compare our approach to state-of-the-art single-view and multi-view novel view synthesis algorithms. For single-view methods, we compare to MPIs [74] and SinSyn [78], trained on indoor real estate videos [84]; 3D Photos [64] and 3D Ken Burns [54] were trained mainly on images in the wild. Since these methods can only compute depth up to an unknown scale and shift, we align the predicted depths with the SfM sparse point clouds before rendering. For multi-view, we compare to a recent dynamic view synthesis method [81]. Since the authors do not provide source code, we reimplemented their approach based on the paper description. We also compare to a video depth prediction method [40] and perform novel view synthesis by rendering the point cloud into novel views while filling in disoccluded regions. Finally, we train a standard NeRF [46], with and without the added time domain, on each dynamic scene. We report the rendering quality of each approach with three standard error metrics: structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), and perceptual similarity through LPIPS [82], on both the entire scene (Full) and in dynamic regions only (Dynamic Only). 4.2. Quantitative evaluation: We evaluate on the Nvidia Dynamic Scenes Dataset [81], which consists of 8 scenes with human and non-human motion recorded by 12 synchronized cameras. As in the original work [81], we simulate a moving monocular camera by extracting images sampled from each camera viewpoint at different time instances, and evaluate the result of view synthesis with respect to known held-out viewpoints and frames. For each scene, we extract 24 frames from the original videos for training and use the remaining 11 held-out images per time instance for evaluation. Figure 8: 
Qualitative comparisons on the Dynamic Scenes dataset. Compared with prior methods, our rendered images more closely match the ground truth, and include fewer artifacts, as shown in the highlighted regions. 
Novel view synthesis. We first evaluate our approach and other baselines on the task of novel view synthesis (at the same time instances as the training sequences). The quantitative results are shown in Table 1. Our approach without the static scene representation (Ours w/o static) already significantly outperforms other single-view and multi-view baselines in both dynamic regions and on the entire scene. NeRF has the second best performance on the entire scene, but cannot model scene dynamics. Moreover, adding the static scene representation improves overall rendering quality by more than 30%, demonstrating the benefits of leveraging global multi-view information from rigid regions where possible. Novel view and time synthesis. We also evaluate the task of novel view and time synthesis by extracting every other frame from the original Dynamic Scenes dataset videos for training, and evaluating on the held-out intermediate time instances at held-out camera viewpoints. Since we are not aware of prior monocular space-time view interpolation methods, we use two state-of-the-art view synthesis baselines [64], [81] to synthesize images at the testing camera viewpoints followed by 2D frame interpolation [52] to render intermediate times, as well as NeRF evaluated directly at the novel space-time views. Table 2 shows that our method significantly outperforms all baselines in both dynamic regions and the entire scene. Table 3: 
Ablation study on the Dynamic Scenes dataset. See Sec. 4.2 for detailed descriptions of each of the ablations.
Ablation study. We analyze the effect of each proposed system component in the task of novel view synthesis by removing (1) all added losses, which gives us NeRF extended to the temporal domain (NeRF (w/ time)); (2) the single view depth prior (w/o ℒz); (3) the geometry consistency prior (w/o ℒgeo); (4) the scene flow cycle consistency term (w/o ℒcyc); (5) the scene flow regularization term (w/o ℒreg); (6) the disocculusion weight fields (w/o {\mathcal{W}_i}); (7) the static representation (w/o static). The results, shown in Table 3, demonstrate the relative importance of each component, with the full system performing the best. 4.3. Qualitative evaluation: We provide qualitative comparisons on the Dynamic Scenes dataset (Fig. 8) and on monocular video clips collected in-the-wild from the internet featuring complex object motions such as jumping, running, or dancing with various occlusions (Fig. 9). NeRF [46] correctly reconstructs most static regions, but produces ghosting in dynamic regions since it treats all the moving objects as view-dependent effects, leading to incorrect interpolation results. The state-of-the-art single-view method [64] tends to synthesize incorrect content at disocclusions, such as the bins and speaker in the last three rows of Fig. 9. In contrast, methods based on reconstructing explicit depth maps [40], [81] have difficulty modeling complex scene appearance and geometry such as the thin structures in the third row of Fig. 8 and the first row of Fig. 9. Figure 9: 
Qualitative comparisons on monocular video clips. When compared to baselines, our approach more correctly synthesizes hidden content in disocclusions (shown in the last three rows), and locations with complex scene structure such as the fence in the first row. 


SECTION 5. Discussion: Limitations. Monocular space-time view synthesis of dynamic scenes is challenging, and we have only scratched the surface with our proposed method. In particular, there are several limitations to our approach. Similar to NeRF, training and rendering times are high, even at limited resolutions. Additionally, each scene has to be reconstructed from scratch and our representation is unable to extrapolate content unseen in the training views (See Fig. 10(a)). Furthermore, we found that rendering quality degrades when either the length of the sequence is increased given default number of model parameters (most of our sequences were trained for 1~2 seconds), or when the amount of motion is extreme (See Fig. 10(b-c), where we train a model on a low frame rate video). Our method can end up in the incorrect local minima if object and camera motions are close to a degenerate case, e.g., colinear, as described in Park et al. [55]. Figure 10: 
Limitations. Our method is unable to extrapolate content unseen in the training views (a), and has difficulty recovering high frequency details if a video involves extreme object motions (b,c). 
Conclusion. We presented an approach for monocular novel view and time synthesis of complex dynamic scenes by Neural Scene Flow Fields, a new representation that implicitly models scene time-variant reflectance, geometry and 3D motion. We have shown that our method can generate compelling space-time view synthesis results for scenes with natural in-the-wild scene motion. In the future, we hope that such methods can enable high-resolution views of dynamic scenes with larger scale and larger viewpoint changes.