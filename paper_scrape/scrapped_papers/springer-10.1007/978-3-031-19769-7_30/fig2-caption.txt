Self-supervised view generation via virtual cameras. Given a starting RGBD image \((I_0, D_0)\) at viewpoint \(c_0\), our training procedure samples two virtual camera trajectories: 1) a cycle to and back from a single virtual view (dashed orange arrows), creating a self-supervised view synthesis signal enforced by the reconstruction loss \(\mathcal {L}_{\text {rec}}\). 2) a longer virtual camera path for which we generate corresponding images via the render-refine-repeat process (black dashed arrows and gray cameras). An adversarial loss \(\mathcal {L}_{\text {adv}}\) between the final view \((\hat{I}_T, \hat{D}_T)\) and the real image \((I_0, D_0)\) enables the network to learn long-range view generation. (Color figure online)