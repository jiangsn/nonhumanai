1 Introduction:
Reinforcement learning (RL) is a general learning paradigm that addresses the problem of how an acting agent can learn an optimal behavioral strategy while interacting with an unknown environment surrounding it [1]. Recently, RL has achieved a variety of successes, most prominently in autonomous driving [2, 3], gaming [4, 5], and robotics manipulation [6, 7]. Notably, most RL research is based on the formalism of Markov decision process (MDP) which models the decision-making problem. Specifically, while interacting with the environment, the agent takes actions based on the observed state of the environment and then receives rewards for its actions. Thus, the goal of RL is to maximize the expected value of the cumulative reward in an episode. However, formulating a near-optimal policy for the agent’s actions usually requires extensive exploration in the action space. For example, OpenAI Five [5] defeated the world champions at an esports (Dota2) game by developing a training system using RL techniques. To fully explore the action space, it trained for 180 days on 256 GPUs and 128,000 CPU cores, based on an average of 180 days worth of self-play. Therefore, training RL models in the context of robotics is particularly challenging, since in such a context it is very difficult to safely collect samples that cover all possible actions in their space.
A common method to address this problem is training RL models with the help of simulations [8,9,10,11,12]. Previous work has shown that simulations can be used as a valuable tool for robotics research, as performing robotic skills in a simulation is comparatively easier than in the real world [3, 13]. The use of simulation greatly facilitates the implementation of RL in robotics by allowing a comprehensive exploration of the robotic action space with less engineering effort than in real world by adjusting the parameters of the simulator. However, policies learned in a simulation are often unsuitable for reality due to the reality gap. To bridge the gap, researchers usually incorporate data from the real world for training, or retrain the models in reality. For example, a table tennis robot was trained with RL in a hybrid simulation and real system [14]. The real trajectories of the ball were recorded and replayed in simulation to use the real data as much as possible. However, one problem in existing RL methods such as Trust Region Policy Optimization (TRPO) [15], Proximal Policy Optimization (PPO) [16], Deep Deterministic Policy Gradient (DDPG) [17], Twin Delayed DDPG (TD3) [18], or Soft Actor-Critic (SAC) [19] is that the fuzzy one-dimensional reward cannot precisely express the interaction with the environment for multi-dimensional actions. Therefore, inspired by the previous work, we propose a novel approach to learn the optimal strokes for our table tennis robot, which is one of the first robots around the world that can handle strong spin balls in reality. A 3D Q-value function is designed to cope with the corresponding 3D reward vector. Two learning steps, including training in simulation and retraining in reality, can be completed in around 1.5 hours for the balls with a wide variety of spins, speeds, and positions. Comparing to the manually designed stroke policy in [20], our approach is capable of adapting to different rackets rapidly within half an hour. The main contributions of this work are as follows: 


We design a realistic table tennis robot simulation system for optimal stroke learning with RL, as shown in Fig. 1 left. The measurement methods for the real dynamic parameters of the ball, the table, and the racket are presented, which makes our simulation environment more realistic. Multiple controllers for the simulated flying ball and the collisions between two entities are implemented according to the effect of the force on dynamic objects in simulation. 


Inspired from the hitting strategies of a human player, we decompose the learning part into two stages: first, the prediction of the ball’s hitting state, and second, the learning of the optimal stroke, which is the focus of this paper. Based on the controllable and applicable actions of the robot, a new exploration strategy, a multidimensional reward function and a Q-value model are proposed. 


We compare our RL method with the predecessors by evaluating them on a dataset of 1000 balls in simulation. An efficient retraining step is used to close the sim-to-real gap. Unlike other work that relies directly on the real robot, our training and retraining steps are able to use simulation to explore more potential actions in order to achieve better performance and reduce learning time. The experiments show that our models trained in real robots (see Fig. 1 right) achieve remarkable performance in three distinct scenarios. 

Fig. 1Left: the simulated table tennis robot in a realistic simulation environment. To approximate reality, Gaussian noise is applied to the 3D position of each ball in simulation. The world coordinate system is identical to that of the robot. Right: our table tennis robot with a KUKA KR 6 R900 robot 

2 Related work:


2.1 Simulation for robotic table tennis:
Just as simulation is being used in various areas of robotics research (e.g., autonomous vehicles [3], industrial robots [21], and humanoid robots [22]), it is also being widely used for facilitating the development of more intelligent robotic table tennis [10, 23]. By using simulation, the robot can explore the action space extensively without worrying about safety during the training steps. Moreover, simulation provides the ability to compare the performance of different approaches in a fair and deterministic environment. For instance, [11] proposed an approach to sample-efficient learning of complex policies in the context of robotic table tennis. The simulation environment was created using PyBullet [24] and connected to a virtual reality setup to capture human actions with instrumented paddles. Similarly, a simulated robotic table tennis system was built using PyBullet [24] to train policies for performing table tennis ball return tasks [10]. In addition, a simulation environment was developed in MATLAB and used to generate the optimal trajectory for robot table tennis [25]. To make use of robotic drivers and devices, [26] developed a flying robot with the Gazebo simulator which can be easily combined with ROS. [14] presented a hybrid simulation and real (HYSR) training method for muscular robots performing a table tennis task. To teach robots how to play table tennis without using real balls, the historical states of the ball in reality were recorded and replayed in simulation, and then the actions in simulation were applied to real robots. The aforementioned works are further evidence of the benefit and importance of using simulations in robotic table tennis research. However, some of these existing simulated table tennis systems have the characteristic that a high effort is required when transferring the trained models from simulation to reality due to the fact that they are not sufficiently realistic. In addition, due to incompatibility with the existing RL libraries that contain advanced RL algorithms, some of these existing simulated table tennis systems are further limited in their generalizability. Therefore, there is an increasing need to develop a more realistic simulation that can better simulate the real situation for advanced robot table tennis research. In this case, we present a general method for measuring the dynamic coefficients of the ball, the table, and the racket, which are used to constructing the simulation environment. The dynamics formula of the ball in simulation is determined based on [27, 28]. The simulated manipulator in our work is controlled in Cartesian space so that it can be easily replaced by other types of robots, such as flying robots or mobile robots. 

2.2 Reinforcement learning in robotic table tennis:
Reinforcement learning has been shown to be an excellent method for training robots to learn complex tasks [29, 30], and therefore could be a solution for robotic table tennis training. Indeed, deep RL has already attracted great interest from researchers and has achieved some success in the field of robotic table tennis [31, 32]. For instance, [10] developed an end-to-end RL algorithm for learning efficient policies to directly control of a simulated table tennis robot in joint space. A multi-modal model-free policy was trained to learn the velocities of each joint at 100 Hz by taking the joint position trajectories and locations of the ball as inputs. The optimal policy was found at about 1 million episodes. In [14], a muscular table tennis robot was trained in joint space in a hybrid simulation and reality (HYSR) system. The PPO was used as backbone. To return and smash the ball with a high success rate, a dense reward function was developed depending on the ball position and the robot state. Without using real balls, the robots were trained to play the ball with a similar initial condition in around 14 hours. [11] incorporated stroke learning into a hierarchical control system using an inverse landing model, an analytic racket controller, a forward racket model and a forward landing model. Each model was trained separately to make the learning process easier and more efficient. A striking policy that can hit the ball to the targets with reasonable errors was learned from about 7,000 demonstrated trajectories, and in addition, the agent can learn from about 24,000 strikes in self-play to make optimal use of the human dynamics models for longer play. To efficiently learn the optimal stroke, a two-stage approach was adopted in [12]. In the first stage, the hitting states of the ball (position and velocity) were determined by an extended Kalman filter (EKF) based predictor. In the second stage, these determined states of the ball were fed as inputs to the DDPG, with the velocities of the racket being the outputs. A reliable performance was achieved within 200,000 simulated episodes. Instead of using DDPG directly, [33] proposed an accelerated parametrized-reward gradients approach to learn the velocity and rotation of the racket from the predicted hitting states. The policy was trained with 200 human demonstrations. To keep the explorations safe and avoid collisions, the racket action were restricted to a narrow range, resulting in the network being trained using a set of very similar trajectories of the ball. In addition, [34] used an LSTM to encode the observed trajectory of the ball into a latent state (50-dimension vector), which was served as the state in the common DRL setting. 

3 Methodology:
To efficiently learn the optimal stroke and successfully return the ball to the desired target position on the table, we propose a novel framework as shown in Fig. 2. Specifically, a realistic simulation environment is developed for robot learning and comparison with other advanced RL algorithms. In the first stage, the hitting state (position, velocity, and spin) of the ball is predicted using the physical model of the ball proposed in our previous work [28, 35]. In the second stage, a novel approach is presented to learn the optimal stroke in simulation, which is compatible with ROS and OpenAI libraries (see Fig. 8) in order to use the existing algorithms for comparisons.
Fig. 2The entire framework for training and testing. Two high-speed cameras were mounted in the corners of the ceiling for detecting the ball at 150 Hz. The goal is returning the ball to the desired target position on the table. In the first stage, the ball’s state s at the hitting time is predicted by [28, 35]. In this work, we focus on the second stage, where an optimal stoke can be learned based on a novel RL algorithm. The upper part is performed in simulation. The RL model is trained within 10,000 episodes. To cross the reality gap, the model is then retrained in reality (see the bottom part) 

3.1 Simulation:
A well-known challenge for deep RL is the safe interaction with the environment. In particular, in robotic table tennis, it is difficult to explore all possibilities in reality, since unexpected collisions would destroy the mechanical robot parts. Moreover, the robot must interact with the environment over a large number of steps to learn a high-level policy. Taken together, this makes the application of deep RL in robotic table tennis more challenging. To address these issues, we develop a realistic simulation that provides a convenient scenario for optimal stroke learning as well as a comparison of different algorithms. The pose and velocity of the simulated racket are controlled by an inverse kinematics (IK) library. The dynamic model for each entity is obtained with the methods described in the following subsections.
3.1.1 Flying ball modelIn addition to the gravitational force Fg, a flying ball is usually influenced by the Magnus force Fm and the air drag Fd [36]. As shown in Fig. 3, Fm is perpendicular to the spin axis and the flight direction, while Fd is opposite to the flight direction. These forces can be computed using the following formulas:
$$ \begin{array}{@{}rcl@{}} F_{g} &=& (0, 0, -mg)^{T} , \end{array} $$
(1)
$$ \begin{array}{@{}rcl@{}} F_{d} &=& - \frac{1}{2} C_{D} \rho_{a} A \|{v} \|v , \end{array} $$
(2)
$$ \begin{array}{@{}rcl@{}} F_{m} &=& \frac{1}{2} C_{M} \rho_{a} A r_{1} (\omega \times v) , \end{array} $$
(3)
where the constants were determined in our previous work [35], including the mass of the ball m = 2.7g, the gravitational constant g = 9.81m/s2, the drag coefficient CD = 0.4, the density of the air ρa = 1.29kg/m3, the lift coefficient CM = 0.6, the radius of the ball r1 = 20mm, the cavity radius r2 = 19.6mm, and the cross-section of the ball \(A = {r_{1}^{2}} \pi \). In addition, ω and v are the linear and angular velocities, respectively, which can be derived from the trajectory of the ball in reality. These two velocities can be further used to predict the hitting state of the ball using the algorithm introduced in [28, 35]. To simulate the accurate dynamics of the ball, the inertia value I should also be considered, which is calculated as
$$ I=\frac{2}{5} m\left( \frac{{r_{1}^{5}}-{r_{2}^{5}}}{{r_{1}^{3}}-{r_{2}^{3}}}\right) . $$
(4)
Fig. 3Force analysis of a flying ball. A sphere shell of radius r1 and mass m, with a centered spherical cavity of radius r2, is created as a simulated ball in Gazebo 3.1.2 Bounce modelSince the physical contact between two objects in reality is a very complex matter, the Open Dynamics Engine (ODE), a popular rigid body dynamics library for robotics, was used to simulate the contact forces between the ball and the table (or the racket) in simulation. ODE has been originally integrated into the Gazebo simulator. To represent the elastic and frictional impacts on the ball, we compute the restitution coefficient κR and the friction coefficient μ similar to [27].
The restitution coefficient κR is defined as the ratio of the energy before and after a collision, e.g., when the ball bounces off the table. Approximately, it can be solved by the free fall of the ball as follows:
$$ {\kappa_{R}^{t}}=\frac{{v_{2}^{t}}-{v_{2}^{b}}}{{v_{1}^{b}}-{v_{1}^{t}}}=\frac{-{v_{2}^{b}}}{{v_{1}^{b}}}=\frac{-\sqrt{2 \cdot g \cdot h_{2}}}{-\sqrt{2 \cdot g\cdot h_{1}}}=\sqrt{\frac{h_{2}}{h_{1}}} , $$
(5)
where \({v_{1}^{b}}\) and \({v_{1}^{t}}\) are the velocities of the ball and table before bouncing, while \({v_{2}^{b}}\) and \({v_{2}^{t}}\) are the velocities after bouncing. h1 and h2 are the corresponding heights when the ball is not moving. Here the table velocity vt = 0. The friction coefficient μ is obtained from the setup in Fig. 4, where three balls are arranged together in the shape of a triangle frame. We first place the balls on the table and then lift the table until the balls start to slide. After obtaining the horizontal angle change 𝜃 of the table, the friction coefficient μt between the table and the ball can be calculated as
$$ \mu^{t}=\frac{3mg \cdot \sin{\theta}}{3mg \cdot \cos{\theta}}=\tan{\theta} . $$
(6)
Fig. 4Setup for measuring the friction coefficient μ. Three balls are arranged together in the shape of a triangle frame and placed on a flat table With the same methods, we calculate the restitution coefficient \({\kappa _{R}^{r}}\) and the friction coefficient μr of the racket. The resulting parameter values are given in the Table 1. The additionally required parameters μ2 and slip are defined as the friction coefficient in the second ODE friction pyramid direction and the coefficients of force-dependent-slip (FDS), respectively. They are manually adjusted to fit the reality.
Table 1 Collision parameters when the ball impacts on the table and racketTo roughly test the accuracy of the simulation, we utilize a ball throwing machine to launch a topspin ball towards the stationary racket mounted on the robot. The entire trajectory of the ball can be recorded as the ground truth using stereo cameras at 150 Hz. The starting spin and velocity of the ball are computed using a spin detector tool and a curve fitting approach, respectively. Then, these initial parameters are fed into the simulated ball and this initialized ball is served in simulation. The simulated trajectory is generated and is shown in Fig. 5.
Fig. 5Trajectory comparison between reality and simulation when serving a topspin ball. The difference between the returned landing positions on the table is about 6.2 cm. In this case, the racket is stationary since it is difficult to understand and simulate all the dynamics parameters of a moving racket in simulation. Therefore, the actions of the racket applied in simulation are always the true actions without any noise and time latency. This reality gap is closed in the retraining Section 4.3 

3.2 Algorithm:
With regard to the different types of inputs, there are generally two ways available when using deep RL algorithms in robotic table tennis. One is the one-stage algorithm that takes the state of the ball at each step as input and learns the pose of the racket in an end-to-end way. The other is the two-stage algorithm that first predicts the hitting state (i.e. position, velocity, and spin) of the ball and then takes it as input for RL. The latter can significantly accelerate the training step and can efficiently deal with different spin balls. In this work, we adopt the second approach to learn the optimal stroke of the racket based on the prediction of the state of the ball at the hitting time. Since there is only a single state vector as input in the second stage, we then parameterize stroke learning as a bandit problem, where actions have no influence on the next states and consequently there are no delayed rewards in an episode. It is a simple version of an Markov Decision Process (MDP), with
$$ M=(S, A, R) , $$
(7)
where S is the set of the observed 11-D states s, including the 3D position pb of the ball, 3D linear velocity vb, 3D angular velocity ωb at the hitting time, and the desired 2D landing target position ptar on the table. A is the set of 3D actions a that can be performed on the robot. Due to the restriction of the current mechanical structure and the control system, we cannot operate the robot as flexibly as a human can move. Therefore, we only learn to change the linear velocity \({v_{x}^{r}}\) of the robot along the x-axis and its orientation angles (βr,γr) around the y and z axes. The target position of the racket is the same as the predicted hitting position of the ball. R is a set of the immediate rewards r. We use a policy μ𝜃(s) as the actor network which can output the actions a with respect to the current state s, as shown in Fig. 6 left. To evaluate the actions, a critic network Qϕ(s,a) is used, which takes as input both the states and actions and outputs a Q-value vector, as shown in Fig. 6 right. 𝜃 and ϕ are the weights of the neural network. The goal of our work is to lean a deterministic policy μ𝜃(s), which provides an action that maximizes the norm of Qϕ(s,a). According to the DDPG algorithm, the critic and the actor can be updated, respectively, by minimizing the losses:
$$ \quad \quad \quad \mathcal{L}(\phi, \mathcal{B})=\underset{\left.(s, a, r\right.) \sim \mathcal{B}}{\mathrm{E}}\left.\|{Q_{\phi}(s, a)-r}\|^{2}\right. ,  $$
(8)
$$ \mathcal{L}(\theta, \mathcal{B})=\underset{s \sim B}{-\mathrm{E}}\left.\|{Q_{\phi}(s, \mu_{\theta}(s)}\|\right. ,  $$
(9)
where B is a minibatch for storing s,a,r. The reward r is the feedback from the environment, which we will discuss in more detail later.
Fig. 6Classic Actor-Critic algorithms. Instead of a 1D Q-value, a 3D Q-value is proposed to train the corresponding 3D actions To accelerate the training step and boost the resulting performance, we apply the following modifications to the classic actor-critic algorithms for training. 3.2.1 ExplorationFor continuous action spaces, several exploration strategies are used in the deterministic environments. For instance, the random strategy selects actions randomly from a Gaussian distribution, while the epsilon − greedy strategy takes the random actions occasionally with probability 𝜖 and uses the output of the current actor μ𝜃(s) with probability 1 − 𝜖. This is the default strategy used in Spinning Up. In this work, we noticed that the actor μ𝜃(s) did not give the action with the maximum Q-value in the earlier training step because of the large loss error. Therefore, we generate the action a as
$$ a=\underset{\mu_{\theta}(s)+\mathcal{N}}{{\arg}{\max}}{\|Q_{\phi}(s, \mu_{\theta}(s)+\mathcal{N})}\| ,  $$
(10)
where \(\mathcal {N}\) is a Gaussian noise. 3.2.2 Reward shapingIn [12], a reward function was developed that depends on the height hb of the ball when crossing the net and the actual landing position preal on the table when the ball is returned. To balance these two independent variables hb and preal, a coefficient is used; however, deciding the value of this coefficient is a tricky issue. To address this problem, we decompose the reward function into three vector components: rx and ry for the x and y of the landing position, and rh for the height of the ball. Each reward function is then normalized to [0,1] by the following equations:
$$ \begin{array}{@{}rcl@{}} r_{x} &=& e^{- \left|{p_{x}^{real}-p_{x}^{tar}}\right|} , \end{array} $$
(11)
$$ \begin{array}{@{}rcl@{}} r_{y} &=& e^{-\left|{p_{y}^{real}-p_{y}^{tar}}\right|} , \end{array} $$
(12)
$$ \begin{array}{@{}rcl@{}} r_{h} &=& e^{-\left|{h^{b}-0.173}\right|} , \end{array} $$
(13)
$$ \begin{array}{@{}rcl@{}} r &=& [r_{x}, r_{y}, r_{h}] \quad \text{if \textit{success} else} \overrightarrow{\text{0}} , \end{array} $$
(14)
where px and py are the landing position (in meters) along the x and y axes, the constant 0.173 is the measured actual height of the net (in meters), and e is the natural exponential operation. When a ball is successfully returned to the opposite table, success is set to true. 3.2.3 3D Q-valueNormally, the Q-value is a 1D vector that is expected to be maximized. To take advantage of the above rewards, we replace the last layer in the critic network from 1D to 3D. This results in a 3D Q-value [Qx,Qy,Qh], which can precisely indicate the quality of the actions. In addition, for the actor-critic model we adopt TD3 as the backbone. The critic is changed to:
$$ Q_{\phi}(s, a)=\left\{\begin{array}{ll} Q_{\phi_{1}}(s, a), \text { if }\left\|{Q_{\phi_{1}}(s, a)}\right\| < \left\|{Q_{\phi_{2}}(s, a)}\right\| \\ Q_{\phi_{2}}(s, a), \text{ otherwise } \end{array}\right. $$
(15)
The whole training process is depicted in Algorithm 1, where the loss functions \({\mathscr{L}}(\phi _{i}, {\mathscr{B}})\) and \({\mathscr{L}}(\theta , {\mathscr{B}})\) are used to update the critic and the actor, respectively.
Algorithm 1Policy Gradient Training with TD3 backbone. 

4 Experiments:


4.1 Training and testing in simulation:
To generalize the trained model, 11,000 serves were randomly sampled from a wide range of values for training (10,000) and evaluation (1,000), as shown in Table 2. To bridge the reality gap, a Gaussian noise is first applied to the 3D position of each ball in simulation. Then, we predict the state of the ball at the hitting time using the methods in [28, 35] instead of using the actual state of the ball in simulation. Since the predicted hitting position is the one to which the simulated racket should actually move, this allows the replication of the real situation and makes the trained model more realistic for the real world. The final state variables and their range of the ball are shown in Table 2, which includes the desired landing target position (\(p_{x}^{tar},p_{y}^{tar}\)), the position of the ball (\({p_{x}^{b}},{p_{y}^{b}},{p_{z}^{b}}\)), linear velocity (\({v_{x}^{b}}, {v_{y}^{b}}, {v_{z}^{b}}\)) and angular velocity (\({\omega _{x}^{b}}, {\omega _{y}^{b}}, {\omega _{z}^{b}}\)) at the hitting time. These state variables are then normalized and used as inputs for training. The \({p_{x}^{b}}\) is consistently equal to 0.675 m to form a virtual hitting plane used for the state prediction in the first stage. Thus, the hitting time is the time when the ball reaches this virtual plane. The home position of the racket in the world coordinates is (0.5,0.0,0.0) in meters.
Table 2 State variables and their range at the hitting time for training and evaluation in simulationConsidering the mechanical setup of the robot, we restrict the linear velocity \({v_{x}^{r}}\) of the robot to a range from 0m/s to 2m/s. The orientation angles βr,γr are between –50 to 50. The third angle αr around the x-axis is calculated as
$$ \begin{array}{@{}rcl@{}} \alpha^{r} = k\cdot \frac{{p^{b}_{y}}}{0.5\cdot w^{t}} , \end{array} $$
(16)
where wt is the table width and k is a weight. In this way, the robot generates a flexible stroke. Here we assume the angle αr has no influence on the impact with the ball. In (10), the added action noise \(\mathcal {N}\) for exploration is a mean-zero Gaussian distribution with a standard deviation of 0.1. The replay buffer \(\mathcal {D}\) has a size of 5,000. The number of training episodes λ is 10,000. Other hyper-parameters used for actor-critic are given in Table 3. The output actions from the actor are scaled to the valid range and then applied to the simulation. These hyper-parameters are tuned manually in order to achieve the best performance.
Table 3 Hyper-parameters used for training in simulation and retraining in realityCompared to other environments that require millions of interactions in OpenAI Gym, our model is able to converge after 30 epochs, which took about 1 hour of training. In addition, 1000 episodes were run for evaluation after each epoch. The resulting rewards and the corresponding 3D Q-value are plotted in Fig. 7. It is observed that the testing rewards reach a stable level starting from the 20th epoch, although the Q-values have not yet converged to the maximum values.
Fig. 7The testing rewards [rx,ry,rh] and the 3D Q-value [Qx,Qy,Qh] over the epochs in simulation 

4.2 Evaluation in simulation:
To use the existing RL algorithms, we build the following evaluation pipeline (see Fig. 8) by combining the Gazebo simulator with ROS, Gym, and Spinning Up. A commonly used metric for evaluation is the distance error between the actual and desired landing position [11, 12, 14]. However, this metric cannot reflect a failed return, e.g., if the landing position is not on the table, then it will be difficult to calculate the distance error. Therefore, we introduced a new metric here: distance error 𝜖d computed by
$$ \begin{array}{@{}rcl@{}} r_{d} = e^{-\left\|{p^{real}-p^{tar}}\right\|_{2}} \quad \text{if \textit{success} else 0} , \end{array} $$
(17)
$$ \begin{array}{@{}rcl@{}} \epsilon_{d} = -\ln \left( \frac{1}{\lambda}\sum\limits_{n=1}^{\lambda} {r_{d}^{n}} \right) , \end{array} $$
(18)
where the \({r_{d}^{n}}\) is the reward for the landing distance error of the nth ball. The value of the \({r_{d}^{n}}\) is 0 if the ball does not land on the opposite table. Furthermore, two additional metrics reflecting the performance of the model were used for evaluation. The flying height error 𝜖h of the ball when crossing the net is calculated by
$$ \epsilon_{h} = -\ln \left( \frac{1}{\lambda}\sum\limits_{n=1}^{\lambda} {r_{h}^{n}} \right) . $$
(19)
As well, the success rate of the ball in returning to the opposite table is calculated. To obtain a fair evaluation, we adopt 1000 episodes to cover a large range for the states (see Table 2 right).
Fig. 8The evaluation architecture is built by combining the Gazebo simulator with ROS, Gym, and Spinning Up. We can subscribe and publish the states (pose and velocity)of the ball and the robot from Gazebo with the Gazebo_msgs in ROS. These states should be fed into the Gym toolkit, which allows us to use the RL algorithms in Spinning Up Since only the 1D Q-value was used in the existing RL algorithms, we thus create a 1D reward function similar to [12] as
$$ r_{eval} = e^{-k(\left\|{p^{real}-p^{tar}}+\right\|{ h^{b}-0.173}|)} , $$
(20)
where k is a scalar coefficient set to 0.5 in this work. This new reward function is used only for training the existing RL algorithms, including TRPO, PPO, SAC, DDPG, and TD3. By evaluating the existing and newly introduced algorithms using different reward functions corresponding to these different algorithms, we compute the distance error 𝜖D, the height error 𝜖h, and the success rate, respectively, as shown in Table 4. The unit of these errors is converted from meters to centimeters for better visualization. The proposed approach, argmax exploration plus 3D Q-value together with TD3 backbone, achieves better performance than the DDPG backbone. The other three approaches, TRPO, PPO, and SAC, learn the optical stroke using a stochastic policy, resulting in much higher errors and lower success rate.
Table 4 Evaluation for different RL algorithmsAs we discussed in Chapter 3.2, two-stage approaches are capable of accelerating the policy learning step and handling the spin balls explicitly, since the ball hitting state, including positions, velocities, and spins, can be directly obtained in the first stage. This helps the RL algorithm to have a more specific understanding about the trajectory of the ball. For example, the one-stage approach in [10, 14] requires around 1 million episodes to converge, but our method only uses 3,000 interactions. Additionally, one-stage approaches are usually used in joint space, which means that the kinematics and dynamics of the manipulator have to be learned explicitly as well. This might prevent the generalization for the application of other types of table tennis robots. Therefore, two-stage approaches are more convenient and much efficient to apply to the real world, and here we compare our approaches against the following three two-stage approaches: Zhu et al. [12], Tebbe et al. [28], Yang et al. [34], as shown in Table 5. The 1D reward reval and the 3D action a are used in them to adapt to our robot system. The input vector for [12] and [28] are the same as ours. For [34], we adopt the idea of using a 50-D latent vector encoded from an LSTM as the input to RL. Although [12] also used DDPG, their performance is slightly worse than our DDPG-based algorithm (see Table 4), because our model have already been fine-tuned. Yang et al. [34] has the same performance as [12]. Its height error 𝜖h is a bit large, as it is not easy for RL to understand the spin value from the latent code and is prone to learn an open style for the racket action. [28] is close to our proposed approach \(DDPG+{{\arg }{\max \limits }}\) that is actually a simplified version of [28]. The difference is that they also applied the \({{\arg }{\max \limits }}\)-based trick during test, which probably tends to the local optima. Finally, our approach \(TD3+{{\arg }{\max \limits }}+3D Q-value\) achieves the best performance than others.
Table 5 Comparison against other two-stage algorithms

4.3 Retraining in reality:
Although we built a high-fidelity simulation by manually measuring the coefficients and applying random noise to the ball, the real robot has many more dynamic and complicated factors that cannot be accurately measured and accounted for. To find the best hyper-parameters for retraining, we change the racket’s restitution coefficient \({\kappa _{R}^{r}}\) and friction coefficient μr in simulation. In this way, we can replicate the situation between two different rackets in reality. Based on the pretrained actor-critic model, we then retrain the model in the new simulation with different batch sizes, episodes per epoch, and learning rates. The best hyper-parameters found in simulation are shown in Table 3 right. A ball throwing machine, TTmatic 404A, is used to provide a variety of balls with sidespin, topspin, and backspin using a group of selected parameters. At the moment our robot can only handle sidespin and topspin, since the backspin ball causes too much acceleration in a robot joint. This could be solved in the future. In addition, the Reflexxes motion library [37] is used for robot trajectory planning in Cartesian space. Each epoch includes both sidespin and topspin balls during retraining. The state variables and the range of the ball for retraining and testing at the hitting time are shown in Table 6. Here, the model is retrained with 20 epochs in 0.5 hours to ensure that it achieves convergence. The hitting position \({p_{x}^{b}}\) along the x axis is fixed to 0.675 m. The retraining process demonstrating the landing distance error 𝜖d and the height error 𝜖h is shown in Fig. 9. Comparing to the trained model in simulation, the retraining process with the real robot improves the performance, as shown in Table 7. The comparison data include 20 topspin balls and 20 sidespin balls.

Table 6 State variables and their range at the hitting time for retraining and testing in realityFig. 9Retraining process for the original racket (a) and for the coefficient-unknown racket (b), with balls served by a ball throwing machine. The metrics 𝜖d and 𝜖h in units of centimeters (cm) are the landing distance error 𝜖d and the height error 𝜖h when the ball crosses the net Table 7 Performance comparison using the first racket in reality, with the trained and the retrained modelsFurthermore, to investigate the generalizability of the algorithms for a coefficient-unknown racket, we retrain a new model for a second racket whose dynamics are completely different from the first. Fig. 9 (b) illustrates the second retraining process. As can be seen, the retraining of the second racket required around 5 more epochs to converge and achieved similar performance to the retraining of the first racket. The performance comparison with the trained and the retrained models is shown in Table 8.
Table 8 Performance comparison using the second racket in reality, with the trained and the retrained models

4.4 Testing in reality:
To test our algorithms comprehensively, we conduct the experiments in three scenarios (see Fig. 10) in which the complexity gradually increases. We use two high-speed cameras to extract the 3D positions of the ball and predict the hitting state of the ball based on our previous work [28, 35].
Fig. 10Testing in three scenarios: Player1, Player2, and Machine In the first scenario, a human player (Player1) serves the ball with different starting positions at the front of the table. In this way, the the hitting position along the y-axis can be completely covered. In the second scenario, a human player (Player2) with higher skills than Player1 plays a long game rally to test the continuous performance of the robot. The state variables and the range of the ball for these two scenarios are shown in the third column of the Table 6. In the last and most complex scenario, the ball throwing machine (Machine) is used to serve balls with various spins and speeds. Since many aspects such as the robot, the racket, the state of the ball, the human players as well as the evaluation metrics are completely different, it is difficult to fairly compare the performance of our algorithm with other previous works. Therefore, we report the Table 9 by directly using the data in [20, 38] or by manually analyzing from [14, 33].
Table 9 Testing in realityThe average distance error 𝜖d and height error 𝜖h in the testing in three real scenarios are 24.9 ± 9.0 cm and 21.9 ± 4.6 cm, respectively. Comparing to others, our approach is able to handle the strong spin with high accuracy. Playing performance including some failure cases can be found at https://youtu.be/SNnqtGLmX4Y. 

5 Conclusions and future work:
In this work, we developed a realistic simulation for a table tennis robot. To learn the optimal stroke movement for the robot, we proposed a new policy gradient approach with TD3 backbone. Different algorithms were fairly evaluated in simulation using 1000 balls with a wide range of spins and speeds. To cross the domain from simulation to reality, a retraining approach was employed for the original racket and a coefficient-unknown racket racket. The test experiments showed a successful return rate of 98% in three complicated scenarios. The total training time is about 1.5 hours, which means that our algorithm is very efficient for application in robotic table tennis. Instead of a constant target position on the table, one can simply train a higher-level policy with a random target to make the game more challenging for human players. Moreover, our approach can be easily adapted to other robots playing racket-based sports, like tennis and badminton. Although we have shown significant improvements in robotic table tennis in both simulation and reality, the current control approach and the mechanical structure of the robot still limits its application in the real world. For example, the robot will fail to return the ball if the incoming ball is too high or too slow, since the target cannot be reached at a fast enough speed. Also, our robot will not have sufficient reaction time if the ball is too fast (e.g. > 10m/s) because the minimum communication time between the controller and the robot is 5 ms. In the future we plan to optimize the Reflexxes motion libraries to produce a more applicable trajectory for back spin balls. Instead of constraining the hitting position \({p_{x}^{b}}\) along the x-axis, we will parameterize it as one action to be learned in RL. For a more offensive stroke, we could also try to learn the angular velocities of the racket so that the robot can initiatively generate a spin ball.