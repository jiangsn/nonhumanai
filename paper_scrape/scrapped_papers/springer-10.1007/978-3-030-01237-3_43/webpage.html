<html class="js" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta content="pc,mobile" name="applicable-device"/>
  <meta content="Yes" name="access"/>
  <meta content="SpringerLink" name="twitter:site"/>
  <meta content="summary" name="twitter:card"/>
  <meta content="Content cover image" name="twitter:image:alt"/>
  <meta content="A Style-Aware Content Loss for Real-Time HD Style Transfer" name="twitter:title"/>
  <meta content="Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but..." name="twitter:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-030-01237-3.jpg" name="twitter:image"/>
  <meta content="10.1007/978-3-030-01237-3_43" name="dc.identifier"/>
  <meta content="10.1007/978-3-030-01237-3_43" name="DOI"/>
  <meta content="Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but..." name="dc.description"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/content/pdf/10.1007/978-3-030-01237-3_43.pdf" name="citation_pdf_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43" name="citation_fulltext_html_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43" name="citation_abstract_html_url"/>
  <meta content="Computer Vision – ECCV 2018" name="citation_inbook_title"/>
  <meta content="A Style-Aware Content Loss for Real-Time HD Style Transfer" name="citation_title"/>
  <meta content="2018" name="citation_publication_date"/>
  <meta content="715" name="citation_firstpage"/>
  <meta content="731" name="citation_lastpage"/>
  <meta content="en" name="citation_language"/>
  <meta content="10.1007/978-3-030-01237-3_43" name="citation_doi"/>
  <meta content="springer/eccv, dblp/eccv" name="citation_conference_series_id"/>
  <meta content="European Conference on Computer Vision" name="citation_conference_title"/>
  <meta content="ECCV" name="citation_conference_abbrev"/>
  <meta content="145555" name="size"/>
  <meta content="Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but..." name="description"/>
  <meta content="Sanakoyeu, Artsiom" name="citation_author"/>
  <meta content="artsiom.sanakoyeu@iwr.uni-heidelberg.de" name="citation_author_email"/>
  <meta content="IWR, Heidelberg University" name="citation_author_institution"/>
  <meta content="Kotovenko, Dmytro" name="citation_author"/>
  <meta content="dmytro.kotovenko@iwr.uni-heidelberg.de" name="citation_author_email"/>
  <meta content="IWR, Heidelberg University" name="citation_author_institution"/>
  <meta content="Lang, Sabine" name="citation_author"/>
  <meta content="sabine.lang@iwr.uni-heidelberg.de" name="citation_author_email"/>
  <meta content="IWR, Heidelberg University" name="citation_author_institution"/>
  <meta content="Ommer, Björn" name="citation_author"/>
  <meta content="bjorn.ommer@iwr.uni-heidelberg.de" name="citation_author_email"/>
  <meta content="IWR, Heidelberg University" name="citation_author_institution"/>
  <meta content="Springer, Cham" name="citation_publisher"/>
  <meta content="http://api.springer-com.proxy.lib.ohio-state.edu/xmldata/jats?q=doi:10.1007/978-3-030-01237-3_43&amp;api_key=" name="citation_springer_api_url"/>
  <meta content="telephone=no" name="format-detection"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43" property="og:url"/>
  <meta content="Paper" property="og:type"/>
  <meta content="SpringerLink" property="og:site_name"/>
  <meta content="A Style-Aware Content Loss for Real-Time HD Style Transfer" property="og:title"/>
  <meta content="Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but..." property="og:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-030-01237-3.jpg" property="og:image"/>
  <title>
   A Style-Aware Content Loss for Real-Time HD Style Transfer | SpringerLink
  </title>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="shortcut icon"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="icon" sizes="16x16 32x32 48x48"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png" rel="icon" sizes="16x16" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png" rel="icon" sizes="32x32" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png" rel="icon" sizes="48x48" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png" rel="apple-touch-icon" sizes="72x72"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png" rel="apple-touch-icon" sizes="76x76"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png" rel="apple-touch-icon" sizes="114x114"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png" rel="apple-touch-icon" sizes="120x120"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png" rel="apple-touch-icon" sizes="144x144"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png" rel="apple-touch-icon" sizes="152x152"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon" sizes="180x180"/>
  <script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_SVG.js">
  </script>
  <script>
   (function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)
  </script>
  <script data-consent="link-springer-com.proxy.lib.ohio-state.edu" src="/static/js/lib/cookie-consent.min.js">
  </script>
  <style>
   @media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}body,img{max-width:100%}body{background:#fcfcfc;font-size:1.125rem;line-height:1.5;min-height:100%}main{display:block}h1{font-family:Georgia,Palatino,serif;font-size:2.25rem;font-style:normal;font-weight:400;line-height:1.4;margin:.67em 0}a{background-color:transparent;color:#004b83;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}sup{font-size:75%;line-height:0;position:relative;top:-.5em;vertical-align:baseline}img{border:0;height:auto;vertical-align:middle}button,input{font-family:inherit;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}*{margin:0}h2{font-family:Georgia,Palatino,serif;font-size:1.75rem;font-style:normal;font-weight:400;line-height:1.4}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input,p{margin:0;padding:0}a>img{vertical-align:middle}p{overflow-wrap:break-word;word-break:break-word}.c-app-header__theme{border-top-left-radius:2px;border-top-right-radius:2px;height:50px;margin:-16px -16px 0;overflow:hidden;position:relative}@media only screen and (min-width:1024px){.c-app-header__theme:after{background-color:hsla(0,0%,100%,.15);bottom:0;content:"";position:absolute;right:0;top:0;width:456px}}.c-app-header__content{padding-top:16px}@media only screen and (min-width:1024px){.c-app-header__content{display:flex}}.c-app-header__main{display:flex;flex:1 1 auto}.c-app-header__cover{margin-right:16px;margin-top:-50px;position:relative;z-index:5}.c-app-header__cover img{border:2px solid #fff;border-radius:4px;box-shadow:0 0 5px 2px hsla(0,0%,50%,.2);max-height:125px;max-width:96px}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-breadcrumbs>li{display:inline}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;list-style:none;margin:0;padding:16px}@media only screen and (min-width:540px){.c-pagination{justify-content:center}}.c-pagination__item{margin-bottom:8px;margin-right:16px}.c-pagination__item:last-child{margin-right:0}.c-pagination__link{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;min-width:30px;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link svg,.c-pagination__link--disabled svg{fill:currentcolor}.c-pagination__link:visited{color:#004b83}.c-pagination__link:focus,.c-pagination__link:hover{border:1px solid #666;text-decoration:none}.c-pagination__link:focus,.c-pagination__link:hover{background-color:#666;background-image:none;color:#fff}.c-pagination__link:focus svg path,.c-pagination__link:hover svg path{fill:#fff}.c-pagination__link--disabled{align-items:center;background-color:transparent;background-image:none;border-radius:2px;color:#333;cursor:default;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;opacity:.67;padding:8px;position:relative;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link--disabled:visited{color:#333}.c-pagination__link--disabled,.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{border:1px solid #ccc;text-decoration:none}.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{background-color:transparent;background-image:none;color:#333}.c-pagination__link--disabled:focus svg path,.c-pagination__link--disabled:hover svg path{fill:#333}.c-pagination__link--active{background-color:#666;background-image:none;border-color:#666;color:#fff;cursor:default}.c-pagination__ellipsis{background:0 0;border:0;min-width:auto;padding-left:0;padding-right:0}.c-pagination__icon{fill:#999;height:12px;width:16px}.c-pagination__icon--active{fill:#004b83}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#666;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-box{background-color:#fff;border:1px solid #ccc;border-radius:2px;line-height:1.3;padding:16px}.c-box--shadowed{box-shadow:0 0 5px 0 hsla(0,0%,50%,.1)}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #f2f2f2;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__section-item .c-article-section__title-number{display:none}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff}.c-pdf-download__link .u-icon{padding-top:2px}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}.u-button svg,.u-button--primary svg{fill:currentcolor}.app-elements .c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;line-height:1.4;padding:8px 0 0}.app-elements .c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav{border-top:2px solid #cedbe0;padding-top:4px;position:relative}.app-elements .c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav-container>:not(:last-child){margin-right:32px}.app-elements .c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.app-elements .c-header__list{list-style:none;margin:0;padding:0}.app-elements .c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.app-elements .c-header__list-item:not(:last-child){border-bottom:2px solid #cedbe0}.app-elements .c-header__item{color:inherit}@media only screen and (min-width:540px){.app-elements .c-header__item--menu{display:none;visibility:hidden}.app-elements .c-header__item--menu:first-child+*{margin-block-start:0}}.app-elements .c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:540px){.app-elements .c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.app-elements .c-header__item--divider:before{border-left:2px solid #cedbe0;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.app-elements .c-header__brand a{display:block;line-height:1;padding:16px 8px;text-decoration:none}.app-elements .c-header__brand img{height:24px;width:auto}.app-elements .c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-elements .c-header__link--static{flex:0 0 auto}.app-elements .c-header__icon{fill:currentcolor;display:inline-block;font-size:24px;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.app-elements .c-header__icon+*{margin-left:8px}.app-elements .c-header__expander{background-color:#ebf1f5}.app-elements .c-header__search{padding:24px 0}@media only screen and (min-width:540px){.app-elements .c-header__search{max-width:70%}}.app-elements .c-header__search-container{position:relative}.app-elements .c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.app-elements .c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.app-elements .c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.app-elements .has-tethered.c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:540px){.app-elements .has-tethered.c-header__expander--menu{display:none;visibility:hidden}}.app-elements .has-tethered .c-header__heading{display:none;visibility:hidden}.app-elements .has-tethered .c-header__heading:first-child+*{margin-block-start:0}.app-elements .has-tethered .c-header__search{margin:auto}.app-elements .c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-justify-content-space-between{justify-content:space-between}.u-flex-shrink{flex:0 1 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-mt-0{margin-top:0}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-float-left{float:left}.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-text-sm{font-size:1rem}.u-text-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-h3{font-family:Georgia,Palatino,serif;font-size:1.5rem;font-style:normal;font-weight:400;line-height:1.4}.c-article-section__content p{line-height:1.8}.c-pagination__input{border:1px solid #bfbfbf;border-radius:2px;box-shadow:inset 0 2px 6px 0 rgba(51,51,51,.2);box-sizing:initial;display:inline-block;height:28px;margin:0;max-width:64px;min-width:16px;padding:0 8px;text-align:center;transition:width .15s ease 0s}.c-pagination__input::-webkit-inner-spin-button,.c-pagination__input::-webkit-outer-spin-button{-webkit-appearance:none;margin:0}.c-article-associated-content__container .c-article-associated-content__collection-label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.063rem}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.063rem;font-weight:400}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-section__title,.c-article-title{font-weight:400}.c-chapter-book-series{font-size:1rem}.c-chapter-identifiers{margin:16px 0 8px}.c-chapter-book-details{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative}.c-chapter-book-details__title{font-weight:700}.c-chapter-book-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-chapter-book-details a{color:inherit}@media only screen and (max-width:539px){.c-chapter-book-details__meta{display:block}}.c-cover-image-lightbox{align-items:center;bottom:0;display:flex;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;transition:all .15s ease-in 0s;visibility:hidden;z-index:-1}.js-cover-image-lightbox--close{background:0 0;border:0;color:#fff;cursor:pointer;font-size:1.875rem;padding:13px;position:absolute;right:10px;top:0}.c-cover-image-lightbox__image{max-height:90vh;width:auto}.c-expand-overlay{background:#fff;color:#333;opacity:.5;padding:2px;position:absolute;right:3px;top:3px}.c-pdf-download__link{padding:13px 24px} }
  </style>
  <link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/enhanced-article-927ffe4eaf.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
  <script>
   window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/978-3-030-01237-3_43","Page":"chapter","page":{"attributes":{"environment":"live"}},"Country":"US","japan":false,"doi":"10.1007-978-3-030-01237-3_43","Keywords":"Style transfer, Generative network, Deep learning","kwrd":["Style_transfer","Generative_network","Deep_learning"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate","cobranding","doNotAutoAssociate","cobranding"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3000266689","8200724141"],"businessPartnerIDString":"3000266689|8200724141"}},"Access Type":"permanently-free","Bpids":"3000266689, 8200724141","Bpnames":"OhioLINK Consortium, Ohio State University Libraries","BPID":["3000266689","8200724141"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-978-3-030-01237-3","Full HTML":"Y","session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1611-3349","pissn":"0302-9743"},"book":{"doi":"10.1007/978-3-030-01237-3","title":"Computer Vision – ECCV 2018","pisbn":"978-3-030-01236-6","eisbn":"978-3-030-01237-3","bookProductType":"Proceedings","seriesTitle":"Lecture Notes in Computer Science","seriesId":"558"},"chapter":{"doi":"10.1007/978-3-030-01237-3_43"},"type":"ConferencePaper","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"SCI","secondarySubjects":{"1":"Image Processing and Computer Vision","2":"Artificial Intelligence"},"secondarySubjectCodes":{"1":"SCI22021","2":"SCI21000"}},"sucode":"SUCO11645"},"attributes":{"deliveryPlatform":"oscar"},"country":"US","Has Preview":"N","subjectCodes":"SCI,SCI22021,SCI21000","PMC":["SCI","SCI22021","SCI21000"]},"Event Category":"Conference Paper","ConferenceSeriesId":"eccv, eccv","productId":"9783030012373"}];
  </script>
  <script>
   window.dataLayer.push({
        ga4MeasurementId: 'G-B3E4QL2TPR',
        ga360TrackingId: 'UA-26408784-1',
        twitterId: 'o47a7',
        ga4ServerUrl: 'https://collect-springer-com.proxy.lib.ohio-state.edu',
        imprint: 'springerlink'
    });
  </script>
  <script data-test="gtm-head">
   window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
  </script>
  <script>
   (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/lib/cookie-consent.min.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
  </script>
  <script>
   (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
  </script>
  <script>
   (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
  </script>
  <script class="js-entry">
   if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-f934ac6316.js', 'async': false},
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/app-es5-bundle-774ca0a0f5.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/app-es6-bundle-047cc3c848.js', 'async': false, 'module': true}
                
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-c14b406246.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
  </script>
  <script src="/oscar-static/js/airbrake-es5-bundle-f934ac6316.js">
  </script>
  <script src="/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js">
  </script>
  <link href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43" rel="canonical"/>
  <script type="application/ld+json">
   {"headline":"A Style-Aware Content Loss for Real-Time HD Style Transfer","pageEnd":"731","pageStart":"715","image":"https://media-springernature-com.proxy.lib.ohio-state.edu/w153/springer-static/cover/book/978-3-030-01237-3.jpg","genre":["Computer Science","Computer Science (R0)"],"isPartOf":{"name":"Computer Vision – ECCV 2018","isbn":["978-3-030-01237-3","978-3-030-01236-6"],"@type":"Book"},"publisher":{"name":"Springer International Publishing","logo":{"url":"https://www-springernature-com.proxy.lib.ohio-state.edu/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Artsiom Sanakoyeu","affiliation":[{"name":"IWR, Heidelberg University","address":{"name":"Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Heidelberg, Germany","@type":"PostalAddress"},"@type":"Organization"}],"email":"artsiom.sanakoyeu@iwr.uni-heidelberg.de","@type":"Person"},{"name":"Dmytro Kotovenko","affiliation":[{"name":"IWR, Heidelberg University","address":{"name":"Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Heidelberg, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Sabine Lang","affiliation":[{"name":"IWR, Heidelberg University","address":{"name":"Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Heidelberg, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Björn Ommer","affiliation":[{"name":"IWR, Heidelberg University","address":{"name":"Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Heidelberg, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"keywords":"Style transfer, Generative network, Deep learning","description":"Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more images. Moreover, previous work has relied on a direct comparison of art in the domain of RGB images or on CNNs pre-trained on ImageNet, which requires millions of labeled object bounding boxes and can introduce an extra bias, since it has been assembled without artistic consideration. To circumvent these issues, we propose a style-aware content loss, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and videos. We propose a quantitative measure for evaluating the quality of a stylized image and also have art historians rank patches from our approach against those from previous work. These and our qualitative results ranging from small image patches to megapixel stylistic images and videos show that our approach better captures the subtle nature in which a style affects content.","datePublished":"2018","isAccessibleForFree":true,"@type":"ScholarlyArticle","@context":"https://schema.org"}
  </script>
  <style type="text/css">
   .c-cookie-banner {
			background-color: #01324b;
			color: white;
			font-size: 1rem;
			position: fixed;
			bottom: 0;
			left: 0;
			right: 0;
			padding: 16px 0;
			font-family: sans-serif;
			z-index: 100002;
			text-align: center;
		}
		.c-cookie-banner__container {
			margin: 0 auto;
			max-width: 1280px;
			padding: 0 16px;
		}
		.c-cookie-banner p {
			margin-bottom: 8px;
		}
		.c-cookie-banner p:last-child {
			margin-bottom: 0;
		}	
		.c-cookie-banner__dismiss {
			background-color: transparent;
			border: 0;
			padding: 0;
			margin-left: 4px;
			color: inherit;
			text-decoration: underline;
			font-size: inherit;
		}
		.c-cookie-banner__dismiss:hover {
			text-decoration: none;
		}
  </style>
  <style type="text/css">
   .MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
  </style>
  <style type="text/css">
   #MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
  </style>
  <style type="text/css">
   .MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
  </style>
  <style type="text/css">
   #MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
  </style>
  <style type="text/css">
   .MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
  </style>
  <style type="text/css">
   .MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <script>
   window.dataLayer = window.dataLayer || [];
            window.dataLayer.push({
                recommendations: {
                    recommender: 'semantic',
                    model: 'specter',
                    policy_id: 'NA',
                    timestamp: 1698023149,
                    embedded_user: 'null'
                }
            });
  </script>
 </head>
 <body class="shared-article-renderer">
  <div id="MathJax_Message" style="">
   Loading [MathJax]/jax/output/HTML-CSS/config.js
  </div>
  <!-- Google Tag Manager (noscript) -->
  <noscript data-test="gtm-body">
   <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0">
   </iframe>
  </noscript>
  <!-- End Google Tag Manager (noscript) -->
  <div class="u-vh-full">
   <a class="c-skip-link" href="#main-content">
    Skip to main content
   </a>
   <div class="u-hide u-show-following-ad">
   </div>
   <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
    <div class="c-ad__inner">
     <p class="c-ad__label">
      Advertisement
     </p>
     <div data-gpt="" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;" data-gpt-unitpath="/270604982/springerlink/book/chapter" data-pa11y-ignore="" data-test="LB1-ad" id="div-gpt-ad-LB1" style="min-width:728px;min-height:90px">
     </div>
    </div>
   </aside>
   <div class="app-elements u-mb-24">
    <header class="c-header" data-header="">
     <div class="c-header__container" data-header-expander-anchor="">
      <div class="c-header__brand">
       <a data-test="logo" data-track="click" data-track-action="click logo link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu">
        <img alt="SpringerLink" src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg"/>
       </a>
      </div>
      <a class="c-header__link c-header__link--static" data-test="login-link" data-track="click" data-track-action="click log in link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-030-01237-3_43">
       <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
        <use xlink:href="#icon-eds-user-single">
        </use>
       </svg>
       <span>
        Log in
       </span>
      </a>
     </div>
     <nav aria-label="header navigation" class="c-header__nav">
      <div class="c-header__nav-container">
       <div class="c-header__item c-header__item--menu">
        <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
         <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
          <use xlink:href="#icon-eds-menu">
          </use>
         </svg>
         <span>
          Menu
         </span>
        </a>
       </div>
       <div class="c-header__item c-header__item--inline-links">
        <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
         Find a journal
        </a>
        <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
         Publish with us
        </a>
       </div>
       <div class="c-header__link-container">
        <div class="c-header__item c-header__item--divider">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
          <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span>
           Search
          </span>
         </a>
        </div>
        <div class="c-header__item">
         <div class="c-header__item ecommerce-cart" id="ecommerce-header-cart-icon-link" style="display:inline-block">
          <a class="c-header__link" href="https://order-springer-com.proxy.lib.ohio-state.edu/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
           <svg aria-hidden="true" focusable="false" height="24" id="eds-i-cart" style="vertical-align:bottom" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <path d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z" fill="currentColor" fill-rule="nonzero">
            </path>
           </svg>
           <span style="padding-left:10px">
            Cart
           </span>
           <span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center">
           </span>
          </a>
          <script>
           (function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    console.log("listen to updatedCart event");
    document.body.addEventListener("updatedCart", function () {
        console.log("updatedCart happened");
        updateCartIcon().then(function () { return console.log("Cart state update upon event"); });
    }, false);
    return updateCartIcon().then(function () { return console.log("Initial cart state update"); });
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function () { return console.log("Could not fetch cart info"); });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart-springer-com.proxy.lib.ohio-state.edu/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()
          </script>
         </div>
        </div>
       </div>
      </div>
     </nav>
    </header>
    <div class="c-header__expander has-tethered u-js-hide" hidden="" id="popup-search">
     <h2 class="c-header__heading">
      Search
     </h2>
     <div class="u-container">
      <div class="c-header__search">
       <form action="//link-springer-com.proxy.lib.ohio-state.edu/search" data-track="submit" data-track-action="submit search form" data-track-category="unified header" data-track-label="form" method="GET" role="search">
        <label class="c-header__search-label" for="header-search">
         Search by keyword or author
        </label>
        <div class="c-header__search-container">
         <input autocomplete="off" class="c-header__search-input" id="header-search" name="query" required="" type="text" value=""/>
         <button class="c-header__search-button" type="submit">
          <svg aria-hidden="true" class="c-header__icon" focusable="false">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span class="u-visually-hidden">
           Search
          </span>
         </button>
        </div>
       </form>
      </div>
     </div>
    </div>
    <div class="c-header__expander c-header__expander--menu has-tethered u-js-hide" hidden="" id="header-nav">
     <h2 class="c-header__heading">
      Navigation
     </h2>
     <ul class="c-header__list">
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
        Find a journal
       </a>
      </li>
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
        Publish with us
       </a>
      </li>
     </ul>
    </div>
   </div>
   <div class="u-container u-mb-32 u-clearfix" data-component="article-container" id="main-content">
    <div class="u-hide-at-lg js-context-bar-sticky-point-mobile">
     <div class="c-pdf-container">
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-030-01237-3.pdf?pdf=button" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book PDF
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-030-01237-3.epub" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book EPUB
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
     </div>
    </div>
    <header class="u-mb-24" data-test="chapter-information-header">
     <div class="c-box c-box--shadowed">
      <div class="c-app-header">
       <div class="c-app-header__theme" style="background-image: url('https://media-springernature-com.proxy.lib.ohio-state.edu/dominant-colour/springer-static/cover/book/978-3-030-01237-3.jpg')">
       </div>
       <div class="c-app-header__content">
        <div class="c-app-header__main">
         <div class="c-app-header__cover">
          <div class="c-app-expand-overlay-wrapper">
           <a data-component="cover-zoom" data-img-src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-030-01237-3" href="/chapter/10.1007/978-3-030-01237-3_43/cover" rel="nofollow">
            <picture>
             <source srcset="                                                         //media.springernature.com/w92/springer-static/cover/book/978-3-030-01237-3.jpg?as=webp 1x,                                                         //media.springernature.com/w184/springer-static/cover/book/978-3-030-01237-3.jpg?as=webp 2x" type="image/webp"/>
             <img alt="Book cover" height="130" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92/springer-static/cover/book/978-3-030-01237-3.jpg" width="90"/>
            </picture>
            <svg aria-hidden="true" class="c-expand-overlay u-icon" data-component="expand-icon" focusable="false" height="18" width="18">
             <use xlink:href="#icon-expand-image" xmlns:xlink="http://www.w3.org/1999/xlink">
             </use>
            </svg>
           </a>
          </div>
         </div>
         <div class="c-cover-image-lightbox u-hide" data-component="cover-lightbox">
          <button aria-label="Close expanded book cover" class="js-cover-image-lightbox--close" data-component="close-cover-lightbox" type="button">
           <span aria-hidden="true">
            ×
           </span>
          </button>
          <picture>
           <source srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-030-01237-3?as=webp" type="image/webp"/>
           <img alt="Book cover" class="c-cover-image-lightbox__image" height="1200" src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-030-01237-3" width="800"/>
          </picture>
         </div>
         <div class="u-flex-shrink">
          <p class="c-chapter-info-details u-mb-8">
           <a data-track="click" data-track-action="open conference" data-track-label="link" href="/conference/eccv eccv">
            European Conference on Computer Vision
           </a>
          </p>
          <p class="c-chapter-book-details right-arrow">
           ECCV 2018:
           <a class="c-chapter-book-details__title" data-test="book-link" data-track="click" data-track-action="open book series" data-track-label="link" href="/book/10.1007/978-3-030-01237-3">
            Computer Vision – ECCV 2018
           </a>
           pp
                                         715–731
           <a class="c-chapter-book-details__cite-as u-hide-print" data-track="click" data-track-action="cite this chapter" data-track-label="link" href="#citeas">
            Cite as
           </a>
          </p>
         </div>
        </div>
       </div>
      </div>
     </div>
    </header>
    <nav aria-label="breadcrumbs" class="u-mb-16" data-test="article-breadcrumbs">
     <ol class="c-breadcrumbs c-breadcrumbs--truncated" itemscope="" itemtype="https://schema.org/BreadcrumbList">
      <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb1" href="/" itemprop="item">
        <span itemprop="name">
         Home
        </span>
       </a>
       <meta content="1" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb2" href="/book/10.1007/978-3-030-01237-3" itemprop="item">
        <span itemprop="name">
         Computer Vision – ECCV 2018
        </span>
       </a>
       <meta content="2" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <span itemprop="name">
        Conference paper
       </span>
       <meta content="3" itemprop="position"/>
      </li>
     </ol>
    </nav>
    <main class="c-article-main-column u-float-left js-main-column u-text-sans-serif" data-track-component="conference paper">
     <div aria-hidden="true" class="c-context-bar u-hide" data-context-bar="" data-context-bar-with-recommendations="" data-test="context-bar">
      <div class="c-context-bar__container u-container">
       <div class="c-context-bar__title">
        A Style-Aware Content Loss for Real-Time HD Style Transfer
       </div>
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-030-01237-3.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-030-01237-3.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div id="recommendations">
       <div class="c-recommendations__container u-container u-display-none" data-component-recommendations="">
        <aside class="c-status-message c-status-message--success u-display-none" data-component-status-msg="">
         <svg aria-label="success:" class="c-status-message__icon" focusable="false" height="24" role="img" width="24">
          <use xlink:href="#icon-success">
          </use>
         </svg>
         <div class="c-status-message__message" id="success-message" tabindex="-1">
          Your content has downloaded
         </div>
        </aside>
        <div class="c-recommendations-header u-display-flex u-justify-content-space-between">
         <h2 class="c-recommendations-title" id="recommendation-heading">
          Similar content being viewed by others
         </h2>
         <button aria-label="Close" class="c-recommendations-close u-flex-static" data-track="click" data-track-action="close recommendations" type="button">
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-close">
           </use>
          </svg>
         </button>
        </div>
        <section aria-labelledby="recommendation-heading" aria-roledescription="carousel">
         <p class="u-visually-hidden">
          Slider with three content items shown per slide. Use the Previous and Next buttons to navigate the slides or the slide controller buttons at the end to navigate through each slide.
         </p>
         <div class="c-recommendations-list-container">
          <div class="c-recommendations-list">
           <div aria-label="Recommendation 1 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-11018-5?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 1" data-track-label="10.1007/978-3-030-11018-5_32" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-11018-5_32" itemprop="url">
                  Multi-style Generative Network for Real-Time Transfer
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2019
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 2 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-29891-3?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 2" data-track-label="10.1007/978-3-030-29891-3_19" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-29891-3_19" itemprop="url">
                  Real-Time Style Transfer with Strength Control
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2019
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 3 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-20071-7?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 3" data-track-label="10.1007/978-3-031-20071-7_7" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-20071-7_7" itemprop="url">
                  Learning Graph Neural Networks for Image Style Transfer
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2022
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 4 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs00371-018-1609-4/MediaObjects/371_2018_1609_Fig1_HTML.jpg"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 4" data-track-label="10.1007/s00371-018-1609-4" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s00371-018-1609-4" itemprop="url">
                  Photographic style transfer
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  10 November 2018
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Li Wang, Zhao Wang, … Jianjun Zhang
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 5 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs44196-021-00021-0/MediaObjects/44196_2021_21_Fig1_HTML.png"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 5" data-track-label="10.1007/s44196-021-00021-0" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s44196-021-00021-0" itemprop="url">
                  Interactive Artistic Multi-style Transfer
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  08 November 2021
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Xiaohui Wang, Yiran Lyu, … Jingyan Qin
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 6 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11042-022-12121-w/MediaObjects/11042_2022_12121_Fig1_HTML.png"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 6" data-track-label="10.1007/s11042-022-12121-w" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11042-022-12121-w" itemprop="url">
                  Research on style transfer for multiple regions
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  25 January 2022
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Wang Yang, Yu Zhenxin &amp; Long Haiyan
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 7 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-981-13-8138-6?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 7" data-track-label="10.1007/978-981-13-8138-6_14" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-981-13-8138-6_14" itemprop="url">
                  Improving Semantic Style Transfer Using Guided Gram Matrices
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2019
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 8 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11263-018-1089-z/MediaObjects/11263_2018_1089_Fig1_HTML.jpg"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 8" data-track-label="10.1007/s11263-018-1089-z" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11263-018-1089-z" itemprop="url">
                  Artistic Style Transfer for Videos and Spherical Images
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  21 April 2018
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Manuel Ruder, Alexey Dosovitskiy &amp; Thomas Brox
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 9 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-44207-0?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 9" data-track-label="10.1007/978-3-031-44207-0_21" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-44207-0_21" itemprop="url">
                  HaarStyle:Revision Style Transfer Based on Multiple Resolutions
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2023
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
          </div>
         </div>
        </section>
       </div>
       <div class="js-greyout-page-background" data-component-grey-background="" style="display:none">
       </div>
      </div>
     </div>
     <article lang="en">
      <header data-test="chapter-detail-header">
       <div class="c-article-header">
        <h1 class="c-article-title" data-chapter-title="" data-test="chapter-title">
         A Style-Aware Content Loss for Real-Time HD Style Transfer
        </h1>
        <ul class="c-article-author-list c-article-author-list--short js-no-scroll" data-component-authors-activator="authors-list" data-test="authors-list">
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Artsiom-Sanakoyeu" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Artsiom-Sanakoyeu">
           Artsiom Sanakoyeu
           <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
            <use xlink:href="#icon-email" xmlns:xlink="http://www.w3.org/1999/xlink">
            </use>
           </svg>
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Dmytro-Kotovenko" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Dmytro-Kotovenko">
           Dmytro Kotovenko
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Sabine-Lang" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Sabine-Lang">
           Sabine Lang
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
          </sup>
          &amp;
         </li>
         <li aria-label="Show all 4 authors for this article" class="c-article-author-list__show-more" title="Show all 4 authors for this article">
          …
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Bj_rn-Ommer" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Bj_rn-Ommer">
           Björn Ommer
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
          </sup>
         </li>
        </ul>
        <button aria-expanded="false" class="c-article-author-list__button">
         <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
          <use xlink:href="#icon-plus" xmlns:xlink="http://www.w3.org/1999/xlink">
          </use>
         </svg>
         <span>
          Show authors
         </span>
        </button>
        <ul class="c-article-identifiers c-chapter-identifiers">
         <li class="c-article-identifiers__item" data-test="article-category">
          Conference paper
         </li>
         <li class="c-article-identifiers__item">
          <a data-track="click" data-track-action="publication date" data-track-label="link" href="#chapter-info">
           First Online:
           <time datetime="2018-10-07">
            07 October 2018
           </time>
          </a>
         </li>
        </ul>
        <div data-test="article-metrics">
         <div id="altmetric-container">
          <div class="c-article-metrics-bar__wrapper u-clear-both">
           <ul class="c-article-metrics-bar u-list-reset">
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              2672
              <span class="c-article-metrics-bar__label">
               Accesses
              </span>
             </p>
            </li>
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              89
              <a class="c-article-metrics-bar__label" data-track="click" data-track-action="Citation count" data-track-label="link" href="http://citations.springer-com.proxy.lib.ohio-state.edu/item?doi=10.1007/978-3-030-01237-3_43" rel="noopener" target="_blank" title="Visit Springer Citations for full citation details">
               Citations
              </a>
             </p>
            </li>
           </ul>
          </div>
         </div>
        </div>
        <p class="c-chapter-book-series">
         Part of the
         <a data-track="click" data-track-action="open book series" data-track-label="link" href="/bookseries/558">
          Lecture Notes in Computer Science
         </a>
         book series (LNIP,volume 11212)
        </p>
       </div>
      </header>
      <div class="c-article-body" data-article-body="true">
       <section aria-labelledby="Abs1" data-title="Abstract" lang="en">
        <div class="c-article-section" id="Abs1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">
          <span class="c-article-section__title-number">
          </span>
          Abstract
         </h2>
         <div class="c-article-section__content" id="Abs1-content">
          <p>
           Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more images. Moreover, previous work has relied on a direct comparison of art in the domain of RGB images or on CNNs pre-trained on ImageNet, which requires millions of labeled object bounding boxes and can introduce an extra bias, since it has been assembled without artistic consideration. To circumvent these issues, we propose a style-aware content loss, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and videos. We propose a quantitative measure for evaluating the quality of a stylized image and also have art historians rank patches from our approach against those from previous work. These and our qualitative results ranging from small image patches to megapixel stylistic images and videos show that our approach better captures the subtle nature in which a style affects content.
          </p>
          <h3 class="c-article__sub-heading">
           Keywords
          </h3>
          <ul class="c-article-subject-list">
           <li class="c-article-subject-list__subject">
            <span>
             Style transfer
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Generative network
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Deep learning
            </span>
           </li>
          </ul>
         </div>
        </div>
       </section>
       <div class="c-article-section__content c-article-section__content--separator">
        <p>
         A. Sanakoyeu and D. Kotovenko—Contributed equally to this work.
        </p>
       </div>
       <div data-test="chapter-cobranding-and-download">
        <div class="note test-pdf-link" id="cobranding-and-download-availability-text">
         <div class="c-article-access-provider" data-component="provided-by-box">
          <p class="c-article-access-provider__text">
           <a class="c-pdf-download__link" data-track="click" data-track-action="Pdf download" data-track-label="inline link" download="" href="/content/pdf/10.1007/978-3-030-01237-3_43.pdf?pdf=inline%20link" id="js-body-chapter-download" rel="noopener" style="display: inline; padding:0px!important;" target="_blank">
            Download
           </a>
           conference paper PDF
          </p>
         </div>
        </div>
       </div>
       <div class="main-content">
        <section data-title="Introduction">
         <div class="c-article-section" id="Sec1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">
           <span class="c-article-section__title-number">
            1
           </span>
           Introduction
          </h2>
          <div class="c-article-section__content" id="Sec1-content">
           <p>
            A picture may be worth a thousand words, but at least it contains a lot of very diverse information. This not only comprises
            <i>
             what
            </i>
            is portrayed, e.g., composition of a scene and individual objects, but also
            <i>
             how
            </i>
            it is depicted, referring to the artistic style of a painting or filters applied to a photo. Especially when considering artistic images, it becomes evident that not only content but also style is a crucial part of the message an image communicates (just imagine van Gogh’s Starry Night in the style of Pop Art). Here, we follow the common wording of our community and refer to ‘content’ as a synonym for ‘subject matter’ or ‘sujet’, preferably used in art history. A vision system then faces the challenge to decompose and separately represent the content and style of an image to enable a direct analysis based on each individually. The ultimate test for this ability is style transfer [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e910" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ] – exchanging the style of an image while retaining its content (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            ).
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 1." id="figure-1">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/1" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig1_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 1" aria-describedby="Fig1" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig1_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc">
               <p>
                Evaluating the fine details preserved by our approach. Can you guess which of the cut-outs are from Monet’s artworks and which are generated? Solution is on p. xx.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/1" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 2." id="figure-2">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/2" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig2_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 2" aria-describedby="Fig2" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig2_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc">
               <p>
                Style transfer using different approaches on 1 and a collection of reference style images. (a) [
                <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e945" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
                 12
                </a>
                ] using van Gogh’s “Road with Cypress and Star” as reference style image; (b) [
                <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e948" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
                 12
                </a>
                ] using van Gogh’s “Starry night”; (c) [
                <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e951" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
                 12
                </a>
                ] using the average Gram matrix computed across the collection of Vincent van Gogh’s artworks; (d) [
                <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e954" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
                 22
                </a>
                ] trained on the collection of van Gogh’s artworks alternating target style images every SGD mini-batch; (e) our approach trained on the same collection of van Gogh’s artworks. Stylizations
                <i>
                 (a)
                </i>
                and
                <i>
                 (b)
                </i>
                depend significantly on the particular style image, but using a collection of the style images
                <i>
                 (c)
                </i>
                ,
                <i>
                 (d)
                </i>
                does not produce visually plausible results, due to oversmoothing over the numerous Gram matrices. In contrast, our approach
                <i>
                 (e)
                </i>
                has learned how van Gogh is altering particular content in a specific manner (edges around objects also stylized, cf. bell tower)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/2" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            In contrast to the seminal work of Gatys et al. [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e985" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ], who have relied on powerful but slow iterative optimization, there has recently been a focus on feed-forward generator networks [
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d66353580e988" title="Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style. In: Proceedings of ICLR (2017)">
             6
            </a>
            ,
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d66353580e991" title="Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)">
             20
            </a>
            ,
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e994" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e997" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d66353580e1001" title="Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: feed-forward synthesis of textures and stylized images. In: ICML, pp. 1349–1357 (2016)">
             40
            </a>
            ,
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d66353580e1004" title="Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. In: Proceedings of CVPR (2017)">
             41
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d66353580e1007" title="Wang, X., Oxholm, G., Zhang, D., Wang, Y.F.: Multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017">
             44
            </a>
            ]. The crucial representation in all these approaches has been based on a VGG16 or VGG19 network [
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d66353580e1010" title="Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint 
                      arXiv:1409.1556
                      
                     (2014)">
             39
            </a>
            ], pre-trained on ImageNet [
            <a aria-label="Reference 34" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR34" id="ref-link-section-d66353580e1013" title="Russakovsky, O.: ImageNet large scale visual recognition challenge. Int. J. Comput. Vis. (IJCV) 115(3), 211–252 (2015). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/s11263-015-0816-y
                      
                    ">
             34
            </a>
            ]. However, a recent trend in deep learning has been to avoid supervised pre-training on a million images with tediously labeled object bounding boxes [
            <a aria-label="Reference 43" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR43" id="ref-link-section-d66353580e1016" title="Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos. arXiv preprint 
                      arXiv:1505.00687
                      
                     (2015)">
             43
            </a>
            ]. In the setting of style transfer this has the particular benefit of avoiding from the outset any bias introduced by ImageNet, which has been assembled without artistic consideration. Rather than utilizing a separate pre-trained VGG network to measure and optimize the quality of the stylistic output [
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d66353580e1020" title="Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style. In: Proceedings of ICLR (2017)">
             6
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1023" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ,
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e1026" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e1029" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d66353580e1032" title="Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: feed-forward synthesis of textures and stylized images. In: ICML, pp. 1349–1357 (2016)">
             40
            </a>
            ,
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d66353580e1035" title="Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. In: Proceedings of CVPR (2017)">
             41
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d66353580e1039" title="Wang, X., Oxholm, G., Zhang, D., Wang, Y.F.: Multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017">
             44
            </a>
            ], we employ an encoder-decoder architecture with adversarial discriminator, Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            , to stylize the input content image and also use the encoder to measure the reconstruction loss. In essence the stylized output image is again run through the encoder and compared with the encoded input content image. Thus, we learn a style-specific content loss from scratch, which adapts to the specific way in which a particular style retains content and is more adaptive than a comparison in the domain of RGB images [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1045" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ].
           </p>
           <p>
            Most importantly, however, previous work has only been based on a
            <i>
             single
            </i>
            style image. This stands in stark contrast to art history which understands “style as an expression of a collective spirit” resulting in a “distinctive manner which permits the grouping of works into related categories” [
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR9" id="ref-link-section-d66353580e1054" title="Fernie, E.: Art History and Its Methods: A Critical Anthology, p. 361. Phaidon, London (1995)">
             9
            </a>
            ]. As a result, art history developed a scheme, which allows to identify groups of artworks based on shared qualities. Artistic style consists of a diverse range of elements, such as form, color, brushstroke, or use of light. Therefore, it is insufficient to only use a single artwork, because it might not represent the full scope of an artistic style. Today, freely available art datasets such as Wikiart [
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d66353580e1057" title="Karayev, S., et al.: Recognizing image style. arXiv preprint 
                      arXiv:1311.3715
                      
                     (2013)">
             23
            </a>
            ] easily contain more than 100K images, thus providing numerous examples for various styles. Previous work [
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d66353580e1060" title="Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style. In: Proceedings of ICLR (2017)">
             6
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1063" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ,
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e1067" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e1070" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d66353580e1073" title="Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: feed-forward synthesis of textures and stylized images. In: ICML, pp. 1349–1357 (2016)">
             40
            </a>
            ,
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d66353580e1076" title="Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. In: Proceedings of CVPR (2017)">
             41
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d66353580e1079" title="Wang, X., Oxholm, G., Zhang, D., Wang, Y.F.: Multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017">
             44
            </a>
            ] has represented style based on the Gram matrix, which captures highly image-specific style statistics, cf. Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            . To combine several style images in [
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d66353580e1086" title="Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style. In: Proceedings of ICLR (2017)">
             6
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1089" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ,
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e1092" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e1095" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d66353580e1098" title="Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: feed-forward synthesis of textures and stylized images. In: ICML, pp. 1349–1357 (2016)">
             40
            </a>
            ,
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d66353580e1101" title="Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. In: Proceedings of CVPR (2017)">
             41
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d66353580e1105" title="Wang, X., Oxholm, G., Zhang, D., Wang, Y.F.: Multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017">
             44
            </a>
            ] one needs to aggregate their Gram matrices. We have evaluated several aggregation strategies and averaging worked the best, Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            (c). But, obviously, neither art history, nor statistics suggests aggregating Gram matrices. Additionally, we investigated alternating the target style images in every mini-batch while training [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e1111" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ], Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            (d). However, all these methods cannot make proper use of several style images, because combining the Gram matrices of several images forfeits the details of style, cf. the analysis in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            . In contrast, our proposed approach allows to combine an arbitrary number of instances of a style during training.
           </p>
           <p>
            We conduct extensive evaluations of the proposed style transfer approach; we quantitatively and qualitatively compare it against numerous baselines. Being able to generate high quality artistic works in high-resolution, our approach produces visually more detailed stylizations than the current state of the art style transfer approaches and yet shows real-time inference speed. The results are quantitatively validated by experts from art history and by adopting in this paper a
            <i>
             deception rate
            </i>
            metric based on a deep neural network for artist classification.
           </p>
           <h3 class="c-article__sub-heading" id="Sec2">
            <span class="c-article-section__title-number">
             1.1
            </span>
            Related Work
           </h3>
           <p>
            In recent years, a lot of research efforts have been devoted to texture synthesis and style transfer problems. Earlier methods [
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d66353580e1134" title="Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analogies. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 327–340. ACM (2001)">
             17
            </a>
            ] are usually non-parametric and are build upon low-level image features. Inspired by Image Analogies [
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d66353580e1137" title="Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analogies. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 327–340. ACM (2001)">
             17
            </a>
            ], approaches [
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d66353580e1140" title="Frigo, O., Sabater, N., Delon, J., Hellier, P.: Split and match: example-based adaptive patch sampling for unsupervised style transfer. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 553–561 (2016)">
             10
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d66353580e1143" title="Liao, J., Yao, Y., Yuan, L., Hua, G., Kang, S.B.: Visual attribute transfer through deep image analogy. arXiv preprint 
                      arXiv:1705.01088
                      
                     (2017)">
             28
            </a>
            ,
            <a aria-label="Reference 37" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR37" id="ref-link-section-d66353580e1146" title="Shih, Y., Paris, S., Barnes, C., Freeman, W.T., Durand, F.: Style transfer for headshot portraits. ACM Trans. Graph. (TOG) 33(4), 148 (2014)">
             37
            </a>
            ,
            <a aria-label="Reference 38" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR38" id="ref-link-section-d66353580e1150" title="Shih, Y., Paris, S., Durand, F., Freeman, W.T.: Data-driven hallucination of different times of day from a single outdoor photo. ACM Trans. Graph. (TOG) 32(6), 200 (2013)">
             38
            </a>
            ] are based on finding dense correspondence between content and style image and often require image pairs to depict similar content. Therefore, these methods do not scale to the setting of arbitrary content images.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 3." id="figure-3">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/3" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig3_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 3" aria-describedby="Fig3" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig3_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc">
               <p>
                Encoder-decoder network for style transfer based on style-aware content loss.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/3" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            In contrast, Gatys et al. [
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d66353580e1174" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks. arXiv preprint 
                      arXiv:1505.07376
                      
                     12 (2015)">
             11
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1177" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ] proposed a more flexible iterative optimization approach based on a pre-trained VGG19 network [
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d66353580e1180" title="Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint 
                      arXiv:1409.1556
                      
                     (2014)">
             39
            </a>
            ]. This method produces high quality results and works on arbitrary inputs, but is costly, since each optimization step requires a forward and backward pass through the VGG19 network. Subsequent methods [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e1183" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66353580e1186" title="Li, C., Wand, M.: Precomputed real-time texture synthesis with markovian generative adversarial networks. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9907, pp. 702–716. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46487-9_43
                      
                    ">
             25
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d66353580e1190" title="Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: feed-forward synthesis of textures and stylized images. In: ICML, pp. 1349–1357 (2016)">
             40
            </a>
            ] aimed to accelerate the optimization procedure [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1193" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ] by approximating it with feed-forward convolutional neural networks. This way, only one forward pass through the network is required to generate a stylized image. Beyond that, a number of methods have been proposed to address different aspects of style transfer, including quality [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d66353580e1196" title="Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv preprint 
                      arXiv:1612.04337
                      
                     (2016)">
             4
            </a>
            ,
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR13" id="ref-link-section-d66353580e1199" title="Gatys, L.A., Ecker, A.S., Bethge, M., Hertzmann, A., Shechtman, E.: Controlling perceptual factors in neural style transfer. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)">
             13
            </a>
            ,
            <a aria-label="Reference 21" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR21" id="ref-link-section-d66353580e1202" title="Jing, Y., Liu, Y., Yang, Y., Feng, Z., Yu, Y., Song, M.: Stroke controllable fast style transfer with adaptive receptive fields. arXiv preprint 
                      arXiv:1802.07101
                      
                     (2018)">
             21
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d66353580e1205" title="Wang, X., Oxholm, G., Zhang, D., Wang, Y.F.: Multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017">
             44
            </a>
            ,
            <a aria-label="Reference 46" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR46" id="ref-link-section-d66353580e1209" title="Wilmot, P., Risser, E., Barnes, C.: Stable and controllable neural texture synthesis and style transfer using histogram losses. arXiv preprint 
                      arXiv:1701.08893
                      
                     (2017)">
             46
            </a>
            ], diversity [
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d66353580e1212" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Diversified texture synthesis with feed-forward networks. In: IEEE Conference on Computer Vision and Pattern Recognition (2017)">
             26
            </a>
            ,
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d66353580e1215" title="Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. In: Proceedings of CVPR (2017)">
             41
            </a>
            ], photorealism [
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d66353580e1218" title="Luan, F., Paris, S., Shechtman, E., Bala, K.: Deep photo style transfer. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)">
             30
            </a>
            ], combining several styles in a single model [
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR3" id="ref-link-section-d66353580e1221" title="Chen, D., Yuan, L., Liao, J., Yu, N., Hua, G.: Stylebank: an explicit representation for neural image style transfer. In: Proceedings of CVPR (2017)">
             3
            </a>
            ,
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d66353580e1224" title="Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style. In: Proceedings of ICLR (2017)">
             6
            </a>
            ,
            <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR42" id="ref-link-section-d66353580e1228" title="Wang, H., Liang, X., Zhang, H., Yeung, D.Y., Xing, E.P.: ZM-Net: real-time zero-shot image manipulation network. arXiv preprint 
                      arXiv:1703.07255
                      
                     (2017)">
             42
            </a>
            ] and generalizing to previously unseen styles [
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR14" id="ref-link-section-d66353580e1231" title="Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V., Shlens, J.: Exploring the structure of a real-time, arbitrary neural artistic stylization network. arXiv preprint 
                      arXiv:1705.06830
                      
                     (2017)">
             14
            </a>
            ,
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d66353580e1234" title="Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)">
             20
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e1237" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ,
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d66353580e1240" title="Shen, F., Yan, S., Zeng, G.: Meta networks for neural style transfer. arXiv preprint 
                      arXiv:1709.04111
                      
                     (2017)">
             36
            </a>
            ]. However, all these methods rely on the fixed style representation which is captured by the features of a VGG [
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d66353580e1243" title="Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint 
                      arXiv:1409.1556
                      
                     (2014)">
             39
            </a>
            ] network pre-trained on ImageNet. Therefore they require a supervised pre-training on millions of labeled object bounding boxes and have a bias introduced by ImageNet, because it has been assembled without artistic consideration. Moreover, the image quality achieved by the costly optimization in [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1247" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ] still remains an upper bound for the performance of recent methods. Other works like [
            <a aria-label="Reference 1" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR1" id="ref-link-section-d66353580e1250" title="Bautista, M.A., Sanakoyeu, A., Tikhoncheva, E., Ommer, B.: CliqueCNN: deep unsupervised exemplar learning. In: Advances in Neural Information Processing Systems, pp. 3846–3854 (2016)">
             1
            </a>
            ,
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66353580e1253" title="Collomosse, J., Bui, T., Wilber, M.J., Fang, C., Jin, H.: Sketching with style: visual search with sketches and aesthetic context. In: The IEEE International Conference on Computer Vision (ICCV), October 2017">
             5
            </a>
            ,
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d66353580e1256" title="Esser, P., Sutter, E., Ommer, B.: A variational U-Net for conditional appearance and shape generation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2018">
             8
            </a>
            ,
            <a aria-label="Reference 32" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR32" id="ref-link-section-d66353580e1259" title="Mao, H., Cheung, M., She, J.: DeepArt: learning joint representations of visual arts. In: Proceedings of the 2017 ACM on Multimedia Conference, pp. 1183–1191. ACM (2017)">
             32
            </a>
            ,
            <a aria-label="Reference 45" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR45" id="ref-link-section-d66353580e1262" title="Wilber, M.J., Fang, C., Jin, H., Hertzmann, A., Collomosse, J., Belongie, S.: BAM! The behance artistic media dataset for recognition beyond photography. In: The IEEE International Conference on Computer Vision (ICCV), October 2017">
             45
            </a>
            ] learn how to discriminate different techniques, styles and contents in the latent space. Zhu et al. [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1266" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ] learn a bidirectional mapping between a domain of content images and paintings using generative adversarial networks. Employing cycle consistency loss, they directly measure the distance between a backprojection of the stylized output and the content image in the RGB pixel space. Measuring distances in the RGB image domain is not just generally prone to be coarse, but, especially for abstract styles, a pixel-wise comparison of backwards mapped stylized images is not suited. Then, either content is preserved and the stylized image is not sufficiently abstract, e.g., not altering object boundaries, or the stylized image has a suitable degree of abstractness and so a pixel-based comparison with the content image must fail. Moreover, the more abstract the style is, the more potential backprojections into the content domain exist, because this mapping is underdetermined (think of the many possible content images for a single cubistic painting). In contrast, we spare the ill-posed backward mapping of styles and compare stylized and content images in the latent space which is trained jointly with the style transfer network. Since both content and stylized images are run through our encoder, the latent space is trained to only pay attention to the commonalities, i.e., the content present in both. Another consequence of the cycle consistency loss is that it requires content and style images used for training to represent similar scenes [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1269" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ], and thus training data preparation for [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1272" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ] involves tedious manual filtering of samples, while our approach can be trained on arbitrary unpaired content and style images.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Approach">
         <div class="c-article-section" id="Sec3-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">
           <span class="c-article-section__title-number">
            2
           </span>
           Approach
          </h2>
          <div class="c-article-section__content" id="Sec3-content">
           <p>
            To enable a fast style transfer that instantly transfers a content image or even frames of a video according to a particular style, we need a feed-forward architecture [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e1284" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ] rather than the slow optimization-based approach of [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1287" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ]. To this end, we adopt an encoder-decoder architecture that utilizes an encoder network
            <i>
             E
            </i>
            to map an input content image
            <i>
             x
            </i>
            onto a latent representation
            <span class="mathjax-tex">
             \(z=E(x)\)
            </span>
            . A generative decoder
            <i>
             G
            </i>
            then plays the role of a painter and generates the stylized output image
            <span class="mathjax-tex">
             \(y=G(z)\)
            </span>
            from the sketchy content representation
            <i>
             z
            </i>
            . Stylization then only requires a single forward pass, thus working in real-time.
           </p>
           <h3 class="c-article__sub-heading" id="Sec4">
            <span class="c-article-section__title-number">
             2.1
            </span>
            Training with a Style-Aware Content Loss
           </h3>
           <p>
            Previous approaches have been limited in that training worked only with a single style image [
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d66353580e1332" title="Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style. In: Proceedings of ICLR (2017)">
             6
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1335" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ,
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d66353580e1338" title="Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)">
             20
            </a>
            ,
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e1341" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e1344" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d66353580e1348" title="Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: feed-forward synthesis of textures and stylized images. In: ICML, pp. 1349–1357 (2016)">
             40
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d66353580e1351" title="Wang, X., Oxholm, G., Zhang, D., Wang, Y.F.: Multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017">
             44
            </a>
            ] or that style images used for training had to be similar in content to the content images [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1354" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ]. In contrast, given a single style image
            <span class="mathjax-tex">
             \(y_0\)
            </span>
            we include a set
            <i>
             Y
            </i>
            of related style images
            <span class="mathjax-tex">
             \(y_j \in Y\)
            </span>
            , which are automatically selected (see Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec5">
             2.2
            </a>
            ) from a large art dataset (Wikiart). We do
            <i>
             not
            </i>
            require the
            <span class="mathjax-tex">
             \(y_j\)
            </span>
            to depict similar content as the set
            <i>
             X
            </i>
            of arbitrary content images
            <span class="mathjax-tex">
             \(x_i \in X\)
            </span>
            , which we simply take from Places365 [
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d66353580e1414" title="Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information Processing Systems, pp. 487–495 (2014)">
             47
            </a>
            ]. Compared to [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1418" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ], we thus can utilize standard datasets for content and style and need no tedious manual selection of the
            <span class="mathjax-tex">
             \(x_i\)
            </span>
            and
            <span class="mathjax-tex">
             \(y_j\)
            </span>
            as described in Sects. 5.1 and 7.1 of [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1443" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ].
           </p>
           <p>
            To train
            <i>
             E
            </i>
            and
            <i>
             G
            </i>
            we employ a standard adversarial discriminator
            <i>
             D
            </i>
            [
            <a aria-label="Reference 15" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR15" id="ref-link-section-d66353580e1458" title="Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Information Processing Systems, pp. 2672–2680 (2014)">
             15
            </a>
            ] to distinguish the stylized output
            <span class="mathjax-tex">
             \(G(E(x_i))\)
            </span>
            from real examples
            <span class="mathjax-tex">
             \(y_j\in Y\)
            </span>
            ,
           </p>
           <div class="c-article-equation" id="Equ1">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} \mathcal {L}_D(E, G, D) = \mathop {\mathbb {E}}_{y \sim p_{Y}(y)} \left[ \log {D(y)} \right] + \mathop {\mathbb {E}}_{x \sim p_{X}(x)} \left[ \log {\left( 1 - D(G(E(x)))\right) } \right] \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (1)
            </div>
           </div>
           <p>
            However, the crucial challenge is to decide which details to retain from the content image, something which is not captured by Eq.
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            . Contrary to previous work, we want to directly enforce
            <i>
             E
            </i>
            to strip the latent space of all image details that the target style disregards. Therefore, the details that need to be retained or ignored in
            <i>
             z
            </i>
            depend on the style. For instance, Cubism would disregard texture, whereas Pointillism would retain low-frequency textures. Therefore, a pre-trained network or fixed similarity measure [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e1506" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ] for measuring the similarity in content between
            <span class="mathjax-tex">
             \(x_i\)
            </span>
            and
            <span class="mathjax-tex">
             \(y_i\)
            </span>
            is violating the art historical premise that the manner, in which content is preserved, depends on the style. Similar issues arise when measuring the distance after projecting the stylized image
            <span class="mathjax-tex">
             \(G(E(x_i))\)
            </span>
            back into the domain
            <i>
             X
            </i>
            of original images with a second pair of encoder and decoder
            <span class="mathjax-tex">
             \(G_2(E_2(G(E(x_i))))\)
            </span>
            . The resulting loss proposed in [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1557" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ],
           </p>
           <div class="c-article-equation" id="Equ2">
            <div class="c-article-equation__content">
             <img alt="" class="u-display-block" src="//media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/474213_1_En_43_Equ2_HTML.gif"/>
            </div>
            <div class="c-article-equation__number">
             (2)
            </div>
           </div>
           <p>
            fails where styles become abstract, since the backward projection of abstract art to the original image is highly underdetermined.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 4." id="figure-4">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/4" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig4_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 4" aria-describedby="Fig4" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig4_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc">
               <p>
                1st row - results of style transfer for different styles. 2nd row - sketchy content visualization reconstructed from the latent space
                <i>
                 E
                </i>
                (
                <i>
                 x
                </i>
                ) using method of [
                <a aria-label="Reference 31" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR31" id="ref-link-section-d66353580e1587" title="Mahendran, A., Vedaldi, A.: Understanding deep image representations by inverting them. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)">
                 31
                </a>
                ]. (a) The encoder for Pollock does not preserve much content due to the abstract style; (b) only rough structure of the content is preserved (coarse patches) because of the distinct style of El Greco; (c) latent space highlights surfaces of the same color and that fine object details are ignored, since Gauguin was less interested in details, often painted plain surfaces and used vivid colors; (d) encodes the thick, wide brushstrokes Cézanne used, but preserves a larger palette of colors. (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/4" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            Therefore, we propose a style-aware content loss that is being optimized, while the network learns to stylize images. Since encoder training is coupled with training of the decoder, which produces artistic images of the specific style, the latent vector
            <i>
             z
            </i>
            produced for the input image
            <i>
             x
            </i>
            can be viewed as its style-dependent sketchy content representation. This latent space representation is changing during training and hence
            <i>
             adapts
            </i>
            to the style. Thus, when measuring the similarity in content between input image
            <span class="mathjax-tex">
             \(x_i\)
            </span>
            and the stylized image
            <span class="mathjax-tex">
             \(y_i = G(E(x_i))\)
            </span>
            in the latent space, we focus only on those details which are relevant for the style. Let the latent space have
            <i>
             d
            </i>
            dimensions, then we define a style-aware content loss as normalized squared Euclidean distance between
            <span class="mathjax-tex">
             \(E(x_i)\)
            </span>
            and
            <span class="mathjax-tex">
             \(E(y_i)\)
            </span>
            :
           </p>
           <div class="c-article-equation" id="Equ3">
            <div class="c-article-equation__content">
             <img alt="" class="u-display-block" src="//media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/474213_1_En_43_Equ3_HTML.gif"/>
            </div>
            <div class="c-article-equation__number">
             (3)
            </div>
           </div>
           <p>
            To show the additional intuition behind the style-aware content loss we used the method [
            <a aria-label="Reference 31" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR31" id="ref-link-section-d66353580e1669" title="Mahendran, A., Vedaldi, A.: Understanding deep image representations by inverting them. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)">
             31
            </a>
            ] to reconstruct the content image from latent representations trained on different styles and illustrated it in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
             4
            </a>
            . It can be seen that latent space encodes a sketchy, style-specific visual content, which is implicitly used by the loss function. For example, Pollock is famous for his abstract paintings, so reconstruction (a) shows that the latent space ignores most of the object structure; Gauguin was less interested in details, painted a lot of plain surfaces and used vivid colors which is reflected in the reconstruction (c), where latent space highlights surfaces of the same color and fine object details are ignored.
           </p>
           <p>
            Since we train our model for altering the artistic style without supervision and from scratch, we now introduce extra signal to initialize training and boost the learning of the primary latent space. The simplest thing to do is to use an autoencoder loss which computes the difference between
            <span class="mathjax-tex">
             \(x_i\)
            </span>
            and
            <span class="mathjax-tex">
             \(y_i\)
            </span>
            in the RGB space. However, this loss would impose a high penalty for any changes in image structure between input
            <span class="mathjax-tex">
             \(x_i\)
            </span>
            and output
            <span class="mathjax-tex">
             \(y_i\)
            </span>
            , because it relies only on low-level pixel information. But we aim to learn image stylization and want the encoder to discard certain details in the content depending on style. Hence the autoencoder loss will contradict with the purpose of the style-aware loss, where the style determines which details to retain and which to disregard. Therefore, we propose to measure the difference after applying a weak image transformation on
            <span class="mathjax-tex">
             \(x_i\)
            </span>
            and
            <span class="mathjax-tex">
             \(y_i\)
            </span>
            , which is learned while learning E and G. We inject in our model a transformer block
            <span class="mathjax-tex">
             \(\mathbf {T}\)
            </span>
            which is essentially a one-layer fully convolutional neural network taking an image as input and producing a transformed image of the same size. We apply
            <i>
             T
            </i>
            to images
            <span class="mathjax-tex">
             \(x_i\)
            </span>
            and
            <span class="mathjax-tex">
             \(y_i=G(E(x_i))\)
            </span>
            before measuring the difference. We refer to this as
            <i>
             transformed image loss
            </i>
            and define it as
           </p>
           <div class="c-article-equation" id="Equ4">
            <div class="c-article-equation__content">
             <img alt="" class="u-display-block" src="//media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/474213_1_En_43_Equ4_HTML.gif"/>
            </div>
            <div class="c-article-equation__number">
             (4)
            </div>
           </div>
           <p>
            where
            <span class="mathjax-tex">
             \(C \times H \times W\)
            </span>
            is the size of image
            <i>
             x
            </i>
            and for training
            <i>
             T
            </i>
            is initialized with uniform weights.
           </p>
           <p>
            Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            illustrates the full pipeline of our approach. To summarize, the full objective of our model is:
           </p>
           <div class="c-article-equation" id="Equ5">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} \mathcal {L}(E, G, D) = \mathcal {L}_c(E, G) + \mathcal {L}_{t}(E, G) + \lambda \mathcal {L}_D(E, G, D) , \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (5)
            </div>
           </div>
           <p>
            where
            <span class="mathjax-tex">
             \(\lambda \)
            </span>
            controls the relative importance of adversarial loss. We solve the following optimization problem:
           </p>
           <div class="c-article-equation" id="Equ6">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} E, G = \arg \;\min \limits _{E,G}\;\max \limits _{D} \mathcal {L}(E, G, D). \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (6)
            </div>
           </div>
           <h3 class="c-article__sub-heading" id="Sec5">
            <span class="c-article-section__title-number">
             2.2
            </span>
            Style Image Grouping
           </h3>
           <p>
            In this section we explain an automatic approach for gathering a set of related style images. Given a single style image
            <span class="mathjax-tex">
             \(y_0\)
            </span>
            we strive to find a set
            <i>
             Y
            </i>
            of related style images
            <span class="mathjax-tex">
             \(y_j \in Y\)
            </span>
            . Contrary to [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e1889" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ] we avoid tedious manual selection of style images and follow a fully automatic approach. To this end, we train a VGG16 [
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d66353580e1892" title="Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint 
                      arXiv:1409.1556
                      
                     (2014)">
             39
            </a>
            ] network
            <i>
             C
            </i>
            from scratch on the Wikiart [
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d66353580e1899" title="Karayev, S., et al.: Recognizing image style. arXiv preprint 
                      arXiv:1311.3715
                      
                     (2013)">
             23
            </a>
            ] dataset to predict an artist given the artwork. The network is trained on the 624 largest (by number of works) artists from the Wikiart dataset. Note that our ultimate goal is stylization and numerous artists can share the same style, e.g., Impressionism, as well as a single artist can exhibit different styles, such as the different stylistic periods of Picasso. However, we do
            <i>
             not
            </i>
            use any
            <i>
             style
            </i>
            labels. Artist classification in this case is the surrogate task for learning meaningful features in the artworks’ domain, which allows to retrieve similar artworks to image
            <span class="mathjax-tex">
             \(y_0\)
            </span>
            .
           </p>
           <p>
            Let
            <span class="mathjax-tex">
             \(\phi (y)\)
            </span>
            be the activations of the fc6 layer of the VGG16 network
            <i>
             C
            </i>
            for input image
            <i>
             y
            </i>
            . To get a set of related style images to
            <span class="mathjax-tex">
             \(y_0\)
            </span>
            from the Wikiart dataset
            <span class="mathjax-tex">
             \(\mathcal {Y}\)
            </span>
            we retrieve all nearest neighbors of
            <span class="mathjax-tex">
             \(y_0\)
            </span>
            based on the cosine distance
            <span class="mathjax-tex">
             \(\delta \)
            </span>
            of the activations
            <span class="mathjax-tex">
             \(\phi (\cdot )\)
            </span>
            , i.e.
           </p>
           <div class="c-article-equation" id="Equ7">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} Y = \{ y \; |\; y \in \mathcal {Y}, \delta (\phi (y), \phi (y_0)) &lt; t \}, \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (7)
            </div>
           </div>
           <p>
            where
            <span class="mathjax-tex">
             \(\delta (a, b) = 1 + \frac{\phi (a)\phi (b)}{||a||_2 ||b||_2}\)
            </span>
            and
            <i>
             t
            </i>
            is the
            <span class="mathjax-tex">
             \(10\%\)
            </span>
            quantile of all pairwise distances in the dataset
            <span class="mathjax-tex">
             \(\mathcal {Y}\)
            </span>
            .
           </p>
          </div>
         </div>
        </section>
        <section data-title="Experiments">
         <div class="c-article-section" id="Sec6-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">
           <span class="c-article-section__title-number">
            3
           </span>
           Experiments
          </h2>
          <div class="c-article-section__content" id="Sec6-content">
           <p>
            To compare our style transfer approach with the state-of-the-art, we first perform extensive qualitative analysis, then we provide quantitative results based on the
            <i>
             deception score
            </i>
            and evaluations of experts from art history. Afterwards in Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec9">
             3.3
            </a>
            we ablate single components of our model and show their importance.
           </p>
           <p>
            <b>
             Implementation Details:
            </b>
            The basis for our style transfer model is an encoder-decoder architecture, cf. [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e2065" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ]. The encoder network contains 5
            <span class="u-monospace">
             conv
            </span>
            layers:
            <span class="mathjax-tex">
             \(1 {\times }\)
            </span>
            <span class="u-monospace">
             conv-stride-1
            </span>
            and
            <span class="mathjax-tex">
             \(4{\times }\)
            </span>
            <span class="u-monospace">
             conv-stride-2
            </span>
            . The decoder network has 9 residual blocks [
            <a aria-label="Reference 16" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR16" id="ref-link-section-d66353580e2098" title="He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778 (2016)">
             16
            </a>
            ], 4 upsampling blocks and
            <span class="mathjax-tex">
             \(1 {\times }\)
            </span>
            <span class="u-monospace">
             conv-stride-1
            </span>
            . For upsampling blocks we used a sequence of nearest-neighbor upscaling and
            <span class="u-monospace">
             conv-stride-1
            </span>
            instead of fractionally strided convolutions [
            <a aria-label="Reference 29" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR29" id="ref-link-section-d66353580e2117" title="Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431–3440 (2015)">
             29
            </a>
            ], which tend to produce heavier artifacts [
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR33" id="ref-link-section-d66353580e2120" title="Odena, A., Dumoulin, V., Olah, C.: Deconvolution and checkerboard artifacts. Distill (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.23915/distill.00003
                      
                    ">
             33
            </a>
            ]. Discriminator is a fully convolutional network with
            <span class="mathjax-tex">
             \(7{\times }\)
            </span>
            <span class="u-monospace">
             conv-stride-2
            </span>
            layers. For a detailed network architecture description we refer to the supplementary material. We set
            <span class="mathjax-tex">
             \(\lambda =0.001\)
            </span>
            in Eq.
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ5">
             5
            </a>
            . During the training process we sample
            <span class="mathjax-tex">
             \(768 \times 768\)
            </span>
            content image patches from the training set of Places365 [
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d66353580e2162" title="Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information Processing Systems, pp. 487–495 (2014)">
             47
            </a>
            ] and
            <span class="mathjax-tex">
             \(768 \times 768\)
            </span>
            style image patches from the Wikiart [
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d66353580e2177" title="Karayev, S., et al.: Recognizing image style. arXiv preprint 
                      arXiv:1311.3715
                      
                     (2013)">
             23
            </a>
            ] dataset. We train for 300000 iterations with batch size 1, learning rate 0.0002 and Adam [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66353580e2180" title="Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint 
                      arXiv:1412.6980
                      
                     (2014)">
             24
            </a>
            ] optimizer. The learning rate is reduced by a factor of 10 after 200000 iterations.
           </p>
           <p>
            <b>
             Baselines:
            </b>
            Since we aim to generate high-resolution stylizations, for comparison we run style transfer on our method and all baselines for input images of size
            <span class="mathjax-tex">
             \(768\times 768\)
            </span>
            , unless otherwise specified. We did not exceed this resolution when comparing, because some other methods were reaching the GPU memory limit. We optimize Gatys et al. [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e2199" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ] for 500 iterations using L-BFGS. For Johnson et al. [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e2202" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ] we used the implementation of [
            <a aria-label="Reference 7" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR7" id="ref-link-section-d66353580e2205" title="Engstrom, L.: Fast style transfer (2016). 
                      https://github.com/lengstrom/fast-style-transfer/
                      
                    . Commit 55809f4e">
             7
            </a>
            ] and trained a separate network for every reference style image on the same content images from Places365 [
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d66353580e2208" title="Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information Processing Systems, pp. 487–495 (2014)">
             47
            </a>
            ] as our method. For Huang et al. [
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d66353580e2212" title="Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)">
             20
            </a>
            ], Chen et al. [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d66353580e2215" title="Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv preprint 
                      arXiv:1612.04337
                      
                     (2016)">
             4
            </a>
            ] and Li et al. [
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e2218" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ] implementations and pre-trained models provided by the authors were used. Zhu et al. [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e2221" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ] was trained on exactly the same content and style images as our approach using the source code provided by the authors. Methods [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d66353580e2224" title="Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv preprint 
                      arXiv:1612.04337
                      
                     (2016)">
             4
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e2227" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ,
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d66353580e2231" title="Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)">
             20
            </a>
            ,
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e2234" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e2237" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ] utilized only one example per style, as they cannot benefit from more (cf. the analysis in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            ).
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 5." id="figure-5">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/5" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig5_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 5" aria-describedby="Fig5" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig5_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc">
               <p>
                Results from different style transfer methods. We compare methods on different styles and content images.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 5" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure5 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/5" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec7">
            <span class="c-article-section__title-number">
             3.1
            </span>
            Qualitative Results
           </h3>
           <p>
            <b>
             Full Image Stylization:
            </b>
            In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            we demonstrate the effectiveness of our approach for stylizing different contents with various styles. Chen et al. [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d66353580e2273" title="Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv preprint 
                      arXiv:1612.04337
                      
                     (2016)">
             4
            </a>
            ] work on the overlapping patches extracted from the content image, swapping the features of the original patch with the features of the most similar patch in the style image, and then averages the features in the overlapping regions, thus producing an over-smoothed image without fine details (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            (d)). [
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d66353580e2279" title="Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)">
             20
            </a>
            ] produces a lot of repetitive artifacts, especially visible on flat surfaces, cf. Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            (e, rows 1, 4–6). Method [
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e2286" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ] fails to understand the content of the image and applies different colors in the wrong locations (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            (f)). Methods [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e2292" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e2295" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ] often fail to alter content image and their effect may be characterized as shifting the color histogram, e.g., Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            (g, rows 3, 7; c, rows 1, 3–4). One reason for such failure cases of [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e2301" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ] is the loss in the RGB pixel space based on the difference between a backward mapping of the stylized output and the content image. Another reason for this is that we utilized the standard Places365 [
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d66353580e2305" title="Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information Processing Systems, pp. 487–495 (2014)">
             47
            </a>
            ] dataset and did not hand-pick training content images, as is advised for [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e2308" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ]. Thus, artworks and content images used for training differed significantly in their content, which is the ultimate test for a stylization that truly alters the input and goes beyond a direct mapping between regions of content and style images. The optimization-based method [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e2311" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ] often works better than other baselines, but produces a lot of prominent artifacts, leading to details of stylizations looking unnatural, cf. Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            (b, rows 4, 5, 6). This is due to an explicit minimization of the loss directly on the pixel level. In contrast to this, our model can not only handle styles, which have salient, simple to spot characteristics, but also styles, such as El Greco’s Mannerism, with less graspable stylistic characteristics, where other methods fail (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            , b–g, 5th row).
           </p>
           <p>
            <b>
             Fine-Grained Style Details:
            </b>
            In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            we show zoomed in cut-outs from the stylized images. Interestingly, the stylizations of methods [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d66353580e2328" title="Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv preprint 
                      arXiv:1612.04337
                      
                     (2016)">
             4
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66353580e2331" title="Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)">
             12
            </a>
            ,
            <a aria-label="Reference 19" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR19" id="ref-link-section-d66353580e2334" title="Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neural networks. Science 313(5786), 504–507 (2006). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1126/science.1127647
                      
                    ">
             19
            </a>
            ,
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d66353580e2337" title="Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)">
             20
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66353580e2341" title="Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)">
             27
            </a>
            ] do not change much across styles (compare Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            (d, f–i, rows 1–3)). Zhu et al. [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e2347" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ] produce more diverse images for different styles, but obviously cannot alter the edges of the content (blades of grass are
            <i>
             clearly visible
            </i>
            on all the cutouts in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            (e)). Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            (c) shows the stylized cutouts of our approach, which exhibit significant changes from one style to another. Another interesting example is the style of Pollock, Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            (row 8), where the style-aware loss allows our model to properly alter content to the point of discarding it – as would be expected from a Pollock action painting. Our approach is able to generate high-resolution stylizations with a lot of style specific details and retains those content details which are necessary for the style.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 6." id="figure-6">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/6" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig6_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 6" aria-describedby="Fig6" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig6_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc">
               <p>
                Artwork examples of the early artistic period of van Gogh (a) and his late period (c). Style transfer of the content image (1st column) onto the early period is presented in (b) and the late period in (d).
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 6" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure6 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/6" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 7." id="figure-7">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/7" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig7_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 7" aria-describedby="Fig7" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig7_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc">
               <p>
                Details from stylized images produced for different styles for a fixed content image (a). (b) is our entire stylized image, (c) the zoomed in cut-out and (d)–(i) the same region for competitors. Note the variation across different styles along the
                <i>
                 column
                </i>
                for our method compared to other approaches. This highlights the ability to adapt content (not just colors or textures) where demanded by a style. Fine grained artistic details with sharp boundaries are produced, while altering the original content edges.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 7" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure7 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/7" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             Style Transfer for Different Periods of van Gogh:
            </b>
            We now investigate our ability to properly model fine differences in style
            <i>
             despite
            </i>
            using a group of style images. Therefore, we take two reference images Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
             6
            </a>
            (a) and (c) from van Gogh’s early and late period, respectively, and acquire related style images for both from Wikiart. It can be clearly seen that the stylizations produced for either period Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
             6
            </a>
            (b, d) are fairly different and indeed depict the content in correspondence with the style of early (b) and late (d) periods of van Gogh. This highlights that collections of style images are properly used and do not lead to an averaging effect.
           </p>
           <p>
            <b>
             High-Resolution Image Generation:
            </b>
            Our approach allows us to produce high quality stylized images in high-resolution. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig8">
             8
            </a>
            illustrates an example of the generated piece of art in the style of Berthe Morisot with resolution
            <span class="mathjax-tex">
             \(1280\times 1280\)
            </span>
            . The result exhibits a lot of fine details such as color transitions of the oil paint and brushstrokes of different sizes. More HD images are in the supplementary.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 8." id="figure-8">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig8">
               Fig. 8.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/8" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig8_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 8" aria-describedby="Fig8" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig8_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc">
               <p>
                High-resolution image (1280
                <span class="mathjax-tex">
                 \(\times \)
                </span>
                1280 pix) generated by our approach. A lot of fine details and brushstrokes are visible. A style example is in the bottom left corner.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 8" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure8 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/8" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             Real-Time HD Video Stylization:
            </b>
            We also apply our method to several videos. Our approach can stylize HD videos (
            <span class="mathjax-tex">
             \(1280 \times 720\)
            </span>
            ) at 9 FPS. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig9">
             9
            </a>
            shows stylized frames from a video. We did not use a temporal regularization to show that our method produces equally good results for consecutive frames with varying appearance w/o extra constraints. Stylized videos are in the supplementary.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 9." id="figure-9">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig9">
               Fig. 9.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/9" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig9_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 9" aria-describedby="Fig9" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig9_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc">
               <p>
                Results of our approach applied to the HD video of Eadweard Muybridge “The horse in motion” (1878). Every frame was
                <i>
                 independently
                </i>
                processed (no smoothing or post-processing) by our model in the style of Picasso. Video resolution
                <span class="mathjax-tex">
                 \(1920 \times 1280\)
                </span>
                pix, here the original aspect ratio was changed to save space.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 9" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure9 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/9" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec8">
            <span class="c-article-section__title-number">
             3.2
            </span>
            Quantitative Evaluation
           </h3>
           <p>
            <b>
             Style Transfer Deception Rate:
            </b>
            While several metrics [
            <a aria-label="Reference 2" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR2" id="ref-link-section-d66353580e2530" title="Che, T., Li, Y., Jacob, A.P., Bengio, Y., Li, W.: Mode regularized generative adversarial networks. arXiv preprint 
                      arXiv:1612.02136
                      
                     (2016)">
             2
            </a>
            ,
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d66353580e2533" title="Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. In: Advances in Neural Information Processing Systems, pp. 6629–6640 (2017)">
             18
            </a>
            ,
            <a aria-label="Reference 35" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR35" id="ref-link-section-d66353580e2536" title="Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Improved techniques for training GANs. In: Advances in Neural Information Processing Systems, pp. 2234–2242 (2016)">
             35
            </a>
            ] have been proposed to evaluate the quality of image generation, until now no evaluation metric has been proposed for an automatic evaluation of style transfer results. To measure the quality of the stylized images, we introduce the
            <i>
             style transfer deception rate
            </i>
            . We use a VGG16 network trained from scratch to classify 624 artists on Wikiart. Style transfer deception rate is calculated as the fraction of generated images which were classified by the network as the artworks of an artist for which the stylization was produced. For fair comparison with other approaches, which used only one style image
            <span class="mathjax-tex">
             \(y_0\)
            </span>
            (hence only one artist), we restricted
            <i>
             Y
            </i>
            to only contain samples coming from the same artist as the query example
            <span class="mathjax-tex">
             \(y_0\)
            </span>
            . We selected 18 different artists (i.e. styles). For every method we generated 5400 stylizations (18 styles, 300 per style). In Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab1">
             1
            </a>
            we report mean deception rate for 18 styles. Our method achieves 0.393 significantly outperforming the baselines. For comparison, mean accuracy of the network on hold-out real images of aforementioned 18 artists from Wikiart is 0.616.
           </p>
           <p>
            <b>
             Human Art History Experts Perceptual Studies:
            </b>
            Three experts (with a PhD in art history with focus on modern and pre-modern paintings) have compared results of our method against recent work. Each expert was shown 1000 groups of images. Each group consists of stylizations which were generated by different methods based on the same content and style images. Experts were asked to choose one image which best and most realistically reflects the current style. The score is computed as the fraction of times a specific method was chosen as the best in the group. We calculate a mean expert score for each method using 18 different styles and report them in Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab1">
             1
            </a>
            . Here, we see that the experts selected our method in around
            <span class="mathjax-tex">
             \(50\%\)
            </span>
            of the cases.
           </p>
           <p>
            <b>
             Speed and Memory:
            </b>
            Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab2">
             2
            </a>
            shows the time and memory required for stylization of a single image of size
            <span class="mathjax-tex">
             \(768\times 768\)
            </span>
            px for different methods. One can see that our approach and that of [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66353580e2609" title="Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). 
                      https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
                      
                    ">
             22
            </a>
            ,
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66353580e2612" title="Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)">
             48
            </a>
            ] have comparable speed and only very modest demands on GPU memory, compared to modern graphics cards.
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab1">
               Table 1. Mean deception rate and mean expert score for different methods. The higher the better.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/tables/1" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-2">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab2">
               Table 2. Average inference time and GPU memory consumption, measured on a Titan X Pascal, for different methods with batch size 1 and input image of
               <span class="mathjax-tex">
                \(768\times 768\)
               </span>
               pix.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 2" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/tables/2" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec9">
            <span class="c-article-section__title-number">
             3.3
            </span>
            Ablation Studies
           </h3>
           <p>
            <b>
             Effect of Different Losses:
            </b>
            We study the effect of different components of our model in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig10">
             10
            </a>
            . Removing the style-aware content loss significantly degrades the results, (c). We observe that without the style-aware loss training becomes instable and often stalls. If we remove the transformed image loss, which we introduced for a proper initialization of our model that is trained from scratch, we notice mode collapse after 5000 iterations. Training directly with pixel-wise L2 distance causes a lot of artifacts (grey blobs and flaky structure), (d). Training only with a discriminator neither exhibits the variability in the painting nor in the content, (e). Therefore we conclude that both the style-aware content loss and the transformed image loss are critical for our approach.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 10." id="figure-10">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig10">
               Fig. 10.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/10" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig10_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 10" aria-describedby="Fig10" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig10_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc">
               <p>
                Different variations of our method for Gauguin stylization. See Sect.
                <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec9">
                 3.3
                </a>
                for details. (a) Content image; (b) full model (
                <span class="mathjax-tex">
                 \(\mathcal {L}_c\)
                </span>
                ,
                <span class="mathjax-tex">
                 \(\mathcal {L}_{rgb}\)
                </span>
                and
                <span class="mathjax-tex">
                 \(\mathcal {L}_D\)
                </span>
                ); (c)
                <span class="mathjax-tex">
                 \(\mathcal {L}_{rgb}\)
                </span>
                and
                <span class="mathjax-tex">
                 \(\mathcal {L}_D\)
                </span>
                ; (d) without transformer block; (e) only
                <span class="mathjax-tex">
                 \(\mathcal {L}_D\)
                </span>
                ; (f) trained with all of Gauguin’s artworks as style images. Please zoom in to compare.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 10" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure10 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/10" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 11." id="figure-11">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig11">
               Fig. 11.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-030-01237-3_43/figures/11" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig11_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 11" aria-describedby="Fig11" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig11_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc">
               <p>
                Encoder ablation studies: (a) stylization using our model; (b) stylization using pre-trained VGG16 encoder instead of
                <i>
                 E
                </i>
                .
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 11" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure11 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-030-01237-3_43/figures/11" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             Single vs Collection of Style Images:
            </b>
            Here, we investigate the importance of the style image grouping. First, we trained a model with only one style image of Gauguin, which led to mode collapse. Second, we trained with all of Gauguin’s artworks as style images (without utilizing style grouping procedure). It produced unsatisfactory results, cf. Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig10">
             10
            </a>
            (f), because style images comprised several distinct styles. Therefore we conclude that to learn a good style transfer model it is important to group style images according to their stylistic similarity.
           </p>
           <p>
            <b>
             Encoder Ablation:
            </b>
            To investigate the effect of our encoder
            <i>
             E
            </i>
            , we substitute it with VGG16 [
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d66353580e3207" title="Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint 
                      arXiv:1409.1556
                      
                     (2014)">
             39
            </a>
            ] encoder (up to
            <span class="u-monospace">
             conv5_3
            </span>
            ) pre-trained on ImageNet. The VGG encoder retains features that separate object classes (since it was trained discriminatively), as opposed to our encoder which is trained to retain style-specific content details. Hence, our encoder is not biased towards class-discriminative features, but is style specific and trained from scratch. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig11">
             11
            </a>
            (a, b) show that our approach produces better results than with pre-trained VGG16 encoder.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Conclusion">
         <div class="c-article-section" id="Sec10-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">
           <span class="c-article-section__title-number">
            4
           </span>
           Conclusion
          </h2>
          <div class="c-article-section__content" id="Sec10-content">
           <p>
            This paper has addressed major conceptual issues in state-of-the-art approaches for style transfer. We overcome the limitation of only a single style image or the need for style and content training images to show similar content. Moreover, we exceed a mere pixel-wise comparison of stylistic images or models that are pre-trained on millions of ImageNet bounding boxes. The proposed style-aware content loss enables a real-time, high-resolution encoder-decoder based stylization of images and videos and significantly improves stylization by capturing how style affects content.
           </p>
           <p>
            <i>
             Solution to
            </i>
            Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            <i>
             : patches
            </i>
            3
            <i>
             and
            </i>
            5
            <i>
             were generated by our approach, others by artists.
            </i>
           </p>
          </div>
         </div>
        </section>
       </div>
       <div id="MagazineFulltextChapterBodySuffix">
        <section aria-labelledby="Bib1" data-title="References">
         <div class="c-article-section" id="Bib1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">
           <span class="c-article-section__title-number">
           </span>
           References
          </h2>
          <div class="c-article-section__content" id="Bib1-content">
           <div data-container-section="references">
            <ol class="c-article-references" data-track-component="outbound reference">
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1.">
              <p class="c-article-references__text" id="ref-CR1">
               Bautista, M.A., Sanakoyeu, A., Tikhoncheva, E., Ommer, B.: CliqueCNN: deep unsupervised exemplar learning. In: Advances in Neural Information Processing Systems, pp. 3846–3854 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR1-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bautista%2C%20M.A.%2C%20Sanakoyeu%2C%20A.%2C%20Tikhoncheva%2C%20E.%2C%20Ommer%2C%20B.%3A%20CliqueCNN%3A%20deep%20unsupervised%20exemplar%20learning.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%203846%E2%80%933854%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2.">
              <p class="c-article-references__text" id="ref-CR2">
               Che, T., Li, Y., Jacob, A.P., Bengio, Y., Li, W.: Mode regularized generative adversarial networks. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1612.02136" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1612.02136">
                arXiv:1612.02136
               </a>
               (2016)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3.">
              <p class="c-article-references__text" id="ref-CR3">
               Chen, D., Yuan, L., Liao, J., Yu, N., Hua, G.: Stylebank: an explicit representation for neural image style transfer. In: Proceedings of CVPR (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR3-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Chen%2C%20D.%2C%20Yuan%2C%20L.%2C%20Liao%2C%20J.%2C%20Yu%2C%20N.%2C%20Hua%2C%20G.%3A%20Stylebank%3A%20an%20explicit%20representation%20for%20neural%20image%20style%20transfer.%20In%3A%20Proceedings%20of%20CVPR%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4.">
              <p class="c-article-references__text" id="ref-CR4">
               Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1612.04337" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1612.04337">
                arXiv:1612.04337
               </a>
               (2016)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5.">
              <p class="c-article-references__text" id="ref-CR5">
               Collomosse, J., Bui, T., Wilber, M.J., Fang, C., Jin, H.: Sketching with style: visual search with sketches and aesthetic context. In: The IEEE International Conference on Computer Vision (ICCV), October 2017
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR5-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Collomosse%2C%20J.%2C%20Bui%2C%20T.%2C%20Wilber%2C%20M.J.%2C%20Fang%2C%20C.%2C%20Jin%2C%20H.%3A%20Sketching%20with%20style%3A%20visual%20search%20with%20sketches%20and%20aesthetic%20context.%20In%3A%20The%20IEEE%20International%20Conference%20on%20Computer%20Vision%20%28ICCV%29%2C%20October%202017">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6.">
              <p class="c-article-references__text" id="ref-CR6">
               Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style. In: Proceedings of ICLR (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR6-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Dumoulin%2C%20V.%2C%20Shlens%2C%20J.%2C%20Kudlur%2C%20M.%3A%20A%20learned%20representation%20for%20artistic%20style.%20In%3A%20Proceedings%20of%20ICLR%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7.">
              <p class="c-article-references__text" id="ref-CR7">
               Engstrom, L.: Fast style transfer (2016).
               <a data-track="click" data-track-action="external reference" data-track-label="https://github.com/lengstrom/fast-style-transfer/" href="https://github.com/lengstrom/fast-style-transfer/">
                https://github.com/lengstrom/fast-style-transfer/
               </a>
               . Commit 55809f4e
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8.">
              <p class="c-article-references__text" id="ref-CR8">
               Esser, P., Sutter, E., Ommer, B.: A variational U-Net for conditional appearance and shape generation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2018
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR8-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Esser%2C%20P.%2C%20Sutter%2C%20E.%2C%20Ommer%2C%20B.%3A%20A%20variational%20U-Net%20for%20conditional%20appearance%20and%20shape%20generation.%20In%3A%20The%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%2C%20July%202018">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9.">
              <p class="c-article-references__text" id="ref-CR9">
               Fernie, E.: Art History and Its Methods: A Critical Anthology, p. 361. Phaidon, London (1995)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR9-links">
               <a aria-label="Google Scholar reference 9" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Art%20History%20and%20Its%20Methods%3A%20A%20Critical%20Anthology&amp;publication_year=1995&amp;author=Fernie%2CE" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10.">
              <p class="c-article-references__text" id="ref-CR10">
               Frigo, O., Sabater, N., Delon, J., Hellier, P.: Split and match: example-based adaptive patch sampling for unsupervised style transfer. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 553–561 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR10-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Frigo%2C%20O.%2C%20Sabater%2C%20N.%2C%20Delon%2C%20J.%2C%20Hellier%2C%20P.%3A%20Split%20and%20match%3A%20example-based%20adaptive%20patch%20sampling%20for%20unsupervised%20style%20transfer.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%20553%E2%80%93561%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11.">
              <p class="c-article-references__text" id="ref-CR11">
               Gatys, L.A., Ecker, A.S., Bethge, M.: Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1505.07376" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1505.07376">
                arXiv:1505.07376
               </a>
               12 (2015)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12.">
              <p class="c-article-references__text" id="ref-CR12">
               Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR12-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Gatys%2C%20L.A.%2C%20Ecker%2C%20A.S.%2C%20Bethge%2C%20M.%3A%20Image%20style%20transfer%20using%20convolutional%20neural%20networks.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%2C%20pp.%202414%E2%80%932423.%20IEEE%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13.">
              <p class="c-article-references__text" id="ref-CR13">
               Gatys, L.A., Ecker, A.S., Bethge, M., Hertzmann, A., Shechtman, E.: Controlling perceptual factors in neural style transfer. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR13-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Gatys%2C%20L.A.%2C%20Ecker%2C%20A.S.%2C%20Bethge%2C%20M.%2C%20Hertzmann%2C%20A.%2C%20Shechtman%2C%20E.%3A%20Controlling%20perceptual%20factors%20in%20neural%20style%20transfer.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14.">
              <p class="c-article-references__text" id="ref-CR14">
               Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V., Shlens, J.: Exploring the structure of a real-time, arbitrary neural artistic stylization network. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.06830" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.06830">
                arXiv:1705.06830
               </a>
               (2017)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15.">
              <p class="c-article-references__text" id="ref-CR15">
               Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Information Processing Systems, pp. 2672–2680 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR15-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Goodfellow%2C%20I.%2C%20et%20al.%3A%20Generative%20adversarial%20nets.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%202672%E2%80%932680%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16.">
              <p class="c-article-references__text" id="ref-CR16">
               He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR16-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=He%2C%20K.%2C%20Zhang%2C%20X.%2C%20Ren%2C%20S.%2C%20Sun%2C%20J.%3A%20Deep%20residual%20learning%20for%20image%20recognition.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%20770%E2%80%93778%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17.">
              <p class="c-article-references__text" id="ref-CR17">
               Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analogies. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 327–340. ACM (2001)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR17-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hertzmann%2C%20A.%2C%20Jacobs%2C%20C.E.%2C%20Oliver%2C%20N.%2C%20Curless%2C%20B.%2C%20Salesin%2C%20D.H.%3A%20Image%20analogies.%20In%3A%20Proceedings%20of%20the%2028th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20327%E2%80%93340.%20ACM%20%282001%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18.">
              <p class="c-article-references__text" id="ref-CR18">
               Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. In: Advances in Neural Information Processing Systems, pp. 6629–6640 (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR18-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Heusel%2C%20M.%2C%20Ramsauer%2C%20H.%2C%20Unterthiner%2C%20T.%2C%20Nessler%2C%20B.%2C%20Hochreiter%2C%20S.%3A%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%206629%E2%80%936640%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19.">
              <p class="c-article-references__text" id="ref-CR19">
               Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neural networks. Science
               <b>
                313
               </b>
               (5786), 504–507 (2006).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1126/science.1127647" href="https://doi-org.proxy.lib.ohio-state.edu/10.1126/science.1127647">
                https://doi-org.proxy.lib.ohio-state.edu/10.1126/science.1127647
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR19-links">
               <a aria-label="CrossRef reference 19" data-doi="10.1126/science.1127647" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1126/science.1127647" href="https://doi-org.proxy.lib.ohio-state.edu/10.1126%2Fscience.1127647" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="MathSciNet reference 19" data-track="click" data-track-action="MathSciNet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=2242509" rel="nofollow noopener">
                MathSciNet
               </a>
               <a aria-label="MATH reference 19" data-track="click" data-track-action="MATH reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1226.68083" rel="nofollow noopener">
                MATH
               </a>
               <a aria-label="Google Scholar reference 19" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks&amp;journal=Science&amp;doi=10.1126%2Fscience.1127647&amp;volume=313&amp;issue=5786&amp;pages=504-507&amp;publication_year=2006&amp;author=Hinton%2CGE&amp;author=Salakhutdinov%2CRR" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20.">
              <p class="c-article-references__text" id="ref-CR20">
               Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR20-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Huang%2C%20X.%2C%20Belongie%2C%20S.%3A%20Arbitrary%20style%20transfer%20in%20real-time%20with%20adaptive%20instance%20normalization.%20In%3A%20ICCV%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21.">
              <p class="c-article-references__text" id="ref-CR21">
               Jing, Y., Liu, Y., Yang, Y., Feng, Z., Yu, Y., Song, M.: Stroke controllable fast style transfer with adaptive receptive fields. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1802.07101" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1802.07101">
                arXiv:1802.07101
               </a>
               (2018)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22.">
              <p class="c-article-references__text" id="ref-CR22">
               Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-46475-6_43" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR22-links">
               <a aria-label="CrossRef reference 22" data-doi="10.1007/978-3-319-46475-6_43" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-319-46475-6_43" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-319-46475-6_43" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 22" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution&amp;pages=694-711&amp;publication_year=2016 2016 2016&amp;author=Johnson%2CJ&amp;author=Alahi%2CA&amp;author=Fei-Fei%2CL" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23.">
              <p class="c-article-references__text" id="ref-CR23">
               Karayev, S., et al.: Recognizing image style. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1311.3715" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1311.3715">
                arXiv:1311.3715
               </a>
               (2013)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24.">
              <p class="c-article-references__text" id="ref-CR24">
               Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980">
                arXiv:1412.6980
               </a>
               (2014)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25.">
              <p class="c-article-references__text" id="ref-CR25">
               Li, C., Wand, M.: Precomputed real-time texture synthesis with markovian generative adversarial networks. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9907, pp. 702–716. Springer, Cham (2016).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-46487-9_43" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46487-9_43">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46487-9_43
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR25-links">
               <a aria-label="CrossRef reference 25" data-doi="10.1007/978-3-319-46487-9_43" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-319-46487-9_43" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-319-46487-9_43" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 25" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Precomputed%20real-time%20texture%20synthesis%20with%20markovian%20generative%20adversarial%20networks&amp;pages=702-716&amp;publication_year=2016 2016 2016&amp;author=Li%2CC&amp;author=Wand%2CM" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26.">
              <p class="c-article-references__text" id="ref-CR26">
               Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Diversified texture synthesis with feed-forward networks. In: IEEE Conference on Computer Vision and Pattern Recognition (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR26-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Li%2C%20Y.%2C%20Fang%2C%20C.%2C%20Yang%2C%20J.%2C%20Wang%2C%20Z.%2C%20Lu%2C%20X.%2C%20Yang%2C%20M.H.%3A%20Diversified%20texture%20synthesis%20with%20feed-forward%20networks.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27.">
              <p class="c-article-references__text" id="ref-CR27">
               Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR27-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Li%2C%20Y.%2C%20Fang%2C%20C.%2C%20Yang%2C%20J.%2C%20Wang%2C%20Z.%2C%20Lu%2C%20X.%2C%20Yang%2C%20M.H.%3A%20Universal%20style%20transfer%20via%20feature%20transforms.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%20385%E2%80%93395%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28.">
              <p class="c-article-references__text" id="ref-CR28">
               Liao, J., Yao, Y., Yuan, L., Hua, G., Kang, S.B.: Visual attribute transfer through deep image analogy. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.01088" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.01088">
                arXiv:1705.01088
               </a>
               (2017)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29.">
              <p class="c-article-references__text" id="ref-CR29">
               Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431–3440 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR29-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Long%2C%20J.%2C%20Shelhamer%2C%20E.%2C%20Darrell%2C%20T.%3A%20Fully%20convolutional%20networks%20for%20semantic%20segmentation.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%203431%E2%80%933440%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30.">
              <p class="c-article-references__text" id="ref-CR30">
               Luan, F., Paris, S., Shechtman, E., Bala, K.: Deep photo style transfer. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR30-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Luan%2C%20F.%2C%20Paris%2C%20S.%2C%20Shechtman%2C%20E.%2C%20Bala%2C%20K.%3A%20Deep%20photo%20style%20transfer.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31.">
              <p class="c-article-references__text" id="ref-CR31">
               Mahendran, A., Vedaldi, A.: Understanding deep image representations by inverting them. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR31-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Mahendran%2C%20A.%2C%20Vedaldi%2C%20A.%3A%20Understanding%20deep%20image%20representations%20by%20inverting%20them.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32.">
              <p class="c-article-references__text" id="ref-CR32">
               Mao, H., Cheung, M., She, J.: DeepArt: learning joint representations of visual arts. In: Proceedings of the 2017 ACM on Multimedia Conference, pp. 1183–1191. ACM (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR32-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Mao%2C%20H.%2C%20Cheung%2C%20M.%2C%20She%2C%20J.%3A%20DeepArt%3A%20learning%20joint%20representations%20of%20visual%20arts.%20In%3A%20Proceedings%20of%20the%202017%20ACM%20on%20Multimedia%20Conference%2C%20pp.%201183%E2%80%931191.%20ACM%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33.">
              <p class="c-article-references__text" id="ref-CR33">
               Odena, A., Dumoulin, V., Olah, C.: Deconvolution and checkerboard artifacts. Distill (2016).
               <a data-track="click" data-track-action="external reference" data-track-label="10.23915/distill.00003" href="https://doi-org.proxy.lib.ohio-state.edu/10.23915/distill.00003">
                https://doi-org.proxy.lib.ohio-state.edu/10.23915/distill.00003
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34.">
              <p class="c-article-references__text" id="ref-CR34">
               Russakovsky, O.: ImageNet large scale visual recognition challenge. Int. J. Comput. Vis. (IJCV)
               <b>
                115
               </b>
               (3), 211–252 (2015).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/s11263-015-0816-y" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/s11263-015-0816-y">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/s11263-015-0816-y
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR34-links">
               <a aria-label="CrossRef reference 34" data-doi="10.1007/s11263-015-0816-y" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/s11263-015-0816-y" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-015-0816-y" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="MathSciNet reference 34" data-track="click" data-track-action="MathSciNet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=3422482" rel="nofollow noopener">
                MathSciNet
               </a>
               <a aria-label="Google Scholar reference 34" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=ImageNet%20large%20scale%20visual%20recognition%20challenge&amp;journal=Int.%20J.%20Comput.%20Vis.%20%28IJCV%29&amp;doi=10.1007%2Fs11263-015-0816-y&amp;volume=115&amp;issue=3&amp;pages=211-252&amp;publication_year=2015&amp;author=Russakovsky%2CO" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35.">
              <p class="c-article-references__text" id="ref-CR35">
               Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Improved techniques for training GANs. In: Advances in Neural Information Processing Systems, pp. 2234–2242 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR35-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Salimans%2C%20T.%2C%20Goodfellow%2C%20I.%2C%20Zaremba%2C%20W.%2C%20Cheung%2C%20V.%2C%20Radford%2C%20A.%2C%20Chen%2C%20X.%3A%20Improved%20techniques%20for%20training%20GANs.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%202234%E2%80%932242%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36.">
              <p class="c-article-references__text" id="ref-CR36">
               Shen, F., Yan, S., Zeng, G.: Meta networks for neural style transfer. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1709.04111" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1709.04111">
                arXiv:1709.04111
               </a>
               (2017)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37.">
              <p class="c-article-references__text" id="ref-CR37">
               Shih, Y., Paris, S., Barnes, C., Freeman, W.T., Durand, F.: Style transfer for headshot portraits. ACM Trans. Graph. (TOG)
               <b>
                33
               </b>
               (4), 148 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR37-links">
               <a aria-label="CrossRef reference 37" data-doi="10.1145/2601097.2601137" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/2601097.2601137" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2601097.2601137" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 37" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Style%20transfer%20for%20headshot%20portraits&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=33&amp;issue=4&amp;publication_year=2014&amp;author=Shih%2CY&amp;author=Paris%2CS&amp;author=Barnes%2CC&amp;author=Freeman%2CWT&amp;author=Durand%2CF" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38.">
              <p class="c-article-references__text" id="ref-CR38">
               Shih, Y., Paris, S., Durand, F., Freeman, W.T.: Data-driven hallucination of different times of day from a single outdoor photo. ACM Trans. Graph. (TOG)
               <b>
                32
               </b>
               (6), 200 (2013)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR38-links">
               <a aria-label="CrossRef reference 38" data-doi="10.1145/2508363.2508419" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/2508363.2508419" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2508363.2508419" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 38" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Data-driven%20hallucination%20of%20different%20times%20of%20day%20from%20a%20single%20outdoor%20photo&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=32&amp;issue=6&amp;publication_year=2013&amp;author=Shih%2CY&amp;author=Paris%2CS&amp;author=Durand%2CF&amp;author=Freeman%2CWT" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39.">
              <p class="c-article-references__text" id="ref-CR39">
               Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1409.1556" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1409.1556">
                arXiv:1409.1556
               </a>
               (2014)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40.">
              <p class="c-article-references__text" id="ref-CR40">
               Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: feed-forward synthesis of textures and stylized images. In: ICML, pp. 1349–1357 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR40-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ulyanov%2C%20D.%2C%20Lebedev%2C%20V.%2C%20Vedaldi%2C%20A.%2C%20Lempitsky%2C%20V.S.%3A%20Texture%20networks%3A%20feed-forward%20synthesis%20of%20textures%20and%20stylized%20images.%20In%3A%20ICML%2C%20pp.%201349%E2%80%931357%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41.">
              <p class="c-article-references__text" id="ref-CR41">
               Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. In: Proceedings of CVPR (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR41-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ulyanov%2C%20D.%2C%20Vedaldi%2C%20A.%2C%20Lempitsky%2C%20V.%3A%20Improved%20texture%20networks%3A%20maximizing%20quality%20and%20diversity%20in%20feed-forward%20stylization%20and%20texture%20synthesis.%20In%3A%20Proceedings%20of%20CVPR%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42.">
              <p class="c-article-references__text" id="ref-CR42">
               Wang, H., Liang, X., Zhang, H., Yeung, D.Y., Xing, E.P.: ZM-Net: real-time zero-shot image manipulation network. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1703.07255" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1703.07255">
                arXiv:1703.07255
               </a>
               (2017)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43.">
              <p class="c-article-references__text" id="ref-CR43">
               Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1505.00687" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1505.00687">
                arXiv:1505.00687
               </a>
               (2015)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44.">
              <p class="c-article-references__text" id="ref-CR44">
               Wang, X., Oxholm, G., Zhang, D., Wang, Y.F.: Multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR44-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wang%2C%20X.%2C%20Oxholm%2C%20G.%2C%20Zhang%2C%20D.%2C%20Wang%2C%20Y.F.%3A%20Multimodal%20transfer%3A%20a%20hierarchical%20deep%20convolutional%20neural%20network%20for%20fast%20artistic%20style%20transfer.%20In%3A%20The%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%2C%20July%202017">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45.">
              <p class="c-article-references__text" id="ref-CR45">
               Wilber, M.J., Fang, C., Jin, H., Hertzmann, A., Collomosse, J., Belongie, S.: BAM! The behance artistic media dataset for recognition beyond photography. In: The IEEE International Conference on Computer Vision (ICCV), October 2017
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR45-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wilber%2C%20M.J.%2C%20Fang%2C%20C.%2C%20Jin%2C%20H.%2C%20Hertzmann%2C%20A.%2C%20Collomosse%2C%20J.%2C%20Belongie%2C%20S.%3A%20BAM%21%20The%20behance%20artistic%20media%20dataset%20for%20recognition%20beyond%20photography.%20In%3A%20The%20IEEE%20International%20Conference%20on%20Computer%20Vision%20%28ICCV%29%2C%20October%202017">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46.">
              <p class="c-article-references__text" id="ref-CR46">
               Wilmot, P., Risser, E., Barnes, C.: Stable and controllable neural texture synthesis and style transfer using histogram losses. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1701.08893" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1701.08893">
                arXiv:1701.08893
               </a>
               (2017)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47.">
              <p class="c-article-references__text" id="ref-CR47">
               Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information Processing Systems, pp. 487–495 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR47-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhou%2C%20B.%2C%20Lapedriza%2C%20A.%2C%20Xiao%2C%20J.%2C%20Torralba%2C%20A.%2C%20Oliva%2C%20A.%3A%20Learning%20deep%20features%20for%20scene%20recognition%20using%20places%20database.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%20487%E2%80%93495%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48.">
              <p class="c-article-references__text" id="ref-CR48">
               Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR48-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhu%2C%20J.Y.%2C%20Park%2C%20T.%2C%20Isola%2C%20P.%2C%20Efros%2C%20A.A.%3A%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks.%20In%3A%20IEEE%20International%20Conference%20on%20Computer%20Vision%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
            </ol>
            <p class="c-article-references__download u-hide-print">
             <a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-030-01237-3_43?format=refman&amp;flavour=references" rel="nofollow">
              Download references
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </p>
           </div>
          </div>
         </div>
        </section>
       </div>
       <section data-title="Acknowledgement" lang="en">
        <div class="c-article-section" id="Ack1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">
          <span class="c-article-section__title-number">
          </span>
          Acknowledgement
         </h2>
         <div class="c-article-section__content" id="Ack1-content">
          <p>
           This work has been supported in part by a DFG grant, the Heidelberg Academy of Science, and an Nvidia hardware donation.
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="author-information" data-title="Author information">
        <div class="c-article-section" id="author-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">
          <span class="c-article-section__title-number">
          </span>
          Author information
         </h2>
         <div class="c-article-section__content" id="author-information-content">
          <h3 class="c-article__sub-heading" id="affiliations">
           Authors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff17">
            <p class="c-article-author-affiliation__address">
             Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Heidelberg, Germany
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Artsiom Sanakoyeu, Dmytro Kotovenko, Sabine Lang &amp; Björn Ommer
            </p>
           </li>
          </ol>
          <div class="u-js-hide u-hide-print" data-test="author-info">
           <span class="c-article__sub-heading">
            Authors
           </span>
           <ol class="c-article-authors-search u-list-reset">
            <li id="auth-Artsiom-Sanakoyeu">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Artsiom Sanakoyeu
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Artsiom%20Sanakoyeu" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Artsiom%20Sanakoyeu" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Artsiom%20Sanakoyeu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Dmytro-Kotovenko">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Dmytro Kotovenko
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Dmytro%20Kotovenko" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Dmytro%20Kotovenko" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dmytro%20Kotovenko%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Sabine-Lang">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Sabine Lang
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Sabine%20Lang" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Sabine%20Lang" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sabine%20Lang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Bj_rn-Ommer">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Björn Ommer
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Bj%C3%B6rn%20Ommer" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Bj%C3%B6rn%20Ommer" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Bj%C3%B6rn%20Ommer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
           </ol>
          </div>
          <h3 class="c-article__sub-heading" id="corresponding-author">
           Corresponding author
          </h3>
          <p id="corresponding-author-list">
           Correspondence to
           <a href="mailto:artsiom.sanakoyeu@iwr.uni-heidelberg.de" id="corresp-c1">
            Artsiom Sanakoyeu
           </a>
           .
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="editor-information" data-title="Editor information">
        <div class="c-article-section" id="editor-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="editor-information">
          <span class="c-article-section__title-number">
          </span>
          Editor information
         </h2>
         <div class="c-article-section__content" id="editor-information-content">
          <h3 class="c-article__sub-heading" id="editor-affiliations">
           Editors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff13">
            <p class="c-article-author-affiliation__address">
             Google Research, Zurich, Switzerland
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Vittorio Ferrari
            </p>
           </li>
           <li id="Aff14">
            <p class="c-article-author-affiliation__address">
             Carnegie Mellon University, Pittsburgh, PA, USA
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Prof. Martial Hebert
            </p>
           </li>
           <li id="Aff15">
            <p class="c-article-author-affiliation__address">
             Google Research, Zurich, Switzerland
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Cristian Sminchisescu
            </p>
           </li>
           <li id="Aff16">
            <p class="c-article-author-affiliation__address">
             Hebrew University of Jerusalem, Jerusalem, Israel
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Yair Weiss
            </p>
           </li>
          </ol>
         </div>
        </div>
       </section>
       <section data-title="Electronic supplementary material">
        <div class="c-article-section" id="Sec11-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">
          <span class="c-article-section__title-number">
           1
          </span>
          Electronic supplementary material
         </h2>
         <div class="c-article-section__content" id="Sec11-content">
          <div data-test="supplementary-info">
           <div class="c-article-figshare-container" data-test="figshare-container" id="figshareContainer">
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary material 1 (pdf 42656 kb)" href="https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_MOESM1_ESM.pdf">
              Supplementary material 1 (pdf 42656 KB)
             </a>
            </h3>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary material 2 (mp4 32389 kb)" href="https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_MOESM2_ESM.mp4">
              Supplementary material 2 (mp4 32389 KB)
             </a>
            </h3>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM3">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="474213_1_en_43_moesm3_esm.jpg" href="https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_MOESM3_ESM.jpg">
              474213_1_En_43_MOESM3_ESM.jpg
             </a>
            </h3>
           </div>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM4">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary material 4 (mp4 4072 kb)" href="https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_MOESM4_ESM.mp4">
              Supplementary material 4 (mp4 4072 KB)
             </a>
            </h3>
           </div>
          </div>
         </div>
        </div>
       </section>
       <section data-title="Rights and permissions" lang="en">
        <div class="c-article-section" id="rightslink-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">
          <span class="c-article-section__title-number">
          </span>
          Rights and permissions
         </h2>
         <div class="c-article-section__content" id="rightslink-content">
          <p class="c-article-rights" data-test="rightslink-content">
           <a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?publisherName=SpringerNature&amp;orderBeanReset=true&amp;orderSource=SpringerLink&amp;title=A%20Style-Aware%20Content%20Loss%20for%20Real-Time%20HD%20Style%20Transfer&amp;author=Artsiom%20Sanakoyeu%2C%20Dmytro%20Kotovenko%2C%20Sabine%20Lang%20et%20al&amp;contentID=10.1007%2F978-3-030-01237-3_43&amp;copyright=Springer%20Nature%20Switzerland%20AG&amp;publication=eBook&amp;publicationDate=2018&amp;startPage=715&amp;endPage=731&amp;imprint=Springer%20Nature%20Switzerland%20AG">
            Reprints and Permissions
           </a>
          </p>
         </div>
        </div>
       </section>
       <section data-title="Copyright information">
        <div class="c-article-section" id="copyright-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="copyright-information">
          <span class="c-article-section__title-number">
          </span>
          Copyright information
         </h2>
         <div class="c-article-section__content" id="copyright-information-content">
          <p>
           © 2018 Springer Nature Switzerland AG
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="chapter-info" data-title="About this paper" lang="en">
        <div class="c-article-section" id="chapter-info-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="chapter-info">
          <span class="c-article-section__title-number">
          </span>
          About this paper
         </h2>
         <div class="c-article-section__content" id="chapter-info-content">
          <div class="c-bibliographic-information">
           <div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border">
            <a data-crossmark="10.1007/978-3-030-01237-3_43" data-test="crossmark" data-track="click" data-track-action="Click Crossmark" data-track-label="link" href="https://crossmark-crossref-org.proxy.lib.ohio-state.edu/dialog/?doi=10.1007/978-3-030-01237-3_43" rel="noopener" target="_blank">
             <img alt="Check for updates. Verify currency and authenticity via CrossMark" height="81" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57"/>
            </a>
           </div>
           <div class="c-bibliographic-information__column">
            <h3 class="c-article__sub-heading" id="citeas">
             Cite this paper
            </h3>
            <p class="c-bibliographic-information__citation" data-test="bibliographic-information__cite_this_chapter">
             Sanakoyeu, A., Kotovenko, D., Lang, S., Ommer, B. (2018).  A Style-Aware Content Loss for Real-Time HD Style Transfer.

                     In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds) Computer Vision – ECCV 2018. ECCV 2018. Lecture Notes in Computer Science(), vol 11212. Springer, Cham. https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-01237-3_43
            </p>
            <h3 class="c-bibliographic-information__download-citation u-mb-8 u-mt-16 u-hide-print">
             Download citation
            </h3>
            <ul class="c-bibliographic-information__download-citation-list">
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-030-01237-3_43?format=refman&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .RIS file">
               .RIS
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-030-01237-3_43?format=endnote&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .ENW file">
               .ENW
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-030-01237-3_43?format=bibtex&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .BIB file">
               .BIB
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
            </ul>
            <ul class="c-bibliographic-information__list u-mb-24" data-test="publication-history">
             <li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--chapter-doi">
              <p data-test="bibliographic-information__doi">
               <abbr title="Digital Object Identifier">
                DOI
               </abbr>
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                https://doi.org/10.1007/978-3-030-01237-3_43
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p>
               Published
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2018-10-07">
                 07 October 2018
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__publisher-name">
               Publisher Name
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                Springer, Cham
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__pisbn">
               Print ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-030-01236-6
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__eisbn">
               Online ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-030-01237-3
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__package">
               eBook Packages
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=11645&amp;facet-start-year=2018&amp;facet-end-year=2018">
                 Computer Science
                </a>
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=43710&amp;facet-start-year=2018&amp;facet-end-year=2018">
                 Computer Science (R0)
                </a>
               </span>
              </p>
             </li>
            </ul>
            <div data-component="share-box">
             <div class="c-article-share-box u-display-block">
              <h3 class="c-article__sub-heading">
               Share this paper
              </h3>
              <p class="c-article-share-box__description">
               Anyone you share the following link with will be able to read this content:
              </p>
              <button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url">
               Get shareable link
              </button>
              <div class="js-no-share-url-container u-display-none" hidden="">
               <p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">
                Sorry, a shareable link is not currently available for this article.
               </p>
              </div>
              <div class="js-share-url-container u-display-none" hidden="">
               <p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url">
               </p>
               <button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url">
                Copy to clipboard
               </button>
              </div>
              <p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
               Provided by the Springer Nature SharedIt content-sharing initiative
              </p>
             </div>
            </div>
            <div data-component="chapter-info-list">
            </div>
           </div>
          </div>
         </div>
        </div>
       </section>
      </div>
     </article>
    </main>
    <div class="c-article-extras u-text-sm u-hide-print" data-container-type="reading-companion" data-track-component="conference paper" id="sidebar">
     <aside>
      <div class="js-context-bar-sticky-point-desktop" data-test="download-article-link-wrapper">
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-030-01237-3.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-030-01237-3.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div data-test="editorial-summary">
      </div>
      <div class="c-reading-companion">
       <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky" style="top: 40px;">
        <ul class="c-reading-companion__tabs" role="tablist">
         <li role="presentation">
          <button aria-controls="tabpanel-sections" aria-selected="true" class="c-reading-companion__tab c-reading-companion__tab--active" data-tab-target="sections" data-track="click" data-track-action="sections tab" data-track-label="tab" id="tab-sections" role="tab">
           Sections
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-figures" aria-selected="false" class="c-reading-companion__tab" data-tab-target="figures" data-track="click" data-track-action="figures tab" data-track-label="tab" id="tab-figures" role="tab" tabindex="-1">
           Figures
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-references" aria-selected="false" class="c-reading-companion__tab" data-tab-target="references" data-track="click" data-track-action="references tab" data-track-label="tab" id="tab-references" role="tab" tabindex="-1">
           References
          </button>
         </li>
        </ul>
        <div aria-labelledby="tab-sections" class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections" role="tabpanel">
         <div class="c-reading-companion__scroll-pane" style="max-height: 4544px;">
          <ul class="c-reading-companion__sections-list">
           <li class="c-reading-companion__section-item" id="rc-sec-Abs1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Abstract" href="#Abs1">
             <span class="c-article-section__title-number">
             </span>
             Abstract
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Introduction" href="#Sec1">
             <span class="c-article-section__title-number">
              1
             </span>
             Introduction
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec3">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Approach" href="#Sec3">
             <span class="c-article-section__title-number">
              2
             </span>
             Approach
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec6">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Experiments" href="#Sec6">
             <span class="c-article-section__title-number">
              3
             </span>
             Experiments
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec10">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Conclusion" href="#Sec10">
             <span class="c-article-section__title-number">
              4
             </span>
             Conclusion
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Bib1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: References" href="#Bib1">
             <span class="c-article-section__title-number">
             </span>
             References
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Ack1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Acknowledgement" href="#Ack1">
             <span class="c-article-section__title-number">
             </span>
             Acknowledgement
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-author-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Author information" href="#author-information">
             <span class="c-article-section__title-number">
             </span>
             Author information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-editor-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Editor information" href="#editor-information">
             <span class="c-article-section__title-number">
             </span>
             Editor information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec11">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Electronic supplementary material" href="#Sec11">
             <span class="c-article-section__title-number">
              1
             </span>
             Electronic supplementary material
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-rightslink">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Rights and permissions" href="#rightslink">
             <span class="c-article-section__title-number">
             </span>
             Rights and permissions
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-copyright-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Copyright information" href="#copyright-information">
             <span class="c-article-section__title-number">
             </span>
             Copyright information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-chapter-info">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: About this paper" href="#chapter-info">
             <span class="c-article-section__title-number">
             </span>
             About this paper
            </a>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-figures" class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ul class="c-reading-companion__figures-list">
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig1_HTML.gif?"/>
              <img alt="figure 1" aria-describedby="rc-Fig1" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig1_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/1" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig2_HTML.gif?"/>
              <img alt="figure 2" aria-describedby="rc-Fig2" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig2_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/2" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig3_HTML.gif?"/>
              <img alt="figure 3" aria-describedby="rc-Fig3" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig3_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/3" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig4_HTML.gif?"/>
              <img alt="figure 4" aria-describedby="rc-Fig4" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig4_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/4" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig5_HTML.gif?"/>
              <img alt="figure 5" aria-describedby="rc-Fig5" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig5_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/5" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig6_HTML.gif?"/>
              <img alt="figure 6" aria-describedby="rc-Fig6" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig6_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/6" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig7_HTML.gif?"/>
              <img alt="figure 7" aria-describedby="rc-Fig7" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig7_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/7" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig8">
               Fig. 8.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig8_HTML.gif?"/>
              <img alt="figure 8" aria-describedby="rc-Fig8" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig8_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig8">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/8" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig9">
               Fig. 9.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig9_HTML.gif?"/>
              <img alt="figure 9" aria-describedby="rc-Fig9" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig9_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig9">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/9" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig10">
               Fig. 10.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig10_HTML.gif?"/>
              <img alt="figure 10" aria-describedby="rc-Fig10" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig10_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig10">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/10" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig11">
               Fig. 11.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig11_HTML.gif?"/>
              <img alt="figure 11" aria-describedby="rc-Fig11" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-030-01237-3_43/MediaObjects/474213_1_En_43_Fig11_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig11">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-030-01237-3_43/figures/11" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-references" class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ol class="c-reading-companion__references-list c-reading-companion__references-list--numeric">
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR1">
             Bautista, M.A., Sanakoyeu, A., Tikhoncheva, E., Ommer, B.: CliqueCNN: deep unsupervised exemplar learning. In: Advances in Neural Information Processing Systems, pp. 3846–3854 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bautista%2C%20M.A.%2C%20Sanakoyeu%2C%20A.%2C%20Tikhoncheva%2C%20E.%2C%20Ommer%2C%20B.%3A%20CliqueCNN%3A%20deep%20unsupervised%20exemplar%20learning.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%203846%E2%80%933854%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR2">
             Che, T., Li, Y., Jacob, A.P., Bengio, Y., Li, W.: Mode regularized generative adversarial networks. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1612.02136" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1612.02136">
              arXiv:1612.02136
             </a>
             (2016)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR3">
             Chen, D., Yuan, L., Liao, J., Yu, N., Hua, G.: Stylebank: an explicit representation for neural image style transfer. In: Proceedings of CVPR (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Chen%2C%20D.%2C%20Yuan%2C%20L.%2C%20Liao%2C%20J.%2C%20Yu%2C%20N.%2C%20Hua%2C%20G.%3A%20Stylebank%3A%20an%20explicit%20representation%20for%20neural%20image%20style%20transfer.%20In%3A%20Proceedings%20of%20CVPR%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR4">
             Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1612.04337" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1612.04337">
              arXiv:1612.04337
             </a>
             (2016)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR5">
             Collomosse, J., Bui, T., Wilber, M.J., Fang, C., Jin, H.: Sketching with style: visual search with sketches and aesthetic context. In: The IEEE International Conference on Computer Vision (ICCV), October 2017
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Collomosse%2C%20J.%2C%20Bui%2C%20T.%2C%20Wilber%2C%20M.J.%2C%20Fang%2C%20C.%2C%20Jin%2C%20H.%3A%20Sketching%20with%20style%3A%20visual%20search%20with%20sketches%20and%20aesthetic%20context.%20In%3A%20The%20IEEE%20International%20Conference%20on%20Computer%20Vision%20%28ICCV%29%2C%20October%202017">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR6">
             Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style. In: Proceedings of ICLR (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Dumoulin%2C%20V.%2C%20Shlens%2C%20J.%2C%20Kudlur%2C%20M.%3A%20A%20learned%20representation%20for%20artistic%20style.%20In%3A%20Proceedings%20of%20ICLR%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR7">
             Engstrom, L.: Fast style transfer (2016).
             <a data-track="click" data-track-action="external reference" data-track-label="https://github.com/lengstrom/fast-style-transfer/" href="https://github.com/lengstrom/fast-style-transfer/">
              https://github.com/lengstrom/fast-style-transfer/
             </a>
             . Commit 55809f4e
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR8">
             Esser, P., Sutter, E., Ommer, B.: A variational U-Net for conditional appearance and shape generation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2018
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Esser%2C%20P.%2C%20Sutter%2C%20E.%2C%20Ommer%2C%20B.%3A%20A%20variational%20U-Net%20for%20conditional%20appearance%20and%20shape%20generation.%20In%3A%20The%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%2C%20July%202018">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR9">
             Fernie, E.: Art History and Its Methods: A Critical Anthology, p. 361. Phaidon, London (1995)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Art%20History%20and%20Its%20Methods%3A%20A%20Critical%20Anthology&amp;publication_year=1995&amp;author=Fernie%2CE">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR10">
             Frigo, O., Sabater, N., Delon, J., Hellier, P.: Split and match: example-based adaptive patch sampling for unsupervised style transfer. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 553–561 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Frigo%2C%20O.%2C%20Sabater%2C%20N.%2C%20Delon%2C%20J.%2C%20Hellier%2C%20P.%3A%20Split%20and%20match%3A%20example-based%20adaptive%20patch%20sampling%20for%20unsupervised%20style%20transfer.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%20553%E2%80%93561%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR11">
             Gatys, L.A., Ecker, A.S., Bethge, M.: Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1505.07376" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1505.07376">
              arXiv:1505.07376
             </a>
             12 (2015)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR12">
             Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414–2423. IEEE (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Gatys%2C%20L.A.%2C%20Ecker%2C%20A.S.%2C%20Bethge%2C%20M.%3A%20Image%20style%20transfer%20using%20convolutional%20neural%20networks.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%2C%20pp.%202414%E2%80%932423.%20IEEE%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR13">
             Gatys, L.A., Ecker, A.S., Bethge, M., Hertzmann, A., Shechtman, E.: Controlling perceptual factors in neural style transfer. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Gatys%2C%20L.A.%2C%20Ecker%2C%20A.S.%2C%20Bethge%2C%20M.%2C%20Hertzmann%2C%20A.%2C%20Shechtman%2C%20E.%3A%20Controlling%20perceptual%20factors%20in%20neural%20style%20transfer.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR14">
             Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V., Shlens, J.: Exploring the structure of a real-time, arbitrary neural artistic stylization network. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.06830" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.06830">
              arXiv:1705.06830
             </a>
             (2017)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR15">
             Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Information Processing Systems, pp. 2672–2680 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Goodfellow%2C%20I.%2C%20et%20al.%3A%20Generative%20adversarial%20nets.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%202672%E2%80%932680%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR16">
             He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=He%2C%20K.%2C%20Zhang%2C%20X.%2C%20Ren%2C%20S.%2C%20Sun%2C%20J.%3A%20Deep%20residual%20learning%20for%20image%20recognition.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%20770%E2%80%93778%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR17">
             Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analogies. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 327–340. ACM (2001)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hertzmann%2C%20A.%2C%20Jacobs%2C%20C.E.%2C%20Oliver%2C%20N.%2C%20Curless%2C%20B.%2C%20Salesin%2C%20D.H.%3A%20Image%20analogies.%20In%3A%20Proceedings%20of%20the%2028th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20327%E2%80%93340.%20ACM%20%282001%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR18">
             Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. In: Advances in Neural Information Processing Systems, pp. 6629–6640 (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Heusel%2C%20M.%2C%20Ramsauer%2C%20H.%2C%20Unterthiner%2C%20T.%2C%20Nessler%2C%20B.%2C%20Hochreiter%2C%20S.%3A%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%206629%E2%80%936640%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR19">
             Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neural networks. Science
             <b>
              313
             </b>
             (5786), 504–507 (2006).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1126/science.1127647" href="https://doi-org.proxy.lib.ohio-state.edu/10.1126/science.1127647">
              https://doi-org.proxy.lib.ohio-state.edu/10.1126/science.1127647
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1126/science.1127647" href="https://doi-org.proxy.lib.ohio-state.edu/10.1126%2Fscience.1127647">
              CrossRef
             </a>
             <a data-track="click" data-track-action="mathscinet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=2242509">
              MathSciNet
             </a>
             <a data-track="click" data-track-action="math reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1226.68083">
              MATH
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks&amp;journal=Science&amp;doi=10.1126%2Fscience.1127647&amp;volume=313&amp;issue=5786&amp;pages=504-507&amp;publication_year=2006&amp;author=Hinton%2CGE&amp;author=Salakhutdinov%2CRR">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR20">
             Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: ICCV (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Huang%2C%20X.%2C%20Belongie%2C%20S.%3A%20Arbitrary%20style%20transfer%20in%20real-time%20with%20adaptive%20instance%20normalization.%20In%3A%20ICCV%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR21">
             Jing, Y., Liu, Y., Yang, Y., Feng, Z., Yu, Y., Song, M.: Stroke controllable fast style transfer with adaptive receptive fields. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1802.07101" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1802.07101">
              arXiv:1802.07101
             </a>
             (2018)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR22">
             Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-46475-6_43" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_43
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-319-46475-6_43" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-319-46475-6_43">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution&amp;pages=694-711&amp;publication_year=2016%202016%202016&amp;author=Johnson%2CJ&amp;author=Alahi%2CA&amp;author=Fei-Fei%2CL">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR23">
             Karayev, S., et al.: Recognizing image style. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1311.3715" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1311.3715">
              arXiv:1311.3715
             </a>
             (2013)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR24">
             Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1412.6980">
              arXiv:1412.6980
             </a>
             (2014)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR25">
             Li, C., Wand, M.: Precomputed real-time texture synthesis with markovian generative adversarial networks. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9907, pp. 702–716. Springer, Cham (2016).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-46487-9_43" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46487-9_43">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46487-9_43
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-319-46487-9_43" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-319-46487-9_43">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Precomputed%20real-time%20texture%20synthesis%20with%20markovian%20generative%20adversarial%20networks&amp;pages=702-716&amp;publication_year=2016%202016%202016&amp;author=Li%2CC&amp;author=Wand%2CM">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR26">
             Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Diversified texture synthesis with feed-forward networks. In: IEEE Conference on Computer Vision and Pattern Recognition (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Li%2C%20Y.%2C%20Fang%2C%20C.%2C%20Yang%2C%20J.%2C%20Wang%2C%20Z.%2C%20Lu%2C%20X.%2C%20Yang%2C%20M.H.%3A%20Diversified%20texture%20synthesis%20with%20feed-forward%20networks.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR27">
             Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer via feature transforms. In: Advances in Neural Information Processing Systems, pp. 385–395 (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Li%2C%20Y.%2C%20Fang%2C%20C.%2C%20Yang%2C%20J.%2C%20Wang%2C%20Z.%2C%20Lu%2C%20X.%2C%20Yang%2C%20M.H.%3A%20Universal%20style%20transfer%20via%20feature%20transforms.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%20385%E2%80%93395%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR28">
             Liao, J., Yao, Y., Yuan, L., Hua, G., Kang, S.B.: Visual attribute transfer through deep image analogy. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.01088" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.01088">
              arXiv:1705.01088
             </a>
             (2017)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR29">
             Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431–3440 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Long%2C%20J.%2C%20Shelhamer%2C%20E.%2C%20Darrell%2C%20T.%3A%20Fully%20convolutional%20networks%20for%20semantic%20segmentation.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%203431%E2%80%933440%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR30">
             Luan, F., Paris, S., Shechtman, E., Bala, K.: Deep photo style transfer. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Luan%2C%20F.%2C%20Paris%2C%20S.%2C%20Shechtman%2C%20E.%2C%20Bala%2C%20K.%3A%20Deep%20photo%20style%20transfer.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR31">
             Mahendran, A., Vedaldi, A.: Understanding deep image representations by inverting them. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Mahendran%2C%20A.%2C%20Vedaldi%2C%20A.%3A%20Understanding%20deep%20image%20representations%20by%20inverting%20them.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR32">
             Mao, H., Cheung, M., She, J.: DeepArt: learning joint representations of visual arts. In: Proceedings of the 2017 ACM on Multimedia Conference, pp. 1183–1191. ACM (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Mao%2C%20H.%2C%20Cheung%2C%20M.%2C%20She%2C%20J.%3A%20DeepArt%3A%20learning%20joint%20representations%20of%20visual%20arts.%20In%3A%20Proceedings%20of%20the%202017%20ACM%20on%20Multimedia%20Conference%2C%20pp.%201183%E2%80%931191.%20ACM%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR33">
             Odena, A., Dumoulin, V., Olah, C.: Deconvolution and checkerboard artifacts. Distill (2016).
             <a data-track="click" data-track-action="external reference" data-track-label="10.23915/distill.00003" href="https://doi-org.proxy.lib.ohio-state.edu/10.23915/distill.00003">
              https://doi-org.proxy.lib.ohio-state.edu/10.23915/distill.00003
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR34">
             Russakovsky, O.: ImageNet large scale visual recognition challenge. Int. J. Comput. Vis. (IJCV)
             <b>
              115
             </b>
             (3), 211–252 (2015).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/s11263-015-0816-y" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/s11263-015-0816-y">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/s11263-015-0816-y
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/s11263-015-0816-y" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-015-0816-y">
              CrossRef
             </a>
             <a data-track="click" data-track-action="mathscinet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=3422482">
              MathSciNet
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=ImageNet%20large%20scale%20visual%20recognition%20challenge&amp;journal=Int.%20J.%20Comput.%20Vis.%20%28IJCV%29&amp;doi=10.1007%2Fs11263-015-0816-y&amp;volume=115&amp;issue=3&amp;pages=211-252&amp;publication_year=2015&amp;author=Russakovsky%2CO">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR35">
             Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Improved techniques for training GANs. In: Advances in Neural Information Processing Systems, pp. 2234–2242 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Salimans%2C%20T.%2C%20Goodfellow%2C%20I.%2C%20Zaremba%2C%20W.%2C%20Cheung%2C%20V.%2C%20Radford%2C%20A.%2C%20Chen%2C%20X.%3A%20Improved%20techniques%20for%20training%20GANs.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%202234%E2%80%932242%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR36">
             Shen, F., Yan, S., Zeng, G.: Meta networks for neural style transfer. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1709.04111" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1709.04111">
              arXiv:1709.04111
             </a>
             (2017)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR37">
             Shih, Y., Paris, S., Barnes, C., Freeman, W.T., Durand, F.: Style transfer for headshot portraits. ACM Trans. Graph. (TOG)
             <b>
              33
             </b>
             (4), 148 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/2601097.2601137" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2601097.2601137">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Style%20transfer%20for%20headshot%20portraits&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=33&amp;issue=4&amp;publication_year=2014&amp;author=Shih%2CY&amp;author=Paris%2CS&amp;author=Barnes%2CC&amp;author=Freeman%2CWT&amp;author=Durand%2CF">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR38">
             Shih, Y., Paris, S., Durand, F., Freeman, W.T.: Data-driven hallucination of different times of day from a single outdoor photo. ACM Trans. Graph. (TOG)
             <b>
              32
             </b>
             (6), 200 (2013)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/2508363.2508419" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2508363.2508419">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Data-driven%20hallucination%20of%20different%20times%20of%20day%20from%20a%20single%20outdoor%20photo&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=32&amp;issue=6&amp;publication_year=2013&amp;author=Shih%2CY&amp;author=Paris%2CS&amp;author=Durand%2CF&amp;author=Freeman%2CWT">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR39">
             Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1409.1556" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1409.1556">
              arXiv:1409.1556
             </a>
             (2014)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR40">
             Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.S.: Texture networks: feed-forward synthesis of textures and stylized images. In: ICML, pp. 1349–1357 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ulyanov%2C%20D.%2C%20Lebedev%2C%20V.%2C%20Vedaldi%2C%20A.%2C%20Lempitsky%2C%20V.S.%3A%20Texture%20networks%3A%20feed-forward%20synthesis%20of%20textures%20and%20stylized%20images.%20In%3A%20ICML%2C%20pp.%201349%E2%80%931357%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR41">
             Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: maximizing quality and diversity in feed-forward stylization and texture synthesis. In: Proceedings of CVPR (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ulyanov%2C%20D.%2C%20Vedaldi%2C%20A.%2C%20Lempitsky%2C%20V.%3A%20Improved%20texture%20networks%3A%20maximizing%20quality%20and%20diversity%20in%20feed-forward%20stylization%20and%20texture%20synthesis.%20In%3A%20Proceedings%20of%20CVPR%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR42">
             Wang, H., Liang, X., Zhang, H., Yeung, D.Y., Xing, E.P.: ZM-Net: real-time zero-shot image manipulation network. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1703.07255" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1703.07255">
              arXiv:1703.07255
             </a>
             (2017)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR43">
             Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1505.00687" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1505.00687">
              arXiv:1505.00687
             </a>
             (2015)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR44">
             Wang, X., Oxholm, G., Zhang, D., Wang, Y.F.: Multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wang%2C%20X.%2C%20Oxholm%2C%20G.%2C%20Zhang%2C%20D.%2C%20Wang%2C%20Y.F.%3A%20Multimodal%20transfer%3A%20a%20hierarchical%20deep%20convolutional%20neural%20network%20for%20fast%20artistic%20style%20transfer.%20In%3A%20The%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%2C%20July%202017">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR45">
             Wilber, M.J., Fang, C., Jin, H., Hertzmann, A., Collomosse, J., Belongie, S.: BAM! The behance artistic media dataset for recognition beyond photography. In: The IEEE International Conference on Computer Vision (ICCV), October 2017
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wilber%2C%20M.J.%2C%20Fang%2C%20C.%2C%20Jin%2C%20H.%2C%20Hertzmann%2C%20A.%2C%20Collomosse%2C%20J.%2C%20Belongie%2C%20S.%3A%20BAM%21%20The%20behance%20artistic%20media%20dataset%20for%20recognition%20beyond%20photography.%20In%3A%20The%20IEEE%20International%20Conference%20on%20Computer%20Vision%20%28ICCV%29%2C%20October%202017">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR46">
             Wilmot, P., Risser, E., Barnes, C.: Stable and controllable neural texture synthesis and style transfer using histogram losses. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1701.08893" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1701.08893">
              arXiv:1701.08893
             </a>
             (2017)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR47">
             Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information Processing Systems, pp. 487–495 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhou%2C%20B.%2C%20Lapedriza%2C%20A.%2C%20Xiao%2C%20J.%2C%20Torralba%2C%20A.%2C%20Oliva%2C%20A.%3A%20Learning%20deep%20features%20for%20scene%20recognition%20using%20places%20database.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20pp.%20487%E2%80%93495%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR48">
             Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: IEEE International Conference on Computer Vision (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhu%2C%20J.Y.%2C%20Park%2C%20T.%2C%20Isola%2C%20P.%2C%20Efros%2C%20A.A.%3A%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks.%20In%3A%20IEEE%20International%20Conference%20on%20Computer%20Vision%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
          </ol>
         </div>
        </div>
       </div>
      </div>
     </aside>
    </div>
   </div>
   <div class="app-elements">
    <footer data-test="universal-footer">
     <div class="c-footer" data-track-component="unified-footer">
      <div class="c-footer__container">
       <div class="c-footer__grid c-footer__group--separator">
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Discover content
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="journals a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
            Journals A-Z
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="books a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/books/a/1">
            Books A-Z
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Publish with us
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="publish your research" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
            Publish your research
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="open access publishing" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/open-research/about/the-fundamentals-of-open-access-and-open-research">
            Open access publishing
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Products and services
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="our products" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/products">
            Our products
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="librarians" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/librarians">
            Librarians
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="societies" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/societies">
            Societies
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="partners and advertisers" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/partners">
            Partners and advertisers
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Our imprints
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Springer" data-track-label="link" href="https://www-springer-com.proxy.lib.ohio-state.edu/">
            Springer
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Nature Portfolio" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/">
            Nature Portfolio
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="BMC" data-track-label="link" href="https://www-biomedcentral-com.proxy.lib.ohio-state.edu/">
            BMC
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Palgrave Macmillan" data-track-label="link" href="https://www.palgrave.com/">
            Palgrave Macmillan
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Apress" data-track-label="link" href="https://www.apress.com/">
            Apress
           </a>
          </li>
         </ul>
        </div>
       </div>
      </div>
      <div class="c-footer__container">
       <nav aria-label="footer navigation">
        <ul class="c-footer__links">
         <li class="c-footer__item">
          <button class="c-footer__link" data-cc-action="preferences" data-track="click" data-track-action="Manage cookies" data-track-label="link">
           <span class="c-footer__button-text">
            Your privacy choices/Manage cookies
           </span>
          </button>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="california privacy statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/legal/ccpa">
           Your US state privacy rights
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="accessibility statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/info/accessibility">
           Accessibility statement
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="terms and conditions" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/termsandconditions">
           Terms and conditions
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="privacy policy" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/privacystatement">
           Privacy policy
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="help and support" data-track-label="link" href="https://support-springernature-com.proxy.lib.ohio-state.edu/en/support/home">
           Help and support
          </a>
         </li>
        </ul>
       </nav>
       <div class="c-footer__user">
        <p class="c-footer__user-info">
         <span data-test="footer-user-ip">
          3.128.143.42
         </span>
        </p>
        <p class="c-footer__user-info" data-test="footer-business-partners">
         OhioLINK Consortium (3000266689)  - Ohio State University Libraries (8200724141)
        </p>
       </div>
       <a class="c-footer__link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/">
        <img alt="Springer Nature" height="20" loading="lazy" src="/oscar-static/images/darwin/footer/img/logo-springernature_white-64dbfad7d8.svg" width="200"/>
       </a>
       <p class="c-footer__legal" data-test="copyright">
        © 2023 Springer Nature
       </p>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <div aria-hidden="true" class="u-visually-hidden">
   <!--?xml version="1.0" encoding="UTF-8"?-->
   <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <defs>
     <path d="M0 .74h56.72v55.24H0z" id="a">
     </path>
    </defs>
    <symbol id="icon-access" viewbox="0 0 18 18">
     <path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-account" viewbox="0 0 18 18">
     <path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-alert" viewbox="0 0 18 18">
     <path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-broad" viewbox="0 0 16 16">
     <path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)">
     </path>
    </symbol>
    <symbol id="icon-arrow-down" viewbox="0 0 16 16">
     <path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-left" viewbox="0 0 16 16">
     <path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-right" viewbox="0 0 16 16">
     <path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-sub" viewbox="0 0 16 16">
     <path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-up" viewbox="0 0 16 16">
     <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-article" viewbox="0 0 18 18">
     <path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-audio" viewbox="0 0 18 18">
     <path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-block" viewbox="0 0 24 24">
     <path d="m0 0h24v24h-24z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-book" viewbox="0 0 18 18">
     <path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-broad" viewbox="0 0 24 24">
     <path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)">
     </path>
    </symbol>
    <symbol id="icon-calendar" viewbox="0 0 18 18">
     <path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-cart" viewbox="0 0 18 18">
     <path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z">
     </path>
    </symbol>
    <symbol id="icon-chevron-less" viewbox="0 0 10 10">
     <path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)">
     </path>
    </symbol>
    <symbol id="icon-chevron-more" viewbox="0 0 10 10">
     <path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-chevron-right" viewbox="0 0 10 10">
     <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
     </path>
    </symbol>
    <symbol id="icon-circle-fill" viewbox="0 0 16 16">
     <path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-circle" viewbox="0 0 16 16">
     <path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-citation" viewbox="0 0 18 18">
     <path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-close" viewbox="0 0 16 16">
     <path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-collections" viewbox="0 0 18 18">
     <path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-compare" viewbox="0 0 18 18">
     <path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download-file" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download" viewbox="0 0 16 16">
     <path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-editors" viewbox="0 0 18 18">
     <path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-email" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-error" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-ethics" viewbox="0 0 18 18">
     <path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-expand">
     <path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-explore" viewbox="0 0 18 18">
     <path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-filter" viewbox="0 0 16 16">
     <path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z">
     </path>
    </symbol>
    <symbol id="icon-home" viewbox="0 0 18 18">
     <path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-image" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-info" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-institution" viewbox="0 0 18 18">
     <path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-location" viewbox="0 0 18 18">
     <path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-minus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-newsletter" viewbox="0 0 18 18">
     <path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-orcid" viewbox="0 0 18 18">
     <path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-plus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-print" viewbox="0 0 18 18">
     <path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-search" viewbox="0 0 22 22">
     <path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-facebook" viewbox="0 0 24 24">
     <path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-twitter" viewbox="0 0 24 24">
     <path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-youtube" viewbox="0 0 24 24">
     <path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-subject-medicine" viewbox="0 0 18 18">
     <path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-success" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-table" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick-circle" viewbox="0 0 24 24">
     <path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick" viewbox="0 0 16 16">
     <path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-update" viewbox="0 0 18 18">
     <path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-upload" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-video" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-warning" viewbox="0 0 18 18">
     <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-altmetric">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm-1.886 9.684-1.101 1.845a1 1 0 0 1-.728.479l-.13.008H3.056a9.001 9.001 0 0 0 17.886 0l-4.564-.001-2.779 4.156c-.454.68-1.467.55-1.758-.179l-.038-.113-1.69-6.195ZM12 3a9.001 9.001 0 0 0-8.947 8.016h4.533l2.017-3.375c.452-.757 1.592-.6 1.824.25l1.73 6.345 1.858-2.777a1 1 0 0 1 .707-.436l.124-.008h5.1A9.001 9.001 0 0 0 12 3Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-checklist-banner" viewbox="0 0 56.69 56.69">
     <path d="M0 0h56.69v56.69H0z" style="fill:none">
     </path>
     <clippath id="b">
      <use style="overflow:visible" xlink:href="#a">
      </use>
     </clippath>
     <path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round">
     </path>
     <path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round">
     </path>
    </symbol>
    <symbol id="icon-chevron-down" viewbox="0 0 16 16">
     <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-citations">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-eds-checklist" viewbox="0 0 32 32">
     <path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-external-link-medium" viewbox="0 0 24 24">
     <path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-info-filled-medium" viewbox="0 0 24 24">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z">
     </path>
    </symbol>
    <symbol id="icon-eds-menu" viewbox="0 0 24 24">
     <path d="M21.09 5c.503 0 .91.448.91 1s-.407 1-.91 1H2.91C2.406 7 2 6.552 2 6s.407-1 .91-1h18.18Zm-3.817 6c.401 0 .727.448.727 1s-.326 1-.727 1H2.727C2.326 13 2 12.552 2 12s.326-1 .727-1h14.546Zm3.818 6c.502 0 .909.448.909 1s-.407 1-.91 1H2.91c-.503 0-.91-.448-.91-1s.407-1 .91-1h18.18Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-search" viewbox="0 0 24 24">
     <path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-small-arrow-right" viewbox="0 0 16 16">
     <g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035">
      </path>
     </g>
    </symbol>
    <symbol id="icon-eds-user-single" viewbox="0 0 24 24">
     <path d="M12 12c5.498 0 10 4.001 10 9a1 1 0 0 1-2 0c0-3.838-3.557-7-8-7s-8 3.162-8 7a1 1 0 0 1-2 0c0-4.999 4.502-9 10-9Zm0-11a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm0 2a3 3 0 1 1 0 6 3 3 0 0 1 0-6Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-email-new" viewbox="0 0 24 24">
     <path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z">
     </path>
    </symbol>
    <symbol id="icon-expand-image" viewbox="0 0 18 18">
     <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-github" viewbox="0 0 100 100">
     <path clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-mentions">
     <g fill-rule="evenodd" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M22 15.255A9.373 9.373 0 0 1 8.745 2L22 15.255ZM15.477 8.523l4.215-4.215">
      </path>
      <path d="m7 13-5 9h10l-1-5">
      </path>
     </g>
    </symbol>
    <symbol id="icon-metrics-accesses">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM7.708 13.308c2.004 0 3.969 1.198 5.802 2.995l.23.23a2.285 2.285 0 0 1 .009 3.233C11.853 21.693 9.799 23 7.707 23c-2.091 0-4.14-1.305-6.033-3.226a2.285 2.285 0 0 1-.007-3.233c1.9-1.93 3.949-3.233 6.04-3.233Zm0 2c-1.396 0-3.064 1.062-4.623 2.644a.285.285 0 0 0 .007.41C4.642 19.938 6.311 21 7.707 21c1.397 0 3.069-1.065 4.623-2.644a.285.285 0 0 0 0-.404l-.23-.229c-1.487-1.451-3.064-2.415-4.393-2.415Zm-.036 1.077a1.77 1.77 0 1 1 .126 3.537 1.77 1.77 0 0 1-.126-3.537Zm.072 1.538a.23.23 0 1 0-.017.461.23.23 0 0 0 .017-.46Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-metrics">
     <path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-left">
     <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-right">
     <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z">
     </path>
    </symbol>
    <symbol id="icon-submit-open" viewbox="0 0 16 17">
     <path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero">
     </path>
    </symbol>
   </svg>
  </div>
  <script nomodule="true" src="/oscar-static/js/app-es5-bundle-774ca0a0f5.js">
  </script>
  <script src="/oscar-static/js/app-es6-bundle-047cc3c848.js" type="module">
  </script>
  <script nomodule="true" src="/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js">
  </script>
  <script src="/oscar-static/js/global-article-es6-bundle-c14b406246.js" type="module">
  </script>
  <div class="c-cookie-banner">
   <div class="c-cookie-banner__container">
    <p>
     This website sets only cookies which are necessary for it to function. They are used to enable core functionality such as security, network management and accessibility. These cookies cannot be switched off in our systems. You may disable these by changing your browser settings, but this may affect how the website functions. Please view our privacy policy for further details on how we process your information.
     <button class="c-cookie-banner__dismiss">
      Dismiss
     </button>
    </p>
   </div>
  </div>
 </body>
</html>
