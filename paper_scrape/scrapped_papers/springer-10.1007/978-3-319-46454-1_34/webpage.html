<html class="js" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta content="pc,mobile" name="applicable-device"/>
  <meta content="Yes" name="access"/>
  <meta content="SpringerLink" name="twitter:site"/>
  <meta content="summary" name="twitter:card"/>
  <meta content="Content cover image" name="twitter:image:alt"/>
  <meta content="Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a S" name="twitter:title"/>
  <meta content="We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The..." name="twitter:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-319-46454-1.jpg" name="twitter:image"/>
  <meta content="10.1007/978-3-319-46454-1_34" name="dc.identifier"/>
  <meta content="10.1007/978-3-319-46454-1_34" name="DOI"/>
  <meta content="We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The..." name="dc.description"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/content/pdf/10.1007/978-3-319-46454-1_34.pdf" name="citation_pdf_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34" name="citation_fulltext_html_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34" name="citation_abstract_html_url"/>
  <meta content="Computer Vision – ECCV 2016" name="citation_inbook_title"/>
  <meta content="Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image" name="citation_title"/>
  <meta content="2016" name="citation_publication_date"/>
  <meta content="561" name="citation_firstpage"/>
  <meta content="578" name="citation_lastpage"/>
  <meta content="en" name="citation_language"/>
  <meta content="10.1007/978-3-319-46454-1_34" name="citation_doi"/>
  <meta content="springer/eccv, dblp/eccv" name="citation_conference_series_id"/>
  <meta content="European Conference on Computer Vision" name="citation_conference_title"/>
  <meta content="ECCV" name="citation_conference_abbrev"/>
  <meta content="246415" name="size"/>
  <meta content="We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The..." name="description"/>
  <meta content="Bogo, Federica" name="citation_author"/>
  <meta content="febogo@microsoft.com" name="citation_author_email"/>
  <meta content="Microsoft Research" name="citation_author_institution"/>
  <meta content="Kanazawa, Angjoo" name="citation_author"/>
  <meta content="kanazawa@umiacs.umd.edu" name="citation_author_email"/>
  <meta content="University of Maryland" name="citation_author_institution"/>
  <meta content="Lassner, Christoph" name="citation_author"/>
  <meta content="christoph.lassner@tue.mpg.de" name="citation_author_email"/>
  <meta content="Max Planck Institute for Intelligent Systems" name="citation_author_institution"/>
  <meta content="University of Tübingen" name="citation_author_institution"/>
  <meta content="Gehler, Peter" name="citation_author"/>
  <meta content="pgehler@tue.mpg.de" name="citation_author_email"/>
  <meta content="Max Planck Institute for Intelligent Systems" name="citation_author_institution"/>
  <meta content="University of Tübingen" name="citation_author_institution"/>
  <meta content="Romero, Javier" name="citation_author"/>
  <meta content="jromero@tue.mpg.de" name="citation_author_email"/>
  <meta content="Max Planck Institute for Intelligent Systems" name="citation_author_institution"/>
  <meta content="Black, Michael J." name="citation_author"/>
  <meta content="black@tue.mpg.de" name="citation_author_email"/>
  <meta content="Max Planck Institute for Intelligent Systems" name="citation_author_institution"/>
  <meta content="Springer, Cham" name="citation_publisher"/>
  <meta content="http://api.springer-com.proxy.lib.ohio-state.edu/xmldata/jats?q=doi:10.1007/978-3-319-46454-1_34&amp;api_key=" name="citation_springer_api_url"/>
  <meta content="telephone=no" name="format-detection"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34" property="og:url"/>
  <meta content="Paper" property="og:type"/>
  <meta content="SpringerLink" property="og:site_name"/>
  <meta content="Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image" property="og:title"/>
  <meta content="We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The..." property="og:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-319-46454-1.jpg" property="og:image"/>
  <title>
   Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image | SpringerLink
  </title>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="shortcut icon"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="icon" sizes="16x16 32x32 48x48"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png" rel="icon" sizes="16x16" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png" rel="icon" sizes="32x32" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png" rel="icon" sizes="48x48" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png" rel="apple-touch-icon" sizes="72x72"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png" rel="apple-touch-icon" sizes="76x76"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png" rel="apple-touch-icon" sizes="114x114"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png" rel="apple-touch-icon" sizes="120x120"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png" rel="apple-touch-icon" sizes="144x144"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png" rel="apple-touch-icon" sizes="152x152"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon" sizes="180x180"/>
  <script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_SVG.js">
  </script>
  <script>
   (function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)
  </script>
  <script data-consent="link-springer-com.proxy.lib.ohio-state.edu" src="/static/js/lib/cookie-consent.min.js">
  </script>
  <style>
   @media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}body,img{max-width:100%}body{background:#fcfcfc;font-size:1.125rem;line-height:1.5;min-height:100%}main{display:block}h1{font-family:Georgia,Palatino,serif;font-size:2.25rem;font-style:normal;font-weight:400;line-height:1.4;margin:.67em 0}a{background-color:transparent;color:#004b83;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}sup{font-size:75%;line-height:0;position:relative;top:-.5em;vertical-align:baseline}img{border:0;height:auto;vertical-align:middle}button,input{font-family:inherit;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}*{margin:0}h2{font-family:Georgia,Palatino,serif;font-size:1.75rem;font-style:normal;font-weight:400;line-height:1.4}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input,p{margin:0;padding:0}a>img{vertical-align:middle}p{overflow-wrap:break-word;word-break:break-word}.c-app-header__theme{border-top-left-radius:2px;border-top-right-radius:2px;height:50px;margin:-16px -16px 0;overflow:hidden;position:relative}@media only screen and (min-width:1024px){.c-app-header__theme:after{background-color:hsla(0,0%,100%,.15);bottom:0;content:"";position:absolute;right:0;top:0;width:456px}}.c-app-header__content{padding-top:16px}@media only screen and (min-width:1024px){.c-app-header__content{display:flex}}.c-app-header__main{display:flex;flex:1 1 auto}.c-app-header__cover{margin-right:16px;margin-top:-50px;position:relative;z-index:5}.c-app-header__cover img{border:2px solid #fff;border-radius:4px;box-shadow:0 0 5px 2px hsla(0,0%,50%,.2);max-height:125px;max-width:96px}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-breadcrumbs>li{display:inline}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;list-style:none;margin:0;padding:16px}@media only screen and (min-width:540px){.c-pagination{justify-content:center}}.c-pagination__item{margin-bottom:8px;margin-right:16px}.c-pagination__item:last-child{margin-right:0}.c-pagination__link{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;min-width:30px;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link svg,.c-pagination__link--disabled svg{fill:currentcolor}.c-pagination__link:visited{color:#004b83}.c-pagination__link:focus,.c-pagination__link:hover{border:1px solid #666;text-decoration:none}.c-pagination__link:focus,.c-pagination__link:hover{background-color:#666;background-image:none;color:#fff}.c-pagination__link:focus svg path,.c-pagination__link:hover svg path{fill:#fff}.c-pagination__link--disabled{align-items:center;background-color:transparent;background-image:none;border-radius:2px;color:#333;cursor:default;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;opacity:.67;padding:8px;position:relative;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link--disabled:visited{color:#333}.c-pagination__link--disabled,.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{border:1px solid #ccc;text-decoration:none}.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{background-color:transparent;background-image:none;color:#333}.c-pagination__link--disabled:focus svg path,.c-pagination__link--disabled:hover svg path{fill:#333}.c-pagination__link--active{background-color:#666;background-image:none;border-color:#666;color:#fff;cursor:default}.c-pagination__ellipsis{background:0 0;border:0;min-width:auto;padding-left:0;padding-right:0}.c-pagination__icon{fill:#999;height:12px;width:16px}.c-pagination__icon--active{fill:#004b83}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#666;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-box{background-color:#fff;border:1px solid #ccc;border-radius:2px;line-height:1.3;padding:16px}.c-box--shadowed{box-shadow:0 0 5px 0 hsla(0,0%,50%,.1)}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #f2f2f2;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__section-item .c-article-section__title-number{display:none}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff}.c-pdf-download__link .u-icon{padding-top:2px}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}.u-button svg,.u-button--primary svg{fill:currentcolor}.app-elements .c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;line-height:1.4;padding:8px 0 0}.app-elements .c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav{border-top:2px solid #cedbe0;padding-top:4px;position:relative}.app-elements .c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav-container>:not(:last-child){margin-right:32px}.app-elements .c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.app-elements .c-header__list{list-style:none;margin:0;padding:0}.app-elements .c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.app-elements .c-header__list-item:not(:last-child){border-bottom:2px solid #cedbe0}.app-elements .c-header__item{color:inherit}@media only screen and (min-width:540px){.app-elements .c-header__item--menu{display:none;visibility:hidden}.app-elements .c-header__item--menu:first-child+*{margin-block-start:0}}.app-elements .c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:540px){.app-elements .c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.app-elements .c-header__item--divider:before{border-left:2px solid #cedbe0;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.app-elements .c-header__brand a{display:block;line-height:1;padding:16px 8px;text-decoration:none}.app-elements .c-header__brand img{height:24px;width:auto}.app-elements .c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-elements .c-header__link--static{flex:0 0 auto}.app-elements .c-header__icon{fill:currentcolor;display:inline-block;font-size:24px;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.app-elements .c-header__icon+*{margin-left:8px}.app-elements .c-header__expander{background-color:#ebf1f5}.app-elements .c-header__search{padding:24px 0}@media only screen and (min-width:540px){.app-elements .c-header__search{max-width:70%}}.app-elements .c-header__search-container{position:relative}.app-elements .c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.app-elements .c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.app-elements .c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.app-elements .has-tethered.c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:540px){.app-elements .has-tethered.c-header__expander--menu{display:none;visibility:hidden}}.app-elements .has-tethered .c-header__heading{display:none;visibility:hidden}.app-elements .has-tethered .c-header__heading:first-child+*{margin-block-start:0}.app-elements .has-tethered .c-header__search{margin:auto}.app-elements .c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-justify-content-space-between{justify-content:space-between}.u-flex-shrink{flex:0 1 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-mt-0{margin-top:0}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-float-left{float:left}.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-text-sm{font-size:1rem}.u-text-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-h3{font-family:Georgia,Palatino,serif;font-size:1.5rem;font-style:normal;font-weight:400;line-height:1.4}.c-article-section__content p{line-height:1.8}.c-pagination__input{border:1px solid #bfbfbf;border-radius:2px;box-shadow:inset 0 2px 6px 0 rgba(51,51,51,.2);box-sizing:initial;display:inline-block;height:28px;margin:0;max-width:64px;min-width:16px;padding:0 8px;text-align:center;transition:width .15s ease 0s}.c-pagination__input::-webkit-inner-spin-button,.c-pagination__input::-webkit-outer-spin-button{-webkit-appearance:none;margin:0}.c-article-associated-content__container .c-article-associated-content__collection-label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.063rem}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.063rem;font-weight:400}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-section__title,.c-article-title{font-weight:400}.c-chapter-book-series{font-size:1rem}.c-chapter-identifiers{margin:16px 0 8px}.c-chapter-book-details{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative}.c-chapter-book-details__title{font-weight:700}.c-chapter-book-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-chapter-book-details a{color:inherit}@media only screen and (max-width:539px){.c-chapter-book-details__meta{display:block}}.c-cover-image-lightbox{align-items:center;bottom:0;display:flex;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;transition:all .15s ease-in 0s;visibility:hidden;z-index:-1}.js-cover-image-lightbox--close{background:0 0;border:0;color:#fff;cursor:pointer;font-size:1.875rem;padding:13px;position:absolute;right:10px;top:0}.c-cover-image-lightbox__image{max-height:90vh;width:auto}.c-expand-overlay{background:#fff;color:#333;opacity:.5;padding:2px;position:absolute;right:3px;top:3px}.c-pdf-download__link{padding:13px 24px} }
  </style>
  <link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/enhanced-article-927ffe4eaf.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
  <script>
   window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/978-3-319-46454-1_34","Page":"chapter","page":{"attributes":{"environment":"live"}},"Country":"US","japan":false,"doi":"10.1007-978-3-319-46454-1_34","Keywords":"3D body shape, Human pose, 2D to 3D, CNN","kwrd":["3D_body_shape","Human_pose","2D_to_3D","CNN"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate","cobranding","doNotAutoAssociate","cobranding"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3000266689","8200724141"],"businessPartnerIDString":"3000266689|8200724141"}},"Access Type":"permanently-free","Bpids":"3000266689, 8200724141","Bpnames":"OhioLINK Consortium, Ohio State University Libraries","BPID":["3000266689","8200724141"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-978-3-319-46454-1","Full HTML":"Y","session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1611-3349","pissn":"0302-9743"},"book":{"doi":"10.1007/978-3-319-46454-1","title":"Computer Vision – ECCV 2016","pisbn":"978-3-319-46453-4","eisbn":"978-3-319-46454-1","bookProductType":"Proceedings","seriesTitle":"Lecture Notes in Computer Science","seriesId":"558"},"chapter":{"doi":"10.1007/978-3-319-46454-1_34"},"type":"ConferencePaper","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"SCI","secondarySubjects":{"1":"Image Processing and Computer Vision","2":"Pattern Recognition","3":"Artificial Intelligence","4":"Computer Graphics"},"secondarySubjectCodes":{"1":"SCI22021","2":"SCI2203X","3":"SCI21000","4":"SCI22013"}},"sucode":"SUCO11645"},"attributes":{"deliveryPlatform":"oscar"},"country":"US","Has Preview":"N","subjectCodes":"SCI,SCI22021,SCI2203X,SCI21000,SCI22013","PMC":["SCI","SCI22021","SCI2203X","SCI21000","SCI22013"]},"Event Category":"Conference Paper","ConferenceSeriesId":"eccv, eccv","productId":"9783319464541"}];
  </script>
  <script>
   window.dataLayer.push({
        ga4MeasurementId: 'G-B3E4QL2TPR',
        ga360TrackingId: 'UA-26408784-1',
        twitterId: 'o47a7',
        ga4ServerUrl: 'https://collect-springer-com.proxy.lib.ohio-state.edu',
        imprint: 'springerlink'
    });
  </script>
  <script data-test="gtm-head">
   window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
  </script>
  <script>
   (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/lib/cookie-consent.min.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
  </script>
  <script>
   (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
  </script>
  <script>
   (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
  </script>
  <script class="js-entry">
   if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-f934ac6316.js', 'async': false},
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/app-es5-bundle-774ca0a0f5.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/app-es6-bundle-047cc3c848.js', 'async': false, 'module': true}
                
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-c14b406246.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
  </script>
  <script src="/oscar-static/js/airbrake-es5-bundle-f934ac6316.js">
  </script>
  <script src="/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js">
  </script>
  <link href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34" rel="canonical"/>
  <script type="application/ld+json">
   {"headline":"Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image","pageEnd":"578","pageStart":"561","image":"https://media-springernature-com.proxy.lib.ohio-state.edu/w153/springer-static/cover/book/978-3-319-46454-1.jpg","genre":["Computer Science","Computer Science (R0)"],"isPartOf":{"name":"Computer Vision – ECCV 2016","isbn":["978-3-319-46454-1","978-3-319-46453-4"],"@type":"Book"},"publisher":{"name":"Springer International Publishing","logo":{"url":"https://www-springernature-com.proxy.lib.ohio-state.edu/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Federica Bogo","affiliation":[{"name":"Microsoft Research","address":{"name":"Microsoft Research, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"email":"febogo@microsoft.com","@type":"Person"},{"name":"Angjoo Kanazawa","affiliation":[{"name":"University of Maryland","address":{"name":"University of Maryland, College Park, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Christoph Lassner","affiliation":[{"name":"Max Planck Institute for Intelligent Systems","address":{"name":"Max Planck Institute for Intelligent Systems, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Tübingen","address":{"name":"University of Tübingen, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Peter Gehler","affiliation":[{"name":"Max Planck Institute for Intelligent Systems","address":{"name":"Max Planck Institute for Intelligent Systems, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Tübingen","address":{"name":"University of Tübingen, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Javier Romero","affiliation":[{"name":"Max Planck Institute for Intelligent Systems","address":{"name":"Max Planck Institute for Intelligent Systems, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Michael J. Black","affiliation":[{"name":"Max Planck Institute for Intelligent Systems","address":{"name":"Max Planck Institute for Intelligent Systems, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"keywords":"3D body shape, Human pose, 2D to 3D, CNN","description":"We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.","datePublished":"2016","isAccessibleForFree":true,"@type":"ScholarlyArticle","@context":"https://schema.org"}
  </script>
  <style type="text/css">
   .c-cookie-banner {
			background-color: #01324b;
			color: white;
			font-size: 1rem;
			position: fixed;
			bottom: 0;
			left: 0;
			right: 0;
			padding: 16px 0;
			font-family: sans-serif;
			z-index: 100002;
			text-align: center;
		}
		.c-cookie-banner__container {
			margin: 0 auto;
			max-width: 1280px;
			padding: 0 16px;
		}
		.c-cookie-banner p {
			margin-bottom: 8px;
		}
		.c-cookie-banner p:last-child {
			margin-bottom: 0;
		}	
		.c-cookie-banner__dismiss {
			background-color: transparent;
			border: 0;
			padding: 0;
			margin-left: 4px;
			color: inherit;
			text-decoration: underline;
			font-size: inherit;
		}
		.c-cookie-banner__dismiss:hover {
			text-decoration: none;
		}
  </style>
  <style type="text/css">
   .MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
  </style>
  <style type="text/css">
   #MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
  </style>
  <style type="text/css">
   .MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
  </style>
  <style type="text/css">
   #MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
  </style>
  <style type="text/css">
   .MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
  </style>
  <style type="text/css">
   .MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <script>
   window.dataLayer = window.dataLayer || [];
            window.dataLayer.push({
                recommendations: {
                    recommender: 'semantic',
                    model: 'specter',
                    policy_id: 'NA',
                    timestamp: 1698023142,
                    embedded_user: 'null'
                }
            });
  </script>
 </head>
 <body class="shared-article-renderer">
  <div id="MathJax_Message" style="">
   Loading [MathJax]/jax/output/HTML-CSS/config.js
  </div>
  <!-- Google Tag Manager (noscript) -->
  <noscript data-test="gtm-body">
   <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0">
   </iframe>
  </noscript>
  <!-- End Google Tag Manager (noscript) -->
  <div class="u-vh-full">
   <a class="c-skip-link" href="#main-content">
    Skip to main content
   </a>
   <div class="u-hide u-show-following-ad">
   </div>
   <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
    <div class="c-ad__inner">
     <p class="c-ad__label">
      Advertisement
     </p>
     <div data-gpt="" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;" data-gpt-unitpath="/270604982/springerlink/book/chapter" data-pa11y-ignore="" data-test="LB1-ad" id="div-gpt-ad-LB1" style="min-width:728px;min-height:90px">
     </div>
    </div>
   </aside>
   <div class="app-elements u-mb-24">
    <header class="c-header" data-header="">
     <div class="c-header__container" data-header-expander-anchor="">
      <div class="c-header__brand">
       <a data-test="logo" data-track="click" data-track-action="click logo link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu">
        <img alt="SpringerLink" src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg"/>
       </a>
      </div>
      <a class="c-header__link c-header__link--static" data-test="login-link" data-track="click" data-track-action="click log in link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-46454-1_34">
       <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
        <use xlink:href="#icon-eds-user-single">
        </use>
       </svg>
       <span>
        Log in
       </span>
      </a>
     </div>
     <nav aria-label="header navigation" class="c-header__nav">
      <div class="c-header__nav-container">
       <div class="c-header__item c-header__item--menu">
        <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
         <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
          <use xlink:href="#icon-eds-menu">
          </use>
         </svg>
         <span>
          Menu
         </span>
        </a>
       </div>
       <div class="c-header__item c-header__item--inline-links">
        <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
         Find a journal
        </a>
        <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
         Publish with us
        </a>
       </div>
       <div class="c-header__link-container">
        <div class="c-header__item c-header__item--divider">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
          <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span>
           Search
          </span>
         </a>
        </div>
        <div class="c-header__item">
         <div class="c-header__item ecommerce-cart" id="ecommerce-header-cart-icon-link" style="display:inline-block">
          <a class="c-header__link" href="https://order-springer-com.proxy.lib.ohio-state.edu/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
           <svg aria-hidden="true" focusable="false" height="24" id="eds-i-cart" style="vertical-align:bottom" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <path d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z" fill="currentColor" fill-rule="nonzero">
            </path>
           </svg>
           <span style="padding-left:10px">
            Cart
           </span>
           <span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center">
           </span>
          </a>
          <script>
           (function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    console.log("listen to updatedCart event");
    document.body.addEventListener("updatedCart", function () {
        console.log("updatedCart happened");
        updateCartIcon().then(function () { return console.log("Cart state update upon event"); });
    }, false);
    return updateCartIcon().then(function () { return console.log("Initial cart state update"); });
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function () { return console.log("Could not fetch cart info"); });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart-springer-com.proxy.lib.ohio-state.edu/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()
          </script>
         </div>
        </div>
       </div>
      </div>
     </nav>
    </header>
    <div class="c-header__expander has-tethered u-js-hide" hidden="" id="popup-search">
     <h2 class="c-header__heading">
      Search
     </h2>
     <div class="u-container">
      <div class="c-header__search">
       <form action="//link-springer-com.proxy.lib.ohio-state.edu/search" data-track="submit" data-track-action="submit search form" data-track-category="unified header" data-track-label="form" method="GET" role="search">
        <label class="c-header__search-label" for="header-search">
         Search by keyword or author
        </label>
        <div class="c-header__search-container">
         <input autocomplete="off" class="c-header__search-input" id="header-search" name="query" required="" type="text" value=""/>
         <button class="c-header__search-button" type="submit">
          <svg aria-hidden="true" class="c-header__icon" focusable="false">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span class="u-visually-hidden">
           Search
          </span>
         </button>
        </div>
       </form>
      </div>
     </div>
    </div>
    <div class="c-header__expander c-header__expander--menu has-tethered u-js-hide" hidden="" id="header-nav">
     <h2 class="c-header__heading">
      Navigation
     </h2>
     <ul class="c-header__list">
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
        Find a journal
       </a>
      </li>
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
        Publish with us
       </a>
      </li>
     </ul>
    </div>
   </div>
   <div class="u-container u-mb-32 u-clearfix" data-component="article-container" id="main-content">
    <div class="u-hide-at-lg js-context-bar-sticky-point-mobile">
     <div class="c-pdf-container">
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-319-46454-1.pdf?pdf=button" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book PDF
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-319-46454-1.epub" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book EPUB
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
     </div>
    </div>
    <header class="u-mb-24" data-test="chapter-information-header">
     <div class="c-box c-box--shadowed">
      <div class="c-app-header">
       <div class="c-app-header__theme" style="background-image: url('https://media-springernature-com.proxy.lib.ohio-state.edu/dominant-colour/springer-static/cover/book/978-3-319-46454-1.jpg')">
       </div>
       <div class="c-app-header__content">
        <div class="c-app-header__main">
         <div class="c-app-header__cover">
          <div class="c-app-expand-overlay-wrapper">
           <a data-component="cover-zoom" data-img-src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-319-46454-1" href="/chapter/10.1007/978-3-319-46454-1_34/cover" rel="nofollow">
            <picture>
             <source srcset="                                                         //media.springernature.com/w92/springer-static/cover/book/978-3-319-46454-1.jpg?as=webp 1x,                                                         //media.springernature.com/w184/springer-static/cover/book/978-3-319-46454-1.jpg?as=webp 2x" type="image/webp"/>
             <img alt="Book cover" height="130" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92/springer-static/cover/book/978-3-319-46454-1.jpg" width="90"/>
            </picture>
            <svg aria-hidden="true" class="c-expand-overlay u-icon" data-component="expand-icon" focusable="false" height="18" width="18">
             <use xlink:href="#icon-expand-image" xmlns:xlink="http://www.w3.org/1999/xlink">
             </use>
            </svg>
           </a>
          </div>
         </div>
         <div class="c-cover-image-lightbox u-hide" data-component="cover-lightbox">
          <button aria-label="Close expanded book cover" class="js-cover-image-lightbox--close" data-component="close-cover-lightbox" type="button">
           <span aria-hidden="true">
            ×
           </span>
          </button>
          <picture>
           <source srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-319-46454-1?as=webp" type="image/webp"/>
           <img alt="Book cover" class="c-cover-image-lightbox__image" height="1200" src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-319-46454-1" width="800"/>
          </picture>
         </div>
         <div class="u-flex-shrink">
          <p class="c-chapter-info-details u-mb-8">
           <a data-track="click" data-track-action="open conference" data-track-label="link" href="/conference/eccv eccv">
            European Conference on Computer Vision
           </a>
          </p>
          <p class="c-chapter-book-details right-arrow">
           ECCV 2016:
           <a class="c-chapter-book-details__title" data-test="book-link" data-track="click" data-track-action="open book series" data-track-label="link" href="/book/10.1007/978-3-319-46454-1">
            Computer Vision – ECCV 2016
           </a>
           pp
                                         561–578
           <a class="c-chapter-book-details__cite-as u-hide-print" data-track="click" data-track-action="cite this chapter" data-track-label="link" href="#citeas">
            Cite as
           </a>
          </p>
         </div>
        </div>
       </div>
      </div>
     </div>
    </header>
    <nav aria-label="breadcrumbs" class="u-mb-16" data-test="article-breadcrumbs">
     <ol class="c-breadcrumbs c-breadcrumbs--truncated" itemscope="" itemtype="https://schema.org/BreadcrumbList">
      <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb1" href="/" itemprop="item">
        <span itemprop="name">
         Home
        </span>
       </a>
       <meta content="1" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb2" href="/book/10.1007/978-3-319-46454-1" itemprop="item">
        <span itemprop="name">
         Computer Vision – ECCV 2016
        </span>
       </a>
       <meta content="2" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <span itemprop="name">
        Conference paper
       </span>
       <meta content="3" itemprop="position"/>
      </li>
     </ol>
    </nav>
    <main class="c-article-main-column u-float-left js-main-column u-text-sans-serif" data-track-component="conference paper">
     <div aria-hidden="true" class="c-context-bar u-hide" data-context-bar="" data-context-bar-with-recommendations="" data-test="context-bar">
      <div class="c-context-bar__container u-container">
       <div class="c-context-bar__title">
        Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image
       </div>
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-319-46454-1.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-319-46454-1.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div id="recommendations">
       <div class="c-recommendations__container u-container u-display-none" data-component-recommendations="">
        <aside class="c-status-message c-status-message--success u-display-none" data-component-status-msg="">
         <svg aria-label="success:" class="c-status-message__icon" focusable="false" height="24" role="img" width="24">
          <use xlink:href="#icon-success">
          </use>
         </svg>
         <div class="c-status-message__message" id="success-message" tabindex="-1">
          Your content has downloaded
         </div>
        </aside>
        <div class="c-recommendations-header u-display-flex u-justify-content-space-between">
         <h2 class="c-recommendations-title" id="recommendation-heading">
          Similar content being viewed by others
         </h2>
         <button aria-label="Close" class="c-recommendations-close u-flex-static" data-track="click" data-track-action="close recommendations" type="button">
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-close">
           </use>
          </svg>
         </button>
        </div>
        <section aria-labelledby="recommendation-heading" aria-roledescription="carousel">
         <p class="u-visually-hidden">
          Slider with three content items shown per slide. Use the Previous and Next buttons to navigate the slides or the slide controller buttons at the end to navigate through each slide.
         </p>
         <div class="c-recommendations-list-container">
          <div class="c-recommendations-list">
           <div aria-label="Recommendation 1 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58565-5?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 1" data-track-label="10.1007/978-3-030-58565-5_44" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58565-5_44" itemprop="url">
                  Human Body Model Fitting by Learned Gradient Descent
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 2 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-01234-2?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 2" data-track-label="10.1007/978-3-030-01234-2_2" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-01234-2_2" itemprop="url">
                  BodyNet: Volumetric Inference of 3D Human Body Shapes
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2018
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 3 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11554-022-01214-2/MediaObjects/11554_2022_1214_Fig1_HTML.jpg"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 3" data-track-label="10.1007/s11554-022-01214-2" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11554-022-01214-2" itemprop="url">
                  SimpleMeshNet: end to end recovery of 3d body mesh with one fully connected layer
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  28 April 2022
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Wenzhang Sun, Shaopeng Ma, … Qinwei Ma
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 4 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58539-6?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 4" data-track-label="10.1007/978-3-030-58539-6_36" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58539-6_36" itemprop="url">
                  STAR: Sparse Trained Articulated Human Body Regressor
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 5 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58607-2?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 5" data-track-label="10.1007/978-3-030-58607-2_2" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58607-2_2" itemprop="url">
                  Monocular Expressive Body Regression Through Body-Driven Attention
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 6 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58539-6?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 6" data-track-label="10.1007/978-3-030-58539-6_28" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58539-6_28" itemprop="url">
                  Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 7 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-01240-3?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 7" data-track-label="10.1007/978-3-030-01240-3_41" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-01240-3_41" itemprop="url">
                  Learning 3D Human Pose from Structure and Motion
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2018
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 8 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-20086-1?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 8" data-track-label="10.1007/978-3-031-20086-1_33" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-20086-1_33" itemprop="url">
                  SUPR: A Sparse Unified Part-Based Human Representation
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2022
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 9 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58545-7?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 9" data-track-label="10.1007/978-3-030-58545-7_17" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58545-7_17" itemprop="url">
                  3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
          </div>
         </div>
        </section>
       </div>
       <div class="js-greyout-page-background" data-component-grey-background="" style="display:none">
       </div>
      </div>
     </div>
     <article lang="en">
      <header data-test="chapter-detail-header">
       <div class="c-article-header">
        <h1 class="c-article-title" data-chapter-title="" data-test="chapter-title">
         Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image
        </h1>
        <ul class="c-article-author-list c-article-author-list--short js-no-scroll" data-component-authors-activator="authors-list" data-test="authors-list">
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Federica-Bogo" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Federica-Bogo">
           Federica Bogo
           <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
            <use xlink:href="#icon-email" xmlns:xlink="http://www.w3.org/1999/xlink">
            </use>
           </svg>
          </a>
          <sup class="u-js-hide">
           <a href="#Aff18" tabindex="-1">
            18
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Angjoo-Kanazawa" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Angjoo-Kanazawa">
           Angjoo Kanazawa
          </a>
          <sup class="u-js-hide">
           <a href="#Aff19" tabindex="-1">
            19
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Christoph-Lassner" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Christoph-Lassner">
           Christoph Lassner
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
           ,
           <a href="#Aff20" tabindex="-1">
            20
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Peter-Gehler" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Peter-Gehler">
           Peter Gehler
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
           ,
           <a href="#Aff20" tabindex="-1">
            20
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Javier-Romero" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Javier-Romero">
           Javier Romero
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
          </sup>
          &amp;
         </li>
         <li aria-label="Show all 6 authors for this article" class="c-article-author-list__show-more" title="Show all 6 authors for this article">
          …
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Michael_J_-Black" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Michael_J_-Black">
           Michael J. Black
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
          </sup>
         </li>
        </ul>
        <button aria-expanded="false" class="c-article-author-list__button">
         <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
          <use xlink:href="#icon-plus" xmlns:xlink="http://www.w3.org/1999/xlink">
          </use>
         </svg>
         <span>
          Show authors
         </span>
        </button>
        <ul class="c-article-identifiers c-chapter-identifiers">
         <li class="c-article-identifiers__item" data-test="article-category">
          Conference paper
         </li>
         <li class="c-article-identifiers__item">
          <a data-track="click" data-track-action="publication date" data-track-label="link" href="#chapter-info">
           First Online:
           <time datetime="2016-09-16">
            16 September 2016
           </time>
          </a>
         </li>
        </ul>
        <div data-test="article-metrics">
         <div id="altmetric-container">
          <div class="c-article-metrics-bar__wrapper u-clear-both">
           <ul class="c-article-metrics-bar u-list-reset">
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              19k
              <span class="c-article-metrics-bar__label">
               Accesses
              </span>
             </p>
            </li>
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              10
              <a class="c-article-metrics-bar__label" data-track="click" data-track-action="Social mentions" data-track-label="link" href="https://link.altmetric.com/details/12138771" rel="noopener" target="_blank" title="Visit Altmetric for full social mention details">
               Altmetric
              </a>
             </p>
            </li>
           </ul>
          </div>
         </div>
        </div>
        <p class="c-chapter-book-series">
         Part of the
         <a data-track="click" data-track-action="open book series" data-track-label="link" href="/bookseries/558">
          Lecture Notes in Computer Science
         </a>
         book series (LNIP,volume 9909)
        </p>
       </div>
      </header>
      <div class="c-article-body" data-article-body="true">
       <section aria-labelledby="Abs1" data-title="Abstract" lang="en">
        <div class="c-article-section" id="Abs1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">
          <span class="c-article-section__title-number">
          </span>
          Abstract
         </h2>
         <div class="c-article-section__content" id="Abs1-content">
          <p>
           We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.
          </p>
          <h3 class="c-article__sub-heading">
           Keywords
          </h3>
          <ul class="c-article-subject-list">
           <li class="c-article-subject-list__subject">
            <span>
             3D body shape
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Human pose
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             2D to 3D
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             CNN
            </span>
           </li>
          </ul>
         </div>
        </div>
       </section>
       <div class="c-article-section__content c-article-section__content--separator">
        <p>
         F. Bogo and A. Kanazawa—The first two authors contributed equally to this work. The work was performed at the MPI for Intelligent Systems.
        </p>
       </div>
       <div data-test="chapter-cobranding-and-download">
        <div class="note test-pdf-link" id="cobranding-and-download-availability-text">
         <div class="c-article-access-provider" data-component="provided-by-box">
          <p class="c-article-access-provider__text">
           <a class="c-pdf-download__link" data-track="click" data-track-action="Pdf download" data-track-label="inline link" download="" href="/content/pdf/10.1007/978-3-319-46454-1_34.pdf?pdf=inline%20link" id="js-body-chapter-download" rel="noopener" style="display: inline; padding:0px!important;" target="_blank">
            Download
           </a>
           conference paper PDF
          </p>
         </div>
        </div>
       </div>
       <div class="main-content">
        <section data-title="Introduction">
         <div class="c-article-section" id="Sec1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">
           <span class="c-article-section__title-number">
            1
           </span>
           Introduction
          </h2>
          <div class="c-article-section__content" id="Sec1-content">
           <p>
            The estimation of 3D human pose from a single image is a longstanding problem with many applications. Most previous approaches focus only on pose and ignore 3D human shape. Here we provide a solution that is
            <i>
             fully automatic
            </i>
            and estimates a 3D mesh capturing both pose and shape from a 2D image. We solve the problem in two steps. First we estimate 2D joints using a recently proposed convolutional neural network (CNN) called DeepCut [
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d171448241e996" title="Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P., Schiele, B.: DeepCut: joint subset partition and labeling for multi person pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4929–4937 (2016)">
             36
            </a>
            ]. So far CNNs have been successful at estimating 2D human pose [
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d171448241e999" title="Jain, A., Tompson, J., LeCun, Y., Bregler, C.: MoDeep: a deep learning framework using motion features for human pose estimation. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014. LNCS, vol. 9004, pp. 302–315. Springer, Heidelberg (2015). doi:
                      10.1007/978-3-319-16808-1_21
                      
                    
        ">
             20
            </a>
            ,
            <a aria-label="Reference 34" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR34" id="ref-link-section-d171448241e1002" title="Pfister, T., Charles, J., Zisserman, A.: Flowing convnets for human pose estimation in videos. In: IEEE International Conference on Computer Vision, ICCV, pp. 1913–1921 (2015)">
             34
            </a>
            –
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d171448241e1005" title="Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P., Schiele, B.: DeepCut: joint subset partition and labeling for multi person pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4929–4937 (2016)">
             36
            </a>
            ,
            <a aria-label="Reference 51" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR51" id="ref-link-section-d171448241e1009" title="Toshev, A., Szegedy, C.: DeepPose: human pose estimation via deep neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1653–1660 (2014)">
             51
            </a>
            ] but not 3D pose and shape from one image. Consequently we add a second step, which estimates 3D pose and shape from the 2D joints using a 3D generative model called SMPL [
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d171448241e1012" title="Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-person linear model. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia 34(6), 248: 1–248: 16 (2015)">
             30
            </a>
            ]. The overall framework, which we call “SMPLify”, fits within a classical paradigm of bottom up estimation (CNN) followed by top down verification (generative model). A few examples are shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            .
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 1." id="figure-1">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46454-1_34/figures/1" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig1_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 1" aria-describedby="Fig1" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig1_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc">
               <p>
                <b>
                 Example results.
                </b>
                3D pose and shape estimated by our method for two images from the Leeds Sports Pose Dataset [
                <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d171448241e1031" title="Johnson, S., Everingham, M.: Clustered pose and nonlinear appearance models for human pose estimation. In: Proceedings of the British Machine Vision Conference, pp. 12.1-12.11 (2010)">
                 22
                </a>
                ]. We show the original image (left), our fitted model (middle), and the 3D model rendered from a different viewpoint (right).
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/figures/1" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            There is a long literature on estimating 3D pose from 2D joints. Unlike previous methods, our approach exploits a high-quality 3D human body model that is trained from thousands of 3D scans and hence captures the statistics of shape variation in the population as well as how people deform with pose. Here we use the SMPL body model [
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d171448241e1046" title="Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-person linear model. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia 34(6), 248: 1–248: 16 (2015)">
             30
            </a>
            ]. The key insight is that such a model can be fit to very little data because it captures so much information of human body shape.
           </p>
           <p>
            We define an objective function and optimize pose and shape directly, so that the projected joints of the 3D model are close to the 2D joints estimated by the CNN. Remarkably, fitting only 2D joints produces plausible estimates of 3D body
            <i>
             shape
            </i>
            . We perform a quantitative evaluation using synthetic data and find that 2D joint locations contain a surprising amount of 3D shape information.
           </p>
           <p>
            In addition to capturing shape statistics, there is a second advantage to using a generative 3D model: it enables us to reason about interpenetration. Most previous work in the area has estimated 3D stick figures from 2D joints. With such models, it is easy to find poses that are impossible because the body parts would intersect in 3D. Such solutions are very common when inferring 3D from 2D because the loss of depth information makes the solution ambiguous.
           </p>
           <p>
            Computing interpenetration of a complex, non-convex, articulated object like the body, however, is expensive. Unlike previous work [
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR14" id="ref-link-section-d171448241e1062" title="Guan, P., Weiss, A., Balan, A., Black, M.J.: Estimating human shape and pose from a single image. In: IEEE International Conference on Computer Vision, ICCV, pp. 1381–1388 (2009)">
             14
            </a>
            ,
            <a aria-label="Reference 15" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR15" id="ref-link-section-d171448241e1065" title="Guan, P.: Virtual human bodies with clothing and hair: From images to animation. Ph.D. thesis, Brown University, Department of Computer Science, December 2012">
             15
            </a>
            ], we provide an interpenetration term that is differentiable with respect to body shape and pose. Given a 3D body shape we define a set of “capsules” that approximates the body shape. Crucially, capsule dimensions are linearly regressed from model shape parameters. This representation lets us compute interpenetration efficiently. We show that this term helps to prevent incorrect poses.
           </p>
           <p>
            SMPL is gender-specific; i.e. it distinguishes the shape space of females and males. To make our method fully automatic, we introduce a gender-neutral model. If we do not know the gender, we fit this model to images. If we know the gender, then we use a gender-specific model for better results.
           </p>
           <p>
            To deal with pose ambiguity, it is important to have a good pose prior. Many recent methods learn sparse, over-complete dictionaries from the CMU dataset [
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR3" id="ref-link-section-d171448241e1074" title="
                    
                      http://mocap.cs.cmu.edu
                      
                    
                  ">
             3
            </a>
            ] or learn dataset-specific priors. We train a prior over pose from SMPL models that have been fit to the CMU mocap
            <i>
             marker
            </i>
            data [
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR3" id="ref-link-section-d171448241e1080" title="
                    
                      http://mocap.cs.cmu.edu
                      
                    
                  ">
             3
            </a>
            ] using MoSh [
            <a aria-label="Reference 29" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR29" id="ref-link-section-d171448241e1083" title="Loper, M., Mahmood, N., Black, M.J.: MoSh: motion and shape capture from sparse markers. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia 33(6), 220:1–220:13 (2014)">
             29
            </a>
            ]. This factors shape from pose with pose represented as relative rotations of the body parts. We then learn a generic multi-modal pose prior from this.
           </p>
           <p>
            We compare the method to recently published methods [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d171448241e1089" title="Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3D human pose reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1446–1455 (2015)">
             4
            </a>
            ,
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d171448241e1092" title="Ramakrishna, V., Kanade, T., Sheikh, Y.: Reconstructing 3D human pose from 2D image landmarks. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7575, pp. 573–586. Springer, Heidelberg (2012). doi:
                      10.1007/978-3-642-33765-9_41
                      
                    
        ">
             39
            </a>
            ,
            <a aria-label="Reference 58" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR58" id="ref-link-section-d171448241e1095" title="Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparse representation for 3D shape estimation: a convex relaxation approach. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2015)">
             58
            </a>
            ] using the exact same 2D joints as input. We show the robustness of the approach qualitatively on images from the challenging Leeds Sports Pose Dataset (LSP) [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d171448241e1098" title="Johnson, S., Everingham, M.: Clustered pose and nonlinear appearance models for human pose estimation. In: Proceedings of the British Machine Vision Conference, pp. 12.1-12.11 (2010)">
             22
            </a>
            ] (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            ). We quantitatively compare the method on HumanEva-I [
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d171448241e1105" title="Sigal, L., Balan, A., Black, M.J.: HumanEva: synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. Int. J. Comput. Vis. IJCV 87(1), 4–27 (2010)">
             41
            </a>
            ] and Human3.6M [
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d171448241e1108" title="Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6M: large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE Trans. Pattern Anal. Mach. Intell. TPAMI 36(7), 1325–1339 (2014)">
             18
            </a>
            ], finding that our method is more accurate than previous methods.
           </p>
           <p>
            In summary our contributions are: (1) the first fully automatic method of estimating 3D body shape and pose from 2D joints; (2) an interpenetration term that is differentiable with respect to shape and pose; (3) a novel objective function that matches a 3D body model to 2D joints; (4) for research purposes, we provide the code, 2D joints, and 3D models for all examples in the paper [
            <a aria-label="Reference 1" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR1" id="ref-link-section-d171448241e1114" title="
                    
                      http://smplify.is.tue.mpg.de
                      
                    
                  ">
             1
            </a>
            ].
           </p>
          </div>
         </div>
        </section>
        <section data-title="Related Work">
         <div class="c-article-section" id="Sec2-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">
           <span class="c-article-section__title-number">
            2
           </span>
           Related Work
          </h2>
          <div class="c-article-section__content" id="Sec2-content">
           <p>
            The recovery of 3D human pose from 2D is fundamentally ambiguous and all methods deal with this ambiguity in different ways. These include user intervention, using rich image features, improving the optimization methods, and, most commonly, introducing prior knowledge. This prior knowledge typically includes both a “shape” prior that enforces anthropometric constraints on bone lengths and a “pose” prior that favors plausible poses and rules out impossible ones. While there is a large literature on estimating body pose and shape from multi-camera images or video sequences [
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d171448241e1125" title="Balan, A.O., Sigal, L., Black, M.J., Davis, J.E., Haussecker, H.W.: Detailed human shape and pose from images. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1–8 (2007)">
             6
            </a>
            ,
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR13" id="ref-link-section-d171448241e1128" title="Grest, D., Koch, R.: Human model fitting from monocular posture images. In: Proceedings of VMV, pp. 665–1344 (2005)">
             13
            </a>
            ,
            <a aria-label="Reference 19" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR19" id="ref-link-section-d171448241e1131" title="Jain, A., Thormählen, T., Seidel, H.P., Theobalt, C.: MovieReshape: tracking and reshaping of humans in videos. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH 29(5), 148:1–148:10 (2010)">
             19
            </a>
            ,
            <a aria-label="Reference 45" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR45" id="ref-link-section-d171448241e1134" title="Sminchisescu, C., Telea, A.: Human pose estimation from silhouettes, a consistent approach using distance level sets. In: WSCG International Conference for Computer Graphics, Visualization and Computer Vision, pp. 413–420 (2002)">
             45
            </a>
            ], here we focus on static image methods. We also focus on methods that do not require a background image for background subtraction, but rather infer 3D pose from 2D joints.
           </p>
           <p>
            Most methods formulate the problem as finding a 3D
            <i>
             skeleton
            </i>
            such that its 3D joints project to known or estimated 2D joints. Note that the previous work often refers to this skeleton in a particular posture as a “shape”. In this work we take shape to mean the pose-invariant surface of the human body in 3D and distinguish this from pose, which is the articulated posture of the limbs.
           </p>
           <p>
            <b>
             3D pose from 2D joints.
            </b>
            These methods all assume known correspondence between 2D joints and a 3D skeleton. Methods make different assumptions about the statistics of limb-length variation. Lee and Chen [
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d171448241e1149" title="Lee, H., Chen, Z.: Determination of 3D human body postures from a single view. Comput. Vis. Graph. Image Process. 30(2), 148–168 (1985)">
             26
            </a>
            ] assume known limb lengths of a stick figure while Taylor [
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d171448241e1152" title="Taylor, C.: Reconstruction of articulated objects from point correspondences in single uncalibrated image. Comput. Vis. Image Underst. CVIU 80(10), 349–363 (2000)">
             48
            </a>
            ] assumes the ratios of limb lengths are known. Parameswaran and Chellappa [
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR33" id="ref-link-section-d171448241e1155" title="Parameswaran, V., Chellappa, R.: View independent human body pose estimation from a single perspective image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 16–22 (2004)">
             33
            </a>
            ] assume that limb lengths are isometric across people, varying only in global scaling. Barron and Kakadiaris [
            <a aria-label="Reference 7" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR7" id="ref-link-section-d171448241e1158" title="Barron, C., Kakadiaris, I.: Estimating anthropometry and pose from a single uncalibrated image. Comput. Vis. Image Underst. CVIU 81(3), 269–284 (2001)">
             7
            </a>
            ] build a statistical model of limb-length variation from extremes taken from anthropometric tables. Jiang [
            <a aria-label="Reference 21" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR21" id="ref-link-section-d171448241e1162" title="Jiang, H.: 3D human pose reconstruction using millions of exemplars. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1674–1677 (2010)">
             21
            </a>
            ] takes a non-parametric approach, treating poses in the CMU dataset [
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR3" id="ref-link-section-d171448241e1165" title="
                    
                      http://mocap.cs.cmu.edu
                      
                    
                  ">
             3
            </a>
            ] as exemplars.
           </p>
           <p>
            Recent methods typically use the CMU dataset and learn a statistical model of limb lengths and poses from it. For example, both [
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d171448241e1171" title="Fan, X., Zheng, K., Zhou, Y., Wang, S.: Pose locality constrained representation for 3D human pose reconstruction. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8689, pp. 174–188. Springer, Heidelberg (2014). doi:
                      10.1007/978-3-319-10590-1_12
                      
                    
        ">
             11
            </a>
            ,
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d171448241e1174" title="Ramakrishna, V., Kanade, T., Sheikh, Y.: Reconstructing 3D human pose from 2D image landmarks. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7575, pp. 573–586. Springer, Heidelberg (2012). doi:
                      10.1007/978-3-642-33765-9_41
                      
                    
        ">
             39
            </a>
            ] learn a dictionary of poses but use a fairly weak anthropometric model on limb lengths. Akhter and Black [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d171448241e1177" title="Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3D human pose reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1446–1455 (2015)">
             4
            </a>
            ] take a similar approach but add a novel pose prior that captures pose-dependent joint angle limits. Zhou et al. [
            <a aria-label="Reference 58" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR58" id="ref-link-section-d171448241e1180" title="Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparse representation for 3D shape estimation: a convex relaxation approach. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2015)">
             58
            </a>
            ] also learn a shape dictionary but they create a sparse basis that also captures how these poses appear from different camera views. They show that the resulting optimization problem is easier to solve. Pons-Moll et al. [
            <a aria-label="Reference 37" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR37" id="ref-link-section-d171448241e1183" title="Pons-Moll, G., Fleet, D., Rosenhahn, B.: Posebits for monocular human pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2345–2352 (2014)">
             37
            </a>
            ] take a different approach: they estimate qualitative “posebits” from mocap and relate these to 3D pose.
           </p>
           <p>
            The above approaches have weak, or non-existent, models of human shape. In contrast, we argue that a stronger model of body shape, learned from thousands of people, captures the anthropometric constraints of the population. Such a model helps reduce ambiguity, making the problem easier. Also, because we have 3D shape, we can model interpenetration, avoiding impossible poses.
           </p>
           <p>
            <b>
             3D pose and shape.
            </b>
            There is also work on estimating 3D body shape from single images. This work often assumes good silhouettes are available. Sigal et al. [
            <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR42" id="ref-link-section-d171448241e1196" title="Sigal, L., Balan, A., Black, M.J.: Combined discriminative and generative articulated pose and non-rigid shape estimation. In: Advances in Neural Information Processing Systems (NIPS), vol. 20, pp. 1337–1344 (2008)">
             42
            </a>
            ] assume that silhouettes are given, compute shape features from them, and then use a mixture of experts to predict 3D body pose and shape from the features. Like us they view the problem as a combination of a bottom-up discriminative method and a top-down generative method. In their case the generative model (SCAPE [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d171448241e1199" title="Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: SCAPE: shape completion and animation of people. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH 24(3), 408–416 (2005)">
             5
            </a>
            ]) is fit to the image silhouettes. Their claim that the method is fully automatic is only true if silhouettes are available, which is often not the case. They show a limited set of results using perfect silhouettes and do not evaluate pose accuracy.
           </p>
           <p>
            Guan et al. [
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR14" id="ref-link-section-d171448241e1205" title="Guan, P., Weiss, A., Balan, A., Black, M.J.: Estimating human shape and pose from a single image. In: IEEE International Conference on Computer Vision, ICCV, pp. 1381–1388 (2009)">
             14
            </a>
            ,
            <a aria-label="Reference 15" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR15" id="ref-link-section-d171448241e1208" title="Guan, P.: Virtual human bodies with clothing and hair: From images to animation. Ph.D. thesis, Brown University, Department of Computer Science, December 2012">
             15
            </a>
            ] take manually marked 2D joints and first estimate the 3D pose of a stick figure using classical methods [
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d171448241e1211" title="Lee, H., Chen, Z.: Determination of 3D human body postures from a single view. Comput. Vis. Graph. Image Process. 30(2), 148–168 (1985)">
             26
            </a>
            ,
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d171448241e1214" title="Taylor, C.: Reconstruction of articulated objects from point correspondences in single uncalibrated image. Comput. Vis. Image Underst. CVIU 80(10), 349–363 (2000)">
             48
            </a>
            ]. They use the pose of this stick figure to pose a SCAPE model, project the model into the image and use this to segment the image with GrabCut [
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d171448241e1217" title="Rother, C., Kolmogorov, V., Blake, A.: Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH 23(3), 309–314 (2004)">
             40
            </a>
            ]. They then fit the SCAPE shape and pose to a variety of features including the silhouette, image edges, and shading cues. They assume the camera focal length is known or approximated, the lighting is roughly initialized, and that the height of the person is known. They use an interpenetration term that models each body part by its convex hull. They then check each of the extremities to see how many other body points fall inside it and define a penalty function that penalizes interpenetration. This does not admit easy optimization.
           </p>
           <p>
            In similar work, Hasler et al. [
            <a aria-label="Reference 16" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR16" id="ref-link-section-d171448241e1223" title="Hasler, N., Ackermann, H., Rosenhahn, B., Thormhlen, T., Seidel, H.P.: Multilinear pose and body shape estimation of dressed subjects from image sets. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1823–1830 (2010)">
             16
            </a>
            ] fit a parametric body model to silhouettes. Typically, they require a known segmentation and a few manually provided correspondences. In cases with simple backgrounds, they use four clicked points on the hands and feet to establish a rough fit and then use GrabCut to segment the person. They demonstrate this on one image. Zhou et al. [
            <a aria-label="Reference 57" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR57" id="ref-link-section-d171448241e1226" title="Zhou, S., Fu, H., Liu, L., Cohen-Or, D., Han, X.: Parametric reshaping of human bodies in images. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH 29(4), 126:1–126:10 (2010)">
             57
            </a>
            ] also fit a parametric model of body shape and pose to a cleanly segmented silhouette using significant manual intervention. Chen et al. [
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR9" id="ref-link-section-d171448241e1229" title="Chen, Y., Kim, T.-K., Cipolla, R.: Inferring 3D shapes and deformations from single views. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6313, pp. 300–313. Springer, Heidelberg (2010). doi:
                      10.1007/978-3-642-15558-1_22
                      
                    
        ">
             9
            </a>
            ] fit a parametric model of body shape and pose to manually extracted silhouettes; they do not evaluate quantitative accuracy.
           </p>
           <p>
            To our knowledge, no previous method estimates
            <i>
             3D body shape
            </i>
            and pose directly from only
            <i>
             2D joints
            </i>
            . A priori, it may seem impossible, but given a good statistical model, our approach works surprisingly well. This is enabled by our use of SMPL [
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d171448241e1241" title="Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-person linear model. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia 34(6), 248: 1–248: 16 (2015)">
             30
            </a>
            ], which unlike SCAPE, has explicit 3D joints; we fit their projection directly to 2D joints. SMPL defines how joint locations are related to the 3D surface of the body, enabling inference of shape from joints. Of course this will not be perfect as a person can have the exact same limb lengths with varying weight. SMPL, however, does not represent anatomical joints, rather it represents them as a function of the surface vertices. This couples joints and shape during model training and means that solving for them together is important.
           </p>
           <p>
            <b>
             Making it automatic.
            </b>
            None of the methods above are automatic, most assume known correspondences, and some involve significant manual intervention. There are, however, a few methods that try to solve the entire problem of inferring 3D pose from a single image.
           </p>
           <p>
            Simo-Serra et al. [
            <a aria-label="Reference 43" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR43" id="ref-link-section-d171448241e1254" title="Simo-Serra, E., Quattoni, A., Torras, C., Moreno-Noguer, F.: A joint model for 2D and 3D pose estimation from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3634–3641 (2013)">
             43
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d171448241e1257" title="Simo-Serra, E., Ramisa, A., Alenya, G., Torras, C., Moreno-Noguer, F.: Single image 3D human pose estimation from noisy observations. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2673–2680 (2012)">
             44
            </a>
            ] take into account that 2D part detections are unreliable and formulate a probabilistic model that estimates the 3D pose and the matches to the 2D image features together. Wang et al. [
            <a aria-label="Reference 52" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR52" id="ref-link-section-d171448241e1260" title="Wang, C., Wang, Y., Lin, Z., Yuille, A., Gao, W.: Robust estimation of 3D human poses from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2369–2376 (2014)">
             52
            </a>
            ] use a weak model of limb lengths [
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d171448241e1263" title="Lee, H., Chen, Z.: Determination of 3D human body postures from a single view. Comput. Vis. Graph. Image Process. 30(2), 148–168 (1985)">
             26
            </a>
            ] but exploit automatically detected joints in the image and match to them robustly using an L1 distance. They use a sparse basis to represent poses as in other methods.
           </p>
           <p>
            Zhou et al. [
            <a aria-label="Reference 56" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR56" id="ref-link-section-d171448241e1269" title="Zhou, F., Torre, F.: Spatio-temporal matching for human detection in video. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8694, pp. 62–77. Springer, Heidelberg (2014). doi:
                      10.1007/978-3-319-10599-4_5
                      
                    
        ">
             56
            </a>
            ] run a 2D pose detector [
            <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d171448241e1272" title="Yang, Y., Ramanan, D.: Articulated pose estimation using flexible mixtures of parts. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3546–3553 (2011)">
             54
            </a>
            ] and then optimize 3D pose, automatically rejecting outliers. Akhter and Black [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d171448241e1275" title="Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3D human pose reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1446–1455 (2015)">
             4
            </a>
            ] run a different 2D detector [
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d171448241e1278" title="Kiefel, M., Gehler, P.V.: Human pose estimation with fields of parts. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp. 331–346. Springer, Heidelberg (2014). doi:
                      10.1007/978-3-319-10602-1_22
                      
                    
        ">
             23
            </a>
            ] and show results for their method on a few images. Both methods are only evaluated qualitatively. Yasin et al. [
            <a aria-label="Reference 55" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR55" id="ref-link-section-d171448241e1281" title="Yasin, H., Iqbal, U., Krüger, B., Weber, A., Gall, J.: A dual-source approach for 3D pose estimation from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4948–4956 (2016)">
             55
            </a>
            ] take a non-parametric approach in which the detected 2D joints are used to look up the nearest 3D poses in a mocap dataset. Kostrikov and Gall [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d171448241e1285" title="Kostrikov, I., Gall, J.: Depth sweep regression forests for estimating 3D human pose from images. In: Proceedings of the British Machine Vision Conference (2014)">
             24
            </a>
            ] combine regression forests and a 3D pictorial model to regress 3D joints. Ionescu et al. [
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d171448241e1288" title="Ionescu, C., Carreira, J., Sminchisescu, C.: Iterated second-order label sensitive pooling for 3D human pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1661–1668 (2014)">
             17
            </a>
            ] train a method to predict 3D pose from images by first predicting body part labels; their results on Human3.6M are good but they do not test on complex images where background segmentation is not available. Kulkarni et al. [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d171448241e1291" title="Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.: Picture: a probabilistic programming language for scene perception. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4390–4399 (2015)">
             25
            </a>
            ] use a generative model of body shape and pose, together with a probabilistic programming framework to estimate body pose from single images. They deal with visually simple images, where the person is well centered and cropped, and do not evaluate 3D pose accuracy.
           </p>
           <p>
            Recent advances in deep learning are producing methods for estimating 2D joint positions accurately [
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d171448241e1297" title="Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P., Schiele, B.: DeepCut: joint subset partition and labeling for multi person pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4929–4937 (2016)">
             36
            </a>
            ,
            <a aria-label="Reference 53" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR53" id="ref-link-section-d171448241e1300" title="Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose machines. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4724–4732 (2016)">
             53
            </a>
            ]. We use the recent DeepCut method [
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d171448241e1303" title="Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P., Schiele, B.: DeepCut: joint subset partition and labeling for multi person pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4929–4937 (2016)">
             36
            </a>
            ], which gives remarkably good 2D detections. Recent work [
            <a aria-label="Reference 59" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR59" id="ref-link-section-d171448241e1306" title="Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2016)">
             59
            </a>
            ] uses a CNN to estimate 2D joint locations and then fit 3D pose to these using a monocular video sequence. They do not show results for single images.
           </p>
           <p>
            None of these automated methods estimate 3D body shape. Here we demonstrate a complete system that uses 2D joint detections and fits pose and shape to them from a single image.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Method">
         <div class="c-article-section" id="Sec3-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">
           <span class="c-article-section__title-number">
            3
           </span>
           Method
          </h2>
          <div class="c-article-section__content" id="Sec3-content">
           <p>
            Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            shows an overview of our system. We take a single input image, and use the DeepCut CNN [
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d171448241e1323" title="Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P., Schiele, B.: DeepCut: joint subset partition and labeling for multi person pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4929–4937 (2016)">
             36
            </a>
            ] to predict 2D body joints,
            <span class="mathjax-tex">
             \(J_{\mathrm {est}}\)
            </span>
            . For each 2D joint
            <i>
             i
            </i>
            the CNN provides a confidence value,
            <span class="mathjax-tex">
             \(w_i\)
            </span>
            . We then fit a 3D body model such that the projected joints of the model minimize a robust weighted error term. In this work we use a skinned vertex-based model, SMPL [
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d171448241e1379" title="Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-person linear model. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia 34(6), 248: 1–248: 16 (2015)">
             30
            </a>
            ], and call the system that takes a 2D image and produces a posed 3D mesh,
            <i>
             SMPLify
            </i>
            .
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 2." id="figure-2">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46454-1_34/figures/2" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig2_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 2" aria-describedby="Fig2" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig2_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc">
               <p>
                <b>
                 System overview.
                </b>
                Left to right: Given a single image, we use a CNN-based method to predict 2D joint locations (hot colors denote high confidence). We then fit a 3D body model to this, to estimate 3D body shape and pose. Here we show a fit on HumanEva [
                <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d171448241e1398" title="Sigal, L., Balan, A., Black, M.J.: HumanEva: synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. Int. J. Comput. Vis. IJCV 87(1), 4–27 (2010)">
                 41
                </a>
                ], projected into the image and shown from different viewpoints. (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/figures/2" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            The body model is defined as a function
            <span class="mathjax-tex">
             \(M(\mathbf {\beta },\mathbf {\theta },\mathbf {\gamma })\)
            </span>
            , parameterized by shape
            <span class="mathjax-tex">
             \({\mathbf {\beta }}\)
            </span>
            , pose
            <span class="mathjax-tex">
             \(\mathbf {\theta }\)
            </span>
            , and translation
            <span class="mathjax-tex">
             \(\mathbf {\gamma }\)
            </span>
            . The output of the function is a triangulated surface,
            <span class="mathjax-tex">
             \(\mathcal {M}\)
            </span>
            , with 6890 vertices. Shape parameters
            <span class="mathjax-tex">
             \(\mathbf {\beta }\)
            </span>
            are coefficients of a low-dimensional shape space, learned from a training set of thousands of registered scans. Here we use one of three shape models: male, female, and gender-neutral. SMPL defines only male and female models. For a fully automatic method, we trained a new gender-neutral model using the approximately 2000 male and 2000 female body shapes used to train the gendered SMPL models. If the gender is known, we use the appropriate model. The model used is indicated by its color: pink for gender-specific and light blue for gender-neutral.
           </p>
           <p>
            The pose of the body is defined by a skeleton rig with 23 joints; pose parameters
            <span class="mathjax-tex">
             \(\mathbf {\theta }\)
            </span>
            represent the axis-angle representation of the relative rotation between parts. Let
            <span class="mathjax-tex">
             \(J(\mathbf {\beta })\)
            </span>
            be the function that predicts 3D skeleton joint locations from body shape. In SMPL, joints are a sparse linear combination of surface vertices or, equivalently, a function of the shape coefficients. Joints can be put in arbitrary poses by applying a global rigid transformation. In the following, we denote posed 3D joints as
            <span class="mathjax-tex">
             \(R_\theta (J(\mathbf {\beta })_i)\)
            </span>
            , for joint
            <i>
             i
            </i>
            , where
            <span class="mathjax-tex">
             \(R_\theta \)
            </span>
            is the global rigid transformation induced by pose
            <span class="mathjax-tex">
             \(\mathbf {\theta }\)
            </span>
            . SMPL defines pose-dependent deformations; for the gender-neutral shape model, we use the female deformations, which are general enough in practice. Note that the SMPL model and DeepCut skeleton have slightly different joints. We associate DeepCut joints with the most similar SMPL joints. To project SMPL joints into the image we use a perspective camera model, defined by parameters
            <i>
             K
            </i>
            .
           </p>
           <h3 class="c-article__sub-heading" id="Sec4">
            <span class="c-article-section__title-number">
             3.1
            </span>
            Approximating Bodies with Capsules
           </h3>
           <p>
            We find that previous methods produce 3D poses that are impossible due to interpenetration between body parts. An advantage of our 3D shape model is that it allows us to detect and prevent this. Computing interpenetration however is expensive for complex, non-convex, surfaces like the body. In graphics it is common to use proxy geometries to compute collisions efficiently [
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d171448241e1725" title="Ericson, C.: Real-Time Collision Detection. The Morgan Kaufmann Series in Interactive 3-D Technology (2004)">
             10
            </a>
            ,
            <a aria-label="Reference 50" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR50" id="ref-link-section-d171448241e1728" title="Thiery, J.M., Guy, E., Boubekeur, T.: Sphere-meshes: shape approximation using spherical quadric error metrics. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia 32(6), 178:1–178:12 (2013)">
             50
            </a>
            ]. We follow this approach and approximate the body surface as a set of “capsules” (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            ). Each capsule has a radius and an axis length.
           </p>
           <p>
            We train a regressor from model shape parameters to capsule parameters (axis length and radius), and pose the capsules according to
            <span class="mathjax-tex">
             \(R_\theta \)
            </span>
            , the rotation induced by the kinematic chain. Specifically, we first fit 20 capsules, one per body part, excluding fingers and toes, to the body surface of the unposed training body shapes used to learn SMPL [
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d171448241e1762" title="Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-person linear model. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia 34(6), 248: 1–248: 16 (2015)">
             30
            </a>
            ]. Starting from capsules manually attached to body joints in the template, we perform gradient-based optimization of their radii and axis lengths to minimize the bidirectional distance between capsules and body surface. We then learn a linear regressor from body shape coefficients,
            <span class="mathjax-tex">
             \(\mathbf {\beta }\)
            </span>
            , to the capsules’ radii and axis lengths using cross-validated ridge regression. Once the regressor is trained, the procedure is iterated once more, initializing the capsules with the regressor output. While previous work uses approximations to detect interpenetrations [
            <a aria-label="Reference 38" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR38" id="ref-link-section-d171448241e1785" title="Pons-Moll, G., Taylor, J., Shotton, J., Hertzmann, A., Fitzgibbon, A.: Metric regression forests for correspondence estimation. Int. J. Comput. Vis. IJCV 113(3), 1–13 (2015)">
             38
            </a>
            ,
            <a aria-label="Reference 46" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR46" id="ref-link-section-d171448241e1788" title="Sminchisescu, C., Triggs, B.: Covariance scaled sampling for monocular 3D body tracking. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 447–454 (2001)">
             46
            </a>
            ], we believe this regression from shape parameters is novel.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 3." id="figure-3">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46454-1_34/figures/3" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig3_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 3" aria-describedby="Fig3" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig3_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc">
               <p>
                <b>
                 Body shape approximation with capsules.
                </b>
                Shown for two subjects. Left to right: original shape, shape approximated with capsules, capsules reposed. Yellow point clouds represent actual vertices of the model that is approximated.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/figures/3" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec5">
            <span class="c-article-section__title-number">
             3.2
            </span>
            Objective Function
           </h3>
           <p>
            To fit the 3D pose and shape to the CNN-detected 2D joints, we minimize an objective function that is the sum of five error terms: a joint-based data term, three pose priors, and a shape prior; that is
            <span class="mathjax-tex">
             \(E(\mathbf {\beta }, \mathbf {\theta }) = \)
            </span>
           </p>
           <div class="c-article-equation" id="Equ1">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} E_J(\mathbf {\beta }, \mathbf {\theta }; K, J_{\mathrm {est}}) + \lambda _{\theta } E_{\theta }(\mathbf {\theta }) + \lambda _a E_a (\mathbf {\theta }) + {\lambda _{sp}} E_{sp}(\mathbf {\theta }; \mathbf {\beta }) + \lambda _\beta E_\beta (\mathbf {\beta }) \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (1)
            </div>
           </div>
           <p>
            where
            <i>
             K
            </i>
            are camera parameters and
            <span class="mathjax-tex">
             \(\lambda _{\theta }\)
            </span>
            ,
            <span class="mathjax-tex">
             \(\lambda _a\)
            </span>
            ,
            <span class="mathjax-tex">
             \(\lambda _{sp}\)
            </span>
            <span class="mathjax-tex">
             \(\lambda _\beta \)
            </span>
            are scalar weights.
           </p>
           <p>
            Our joint-based data term penalizes the weighted 2D distance between estimated joints,
            <span class="mathjax-tex">
             \(J_{\mathrm {est}}\)
            </span>
            , and corresponding projected SMPL joints:
           </p>
           <div class="c-article-equation" id="Equ2">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} E_J(\mathbf {\beta }, \mathbf {\theta }; K, J_{\mathrm {\mathrm {est}}}) = \sum _{\mathrm {joint}\,i} w_i \rho (\varPi _{K}(R_\theta (J(\mathbf {\beta })_i)) - J_{\mathrm {est},i}) \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (2)
            </div>
           </div>
           <p>
            where
            <span class="mathjax-tex">
             \(\varPi _K\)
            </span>
            is the projection from 3D to 2D induced by a camera with parameters
            <i>
             K
            </i>
            . We weight the contribution of each joint by the confidence of its estimate,
            <span class="mathjax-tex">
             \(w_i\)
            </span>
            , provided by the CNN. For occluded joints, this value is usually low; pose in this case is driven by our pose priors. To deal with noisy estimates, we use a robust differentiable Geman-McClure penalty function,
            <span class="mathjax-tex">
             \(\rho \)
            </span>
            , [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d171448241e2466" title="Geman, S., McClure, D.: Statistical methods for tomographic image reconstruction. Bull. Int. Stat. Inst. 52(4), 5–21 (1987)">
             12
            </a>
            ].
           </p>
           <p>
            We introduce a pose prior penalizing elbows and knees that bend unnaturally:
           </p>
           <div class="c-article-equation" id="Equ3">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} E_a (\mathbf {\theta }) = \sum _{i} \mathrm {exp}{(\mathbf {\theta }_i)}, \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (3)
            </div>
           </div>
           <p>
            where
            <i>
             i
            </i>
            sums over pose parameters (rotations) corresponding to the bending of knees and elbows. The exponential strongly penalizes rotations violating natural constraints (e.g. elbow and knee hyperextending). Note that when the joint is not bent,
            <span class="mathjax-tex">
             \(\theta _i\)
            </span>
            is zero. Negative bending is natural and is not penalized heavily while positive bending is unnatural and is penalized more.
           </p>
           <p>
            Most methods for 3D pose estimation use some sort of pose prior to favor probable poses over improbable ones. Like many previous methods we train our pose prior using the CMU dataset [
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR3" id="ref-link-section-d171448241e2588" title="
                    
                      http://mocap.cs.cmu.edu
                      
                    
                  ">
             3
            </a>
            ]. Given that poses vary significantly, it is important to represent the multi-modal nature of the data, yet also keep the prior computationally tractable. To build a prior, we use poses obtained by fitting SMPL to the CMU marker data using MoSh [
            <a aria-label="Reference 29" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR29" id="ref-link-section-d171448241e2591" title="Loper, M., Mahmood, N., Black, M.J.: MoSh: motion and shape capture from sparse markers. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia 33(6), 220:1–220:13 (2014)">
             29
            </a>
            ]. We then fit a mixture of Gaussians to approximately 1 million poses, spanning 100 subjects. Using the mixture model directly in our optimization framework is problematic computationally because we need to optimize the negative logarithm of a sum. As described in [
            <a aria-label="Reference 32" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR32" id="ref-link-section-d171448241e2594" title="Olson, E., Agarwal, P.: Inference on networks of mixtures for robust robot mapping. Int. J. Robot. Res. 32(7), 826–840 (2013)">
             32
            </a>
            ], we approximate the sum in the mixture of Gaussians by a max operator:
           </p>
           <div class="c-article-equation" id="Equ4">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} E_\theta (\mathbf {\theta }) \equiv -\log \sum _j(g_j \mathcal {N}(\mathbf {\theta };\mathbf {\mu }_{\theta ,j}, \Sigma _{\theta ,j}))&amp;\approx -\log (\max _j(c g_j \mathcal {N}(\mathbf {\theta };\mathbf {\mu }_{\theta ,j}, \Sigma _{\theta ,j}))) \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (4)
            </div>
           </div>
           <div class="c-article-equation" id="Equ5">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned}&amp;= \min _j \left( -\log (c g_j \mathcal {N}(\mathbf {\theta };\mathbf {\mu }_{\theta ,j}, \Sigma _{\theta ,j})) \right) \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (5)
            </div>
           </div>
           <p>
            where
            <span class="mathjax-tex">
             \(g_j\)
            </span>
            are the mixture model weights of
            <span class="mathjax-tex">
             \({N=8}\)
            </span>
            Gaussians, and
            <i>
             c
            </i>
            a positive constant required by our solver implementation. Although
            <span class="mathjax-tex">
             \(E_\theta \)
            </span>
            is not differentiable at points where the mode with minimum energy changes, we approximate its Jacobian by the Jacobian of the mode with minimum energy in the current optimization step.
           </p>
           <p>
            We define an interpenetration error term that exploits the capsule approximation introduced in Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec4">
             3.1
            </a>
            . We relate the error term to the intersection volume between “incompatible” capsules (i.e. capsules that do not intersect in natural poses). Since the volume of capsule intersections is not simple to compute, we further simplify our capsules into spheres with centers
            <span class="mathjax-tex">
             \(C(\mathbf {\theta },\mathbf {\beta })\)
            </span>
            along the capsule axis and radius
            <span class="mathjax-tex">
             \(r(\mathbf {\beta })\)
            </span>
            corresponding to the capsule radius. Our penalty term is inspired by the mixture of 3D Gaussians model in [
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d171448241e3106" title="Sridhar, S., Mueller, F., Oulasvirta, A., Theobalt, C.: Fast and robust hand tracking using detection-guided optimization. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3121–3221 (2015)">
             47
            </a>
            ]. We consider a 3D isotropic Gaussian with
            <span class="mathjax-tex">
             \(\sigma (\mathbf {\beta }) = \frac{r(\mathbf {\beta })}{3}\)
            </span>
            for each sphere, and define the penalty as a scaled version of the integral of the product of Gaussians corresponding to “incompatible” parts
           </p>
           <div class="c-article-equation" id="Equ6">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} E_{sp}(\mathbf {\theta }; \mathbf {\beta }) = \sum _{i} \sum _{j \in I(i)} \mathrm {exp}\left( \frac{||C_{i}(\mathbf {\theta },\mathbf {\beta })-C_{j}(\mathbf {\theta },\mathbf {\beta })||^2}{\sigma ^2_{i}(\mathbf {\beta })+\sigma ^2_{j}(\mathbf {\beta })}\right) \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (6)
            </div>
           </div>
           <p>
            where the summation is over all spheres
            <i>
             i
            </i>
            and
            <i>
             I
            </i>
            (
            <i>
             i
            </i>
            ) are the spheres incompatible with
            <i>
             i
            </i>
            . Note that the term penalizes, but does not strictly avoid, interpenetrations. As desired, however, this term is differentiable with respect to pose and shape. Note also that we do not use this term in optimizing shape since this would bias the body shape to be thin to avoid interpenetration.
           </p>
           <p>
            We use a shape prior
            <span class="mathjax-tex">
             \(E_\beta (\mathbf {\beta })\)
            </span>
            , defined as
           </p>
           <div class="c-article-equation" id="Equ7">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              $$\begin{aligned} E_\beta (\mathbf {\beta }) = \mathbf {\beta }^T\Sigma ^{-1}_\beta \mathbf {\beta } \end{aligned}$$
             </span>
            </div>
            <div class="c-article-equation__number">
             (7)
            </div>
           </div>
           <p>
            where
            <span class="mathjax-tex">
             \(\Sigma ^{-1}_\beta \)
            </span>
            is a diagonal matrix with the squared singular values estimated via Principal Component Analysis from the shapes in the SMPL training set. Note that the shape coefficients
            <span class="mathjax-tex">
             \(\mathbf {\beta }\)
            </span>
            are zero-mean by construction.
           </p>
           <h3 class="c-article__sub-heading" id="Sec6">
            <span class="c-article-section__title-number">
             3.3
            </span>
            Optimization
           </h3>
           <p>
            We assume that camera translation and body orientation are unknown; we require, however, that the camera focal length or its rough estimate is known. We initialize the camera translation (equivalently
            <span class="mathjax-tex">
             \(\mathbf {\gamma }\)
            </span>
            ) by assuming that the person is standing parallel to the image plane. Specifically, we estimate the depth via the ratio of similar triangles, defined by the torso length of the mean SMPL shape and the predicted 2D joints. Since this assumption is not always true, we further refine this estimate by minimizing
            <span class="mathjax-tex">
             \(E_J\)
            </span>
            over the torso joints alone with respect to camera translation and body orientation; we keep
            <span class="mathjax-tex">
             \(\mathbf {\beta }\)
            </span>
            fixed to the mean shape during this optimization. We do not optimize focal length, since the problem is too unconstrained to optimize it together with translation.
           </p>
           <p>
            After estimating camera translation, we fit our model by minimizing Eq. (
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            ) in a staged approach. We observed that starting with a high value for
            <span class="mathjax-tex">
             \(\lambda _{\theta }\)
            </span>
            and
            <span class="mathjax-tex">
             \(\lambda _{\beta }\)
            </span>
            and gradually decreasing them in the subsequent optimization stages is effective for avoiding local minima.
           </p>
           <p>
            When the subject is captured in a side view, assessing in which direction the body is facing might be ambiguous. To address this, we try two initializations when the 2D distance between the CNN-estimated 2D shoulder joints is below a threshold: first with body orientation estimated as above and then with that orientation rotated by 180 degrees. Finally we pick the fit with lowest
            <span class="mathjax-tex">
             \(E_J\)
            </span>
            .
           </p>
           <p>
            We minimize Eq. (
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            ) using Powell’s dogleg method [
            <a aria-label="Reference 31" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR31" id="ref-link-section-d171448241e3755" title="Nocedal, J., Wright, S.: Numerical Optimization. Springer, New York (2006)">
             31
            </a>
            ], using OpenDR and Chumpy [
            <a aria-label="Reference 2" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR2" id="ref-link-section-d171448241e3758" title="
                    
                      http://chumpy.org
                      
                    
                  ">
             2
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d171448241e3761" title="Loper, M.M., Black, M.J.: OpenDR: an approximate differentiable renderer. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8695, pp. 154–169. Springer, Heidelberg (2014). doi:
                      10.1007/978-3-319-10584-0_11
                      
                    
        ">
             28
            </a>
            ]. Optimization for a single image takes less than 1 min on a common desktop machine.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Evaluation">
         <div class="c-article-section" id="Sec7-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">
           <span class="c-article-section__title-number">
            4
           </span>
           Evaluation
          </h2>
          <div class="c-article-section__content" id="Sec7-content">
           <p>
            We evaluate the accuracy of both 3D pose and 3D shape estimation. For quantitative evaluation of 3D pose, we use two publicly available datasets: HumanEva-I [
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d171448241e3773" title="Sigal, L., Balan, A., Black, M.J.: HumanEva: synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. Int. J. Comput. Vis. IJCV 87(1), 4–27 (2010)">
             41
            </a>
            ] and Human3.6M [
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d171448241e3776" title="Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6M: large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE Trans. Pattern Anal. Mach. Intell. TPAMI 36(7), 1325–1339 (2014)">
             18
            </a>
            ]. We compare our approach to three state-of-the-art methods [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d171448241e3779" title="Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3D human pose reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1446–1455 (2015)">
             4
            </a>
            ,
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d171448241e3782" title="Ramakrishna, V., Kanade, T., Sheikh, Y.: Reconstructing 3D human pose from 2D image landmarks. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7575, pp. 573–586. Springer, Heidelberg (2012). doi:
                      10.1007/978-3-642-33765-9_41
                      
                    
        ">
             39
            </a>
            ,
            <a aria-label="Reference 58" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR58" id="ref-link-section-d171448241e3785" title="Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparse representation for 3D shape estimation: a convex relaxation approach. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2015)">
             58
            </a>
            ] and also use these data for an ablation analysis. Both of the ground truth datasets have restricted laboratory environments and limited poses. Consequently, we perform a qualitative analysis on more challenging data from the Leeds Sports Dataset (LSP) [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d171448241e3789" title="Johnson, S., Everingham, M.: Clustered pose and nonlinear appearance models for human pose estimation. In: Proceedings of the British Machine Vision Conference, pp. 12.1-12.11 (2010)">
             22
            </a>
            ]. Evaluating shape quantitatively is harder since there are few images with ground truth 3D shape. Therefore, we perform a quantitative evaluation using synthetic data to evaluate how well shape can be recovered from 2D joints corrupted by noise. For all experiments, we use 10 body shape coefficients. We tune the
            <span class="mathjax-tex">
             \(\lambda _{i}\)
            </span>
            weights in Eq. (
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            ) on the HumanEva training data and use these values for all experiments.
           </p>
           <h3 class="c-article__sub-heading" id="Sec8">
            <span class="c-article-section__title-number">
             4.1
            </span>
            Quantitative Evaluation: Synthetic Data
           </h3>
           <p>
            We sample synthetic bodies from the SMPL shape and pose space and project their joints into the image with a known camera. We generate 1000 images for male shapes and 1000 for female shapes, at
            <span class="mathjax-tex">
             \(640 \times 480\)
            </span>
            resolution.
           </p>
           <p>
            In the first experiment, we add varying amounts of i.i.d. Gaussian noise (standard deviation (std) from 1 to 5 pixels) to each 2D joint. We solve for pose and shape by minimizing Eq. (
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            ), setting the confidence weights for the joints in Eq. (
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ2">
             2
            </a>
            ) to 1. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
             4
            </a>
            (left) shows the mean vertex-to-vertex Euclidean error between the estimated and true shape in a canonical pose. Here we fit gender-specific models. The results of shape estimation are more accurate than simply guessing the average shape (red lines in the figure). This shows that joints carry information about body shape that is relatively robust to noise.
           </p>
           <p>
            In the second experiment, we assume that the pose is known, and try to understand how many joints one needs to accurately estimate body shape. We fit SMPL to ground-truth 2D joints by minimizing Eq. (
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ2">
             2
            </a>
            ) with respect to: the full set of 23 SMPL joints; the subset of 12 joints corresponding to torso and limbs (excluding head, spine, hands and feet); and the 4 joints of the torso. As above, we measure the mean Euclidean error between the estimated and true shape in a canonical pose. Results are shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
             4
            </a>
            (right). The more joints we have, the better body shape is estimated. To our knowledge, this is the first demonstration of estimating 3D body shape from only 2D joints. Of course some joints may be difficult to estimate reliably; we evaluate on real data below.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 4." id="figure-4">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46454-1_34/figures/4" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig4_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 4" aria-describedby="Fig4" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig4_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc">
               <p>
                <b>
                 Evaluation on synthetic data.
                </b>
                Left: Mean vertex-to-vertex Euclidean error between the estimated and true shape in a canonical pose, when Gaussian noise is added to 2D joints. Dashed and dotted lines represent the error obtained by guessing the mean shape for males and females, respectively. Right: Error between estimated and true shape when considering only a subset of joints during fitting.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/figures/4" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec9">
            <span class="c-article-section__title-number">
             4.2
            </span>
            Quantitative Evaluation: Real Data
           </h3>
           <p>
            <i>
             HumanEva-I.
            </i>
            We evaluate pose estimation accuracy on single frames from the HumanEva dataset [
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d171448241e3907" title="Sigal, L., Balan, A., Black, M.J.: HumanEva: synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. Int. J. Comput. Vis. IJCV 87(1), 4–27 (2010)">
             41
            </a>
            ]. Following the standard procedure, we evaluate on the Walking and Box sequences of subjects 1, 2, and 3 from the “validation” set [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d171448241e3910" title="Bo, L., Sminchisescu, C.: Twin Gaussian processes for structured prediction. Int. J. Comput. Vis. IJCV 87(1–2), 28–52 (2010)">
             8
            </a>
            ,
            <a aria-label="Reference 49" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR49" id="ref-link-section-d171448241e3913" title="Tekin, B., Rozantsev, A., Lepetit, V., Fua, P.: Direct prediction of 3D body poses from motion compensated sequences. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 991–1000 (2016)">
             49
            </a>
            ]. We assume the gender is known and apply the gender-specific SMPL models.
           </p>
           <p>
            Many methods train sequence-specific pose priors for HumanEva; we do not do this. We do, however, tune our weights on HumanEva training set and learn a mapping from the SMPL joints to the 3D skeletal representation of HumanEva. To that end we fit the SMPL model to the raw mocap marker data in the training set using MoSh to estimate body shape and pose. We then train a linear regressor from body vertices (equivalently shape parameters
            <span class="mathjax-tex">
             \(\mathbf {\beta }\)
            </span>
            ) to the HumanEva 3D joints. This is done once on training data for all subjects together and kept fixed. We use the regressed 3D joints as our output for evaluation.
           </p>
           <p>
            We compare our method against three state-of-the-art methods [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d171448241e3942" title="Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3D human pose reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1446–1455 (2015)">
             4
            </a>
            ,
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d171448241e3945" title="Ramakrishna, V., Kanade, T., Sheikh, Y.: Reconstructing 3D human pose from 2D image landmarks. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7575, pp. 573–586. Springer, Heidelberg (2012). doi:
                      10.1007/978-3-642-33765-9_41
                      
                    
        ">
             39
            </a>
            ,
            <a aria-label="Reference 58" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR58" id="ref-link-section-d171448241e3948" title="Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparse representation for 3D shape estimation: a convex relaxation approach. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2015)">
             58
            </a>
            ], which, like us, predict 3D pose from 2D joints. We report the average Euclidean distance between the ground-truth and predicted 3D joint positions. Before computing the error we apply a similarity transform to align the reconstructed 3D joints to a common frame via the Procrustes analysis on every frame. Input to all methods is the same: 2D joints detected by DeepCut [
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d171448241e3951" title="Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P., Schiele, B.: DeepCut: joint subset partition and labeling for multi person pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4929–4937 (2016)">
             36
            </a>
            ]. Recall that DeepCut has not been trained on either dataset used for quantitative evaluation. Note that these approaches have different skeletal structures of 3D joints. We evaluate on the subset of 14 joints that semantically correspond across all representations. For this dataset we use the ground truth focal length.
           </p>
           <p>
            Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab1">
             1
            </a>
            shows quantitative results where SMPLify achieves the lowest errors on all sequences. While the recent method of Zhou et al. [
            <a aria-label="Reference 58" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR58" id="ref-link-section-d171448241e3960" title="Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparse representation for 3D shape estimation: a convex relaxation approach. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2015)">
             58
            </a>
            ] is very good, we argue that our approach is conceptually simpler and more accurate. We simply fit the body model to the 2D data and let the model constrain the solution. Not only does this “lift” the 2D joints to 3D, but SMPLify also produces a skinned vertex-based model that can be immediately used in a variety of applications.
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab1">
               Table 1.
               <b>
                HumanEva-I results.
               </b>
               3D joint errors in mm.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/tables/1" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            To gain insight about the method, we perform an ablation study (Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab2">
             2
            </a>
            ) where we evaluate different pose priors and the interpenetration penalty term. First we replace the mixture-model-based pose prior with
            <span class="mathjax-tex">
             \(E_{\theta '}\)
            </span>
            , which uses a single Gaussian trained from the same data. This significantly degrades performance. Next we add the interpenetration term, but this does not have a significant impact on the 3D joint error. However, qualitatively, we find that it makes a difference in more complex datasets with varied poses and viewing angles as illustrated in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            .
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 5." id="figure-5">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46454-1_34/figures/5" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig5_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 5" aria-describedby="Fig5" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig5_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc">
               <p>
                <b>
                 Interpenetration error term.
                </b>
                Examples where the interpenetration term avoids unnatural poses. For each example we show, from left to right, CNN estimated joints, and the result of the optimization
                <i>
                 without
                </i>
                and
                <i>
                 with
                </i>
                interpenetration error term.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 5" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure5 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/figures/5" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-2">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab2">
               Table 2.
               <b>
                HumanEva-I ablation study.
               </b>
               3D joint errors in mm. The first row drops the interpenetration term and replaces the pose prior with a uni-modal prior. The second row keeps the uni-modal pose prior but adds the interpenetration penalty. The third row shows the proposed SMPLify model.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 2" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/tables/2" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <i>
             Human3.6M.
            </i>
            We perform the same analysis on the Human 3.6M dataset [
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d171448241e4791" title="Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6M: large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE Trans. Pattern Anal. Mach. Intell. TPAMI 36(7), 1325–1339 (2014)">
             18
            </a>
            ], which has a wider range of poses. Following [
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d171448241e4794" title="Li, S., Chan, A.B.: 3D human pose estimation from monocular images with deep convolutional neural network. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014. LNCS, vol. 9004, pp. 332–347. Springer, Heidelberg (2015). doi:
                      10.1007/978-3-319-16808-1_23
                      
                    
        ">
             27
            </a>
            ,
            <a aria-label="Reference 49" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR49" id="ref-link-section-d171448241e4797" title="Tekin, B., Rozantsev, A., Lepetit, V., Fua, P.: Direct prediction of 3D body poses from motion compensated sequences. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 991–1000 (2016)">
             49
            </a>
            ,
            <a aria-label="Reference 59" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR59" id="ref-link-section-d171448241e4800" title="Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2016)">
             59
            </a>
            ], we report results on sequences of subjects S9 and S11. We evaluate on five different action sequences captured from the frontal camera (“cam3”) from trial 1. These sequences consist of 2000 frames on average and we evaluate on all frames
            <i>
             individually
            </i>
            . As above, we use training mocap and MoSh to train a regressor from the SMPL body shape to the 3D joint representation used in the dataset. Other than this we do not use the training set in any manner. We assume that the focal length as well as the distortion coefficients are known since the subjects are closer to the borders of the image. Evaluation on Human3.6 M is shown in Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab3">
             3
            </a>
            where our method again achieves the lowest average 3D error. While not directly comparable, Ionescu et al. [
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d171448241e4810" title="Ionescu, C., Carreira, J., Sminchisescu, C.: Iterated second-order label sensitive pooling for 3D human pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1661–1668 (2014)">
             17
            </a>
            ] report an error of 92 mm on this dataset.
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-3">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab3">
               Table 3.
               <b>
                Human 3.6M.
               </b>
               3D joint errors in mm.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 3" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/tables/3" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 6." id="figure-6">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46454-1_34/figures/6" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig6_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 6" aria-describedby="Fig6" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig6_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc">
               <p>
                <b>
                 Leeds Sports Dataset.
                </b>
                Each sub-image shows the original image with the 2D joints fit by the CNN. To the right of that is our estimated 3D pose and shape and the model seen from another view. The top row shows examples using the gender-neutral body model; the bottom row show fits using the gender-specific models.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 6" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure6 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/figures/6" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 7." id="figure-7">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46454-1_34/figures/7" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig7_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 7" aria-describedby="Fig7" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig7_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc">
               <p>
                <b>
                 LSP Failure cases.
                </b>
                Some representative failure cases: misplaced limbs, limbs matched with the limbs of other people, depth ambiguities.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 7" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure7 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/figures/7" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec10">
            <span class="c-article-section__title-number">
             4.3
            </span>
            Qualitative Evaluation
           </h3>
           <p>
            Here we apply SMPLify to images from the Leeds Sports Pose (LSP) dataset [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d171448241e5553" title="Johnson, S., Everingham, M.: Clustered pose and nonlinear appearance models for human pose estimation. In: Proceedings of the British Machine Vision Conference, pp. 12.1-12.11 (2010)">
             22
            </a>
            ]. These are much more complex in terms of pose, image resolution, clothing, illumination, and background than HumanEva or Human3.6M. The CNN, however, still does a good job of estimating the 2D poses. We only show results on the LSP test set. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
             6
            </a>
            shows several representative examples where the system works well. The figure shows results with both gender-neutral and gender-specific SMPL models; the choice has little visual effect on pose. For the gender-specific models, we manually label the images according to gender.
           </p>
           <p>
            Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig8">
             8
            </a>
            visually compares the results of the different methods on a few images from each of the datasets. The other methods suffer from not having a strong model of how limb lengths are correlated. LSP contains complex poses and these often show the value of the interpenetration term. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            shows two illustrative examples. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            shows a few failure cases on LSP. Some of these result from CNN failures where limbs are mis-detected or are matched with those of other people. Other failures are due to challenging depth ambiguities. See Supplementary Material [
            <a aria-label="Reference 1" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR1" id="ref-link-section-d171448241e5571" title="
                    
                      http://smplify.is.tue.mpg.de
                      
                    
                  ">
             1
            </a>
            ] for more results.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 8." id="figure-8">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig8">
               Fig. 8.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46454-1_34/figures/8" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig8_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 8" aria-describedby="Fig8" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig8_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc">
               <p>
                <b>
                 Qualitative comparison.
                </b>
                From top to bottom: Input image. Akhter and Black [
                <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d171448241e5587" title="Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3D human pose reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1446–1455 (2015)">
                 4
                </a>
                ]. Ramakrishna et al. [
                <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d171448241e5590" title="Ramakrishna, V., Kanade, T., Sheikh, Y.: Reconstructing 3D human pose from 2D image landmarks. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7575, pp. 573–586. Springer, Heidelberg (2012). doi:
                      10.1007/978-3-642-33765-9_41
                      
                    
        ">
                 39
                </a>
                ]. Zhou et al. [
                <a aria-label="Reference 58" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR58" id="ref-link-section-d171448241e5593" title="Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparse representation for 3D shape estimation: a convex relaxation approach. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2015)">
                 58
                </a>
                ]. SMPLify.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 8" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure8 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46454-1_34/figures/8" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
          </div>
         </div>
        </section>
        <section data-title="Conclusions">
         <div class="c-article-section" id="Sec11-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">
           <span class="c-article-section__title-number">
            5
           </span>
           Conclusions
          </h2>
          <div class="c-article-section__content" id="Sec11-content">
           <p>
            We have presented SMPLify, a fully automated method for estimating 3D body shape and pose from 2D joints in single images. SMPLify uses a CNN to estimate 2D joint locations, and then fits a 3D human body model to these joints. We use the recently proposed SMPL body model, which captures correlations in body shape, highly constraining the fitting process. We exploit this to define an objective function and optimize pose and shape directly by minimizing the error between the projected joints of the model and the estimated 2D joints. This gives a simple, yet very effective, solution to estimate 3D pose and approximate shape. The resulting model can be immediately posed and animated. We extensively evaluate our method on various datasets and find that SMPLify outperforms state-of-the-art methods.
           </p>
           <p>
            Our formulation opens many directions for future work. In particular, body shape and pose can benefit from other cues such as silhouettes and we plan to extend the method to use multiple camera views and multiple frames. Additionally a facial pose detector would improve head pose estimation and automatic gender detection would allow the use of the appropriate gender-specific model. It would be useful to train CNNs to predict more than 2D joints, such as features related directly to 3D shape. Our method provides approximate 3D meshes in correspondence with images, which could be useful for such training. The method can be extended to deal with multiple people in an image; having 3D meshes should help with reasoning about occlusion.
           </p>
          </div>
         </div>
        </section>
       </div>
       <div id="MagazineFulltextChapterBodySuffix">
        <section aria-labelledby="Bib1" data-title="References">
         <div class="c-article-section" id="Bib1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">
           <span class="c-article-section__title-number">
           </span>
           References
          </h2>
          <div class="c-article-section__content" id="Bib1-content">
           <div data-container-section="references">
            <ol class="c-article-references" data-track-component="outbound reference">
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1.">
              <p class="c-article-references__text" id="ref-CR1">
               <a data-track="click" data-track-action="external reference" data-track-label="http://smplify.is.tue.mpg.de" href="http://smplify.is.tue.mpg.de">
                http://smplify.is.tue.mpg.de
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2.">
              <p class="c-article-references__text" id="ref-CR2">
               <a data-track="click" data-track-action="external reference" data-track-label="http://chumpy.org" href="http://chumpy.org">
                http://chumpy.org
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3.">
              <p class="c-article-references__text" id="ref-CR3">
               <a data-track="click" data-track-action="external reference" data-track-label="http://mocap.cs.cmu.edu" href="http://mocap.cs.cmu.edu">
                http://mocap.cs.cmu.edu
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4.">
              <p class="c-article-references__text" id="ref-CR4">
               Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3D human pose reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1446–1455 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR4-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Akhter%2C%20I.%2C%20Black%2C%20M.J.%3A%20Pose-conditioned%20joint%20angle%20limits%20for%203D%20human%20pose%20reconstruction.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201446%E2%80%931455%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5.">
              <p class="c-article-references__text" id="ref-CR5">
               Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: SCAPE: shape completion and animation of people. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH
               <b>
                24
               </b>
               (3), 408–416 (2005)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR5-links">
               <a aria-label="CrossRef reference 5" data-doi="10.1145/1073204.1073207" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/1073204.1073207" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F1073204.1073207" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 5" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=SCAPE%3A%20shape%20completion%20and%20animation%20of%20people&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH&amp;volume=24&amp;issue=3&amp;pages=408-416&amp;publication_year=2005&amp;author=Anguelov%2CD&amp;author=Srinivasan%2CP&amp;author=Koller%2CD&amp;author=Thrun%2CS&amp;author=Rodgers%2CJ&amp;author=Davis%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6.">
              <p class="c-article-references__text" id="ref-CR6">
               Balan, A.O., Sigal, L., Black, M.J., Davis, J.E., Haussecker, H.W.: Detailed human shape and pose from images. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1–8 (2007)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR6-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Balan%2C%20A.O.%2C%20Sigal%2C%20L.%2C%20Black%2C%20M.J.%2C%20Davis%2C%20J.E.%2C%20Haussecker%2C%20H.W.%3A%20Detailed%20human%20shape%20and%20pose%20from%20images.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201%E2%80%938%20%282007%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7.">
              <p class="c-article-references__text" id="ref-CR7">
               Barron, C., Kakadiaris, I.: Estimating anthropometry and pose from a single uncalibrated image. Comput. Vis. Image Underst. CVIU
               <b>
                81
               </b>
               (3), 269–284 (2001)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR7-links">
               <a aria-label="CrossRef reference 7" data-doi="10.1006/cviu.2000.0888" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1006/cviu.2000.0888" href="https://doi-org.proxy.lib.ohio-state.edu/10.1006%2Fcviu.2000.0888" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="MATH reference 7" data-track="click" data-track-action="MATH reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1011.68549" rel="nofollow noopener">
                MATH
               </a>
               <a aria-label="Google Scholar reference 7" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Estimating%20anthropometry%20and%20pose%20from%20a%20single%20uncalibrated%20image&amp;journal=Comput.%20Vis.%20Image%20Underst.%20CVIU&amp;volume=81&amp;issue=3&amp;pages=269-284&amp;publication_year=2001&amp;author=Barron%2CC&amp;author=Kakadiaris%2CI" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8.">
              <p class="c-article-references__text" id="ref-CR8">
               Bo, L., Sminchisescu, C.: Twin Gaussian processes for structured prediction. Int. J. Comput. Vis. IJCV
               <b>
                87
               </b>
               (1–2), 28–52 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR8-links">
               <a aria-label="CrossRef reference 8" data-doi="10.1007/s11263-008-0204-y" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/s11263-008-0204-y" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-008-0204-y" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 8" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Twin%20Gaussian%20processes%20for%20structured%20prediction&amp;journal=Int.%20J.%20Comput.%20Vis.%20IJCV&amp;volume=87&amp;issue=1%E2%80%932&amp;pages=28-52&amp;publication_year=2010&amp;author=Bo%2CL&amp;author=Sminchisescu%2CC" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9.">
              <p class="c-article-references__text" id="ref-CR9">
               Chen, Y., Kim, T.-K., Cipolla, R.: Inferring 3D shapes and deformations from single views. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6313, pp. 300–313. Springer, Heidelberg (2010). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-15558-1_22" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-15558-1_22">
                10.1007/978-3-642-15558-1_22
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR9-links">
               <a aria-label="CrossRef reference 9" data-doi="10.1007/978-3-642-15558-1_22" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-642-15558-1_22" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-642-15558-1_22" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 9" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Inferring%203D%20shapes%20and%20deformations%20from%20single%20views&amp;pages=300-313&amp;publication_year=2010&amp;author=Chen%2CY&amp;author=Kim%2CT-K&amp;author=Cipolla%2CR" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10.">
              <p class="c-article-references__text" id="ref-CR10">
               Ericson, C.: Real-Time Collision Detection. The Morgan Kaufmann Series in Interactive 3-D Technology (2004)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR10-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ericson%2C%20C.%3A%20Real-Time%20Collision%20Detection.%20The%20Morgan%20Kaufmann%20Series%20in%20Interactive%203-D%20Technology%20%282004%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11.">
              <p class="c-article-references__text" id="ref-CR11">
               Fan, X., Zheng, K., Zhou, Y., Wang, S.: Pose locality constrained representation for 3D human pose reconstruction. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8689, pp. 174–188. Springer, Heidelberg (2014). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-10590-1_12" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-10590-1_12">
                10.1007/978-3-319-10590-1_12
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR11-links">
               <a aria-label="Google Scholar reference 11" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Pose%20locality%20constrained%20representation%20for%203D%20human%20pose%20reconstruction&amp;pages=174-188&amp;publication_year=2014&amp;author=Fan%2CX&amp;author=Zheng%2CK&amp;author=Zhou%2CY&amp;author=Wang%2CS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12.">
              <p class="c-article-references__text" id="ref-CR12">
               Geman, S., McClure, D.: Statistical methods for tomographic image reconstruction. Bull. Int. Stat. Inst.
               <b>
                52
               </b>
               (4), 5–21 (1987)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR12-links">
               <a aria-label="MathSciNet reference 12" data-track="click" data-track-action="MathSciNet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=1027188" rel="nofollow noopener">
                MathSciNet
               </a>
               <a aria-label="Google Scholar reference 12" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Statistical%20methods%20for%20tomographic%20image%20reconstruction&amp;journal=Bull.%20Int.%20Stat.%20Inst.&amp;volume=52&amp;issue=4&amp;pages=5-21&amp;publication_year=1987&amp;author=Geman%2CS&amp;author=McClure%2CD" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13.">
              <p class="c-article-references__text" id="ref-CR13">
               Grest, D., Koch, R.: Human model fitting from monocular posture images. In: Proceedings of VMV, pp. 665–1344 (2005)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR13-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Grest%2C%20D.%2C%20Koch%2C%20R.%3A%20Human%20model%20fitting%20from%20monocular%20posture%20images.%20In%3A%20Proceedings%20of%20VMV%2C%20pp.%20665%E2%80%931344%20%282005%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14.">
              <p class="c-article-references__text" id="ref-CR14">
               Guan, P., Weiss, A., Balan, A., Black, M.J.: Estimating human shape and pose from a single image. In: IEEE International Conference on Computer Vision, ICCV, pp. 1381–1388 (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR14-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Guan%2C%20P.%2C%20Weiss%2C%20A.%2C%20Balan%2C%20A.%2C%20Black%2C%20M.J.%3A%20Estimating%20human%20shape%20and%20pose%20from%20a%20single%20image.%20In%3A%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20ICCV%2C%20pp.%201381%E2%80%931388%20%282009%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15.">
              <p class="c-article-references__text" id="ref-CR15">
               Guan, P.: Virtual human bodies with clothing and hair: From images to animation. Ph.D. thesis, Brown University, Department of Computer Science, December 2012
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR15-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Guan%2C%20P.%3A%20Virtual%20human%20bodies%20with%20clothing%20and%20hair%3A%20From%20images%20to%20animation.%20Ph.D.%20thesis%2C%20Brown%20University%2C%20Department%20of%20Computer%20Science%2C%20December%202012">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16.">
              <p class="c-article-references__text" id="ref-CR16">
               Hasler, N., Ackermann, H., Rosenhahn, B., Thormhlen, T., Seidel, H.P.: Multilinear pose and body shape estimation of dressed subjects from image sets. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1823–1830 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR16-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hasler%2C%20N.%2C%20Ackermann%2C%20H.%2C%20Rosenhahn%2C%20B.%2C%20Thormhlen%2C%20T.%2C%20Seidel%2C%20H.P.%3A%20Multilinear%20pose%20and%20body%20shape%20estimation%20of%20dressed%20subjects%20from%20image%20sets.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201823%E2%80%931830%20%282010%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17.">
              <p class="c-article-references__text" id="ref-CR17">
               Ionescu, C., Carreira, J., Sminchisescu, C.: Iterated second-order label sensitive pooling for 3D human pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1661–1668 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR17-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ionescu%2C%20C.%2C%20Carreira%2C%20J.%2C%20Sminchisescu%2C%20C.%3A%20Iterated%20second-order%20label%20sensitive%20pooling%20for%203D%20human%20pose%20estimation.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201661%E2%80%931668%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18.">
              <p class="c-article-references__text" id="ref-CR18">
               Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6M: large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE Trans. Pattern Anal. Mach. Intell. TPAMI
               <b>
                36
               </b>
               (7), 1325–1339 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR18-links">
               <a aria-label="CrossRef reference 18" data-doi="10.1109/TPAMI.2013.248" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1109/TPAMI.2013.248" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2013.248" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 18" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Human3.6M%3A%20large%20scale%20datasets%20and%20predictive%20methods%20for%203D%20human%20sensing%20in%20natural%20environments&amp;journal=IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.%20TPAMI&amp;volume=36&amp;issue=7&amp;pages=1325-1339&amp;publication_year=2014&amp;author=Ionescu%2CC&amp;author=Papava%2CD&amp;author=Olaru%2CV&amp;author=Sminchisescu%2CC" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19.">
              <p class="c-article-references__text" id="ref-CR19">
               Jain, A., Thormählen, T., Seidel, H.P., Theobalt, C.: MovieReshape: tracking and reshaping of humans in videos. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH
               <b>
                29
               </b>
               (5), 148:1–148:10 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR19-links">
               <a aria-label="Google Scholar reference 19" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=MovieReshape%3A%20tracking%20and%20reshaping%20of%20humans%20in%20videos&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH&amp;volume=29&amp;issue=5&amp;pages=148%3A1-148%3A10&amp;publication_year=2010&amp;author=Jain%2CA&amp;author=Thorm%C3%A4hlen%2CT&amp;author=Seidel%2CHP&amp;author=Theobalt%2CC" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20.">
              <p class="c-article-references__text" id="ref-CR20">
               Jain, A., Tompson, J., LeCun, Y., Bregler, C.: MoDeep: a deep learning framework using motion features for human pose estimation. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014. LNCS, vol. 9004, pp. 302–315. Springer, Heidelberg (2015). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-16808-1_21" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-16808-1_21">
                10.1007/978-3-319-16808-1_21
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR20-links">
               <a aria-label="Google Scholar reference 20" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=MoDeep%3A%20a%20deep%20learning%20framework%20using%20motion%20features%20for%20human%20pose%20estimation&amp;pages=302-315&amp;publication_year=2015&amp;author=Jain%2CA&amp;author=Tompson%2CJ&amp;author=LeCun%2CY&amp;author=Bregler%2CC" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21.">
              <p class="c-article-references__text" id="ref-CR21">
               Jiang, H.: 3D human pose reconstruction using millions of exemplars. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1674–1677 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR21-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Jiang%2C%20H.%3A%203D%20human%20pose%20reconstruction%20using%20millions%20of%20exemplars.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201674%E2%80%931677%20%282010%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22.">
              <p class="c-article-references__text" id="ref-CR22">
               Johnson, S., Everingham, M.: Clustered pose and nonlinear appearance models for human pose estimation. In: Proceedings of the British Machine Vision Conference, pp. 12.1-12.11 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR22-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Johnson%2C%20S.%2C%20Everingham%2C%20M.%3A%20Clustered%20pose%20and%20nonlinear%20appearance%20models%20for%20human%20pose%20estimation.%20In%3A%20Proceedings%20of%20the%20British%20Machine%20Vision%20Conference%2C%20pp.%2012.1-12.11%20%282010%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23.">
              <p class="c-article-references__text" id="ref-CR23">
               Kiefel, M., Gehler, P.V.: Human pose estimation with fields of parts. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp. 331–346. Springer, Heidelberg (2014). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-10602-1_22" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-10602-1_22">
                10.1007/978-3-319-10602-1_22
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR23-links">
               <a aria-label="Google Scholar reference 23" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Human%20pose%20estimation%20with%20fields%20of%20parts&amp;pages=331-346&amp;publication_year=2014&amp;author=Kiefel%2CM&amp;author=Gehler%2CPV" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24.">
              <p class="c-article-references__text" id="ref-CR24">
               Kostrikov, I., Gall, J.: Depth sweep regression forests for estimating 3D human pose from images. In: Proceedings of the British Machine Vision Conference (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR24-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kostrikov%2C%20I.%2C%20Gall%2C%20J.%3A%20Depth%20sweep%20regression%20forests%20for%20estimating%203D%20human%20pose%20from%20images.%20In%3A%20Proceedings%20of%20the%20British%20Machine%20Vision%20Conference%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25.">
              <p class="c-article-references__text" id="ref-CR25">
               Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.: Picture: a probabilistic programming language for scene perception. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4390–4399 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR25-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kulkarni%2C%20T.D.%2C%20Kohli%2C%20P.%2C%20Tenenbaum%2C%20J.B.%2C%20Mansinghka%2C%20V.%3A%20Picture%3A%20a%20probabilistic%20programming%20language%20for%20scene%20perception.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204390%E2%80%934399%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26.">
              <p class="c-article-references__text" id="ref-CR26">
               Lee, H., Chen, Z.: Determination of 3D human body postures from a single view. Comput. Vis. Graph. Image Process.
               <b>
                30
               </b>
               (2), 148–168 (1985)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR26-links">
               <a aria-label="CrossRef reference 26" data-doi="10.1016/0734-189X(85)90094-5" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1016/0734-189X(85)90094-5" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2F0734-189X%2885%2990094-5" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 26" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Determination%20of%203D%20human%20body%20postures%20from%20a%20single%20view&amp;journal=Comput.%20Vis.%20Graph.%20Image%20Process.&amp;volume=30&amp;issue=2&amp;pages=148-168&amp;publication_year=1985&amp;author=Lee%2CH&amp;author=Chen%2CZ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27.">
              <p class="c-article-references__text" id="ref-CR27">
               Li, S., Chan, A.B.: 3D human pose estimation from monocular images with deep convolutional neural network. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014. LNCS, vol. 9004, pp. 332–347. Springer, Heidelberg (2015). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-16808-1_23" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-16808-1_23">
                10.1007/978-3-319-16808-1_23
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR27-links">
               <a aria-label="Google Scholar reference 27" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=3D%20human%20pose%20estimation%20from%20monocular%20images%20with%20deep%20convolutional%20neural%20network&amp;pages=332-347&amp;publication_year=2015&amp;author=Li%2CS&amp;author=Chan%2CAB" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28.">
              <p class="c-article-references__text" id="ref-CR28">
               Loper, M.M., Black, M.J.: OpenDR: an approximate differentiable renderer. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8695, pp. 154–169. Springer, Heidelberg (2014). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-10584-0_11" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-10584-0_11">
                10.1007/978-3-319-10584-0_11
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR28-links">
               <a aria-label="Google Scholar reference 28" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=OpenDR%3A%20an%20approximate%20differentiable%20renderer&amp;pages=154-169&amp;publication_year=2014&amp;author=Loper%2CMM&amp;author=Black%2CMJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29.">
              <p class="c-article-references__text" id="ref-CR29">
               Loper, M., Mahmood, N., Black, M.J.: MoSh: motion and shape capture from sparse markers. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia
               <b>
                33
               </b>
               (6), 220:1–220:13 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR29-links">
               <a aria-label="Google Scholar reference 29" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=MoSh%3A%20motion%20and%20shape%20capture%20from%20sparse%20markers&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH%20Asia&amp;volume=33&amp;issue=6&amp;pages=220%3A1-220%3A13&amp;publication_year=2014&amp;author=Loper%2CM&amp;author=Mahmood%2CN&amp;author=Black%2CMJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30.">
              <p class="c-article-references__text" id="ref-CR30">
               Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-person linear model. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia
               <b>
                34
               </b>
               (6), 248: 1–248: 16 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR30-links">
               <a aria-label="Google Scholar reference 30" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=SMPL%3A%20a%20skinned%20multi-person%20linear%20model&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH%20Asia&amp;volume=34&amp;issue=6&amp;pages=248%3A%201-248%3A%2016&amp;publication_year=2015&amp;author=Loper%2CM&amp;author=Mahmood%2CN&amp;author=Romero%2CJ&amp;author=Pons-Moll%2CG&amp;author=Black%2CMJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31.">
              <p class="c-article-references__text" id="ref-CR31">
               Nocedal, J., Wright, S.: Numerical Optimization. Springer, New York (2006)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR31-links">
               <a aria-label="MATH reference 31" data-track="click" data-track-action="MATH reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1104.65059" rel="nofollow noopener">
                MATH
               </a>
               <a aria-label="Google Scholar reference 31" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Numerical%20Optimization&amp;publication_year=2006&amp;author=Nocedal%2CJ&amp;author=Wright%2CS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32.">
              <p class="c-article-references__text" id="ref-CR32">
               Olson, E., Agarwal, P.: Inference on networks of mixtures for robust robot mapping. Int. J. Robot. Res.
               <b>
                32
               </b>
               (7), 826–840 (2013)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR32-links">
               <a aria-label="CrossRef reference 32" data-doi="10.1177/0278364913479413" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1177/0278364913479413" href="https://doi-org.proxy.lib.ohio-state.edu/10.1177%2F0278364913479413" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 32" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Inference%20on%20networks%20of%20mixtures%20for%20robust%20robot%20mapping&amp;journal=Int.%20J.%20Robot.%20Res.&amp;volume=32&amp;issue=7&amp;pages=826-840&amp;publication_year=2013&amp;author=Olson%2CE&amp;author=Agarwal%2CP" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33.">
              <p class="c-article-references__text" id="ref-CR33">
               Parameswaran, V., Chellappa, R.: View independent human body pose estimation from a single perspective image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 16–22 (2004)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR33-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Parameswaran%2C%20V.%2C%20Chellappa%2C%20R.%3A%20View%20independent%20human%20body%20pose%20estimation%20from%20a%20single%20perspective%20image.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%2016%E2%80%9322%20%282004%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34.">
              <p class="c-article-references__text" id="ref-CR34">
               Pfister, T., Charles, J., Zisserman, A.: Flowing convnets for human pose estimation in videos. In: IEEE International Conference on Computer Vision, ICCV, pp. 1913–1921 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR34-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Pfister%2C%20T.%2C%20Charles%2C%20J.%2C%20Zisserman%2C%20A.%3A%20Flowing%20convnets%20for%20human%20pose%20estimation%20in%20videos.%20In%3A%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20ICCV%2C%20pp.%201913%E2%80%931921%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35.">
              <p class="c-article-references__text" id="ref-CR35">
               Pfister, T., Simonyan, K., Charles, J., Zisserman, A.: Deep convolutional neural networks for efficient pose estimation in gesture videos. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014. LNCS, vol. 9003, pp. 538–552. Springer, Heidelberg (2015). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-16865-4_35" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-16865-4_35">
                10.1007/978-3-319-16865-4_35
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR35-links">
               <a aria-label="Google Scholar reference 35" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Deep%20convolutional%20neural%20networks%20for%20efficient%20pose%20estimation%20in%20gesture%20videos&amp;pages=538-552&amp;publication_year=2015&amp;author=Pfister%2CT&amp;author=Simonyan%2CK&amp;author=Charles%2CJ&amp;author=Zisserman%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36.">
              <p class="c-article-references__text" id="ref-CR36">
               Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P., Schiele, B.: DeepCut: joint subset partition and labeling for multi person pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4929–4937 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR36-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Pishchulin%2C%20L.%2C%20Insafutdinov%2C%20E.%2C%20Tang%2C%20S.%2C%20Andres%2C%20B.%2C%20Andriluka%2C%20M.%2C%20Gehler%2C%20P.%2C%20Schiele%2C%20B.%3A%20DeepCut%3A%20joint%20subset%20partition%20and%20labeling%20for%20multi%20person%20pose%20estimation.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204929%E2%80%934937%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37.">
              <p class="c-article-references__text" id="ref-CR37">
               Pons-Moll, G., Fleet, D., Rosenhahn, B.: Posebits for monocular human pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2345–2352 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR37-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Pons-Moll%2C%20G.%2C%20Fleet%2C%20D.%2C%20Rosenhahn%2C%20B.%3A%20Posebits%20for%20monocular%20human%20pose%20estimation.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%202345%E2%80%932352%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38.">
              <p class="c-article-references__text" id="ref-CR38">
               Pons-Moll, G., Taylor, J., Shotton, J., Hertzmann, A., Fitzgibbon, A.: Metric regression forests for correspondence estimation. Int. J. Comput. Vis. IJCV
               <b>
                113
               </b>
               (3), 1–13 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR38-links">
               <a aria-label="MathSciNet reference 38" data-track="click" data-track-action="MathSciNet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=3356158" rel="nofollow noopener">
                MathSciNet
               </a>
               <a aria-label="Google Scholar reference 38" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Metric%20regression%20forests%20for%20correspondence%20estimation&amp;journal=Int.%20J.%20Comput.%20Vis.%20IJCV&amp;volume=113&amp;issue=3&amp;pages=1-13&amp;publication_year=2015&amp;author=Pons-Moll%2CG&amp;author=Taylor%2CJ&amp;author=Shotton%2CJ&amp;author=Hertzmann%2CA&amp;author=Fitzgibbon%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39.">
              <p class="c-article-references__text" id="ref-CR39">
               Ramakrishna, V., Kanade, T., Sheikh, Y.: Reconstructing 3D human pose from 2D image landmarks. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7575, pp. 573–586. Springer, Heidelberg (2012). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-33765-9_41" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-33765-9_41">
                10.1007/978-3-642-33765-9_41
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR39-links">
               <a aria-label="Google Scholar reference 39" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Reconstructing%203D%20human%20pose%20from%202D%20image%20landmarks&amp;pages=573-586&amp;publication_year=2012&amp;author=Ramakrishna%2CV&amp;author=Kanade%2CT&amp;author=Sheikh%2CY" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40.">
              <p class="c-article-references__text" id="ref-CR40">
               Rother, C., Kolmogorov, V., Blake, A.: Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH
               <b>
                23
               </b>
               (3), 309–314 (2004)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR40-links">
               <a aria-label="CrossRef reference 40" data-doi="10.1145/1015706.1015720" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/1015706.1015720" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F1015706.1015720" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 40" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Grabcut%3A%20interactive%20foreground%20extraction%20using%20iterated%20graph%20cuts&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH&amp;volume=23&amp;issue=3&amp;pages=309-314&amp;publication_year=2004&amp;author=Rother%2CC&amp;author=Kolmogorov%2CV&amp;author=Blake%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41.">
              <p class="c-article-references__text" id="ref-CR41">
               Sigal, L., Balan, A., Black, M.J.: HumanEva: synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. Int. J. Comput. Vis. IJCV
               <b>
                87
               </b>
               (1), 4–27 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR41-links">
               <a aria-label="CrossRef reference 41" data-doi="10.1007/s11263-009-0273-6" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/s11263-009-0273-6" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-009-0273-6" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 41" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=HumanEva%3A%20synchronized%20video%20and%20motion%20capture%20dataset%20and%20baseline%20algorithm%20for%20evaluation%20of%20articulated%20human%20motion&amp;journal=Int.%20J.%20Comput.%20Vis.%20IJCV&amp;volume=87&amp;issue=1&amp;pages=4-27&amp;publication_year=2010&amp;author=Sigal%2CL&amp;author=Balan%2CA&amp;author=Black%2CMJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42.">
              <p class="c-article-references__text" id="ref-CR42">
               Sigal, L., Balan, A., Black, M.J.: Combined discriminative and generative articulated pose and non-rigid shape estimation. In: Advances in Neural Information Processing Systems (NIPS), vol. 20, pp. 1337–1344 (2008)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR42-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sigal%2C%20L.%2C%20Balan%2C%20A.%2C%20Black%2C%20M.J.%3A%20Combined%20discriminative%20and%20generative%20articulated%20pose%20and%20non-rigid%20shape%20estimation.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%20%28NIPS%29%2C%20vol.%2020%2C%20pp.%201337%E2%80%931344%20%282008%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43.">
              <p class="c-article-references__text" id="ref-CR43">
               Simo-Serra, E., Quattoni, A., Torras, C., Moreno-Noguer, F.: A joint model for 2D and 3D pose estimation from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3634–3641 (2013)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR43-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Simo-Serra%2C%20E.%2C%20Quattoni%2C%20A.%2C%20Torras%2C%20C.%2C%20Moreno-Noguer%2C%20F.%3A%20A%20joint%20model%20for%202D%20and%203D%20pose%20estimation%20from%20a%20single%20image.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%203634%E2%80%933641%20%282013%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44.">
              <p class="c-article-references__text" id="ref-CR44">
               Simo-Serra, E., Ramisa, A., Alenya, G., Torras, C., Moreno-Noguer, F.: Single image 3D human pose estimation from noisy observations. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2673–2680 (2012)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR44-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Simo-Serra%2C%20E.%2C%20Ramisa%2C%20A.%2C%20Alenya%2C%20G.%2C%20Torras%2C%20C.%2C%20Moreno-Noguer%2C%20F.%3A%20Single%20image%203D%20human%20pose%20estimation%20from%20noisy%20observations.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%202673%E2%80%932680%20%282012%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45.">
              <p class="c-article-references__text" id="ref-CR45">
               Sminchisescu, C., Telea, A.: Human pose estimation from silhouettes, a consistent approach using distance level sets. In: WSCG International Conference for Computer Graphics, Visualization and Computer Vision, pp. 413–420 (2002)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR45-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sminchisescu%2C%20C.%2C%20Telea%2C%20A.%3A%20Human%20pose%20estimation%20from%20silhouettes%2C%20a%20consistent%20approach%20using%20distance%20level%20sets.%20In%3A%20WSCG%20International%20Conference%20for%20Computer%20Graphics%2C%20Visualization%20and%20Computer%20Vision%2C%20pp.%20413%E2%80%93420%20%282002%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46.">
              <p class="c-article-references__text" id="ref-CR46">
               Sminchisescu, C., Triggs, B.: Covariance scaled sampling for monocular 3D body tracking. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 447–454 (2001)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR46-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sminchisescu%2C%20C.%2C%20Triggs%2C%20B.%3A%20Covariance%20scaled%20sampling%20for%20monocular%203D%20body%20tracking.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%20447%E2%80%93454%20%282001%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47.">
              <p class="c-article-references__text" id="ref-CR47">
               Sridhar, S., Mueller, F., Oulasvirta, A., Theobalt, C.: Fast and robust hand tracking using detection-guided optimization. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3121–3221 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR47-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sridhar%2C%20S.%2C%20Mueller%2C%20F.%2C%20Oulasvirta%2C%20A.%2C%20Theobalt%2C%20C.%3A%20Fast%20and%20robust%20hand%20tracking%20using%20detection-guided%20optimization.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%203121%E2%80%933221%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48.">
              <p class="c-article-references__text" id="ref-CR48">
               Taylor, C.: Reconstruction of articulated objects from point correspondences in single uncalibrated image. Comput. Vis. Image Underst. CVIU
               <b>
                80
               </b>
               (10), 349–363 (2000)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR48-links">
               <a aria-label="CrossRef reference 48" data-doi="10.1006/cviu.2000.0878" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1006/cviu.2000.0878" href="https://doi-org.proxy.lib.ohio-state.edu/10.1006%2Fcviu.2000.0878" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="MATH reference 48" data-track="click" data-track-action="MATH reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1011.68537" rel="nofollow noopener">
                MATH
               </a>
               <a aria-label="Google Scholar reference 48" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Reconstruction%20of%20articulated%20objects%20from%20point%20correspondences%20in%20single%20uncalibrated%20image&amp;journal=Comput.%20Vis.%20Image%20Underst.%20CVIU&amp;volume=80&amp;issue=10&amp;pages=349-363&amp;publication_year=2000&amp;author=Taylor%2CC" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49.">
              <p class="c-article-references__text" id="ref-CR49">
               Tekin, B., Rozantsev, A., Lepetit, V., Fua, P.: Direct prediction of 3D body poses from motion compensated sequences. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 991–1000 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR49-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tekin%2C%20B.%2C%20Rozantsev%2C%20A.%2C%20Lepetit%2C%20V.%2C%20Fua%2C%20P.%3A%20Direct%20prediction%20of%203D%20body%20poses%20from%20motion%20compensated%20sequences.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%20991%E2%80%931000%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50.">
              <p class="c-article-references__text" id="ref-CR50">
               Thiery, J.M., Guy, E., Boubekeur, T.: Sphere-meshes: shape approximation using spherical quadric error metrics. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia
               <b>
                32
               </b>
               (6), 178:1–178:12 (2013)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR50-links">
               <a aria-label="Google Scholar reference 50" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Sphere-meshes%3A%20shape%20approximation%20using%20spherical%20quadric%20error%20metrics&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH%20Asia&amp;volume=32&amp;issue=6&amp;pages=178%3A1-178%3A12&amp;publication_year=2013&amp;author=Thiery%2CJM&amp;author=Guy%2CE&amp;author=Boubekeur%2CT" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51.">
              <p class="c-article-references__text" id="ref-CR51">
               Toshev, A., Szegedy, C.: DeepPose: human pose estimation via deep neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1653–1660 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR51-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Toshev%2C%20A.%2C%20Szegedy%2C%20C.%3A%20DeepPose%3A%20human%20pose%20estimation%20via%20deep%20neural%20networks.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201653%E2%80%931660%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52.">
              <p class="c-article-references__text" id="ref-CR52">
               Wang, C., Wang, Y., Lin, Z., Yuille, A., Gao, W.: Robust estimation of 3D human poses from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2369–2376 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR52-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wang%2C%20C.%2C%20Wang%2C%20Y.%2C%20Lin%2C%20Z.%2C%20Yuille%2C%20A.%2C%20Gao%2C%20W.%3A%20Robust%20estimation%20of%203D%20human%20poses%20from%20a%20single%20image.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%202369%E2%80%932376%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53.">
              <p class="c-article-references__text" id="ref-CR53">
               Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose machines. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4724–4732 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR53-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wei%2C%20S.E.%2C%20Ramakrishna%2C%20V.%2C%20Kanade%2C%20T.%2C%20Sheikh%2C%20Y.%3A%20Convolutional%20pose%20machines.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204724%E2%80%934732%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54.">
              <p class="c-article-references__text" id="ref-CR54">
               Yang, Y., Ramanan, D.: Articulated pose estimation using flexible mixtures of parts. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3546–3553 (2011)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR54-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yang%2C%20Y.%2C%20Ramanan%2C%20D.%3A%20Articulated%20pose%20estimation%20using%20flexible%20mixtures%20of%20parts.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%203546%E2%80%933553%20%282011%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55.">
              <p class="c-article-references__text" id="ref-CR55">
               Yasin, H., Iqbal, U., Krüger, B., Weber, A., Gall, J.: A dual-source approach for 3D pose estimation from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4948–4956 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR55-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yasin%2C%20H.%2C%20Iqbal%2C%20U.%2C%20Kr%C3%BCger%2C%20B.%2C%20Weber%2C%20A.%2C%20Gall%2C%20J.%3A%20A%20dual-source%20approach%20for%203D%20pose%20estimation%20from%20a%20single%20image.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204948%E2%80%934956%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56.">
              <p class="c-article-references__text" id="ref-CR56">
               Zhou, F., Torre, F.: Spatio-temporal matching for human detection in video. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8694, pp. 62–77. Springer, Heidelberg (2014). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-10599-4_5" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-10599-4_5">
                10.1007/978-3-319-10599-4_5
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR56-links">
               <a aria-label="Google Scholar reference 56" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Spatio-temporal%20matching%20for%20human%20detection%20in%20video&amp;pages=62-77&amp;publication_year=2014&amp;author=Zhou%2CF&amp;author=Torre%2CF" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57.">
              <p class="c-article-references__text" id="ref-CR57">
               Zhou, S., Fu, H., Liu, L., Cohen-Or, D., Han, X.: Parametric reshaping of human bodies in images. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH
               <b>
                29
               </b>
               (4), 126:1–126:10 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR57-links">
               <a aria-label="Google Scholar reference 57" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Parametric%20reshaping%20of%20human%20bodies%20in%20images&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH&amp;volume=29&amp;issue=4&amp;pages=126%3A1-126%3A10&amp;publication_year=2010&amp;author=Zhou%2CS&amp;author=Fu%2CH&amp;author=Liu%2CL&amp;author=Cohen-Or%2CD&amp;author=Han%2CX" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58.">
              <p class="c-article-references__text" id="ref-CR58">
               Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparse representation for 3D shape estimation: a convex relaxation approach. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR58-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhou%2C%20X.%2C%20Zhu%2C%20M.%2C%20Leonardos%2C%20S.%2C%20Derpanis%2C%20K.%2C%20Daniilidis%2C%20K.%3A%20Sparse%20representation%20for%203D%20shape%20estimation%3A%20a%20convex%20relaxation%20approach.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204447%E2%80%934455%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59.">
              <p class="c-article-references__text" id="ref-CR59">
               Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR59-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhou%2C%20X.%2C%20Zhu%2C%20M.%2C%20Leonardos%2C%20S.%2C%20Derpanis%2C%20K.%2C%20Daniilidis%2C%20K.%3A%20Sparseness%20meets%20deepness%3A%203D%20human%20pose%20estimation%20from%20monocular%20video.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204447%E2%80%934455%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
            </ol>
            <p class="c-article-references__download u-hide-print">
             <a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-319-46454-1_34?format=refman&amp;flavour=references" rel="nofollow">
              Download references
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </p>
           </div>
          </div>
         </div>
        </section>
       </div>
       <section data-title="Acknowledgements" lang="en">
        <div class="c-article-section" id="Ack1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">
          <span class="c-article-section__title-number">
          </span>
          Acknowledgements
         </h2>
         <div class="c-article-section__content" id="Ack1-content">
          <p>
           We thank M. Al Borno for inspiring the capsule representation, N. Mahmood for help with the figures, I. Akhter for helpful discussions.
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="author-information" data-title="Author information">
        <div class="c-article-section" id="author-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">
          <span class="c-article-section__title-number">
          </span>
          Author information
         </h2>
         <div class="c-article-section__content" id="author-information-content">
          <h3 class="c-article__sub-heading" id="affiliations">
           Authors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff17">
            <p class="c-article-author-affiliation__address">
             Max Planck Institute for Intelligent Systems, Tübingen, Germany
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Christoph Lassner, Peter Gehler, Javier Romero &amp; Michael J. Black
            </p>
           </li>
           <li id="Aff18">
            <p class="c-article-author-affiliation__address">
             Microsoft Research, Cambridge, UK
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Federica Bogo
            </p>
           </li>
           <li id="Aff19">
            <p class="c-article-author-affiliation__address">
             University of Maryland, College Park, USA
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Angjoo Kanazawa
            </p>
           </li>
           <li id="Aff20">
            <p class="c-article-author-affiliation__address">
             University of Tübingen, Tübingen, Germany
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Christoph Lassner &amp; Peter Gehler
            </p>
           </li>
          </ol>
          <div class="u-js-hide u-hide-print" data-test="author-info">
           <span class="c-article__sub-heading">
            Authors
           </span>
           <ol class="c-article-authors-search u-list-reset">
            <li id="auth-Federica-Bogo">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Federica Bogo
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Federica%20Bogo" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Federica%20Bogo" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Federica%20Bogo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Angjoo-Kanazawa">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Angjoo Kanazawa
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Angjoo%20Kanazawa" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Angjoo%20Kanazawa" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Angjoo%20Kanazawa%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Christoph-Lassner">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Christoph Lassner
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Christoph%20Lassner" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Christoph%20Lassner" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Christoph%20Lassner%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Peter-Gehler">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Peter Gehler
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Peter%20Gehler" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Peter%20Gehler" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Peter%20Gehler%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Javier-Romero">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Javier Romero
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Javier%20Romero" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Javier%20Romero" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Javier%20Romero%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Michael_J_-Black">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Michael J. Black
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Michael%20J.%20Black" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Michael%20J.%20Black" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michael%20J.%20Black%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
           </ol>
          </div>
          <h3 class="c-article__sub-heading" id="corresponding-author">
           Corresponding author
          </h3>
          <p id="corresponding-author-list">
           Correspondence to
           <a href="mailto:febogo@microsoft.com" id="corresp-c1">
            Federica Bogo
           </a>
           .
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="editor-information" data-title="Editor information">
        <div class="c-article-section" id="editor-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="editor-information">
          <span class="c-article-section__title-number">
          </span>
          Editor information
         </h2>
         <div class="c-article-section__content" id="editor-information-content">
          <h3 class="c-article__sub-heading" id="editor-affiliations">
           Editors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff13">
            <p class="c-article-author-affiliation__address">
             RWTH Aachen , Aachen, Germany
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Bastian Leibe
            </p>
           </li>
           <li id="Aff14">
            <p class="c-article-author-affiliation__address">
             Czech Technical University , Prague 2, Czech Republic
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Jiri Matas
            </p>
           </li>
           <li id="Aff15">
            <p class="c-article-author-affiliation__address">
             University of Trento , Povo - Trento, Italy
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Nicu Sebe
            </p>
           </li>
           <li id="Aff16">
            <p class="c-article-author-affiliation__address">
             University of Amsterdam , Amsterdam, The Netherlands
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Max Welling
            </p>
           </li>
          </ol>
         </div>
        </div>
       </section>
       <section data-title="Rights and permissions" lang="en">
        <div class="c-article-section" id="rightslink-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">
          <span class="c-article-section__title-number">
          </span>
          Rights and permissions
         </h2>
         <div class="c-article-section__content" id="rightslink-content">
          <p class="c-article-rights" data-test="rightslink-content">
           <a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?publisherName=SpringerNature&amp;orderBeanReset=true&amp;orderSource=SpringerLink&amp;title=Keep%20It%20SMPL%3A%20Automatic%20Estimation%20of%203D%20Human%20Pose%20and%20Shape%20from%20a%20Single%20Image&amp;author=Federica%20Bogo%2C%20Angjoo%20Kanazawa%2C%20Christoph%20Lassner%20et%20al&amp;contentID=10.1007%2F978-3-319-46454-1_34&amp;copyright=Springer%20International%20Publishing%20AG&amp;publication=eBook&amp;publicationDate=2016&amp;startPage=561&amp;endPage=578&amp;imprint=Springer%20International%20Publishing%20AG">
            Reprints and Permissions
           </a>
          </p>
         </div>
        </div>
       </section>
       <section data-title="Copyright information">
        <div class="c-article-section" id="copyright-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="copyright-information">
          <span class="c-article-section__title-number">
          </span>
          Copyright information
         </h2>
         <div class="c-article-section__content" id="copyright-information-content">
          <p>
           © 2016 Springer International Publishing AG
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="chapter-info" data-title="About this paper" lang="en">
        <div class="c-article-section" id="chapter-info-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="chapter-info">
          <span class="c-article-section__title-number">
          </span>
          About this paper
         </h2>
         <div class="c-article-section__content" id="chapter-info-content">
          <div class="c-bibliographic-information">
           <div class="c-bibliographic-information__column">
            <h3 class="c-article__sub-heading" id="citeas">
             Cite this paper
            </h3>
            <p class="c-bibliographic-information__citation" data-test="bibliographic-information__cite_this_chapter">
             Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J. (2016).  Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image.

                     In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds) Computer Vision – ECCV 2016. ECCV 2016. Lecture Notes in Computer Science(), vol 9909. Springer, Cham. https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46454-1_34
            </p>
            <h3 class="c-bibliographic-information__download-citation u-mb-8 u-mt-16 u-hide-print">
             Download citation
            </h3>
            <ul class="c-bibliographic-information__download-citation-list">
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-319-46454-1_34?format=refman&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .RIS file">
               .RIS
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-319-46454-1_34?format=endnote&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .ENW file">
               .ENW
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-319-46454-1_34?format=bibtex&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .BIB file">
               .BIB
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
            </ul>
            <ul class="c-bibliographic-information__list u-mb-24" data-test="publication-history">
             <li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--chapter-doi">
              <p data-test="bibliographic-information__doi">
               <abbr title="Digital Object Identifier">
                DOI
               </abbr>
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                https://doi.org/10.1007/978-3-319-46454-1_34
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p>
               Published
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2016-09-16">
                 16 September 2016
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__publisher-name">
               Publisher Name
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                Springer, Cham
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__pisbn">
               Print ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-319-46453-4
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__eisbn">
               Online ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-319-46454-1
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__package">
               eBook Packages
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=11645&amp;facet-start-year=2016&amp;facet-end-year=2016">
                 Computer Science
                </a>
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=43710&amp;facet-start-year=2016&amp;facet-end-year=2016">
                 Computer Science (R0)
                </a>
               </span>
              </p>
             </li>
            </ul>
            <div data-component="share-box">
             <div class="c-article-share-box u-display-block">
              <h3 class="c-article__sub-heading">
               Share this paper
              </h3>
              <p class="c-article-share-box__description">
               Anyone you share the following link with will be able to read this content:
              </p>
              <button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url">
               Get shareable link
              </button>
              <div class="js-no-share-url-container u-display-none" hidden="">
               <p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">
                Sorry, a shareable link is not currently available for this article.
               </p>
              </div>
              <div class="js-share-url-container u-display-none" hidden="">
               <p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url">
               </p>
               <button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url">
                Copy to clipboard
               </button>
              </div>
              <p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
               Provided by the Springer Nature SharedIt content-sharing initiative
              </p>
             </div>
            </div>
            <div data-component="chapter-info-list">
            </div>
           </div>
          </div>
         </div>
        </div>
       </section>
      </div>
     </article>
    </main>
    <div class="c-article-extras u-text-sm u-hide-print" data-container-type="reading-companion" data-track-component="conference paper" id="sidebar">
     <aside>
      <div class="js-context-bar-sticky-point-desktop" data-test="download-article-link-wrapper">
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-319-46454-1.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-319-46454-1.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div data-test="editorial-summary">
      </div>
      <div class="c-reading-companion">
       <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky" style="top: 40px;">
        <ul class="c-reading-companion__tabs" role="tablist">
         <li role="presentation">
          <button aria-controls="tabpanel-sections" aria-selected="true" class="c-reading-companion__tab c-reading-companion__tab--active" data-tab-target="sections" data-track="click" data-track-action="sections tab" data-track-label="tab" id="tab-sections" role="tab">
           Sections
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-figures" aria-selected="false" class="c-reading-companion__tab" data-tab-target="figures" data-track="click" data-track-action="figures tab" data-track-label="tab" id="tab-figures" role="tab" tabindex="-1">
           Figures
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-references" aria-selected="false" class="c-reading-companion__tab" data-tab-target="references" data-track="click" data-track-action="references tab" data-track-label="tab" id="tab-references" role="tab" tabindex="-1">
           References
          </button>
         </li>
        </ul>
        <div aria-labelledby="tab-sections" class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections" role="tabpanel">
         <div class="c-reading-companion__scroll-pane" style="max-height: 4544px;">
          <ul class="c-reading-companion__sections-list">
           <li class="c-reading-companion__section-item c-reading-companion__section-item--active" id="rc-sec-Abs1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Abstract" href="#Abs1">
             <span class="c-article-section__title-number">
             </span>
             Abstract
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Introduction" href="#Sec1">
             <span class="c-article-section__title-number">
              1
             </span>
             Introduction
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec2">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Related Work" href="#Sec2">
             <span class="c-article-section__title-number">
              2
             </span>
             Related Work
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec3">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Method" href="#Sec3">
             <span class="c-article-section__title-number">
              3
             </span>
             Method
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec7">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Evaluation" href="#Sec7">
             <span class="c-article-section__title-number">
              4
             </span>
             Evaluation
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec11">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Conclusions" href="#Sec11">
             <span class="c-article-section__title-number">
              5
             </span>
             Conclusions
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Bib1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: References" href="#Bib1">
             <span class="c-article-section__title-number">
             </span>
             References
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Ack1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Acknowledgements" href="#Ack1">
             <span class="c-article-section__title-number">
             </span>
             Acknowledgements
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-author-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Author information" href="#author-information">
             <span class="c-article-section__title-number">
             </span>
             Author information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-editor-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Editor information" href="#editor-information">
             <span class="c-article-section__title-number">
             </span>
             Editor information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-rightslink">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Rights and permissions" href="#rightslink">
             <span class="c-article-section__title-number">
             </span>
             Rights and permissions
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-copyright-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Copyright information" href="#copyright-information">
             <span class="c-article-section__title-number">
             </span>
             Copyright information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-chapter-info">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: About this paper" href="#chapter-info">
             <span class="c-article-section__title-number">
             </span>
             About this paper
            </a>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-figures" class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ul class="c-reading-companion__figures-list">
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig1_HTML.gif?"/>
              <img alt="figure 1" aria-describedby="rc-Fig1" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig1_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34/figures/1" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig2_HTML.gif?"/>
              <img alt="figure 2" aria-describedby="rc-Fig2" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig2_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34/figures/2" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig3_HTML.gif?"/>
              <img alt="figure 3" aria-describedby="rc-Fig3" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig3_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34/figures/3" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig4_HTML.gif?"/>
              <img alt="figure 4" aria-describedby="rc-Fig4" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig4_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34/figures/4" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig5_HTML.gif?"/>
              <img alt="figure 5" aria-describedby="rc-Fig5" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig5_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34/figures/5" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig6_HTML.gif?"/>
              <img alt="figure 6" aria-describedby="rc-Fig6" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig6_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34/figures/6" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig7_HTML.gif?"/>
              <img alt="figure 7" aria-describedby="rc-Fig7" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig7_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34/figures/7" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig8">
               Fig. 8.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig8_HTML.gif?"/>
              <img alt="figure 8" aria-describedby="rc-Fig8" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46454-1_34/MediaObjects/419978_1_En_34_Fig8_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig8">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46454-1_34/figures/8" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-references" class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ol class="c-reading-companion__references-list c-reading-companion__references-list--numeric">
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR1">
             <a data-track="click" data-track-action="external reference" data-track-label="http://smplify.is.tue.mpg.de" href="http://smplify.is.tue.mpg.de">
              http://smplify.is.tue.mpg.de
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR2">
             <a data-track="click" data-track-action="external reference" data-track-label="http://chumpy.org" href="http://chumpy.org">
              http://chumpy.org
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR3">
             <a data-track="click" data-track-action="external reference" data-track-label="http://mocap.cs.cmu.edu" href="http://mocap.cs.cmu.edu">
              http://mocap.cs.cmu.edu
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR4">
             Akhter, I., Black, M.J.: Pose-conditioned joint angle limits for 3D human pose reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1446–1455 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Akhter%2C%20I.%2C%20Black%2C%20M.J.%3A%20Pose-conditioned%20joint%20angle%20limits%20for%203D%20human%20pose%20reconstruction.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201446%E2%80%931455%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR5">
             Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: SCAPE: shape completion and animation of people. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH
             <b>
              24
             </b>
             (3), 408–416 (2005)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/1073204.1073207" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F1073204.1073207">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=SCAPE%3A%20shape%20completion%20and%20animation%20of%20people&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH&amp;volume=24&amp;issue=3&amp;pages=408-416&amp;publication_year=2005&amp;author=Anguelov%2CD&amp;author=Srinivasan%2CP&amp;author=Koller%2CD&amp;author=Thrun%2CS&amp;author=Rodgers%2CJ&amp;author=Davis%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR6">
             Balan, A.O., Sigal, L., Black, M.J., Davis, J.E., Haussecker, H.W.: Detailed human shape and pose from images. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1–8 (2007)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Balan%2C%20A.O.%2C%20Sigal%2C%20L.%2C%20Black%2C%20M.J.%2C%20Davis%2C%20J.E.%2C%20Haussecker%2C%20H.W.%3A%20Detailed%20human%20shape%20and%20pose%20from%20images.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201%E2%80%938%20%282007%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR7">
             Barron, C., Kakadiaris, I.: Estimating anthropometry and pose from a single uncalibrated image. Comput. Vis. Image Underst. CVIU
             <b>
              81
             </b>
             (3), 269–284 (2001)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1006/cviu.2000.0888" href="https://doi-org.proxy.lib.ohio-state.edu/10.1006%2Fcviu.2000.0888">
              CrossRef
             </a>
             <a data-track="click" data-track-action="math reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1011.68549">
              MATH
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Estimating%20anthropometry%20and%20pose%20from%20a%20single%20uncalibrated%20image&amp;journal=Comput.%20Vis.%20Image%20Underst.%20CVIU&amp;volume=81&amp;issue=3&amp;pages=269-284&amp;publication_year=2001&amp;author=Barron%2CC&amp;author=Kakadiaris%2CI">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR8">
             Bo, L., Sminchisescu, C.: Twin Gaussian processes for structured prediction. Int. J. Comput. Vis. IJCV
             <b>
              87
             </b>
             (1–2), 28–52 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/s11263-008-0204-y" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-008-0204-y">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Twin%20Gaussian%20processes%20for%20structured%20prediction&amp;journal=Int.%20J.%20Comput.%20Vis.%20IJCV&amp;volume=87&amp;issue=1%E2%80%932&amp;pages=28-52&amp;publication_year=2010&amp;author=Bo%2CL&amp;author=Sminchisescu%2CC">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR9">
             Chen, Y., Kim, T.-K., Cipolla, R.: Inferring 3D shapes and deformations from single views. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6313, pp. 300–313. Springer, Heidelberg (2010). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-15558-1_22" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-15558-1_22">
              10.1007/978-3-642-15558-1_22
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-642-15558-1_22" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-642-15558-1_22">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Inferring%203D%20shapes%20and%20deformations%20from%20single%20views&amp;pages=300-313&amp;publication_year=2010&amp;author=Chen%2CY&amp;author=Kim%2CT-K&amp;author=Cipolla%2CR">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR10">
             Ericson, C.: Real-Time Collision Detection. The Morgan Kaufmann Series in Interactive 3-D Technology (2004)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ericson%2C%20C.%3A%20Real-Time%20Collision%20Detection.%20The%20Morgan%20Kaufmann%20Series%20in%20Interactive%203-D%20Technology%20%282004%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR11">
             Fan, X., Zheng, K., Zhou, Y., Wang, S.: Pose locality constrained representation for 3D human pose reconstruction. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8689, pp. 174–188. Springer, Heidelberg (2014). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-10590-1_12" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-10590-1_12">
              10.1007/978-3-319-10590-1_12
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Pose%20locality%20constrained%20representation%20for%203D%20human%20pose%20reconstruction&amp;pages=174-188&amp;publication_year=2014&amp;author=Fan%2CX&amp;author=Zheng%2CK&amp;author=Zhou%2CY&amp;author=Wang%2CS">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR12">
             Geman, S., McClure, D.: Statistical methods for tomographic image reconstruction. Bull. Int. Stat. Inst.
             <b>
              52
             </b>
             (4), 5–21 (1987)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="mathscinet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=1027188">
              MathSciNet
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Statistical%20methods%20for%20tomographic%20image%20reconstruction&amp;journal=Bull.%20Int.%20Stat.%20Inst.&amp;volume=52&amp;issue=4&amp;pages=5-21&amp;publication_year=1987&amp;author=Geman%2CS&amp;author=McClure%2CD">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR13">
             Grest, D., Koch, R.: Human model fitting from monocular posture images. In: Proceedings of VMV, pp. 665–1344 (2005)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Grest%2C%20D.%2C%20Koch%2C%20R.%3A%20Human%20model%20fitting%20from%20monocular%20posture%20images.%20In%3A%20Proceedings%20of%20VMV%2C%20pp.%20665%E2%80%931344%20%282005%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR14">
             Guan, P., Weiss, A., Balan, A., Black, M.J.: Estimating human shape and pose from a single image. In: IEEE International Conference on Computer Vision, ICCV, pp. 1381–1388 (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Guan%2C%20P.%2C%20Weiss%2C%20A.%2C%20Balan%2C%20A.%2C%20Black%2C%20M.J.%3A%20Estimating%20human%20shape%20and%20pose%20from%20a%20single%20image.%20In%3A%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20ICCV%2C%20pp.%201381%E2%80%931388%20%282009%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR15">
             Guan, P.: Virtual human bodies with clothing and hair: From images to animation. Ph.D. thesis, Brown University, Department of Computer Science, December 2012
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Guan%2C%20P.%3A%20Virtual%20human%20bodies%20with%20clothing%20and%20hair%3A%20From%20images%20to%20animation.%20Ph.D.%20thesis%2C%20Brown%20University%2C%20Department%20of%20Computer%20Science%2C%20December%202012">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR16">
             Hasler, N., Ackermann, H., Rosenhahn, B., Thormhlen, T., Seidel, H.P.: Multilinear pose and body shape estimation of dressed subjects from image sets. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1823–1830 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hasler%2C%20N.%2C%20Ackermann%2C%20H.%2C%20Rosenhahn%2C%20B.%2C%20Thormhlen%2C%20T.%2C%20Seidel%2C%20H.P.%3A%20Multilinear%20pose%20and%20body%20shape%20estimation%20of%20dressed%20subjects%20from%20image%20sets.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201823%E2%80%931830%20%282010%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR17">
             Ionescu, C., Carreira, J., Sminchisescu, C.: Iterated second-order label sensitive pooling for 3D human pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1661–1668 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ionescu%2C%20C.%2C%20Carreira%2C%20J.%2C%20Sminchisescu%2C%20C.%3A%20Iterated%20second-order%20label%20sensitive%20pooling%20for%203D%20human%20pose%20estimation.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201661%E2%80%931668%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR18">
             Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6M: large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE Trans. Pattern Anal. Mach. Intell. TPAMI
             <b>
              36
             </b>
             (7), 1325–1339 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1109/TPAMI.2013.248" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2013.248">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Human3.6M%3A%20large%20scale%20datasets%20and%20predictive%20methods%20for%203D%20human%20sensing%20in%20natural%20environments&amp;journal=IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.%20TPAMI&amp;volume=36&amp;issue=7&amp;pages=1325-1339&amp;publication_year=2014&amp;author=Ionescu%2CC&amp;author=Papava%2CD&amp;author=Olaru%2CV&amp;author=Sminchisescu%2CC">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR19">
             Jain, A., Thormählen, T., Seidel, H.P., Theobalt, C.: MovieReshape: tracking and reshaping of humans in videos. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH
             <b>
              29
             </b>
             (5), 148:1–148:10 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=MovieReshape%3A%20tracking%20and%20reshaping%20of%20humans%20in%20videos&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH&amp;volume=29&amp;issue=5&amp;pages=148%3A1-148%3A10&amp;publication_year=2010&amp;author=Jain%2CA&amp;author=Thorm%C3%A4hlen%2CT&amp;author=Seidel%2CHP&amp;author=Theobalt%2CC">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR20">
             Jain, A., Tompson, J., LeCun, Y., Bregler, C.: MoDeep: a deep learning framework using motion features for human pose estimation. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014. LNCS, vol. 9004, pp. 302–315. Springer, Heidelberg (2015). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-16808-1_21" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-16808-1_21">
              10.1007/978-3-319-16808-1_21
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=MoDeep%3A%20a%20deep%20learning%20framework%20using%20motion%20features%20for%20human%20pose%20estimation&amp;pages=302-315&amp;publication_year=2015&amp;author=Jain%2CA&amp;author=Tompson%2CJ&amp;author=LeCun%2CY&amp;author=Bregler%2CC">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR21">
             Jiang, H.: 3D human pose reconstruction using millions of exemplars. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1674–1677 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Jiang%2C%20H.%3A%203D%20human%20pose%20reconstruction%20using%20millions%20of%20exemplars.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201674%E2%80%931677%20%282010%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR22">
             Johnson, S., Everingham, M.: Clustered pose and nonlinear appearance models for human pose estimation. In: Proceedings of the British Machine Vision Conference, pp. 12.1-12.11 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Johnson%2C%20S.%2C%20Everingham%2C%20M.%3A%20Clustered%20pose%20and%20nonlinear%20appearance%20models%20for%20human%20pose%20estimation.%20In%3A%20Proceedings%20of%20the%20British%20Machine%20Vision%20Conference%2C%20pp.%2012.1-12.11%20%282010%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR23">
             Kiefel, M., Gehler, P.V.: Human pose estimation with fields of parts. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp. 331–346. Springer, Heidelberg (2014). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-10602-1_22" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-10602-1_22">
              10.1007/978-3-319-10602-1_22
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Human%20pose%20estimation%20with%20fields%20of%20parts&amp;pages=331-346&amp;publication_year=2014&amp;author=Kiefel%2CM&amp;author=Gehler%2CPV">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR24">
             Kostrikov, I., Gall, J.: Depth sweep regression forests for estimating 3D human pose from images. In: Proceedings of the British Machine Vision Conference (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kostrikov%2C%20I.%2C%20Gall%2C%20J.%3A%20Depth%20sweep%20regression%20forests%20for%20estimating%203D%20human%20pose%20from%20images.%20In%3A%20Proceedings%20of%20the%20British%20Machine%20Vision%20Conference%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR25">
             Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.: Picture: a probabilistic programming language for scene perception. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4390–4399 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kulkarni%2C%20T.D.%2C%20Kohli%2C%20P.%2C%20Tenenbaum%2C%20J.B.%2C%20Mansinghka%2C%20V.%3A%20Picture%3A%20a%20probabilistic%20programming%20language%20for%20scene%20perception.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204390%E2%80%934399%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR26">
             Lee, H., Chen, Z.: Determination of 3D human body postures from a single view. Comput. Vis. Graph. Image Process.
             <b>
              30
             </b>
             (2), 148–168 (1985)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1016/0734-189X(85)90094-5" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2F0734-189X%2885%2990094-5">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Determination%20of%203D%20human%20body%20postures%20from%20a%20single%20view&amp;journal=Comput.%20Vis.%20Graph.%20Image%20Process.&amp;volume=30&amp;issue=2&amp;pages=148-168&amp;publication_year=1985&amp;author=Lee%2CH&amp;author=Chen%2CZ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR27">
             Li, S., Chan, A.B.: 3D human pose estimation from monocular images with deep convolutional neural network. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014. LNCS, vol. 9004, pp. 332–347. Springer, Heidelberg (2015). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-16808-1_23" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-16808-1_23">
              10.1007/978-3-319-16808-1_23
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=3D%20human%20pose%20estimation%20from%20monocular%20images%20with%20deep%20convolutional%20neural%20network&amp;pages=332-347&amp;publication_year=2015&amp;author=Li%2CS&amp;author=Chan%2CAB">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR28">
             Loper, M.M., Black, M.J.: OpenDR: an approximate differentiable renderer. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8695, pp. 154–169. Springer, Heidelberg (2014). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-10584-0_11" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-10584-0_11">
              10.1007/978-3-319-10584-0_11
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=OpenDR%3A%20an%20approximate%20differentiable%20renderer&amp;pages=154-169&amp;publication_year=2014&amp;author=Loper%2CMM&amp;author=Black%2CMJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR29">
             Loper, M., Mahmood, N., Black, M.J.: MoSh: motion and shape capture from sparse markers. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia
             <b>
              33
             </b>
             (6), 220:1–220:13 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=MoSh%3A%20motion%20and%20shape%20capture%20from%20sparse%20markers&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH%20Asia&amp;volume=33&amp;issue=6&amp;pages=220%3A1-220%3A13&amp;publication_year=2014&amp;author=Loper%2CM&amp;author=Mahmood%2CN&amp;author=Black%2CMJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR30">
             Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-person linear model. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia
             <b>
              34
             </b>
             (6), 248: 1–248: 16 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=SMPL%3A%20a%20skinned%20multi-person%20linear%20model&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH%20Asia&amp;volume=34&amp;issue=6&amp;pages=248%3A%201-248%3A%2016&amp;publication_year=2015&amp;author=Loper%2CM&amp;author=Mahmood%2CN&amp;author=Romero%2CJ&amp;author=Pons-Moll%2CG&amp;author=Black%2CMJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR31">
             Nocedal, J., Wright, S.: Numerical Optimization. Springer, New York (2006)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="math reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1104.65059">
              MATH
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Numerical%20Optimization&amp;publication_year=2006&amp;author=Nocedal%2CJ&amp;author=Wright%2CS">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR32">
             Olson, E., Agarwal, P.: Inference on networks of mixtures for robust robot mapping. Int. J. Robot. Res.
             <b>
              32
             </b>
             (7), 826–840 (2013)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1177/0278364913479413" href="https://doi-org.proxy.lib.ohio-state.edu/10.1177%2F0278364913479413">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Inference%20on%20networks%20of%20mixtures%20for%20robust%20robot%20mapping&amp;journal=Int.%20J.%20Robot.%20Res.&amp;volume=32&amp;issue=7&amp;pages=826-840&amp;publication_year=2013&amp;author=Olson%2CE&amp;author=Agarwal%2CP">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR33">
             Parameswaran, V., Chellappa, R.: View independent human body pose estimation from a single perspective image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 16–22 (2004)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Parameswaran%2C%20V.%2C%20Chellappa%2C%20R.%3A%20View%20independent%20human%20body%20pose%20estimation%20from%20a%20single%20perspective%20image.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%2016%E2%80%9322%20%282004%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR34">
             Pfister, T., Charles, J., Zisserman, A.: Flowing convnets for human pose estimation in videos. In: IEEE International Conference on Computer Vision, ICCV, pp. 1913–1921 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Pfister%2C%20T.%2C%20Charles%2C%20J.%2C%20Zisserman%2C%20A.%3A%20Flowing%20convnets%20for%20human%20pose%20estimation%20in%20videos.%20In%3A%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20ICCV%2C%20pp.%201913%E2%80%931921%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR35">
             Pfister, T., Simonyan, K., Charles, J., Zisserman, A.: Deep convolutional neural networks for efficient pose estimation in gesture videos. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014. LNCS, vol. 9003, pp. 538–552. Springer, Heidelberg (2015). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-16865-4_35" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-16865-4_35">
              10.1007/978-3-319-16865-4_35
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Deep%20convolutional%20neural%20networks%20for%20efficient%20pose%20estimation%20in%20gesture%20videos&amp;pages=538-552&amp;publication_year=2015&amp;author=Pfister%2CT&amp;author=Simonyan%2CK&amp;author=Charles%2CJ&amp;author=Zisserman%2CA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR36">
             Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P., Schiele, B.: DeepCut: joint subset partition and labeling for multi person pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4929–4937 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Pishchulin%2C%20L.%2C%20Insafutdinov%2C%20E.%2C%20Tang%2C%20S.%2C%20Andres%2C%20B.%2C%20Andriluka%2C%20M.%2C%20Gehler%2C%20P.%2C%20Schiele%2C%20B.%3A%20DeepCut%3A%20joint%20subset%20partition%20and%20labeling%20for%20multi%20person%20pose%20estimation.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204929%E2%80%934937%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR37">
             Pons-Moll, G., Fleet, D., Rosenhahn, B.: Posebits for monocular human pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2345–2352 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Pons-Moll%2C%20G.%2C%20Fleet%2C%20D.%2C%20Rosenhahn%2C%20B.%3A%20Posebits%20for%20monocular%20human%20pose%20estimation.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%202345%E2%80%932352%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR38">
             Pons-Moll, G., Taylor, J., Shotton, J., Hertzmann, A., Fitzgibbon, A.: Metric regression forests for correspondence estimation. Int. J. Comput. Vis. IJCV
             <b>
              113
             </b>
             (3), 1–13 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="mathscinet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=3356158">
              MathSciNet
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Metric%20regression%20forests%20for%20correspondence%20estimation&amp;journal=Int.%20J.%20Comput.%20Vis.%20IJCV&amp;volume=113&amp;issue=3&amp;pages=1-13&amp;publication_year=2015&amp;author=Pons-Moll%2CG&amp;author=Taylor%2CJ&amp;author=Shotton%2CJ&amp;author=Hertzmann%2CA&amp;author=Fitzgibbon%2CA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR39">
             Ramakrishna, V., Kanade, T., Sheikh, Y.: Reconstructing 3D human pose from 2D image landmarks. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7575, pp. 573–586. Springer, Heidelberg (2012). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-33765-9_41" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-33765-9_41">
              10.1007/978-3-642-33765-9_41
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Reconstructing%203D%20human%20pose%20from%202D%20image%20landmarks&amp;pages=573-586&amp;publication_year=2012&amp;author=Ramakrishna%2CV&amp;author=Kanade%2CT&amp;author=Sheikh%2CY">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR40">
             Rother, C., Kolmogorov, V., Blake, A.: Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH
             <b>
              23
             </b>
             (3), 309–314 (2004)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/1015706.1015720" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F1015706.1015720">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Grabcut%3A%20interactive%20foreground%20extraction%20using%20iterated%20graph%20cuts&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH&amp;volume=23&amp;issue=3&amp;pages=309-314&amp;publication_year=2004&amp;author=Rother%2CC&amp;author=Kolmogorov%2CV&amp;author=Blake%2CA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR41">
             Sigal, L., Balan, A., Black, M.J.: HumanEva: synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. Int. J. Comput. Vis. IJCV
             <b>
              87
             </b>
             (1), 4–27 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/s11263-009-0273-6" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-009-0273-6">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=HumanEva%3A%20synchronized%20video%20and%20motion%20capture%20dataset%20and%20baseline%20algorithm%20for%20evaluation%20of%20articulated%20human%20motion&amp;journal=Int.%20J.%20Comput.%20Vis.%20IJCV&amp;volume=87&amp;issue=1&amp;pages=4-27&amp;publication_year=2010&amp;author=Sigal%2CL&amp;author=Balan%2CA&amp;author=Black%2CMJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR42">
             Sigal, L., Balan, A., Black, M.J.: Combined discriminative and generative articulated pose and non-rigid shape estimation. In: Advances in Neural Information Processing Systems (NIPS), vol. 20, pp. 1337–1344 (2008)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sigal%2C%20L.%2C%20Balan%2C%20A.%2C%20Black%2C%20M.J.%3A%20Combined%20discriminative%20and%20generative%20articulated%20pose%20and%20non-rigid%20shape%20estimation.%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%20%28NIPS%29%2C%20vol.%2020%2C%20pp.%201337%E2%80%931344%20%282008%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR43">
             Simo-Serra, E., Quattoni, A., Torras, C., Moreno-Noguer, F.: A joint model for 2D and 3D pose estimation from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3634–3641 (2013)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Simo-Serra%2C%20E.%2C%20Quattoni%2C%20A.%2C%20Torras%2C%20C.%2C%20Moreno-Noguer%2C%20F.%3A%20A%20joint%20model%20for%202D%20and%203D%20pose%20estimation%20from%20a%20single%20image.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%203634%E2%80%933641%20%282013%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR44">
             Simo-Serra, E., Ramisa, A., Alenya, G., Torras, C., Moreno-Noguer, F.: Single image 3D human pose estimation from noisy observations. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2673–2680 (2012)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Simo-Serra%2C%20E.%2C%20Ramisa%2C%20A.%2C%20Alenya%2C%20G.%2C%20Torras%2C%20C.%2C%20Moreno-Noguer%2C%20F.%3A%20Single%20image%203D%20human%20pose%20estimation%20from%20noisy%20observations.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%202673%E2%80%932680%20%282012%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR45">
             Sminchisescu, C., Telea, A.: Human pose estimation from silhouettes, a consistent approach using distance level sets. In: WSCG International Conference for Computer Graphics, Visualization and Computer Vision, pp. 413–420 (2002)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sminchisescu%2C%20C.%2C%20Telea%2C%20A.%3A%20Human%20pose%20estimation%20from%20silhouettes%2C%20a%20consistent%20approach%20using%20distance%20level%20sets.%20In%3A%20WSCG%20International%20Conference%20for%20Computer%20Graphics%2C%20Visualization%20and%20Computer%20Vision%2C%20pp.%20413%E2%80%93420%20%282002%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR46">
             Sminchisescu, C., Triggs, B.: Covariance scaled sampling for monocular 3D body tracking. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 447–454 (2001)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sminchisescu%2C%20C.%2C%20Triggs%2C%20B.%3A%20Covariance%20scaled%20sampling%20for%20monocular%203D%20body%20tracking.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%20447%E2%80%93454%20%282001%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR47">
             Sridhar, S., Mueller, F., Oulasvirta, A., Theobalt, C.: Fast and robust hand tracking using detection-guided optimization. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3121–3221 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sridhar%2C%20S.%2C%20Mueller%2C%20F.%2C%20Oulasvirta%2C%20A.%2C%20Theobalt%2C%20C.%3A%20Fast%20and%20robust%20hand%20tracking%20using%20detection-guided%20optimization.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%203121%E2%80%933221%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR48">
             Taylor, C.: Reconstruction of articulated objects from point correspondences in single uncalibrated image. Comput. Vis. Image Underst. CVIU
             <b>
              80
             </b>
             (10), 349–363 (2000)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1006/cviu.2000.0878" href="https://doi-org.proxy.lib.ohio-state.edu/10.1006%2Fcviu.2000.0878">
              CrossRef
             </a>
             <a data-track="click" data-track-action="math reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1011.68537">
              MATH
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Reconstruction%20of%20articulated%20objects%20from%20point%20correspondences%20in%20single%20uncalibrated%20image&amp;journal=Comput.%20Vis.%20Image%20Underst.%20CVIU&amp;volume=80&amp;issue=10&amp;pages=349-363&amp;publication_year=2000&amp;author=Taylor%2CC">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR49">
             Tekin, B., Rozantsev, A., Lepetit, V., Fua, P.: Direct prediction of 3D body poses from motion compensated sequences. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 991–1000 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tekin%2C%20B.%2C%20Rozantsev%2C%20A.%2C%20Lepetit%2C%20V.%2C%20Fua%2C%20P.%3A%20Direct%20prediction%20of%203D%20body%20poses%20from%20motion%20compensated%20sequences.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%20991%E2%80%931000%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR50">
             Thiery, J.M., Guy, E., Boubekeur, T.: Sphere-meshes: shape approximation using spherical quadric error metrics. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH Asia
             <b>
              32
             </b>
             (6), 178:1–178:12 (2013)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Sphere-meshes%3A%20shape%20approximation%20using%20spherical%20quadric%20error%20metrics&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH%20Asia&amp;volume=32&amp;issue=6&amp;pages=178%3A1-178%3A12&amp;publication_year=2013&amp;author=Thiery%2CJM&amp;author=Guy%2CE&amp;author=Boubekeur%2CT">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR51">
             Toshev, A., Szegedy, C.: DeepPose: human pose estimation via deep neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 1653–1660 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Toshev%2C%20A.%2C%20Szegedy%2C%20C.%3A%20DeepPose%3A%20human%20pose%20estimation%20via%20deep%20neural%20networks.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%201653%E2%80%931660%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR52">
             Wang, C., Wang, Y., Lin, Z., Yuille, A., Gao, W.: Robust estimation of 3D human poses from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2369–2376 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wang%2C%20C.%2C%20Wang%2C%20Y.%2C%20Lin%2C%20Z.%2C%20Yuille%2C%20A.%2C%20Gao%2C%20W.%3A%20Robust%20estimation%20of%203D%20human%20poses%20from%20a%20single%20image.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%202369%E2%80%932376%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR53">
             Wei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional pose machines. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4724–4732 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wei%2C%20S.E.%2C%20Ramakrishna%2C%20V.%2C%20Kanade%2C%20T.%2C%20Sheikh%2C%20Y.%3A%20Convolutional%20pose%20machines.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204724%E2%80%934732%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR54">
             Yang, Y., Ramanan, D.: Articulated pose estimation using flexible mixtures of parts. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3546–3553 (2011)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yang%2C%20Y.%2C%20Ramanan%2C%20D.%3A%20Articulated%20pose%20estimation%20using%20flexible%20mixtures%20of%20parts.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%203546%E2%80%933553%20%282011%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR55">
             Yasin, H., Iqbal, U., Krüger, B., Weber, A., Gall, J.: A dual-source approach for 3D pose estimation from a single image. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4948–4956 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yasin%2C%20H.%2C%20Iqbal%2C%20U.%2C%20Kr%C3%BCger%2C%20B.%2C%20Weber%2C%20A.%2C%20Gall%2C%20J.%3A%20A%20dual-source%20approach%20for%203D%20pose%20estimation%20from%20a%20single%20image.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204948%E2%80%934956%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR56">
             Zhou, F., Torre, F.: Spatio-temporal matching for human detection in video. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8694, pp. 62–77. Springer, Heidelberg (2014). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-10599-4_5" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-10599-4_5">
              10.1007/978-3-319-10599-4_5
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Spatio-temporal%20matching%20for%20human%20detection%20in%20video&amp;pages=62-77&amp;publication_year=2014&amp;author=Zhou%2CF&amp;author=Torre%2CF">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR57">
             Zhou, S., Fu, H., Liu, L., Cohen-Or, D., Han, X.: Parametric reshaping of human bodies in images. ACM Trans. Graph. (TOG) - Proc. ACM SIGGRAPH
             <b>
              29
             </b>
             (4), 126:1–126:10 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Parametric%20reshaping%20of%20human%20bodies%20in%20images&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29%20-%20Proc.%20ACM%20SIGGRAPH&amp;volume=29&amp;issue=4&amp;pages=126%3A1-126%3A10&amp;publication_year=2010&amp;author=Zhou%2CS&amp;author=Fu%2CH&amp;author=Liu%2CL&amp;author=Cohen-Or%2CD&amp;author=Han%2CX">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR58">
             Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparse representation for 3D shape estimation: a convex relaxation approach. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhou%2C%20X.%2C%20Zhu%2C%20M.%2C%20Leonardos%2C%20S.%2C%20Derpanis%2C%20K.%2C%20Daniilidis%2C%20K.%3A%20Sparse%20representation%20for%203D%20shape%20estimation%3A%20a%20convex%20relaxation%20approach.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204447%E2%80%934455%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR59">
             Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4447–4455 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhou%2C%20X.%2C%20Zhu%2C%20M.%2C%20Leonardos%2C%20S.%2C%20Derpanis%2C%20K.%2C%20Daniilidis%2C%20K.%3A%20Sparseness%20meets%20deepness%3A%203D%20human%20pose%20estimation%20from%20monocular%20video.%20In%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20CVPR%2C%20pp.%204447%E2%80%934455%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
          </ol>
         </div>
        </div>
       </div>
      </div>
     </aside>
    </div>
   </div>
   <div class="app-elements">
    <footer data-test="universal-footer">
     <div class="c-footer" data-track-component="unified-footer">
      <div class="c-footer__container">
       <div class="c-footer__grid c-footer__group--separator">
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Discover content
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="journals a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
            Journals A-Z
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="books a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/books/a/1">
            Books A-Z
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Publish with us
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="publish your research" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
            Publish your research
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="open access publishing" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/open-research/about/the-fundamentals-of-open-access-and-open-research">
            Open access publishing
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Products and services
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="our products" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/products">
            Our products
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="librarians" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/librarians">
            Librarians
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="societies" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/societies">
            Societies
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="partners and advertisers" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/partners">
            Partners and advertisers
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Our imprints
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Springer" data-track-label="link" href="https://www-springer-com.proxy.lib.ohio-state.edu/">
            Springer
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Nature Portfolio" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/">
            Nature Portfolio
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="BMC" data-track-label="link" href="https://www-biomedcentral-com.proxy.lib.ohio-state.edu/">
            BMC
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Palgrave Macmillan" data-track-label="link" href="https://www.palgrave.com/">
            Palgrave Macmillan
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Apress" data-track-label="link" href="https://www.apress.com/">
            Apress
           </a>
          </li>
         </ul>
        </div>
       </div>
      </div>
      <div class="c-footer__container">
       <nav aria-label="footer navigation">
        <ul class="c-footer__links">
         <li class="c-footer__item">
          <button class="c-footer__link" data-cc-action="preferences" data-track="click" data-track-action="Manage cookies" data-track-label="link">
           <span class="c-footer__button-text">
            Your privacy choices/Manage cookies
           </span>
          </button>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="california privacy statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/legal/ccpa">
           Your US state privacy rights
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="accessibility statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/info/accessibility">
           Accessibility statement
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="terms and conditions" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/termsandconditions">
           Terms and conditions
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="privacy policy" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/privacystatement">
           Privacy policy
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="help and support" data-track-label="link" href="https://support-springernature-com.proxy.lib.ohio-state.edu/en/support/home">
           Help and support
          </a>
         </li>
        </ul>
       </nav>
       <div class="c-footer__user">
        <p class="c-footer__user-info">
         <span data-test="footer-user-ip">
          3.128.143.42
         </span>
        </p>
        <p class="c-footer__user-info" data-test="footer-business-partners">
         OhioLINK Consortium (3000266689)  - Ohio State University Libraries (8200724141)
        </p>
       </div>
       <a class="c-footer__link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/">
        <img alt="Springer Nature" height="20" loading="lazy" src="/oscar-static/images/darwin/footer/img/logo-springernature_white-64dbfad7d8.svg" width="200"/>
       </a>
       <p class="c-footer__legal" data-test="copyright">
        © 2023 Springer Nature
       </p>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <div aria-hidden="true" class="u-visually-hidden">
   <!--?xml version="1.0" encoding="UTF-8"?-->
   <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <defs>
     <path d="M0 .74h56.72v55.24H0z" id="a">
     </path>
    </defs>
    <symbol id="icon-access" viewbox="0 0 18 18">
     <path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-account" viewbox="0 0 18 18">
     <path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-alert" viewbox="0 0 18 18">
     <path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-broad" viewbox="0 0 16 16">
     <path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)">
     </path>
    </symbol>
    <symbol id="icon-arrow-down" viewbox="0 0 16 16">
     <path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-left" viewbox="0 0 16 16">
     <path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-right" viewbox="0 0 16 16">
     <path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-sub" viewbox="0 0 16 16">
     <path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-up" viewbox="0 0 16 16">
     <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-article" viewbox="0 0 18 18">
     <path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-audio" viewbox="0 0 18 18">
     <path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-block" viewbox="0 0 24 24">
     <path d="m0 0h24v24h-24z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-book" viewbox="0 0 18 18">
     <path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-broad" viewbox="0 0 24 24">
     <path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)">
     </path>
    </symbol>
    <symbol id="icon-calendar" viewbox="0 0 18 18">
     <path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-cart" viewbox="0 0 18 18">
     <path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z">
     </path>
    </symbol>
    <symbol id="icon-chevron-less" viewbox="0 0 10 10">
     <path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)">
     </path>
    </symbol>
    <symbol id="icon-chevron-more" viewbox="0 0 10 10">
     <path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-chevron-right" viewbox="0 0 10 10">
     <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
     </path>
    </symbol>
    <symbol id="icon-circle-fill" viewbox="0 0 16 16">
     <path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-circle" viewbox="0 0 16 16">
     <path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-citation" viewbox="0 0 18 18">
     <path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-close" viewbox="0 0 16 16">
     <path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-collections" viewbox="0 0 18 18">
     <path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-compare" viewbox="0 0 18 18">
     <path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download-file" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download" viewbox="0 0 16 16">
     <path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-editors" viewbox="0 0 18 18">
     <path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-email" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-error" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-ethics" viewbox="0 0 18 18">
     <path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-expand">
     <path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-explore" viewbox="0 0 18 18">
     <path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-filter" viewbox="0 0 16 16">
     <path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z">
     </path>
    </symbol>
    <symbol id="icon-home" viewbox="0 0 18 18">
     <path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-image" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-info" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-institution" viewbox="0 0 18 18">
     <path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-location" viewbox="0 0 18 18">
     <path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-minus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-newsletter" viewbox="0 0 18 18">
     <path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-orcid" viewbox="0 0 18 18">
     <path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-plus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-print" viewbox="0 0 18 18">
     <path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-search" viewbox="0 0 22 22">
     <path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-facebook" viewbox="0 0 24 24">
     <path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-twitter" viewbox="0 0 24 24">
     <path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-youtube" viewbox="0 0 24 24">
     <path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-subject-medicine" viewbox="0 0 18 18">
     <path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-success" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-table" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick-circle" viewbox="0 0 24 24">
     <path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick" viewbox="0 0 16 16">
     <path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-update" viewbox="0 0 18 18">
     <path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-upload" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-video" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-warning" viewbox="0 0 18 18">
     <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-altmetric">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm-1.886 9.684-1.101 1.845a1 1 0 0 1-.728.479l-.13.008H3.056a9.001 9.001 0 0 0 17.886 0l-4.564-.001-2.779 4.156c-.454.68-1.467.55-1.758-.179l-.038-.113-1.69-6.195ZM12 3a9.001 9.001 0 0 0-8.947 8.016h4.533l2.017-3.375c.452-.757 1.592-.6 1.824.25l1.73 6.345 1.858-2.777a1 1 0 0 1 .707-.436l.124-.008h5.1A9.001 9.001 0 0 0 12 3Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-checklist-banner" viewbox="0 0 56.69 56.69">
     <path d="M0 0h56.69v56.69H0z" style="fill:none">
     </path>
     <clippath id="b">
      <use style="overflow:visible" xlink:href="#a">
      </use>
     </clippath>
     <path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round">
     </path>
     <path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round">
     </path>
    </symbol>
    <symbol id="icon-chevron-down" viewbox="0 0 16 16">
     <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-citations">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-eds-checklist" viewbox="0 0 32 32">
     <path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-external-link-medium" viewbox="0 0 24 24">
     <path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-info-filled-medium" viewbox="0 0 24 24">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z">
     </path>
    </symbol>
    <symbol id="icon-eds-menu" viewbox="0 0 24 24">
     <path d="M21.09 5c.503 0 .91.448.91 1s-.407 1-.91 1H2.91C2.406 7 2 6.552 2 6s.407-1 .91-1h18.18Zm-3.817 6c.401 0 .727.448.727 1s-.326 1-.727 1H2.727C2.326 13 2 12.552 2 12s.326-1 .727-1h14.546Zm3.818 6c.502 0 .909.448.909 1s-.407 1-.91 1H2.91c-.503 0-.91-.448-.91-1s.407-1 .91-1h18.18Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-search" viewbox="0 0 24 24">
     <path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-small-arrow-right" viewbox="0 0 16 16">
     <g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035">
      </path>
     </g>
    </symbol>
    <symbol id="icon-eds-user-single" viewbox="0 0 24 24">
     <path d="M12 12c5.498 0 10 4.001 10 9a1 1 0 0 1-2 0c0-3.838-3.557-7-8-7s-8 3.162-8 7a1 1 0 0 1-2 0c0-4.999 4.502-9 10-9Zm0-11a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm0 2a3 3 0 1 1 0 6 3 3 0 0 1 0-6Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-email-new" viewbox="0 0 24 24">
     <path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z">
     </path>
    </symbol>
    <symbol id="icon-expand-image" viewbox="0 0 18 18">
     <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-github" viewbox="0 0 100 100">
     <path clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-mentions">
     <g fill-rule="evenodd" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M22 15.255A9.373 9.373 0 0 1 8.745 2L22 15.255ZM15.477 8.523l4.215-4.215">
      </path>
      <path d="m7 13-5 9h10l-1-5">
      </path>
     </g>
    </symbol>
    <symbol id="icon-metrics-accesses">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM7.708 13.308c2.004 0 3.969 1.198 5.802 2.995l.23.23a2.285 2.285 0 0 1 .009 3.233C11.853 21.693 9.799 23 7.707 23c-2.091 0-4.14-1.305-6.033-3.226a2.285 2.285 0 0 1-.007-3.233c1.9-1.93 3.949-3.233 6.04-3.233Zm0 2c-1.396 0-3.064 1.062-4.623 2.644a.285.285 0 0 0 .007.41C4.642 19.938 6.311 21 7.707 21c1.397 0 3.069-1.065 4.623-2.644a.285.285 0 0 0 0-.404l-.23-.229c-1.487-1.451-3.064-2.415-4.393-2.415Zm-.036 1.077a1.77 1.77 0 1 1 .126 3.537 1.77 1.77 0 0 1-.126-3.537Zm.072 1.538a.23.23 0 1 0-.017.461.23.23 0 0 0 .017-.46Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-metrics">
     <path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-left">
     <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-right">
     <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z">
     </path>
    </symbol>
    <symbol id="icon-submit-open" viewbox="0 0 16 17">
     <path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero">
     </path>
    </symbol>
   </svg>
  </div>
  <script nomodule="true" src="/oscar-static/js/app-es5-bundle-774ca0a0f5.js">
  </script>
  <script src="/oscar-static/js/app-es6-bundle-047cc3c848.js" type="module">
  </script>
  <script nomodule="true" src="/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js">
  </script>
  <script src="/oscar-static/js/global-article-es6-bundle-c14b406246.js" type="module">
  </script>
  <div class="c-cookie-banner">
   <div class="c-cookie-banner__container">
    <p>
     This website sets only cookies which are necessary for it to function. They are used to enable core functionality such as security, network management and accessibility. These cookies cannot be switched off in our systems. You may disable these by changing your browser settings, but this may affect how the website functions. Please view our privacy policy for further details on how we process your information.
     <button class="c-cookie-banner__dismiss">
      Dismiss
     </button>
    </p>
   </div>
  </div>
 </body>
</html>
