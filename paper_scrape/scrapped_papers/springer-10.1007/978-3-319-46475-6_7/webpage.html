<html class="js" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta content="pc,mobile" name="applicable-device"/>
  <meta content="Yes" name="access"/>
  <meta content="SpringerLink" name="twitter:site"/>
  <meta content="summary" name="twitter:card"/>
  <meta content="Content cover image" name="twitter:image:alt"/>
  <meta content="Playing for Data: Ground Truth from Computer Games" name="twitter:title"/>
  <meta content="Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an..." name="twitter:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-319-46475-6.jpg" name="twitter:image"/>
  <meta content="10.1007/978-3-319-46475-6_7" name="dc.identifier"/>
  <meta content="10.1007/978-3-319-46475-6_7" name="DOI"/>
  <meta content="Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an..." name="dc.description"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/content/pdf/10.1007/978-3-319-46475-6_7.pdf" name="citation_pdf_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7" name="citation_fulltext_html_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7" name="citation_abstract_html_url"/>
  <meta content="Computer Vision – ECCV 2016" name="citation_inbook_title"/>
  <meta content="Playing for Data: Ground Truth from Computer Games" name="citation_title"/>
  <meta content="2016" name="citation_publication_date"/>
  <meta content="102" name="citation_firstpage"/>
  <meta content="118" name="citation_lastpage"/>
  <meta content="en" name="citation_language"/>
  <meta content="10.1007/978-3-319-46475-6_7" name="citation_doi"/>
  <meta content="springer/eccv, dblp/eccv" name="citation_conference_series_id"/>
  <meta content="European Conference on Computer Vision" name="citation_conference_title"/>
  <meta content="ECCV" name="citation_conference_abbrev"/>
  <meta content="172342" name="size"/>
  <meta content="Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an..." name="description"/>
  <meta content="Richter, Stephan R." name="citation_author"/>
  <meta content="stephan.richter@visinf.tu-darmstadt.de" name="citation_author_email"/>
  <meta content="TU Darmstadt" name="citation_author_institution"/>
  <meta content="Vineet, Vibhav" name="citation_author"/>
  <meta content="Intel Labs" name="citation_author_institution"/>
  <meta content="Roth, Stefan" name="citation_author"/>
  <meta content="TU Darmstadt" name="citation_author_institution"/>
  <meta content="Koltun, Vladlen" name="citation_author"/>
  <meta content="Intel Labs" name="citation_author_institution"/>
  <meta content="Springer, Cham" name="citation_publisher"/>
  <meta content="http://api.springer-com.proxy.lib.ohio-state.edu/xmldata/jats?q=doi:10.1007/978-3-319-46475-6_7&amp;api_key=" name="citation_springer_api_url"/>
  <meta content="telephone=no" name="format-detection"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7" property="og:url"/>
  <meta content="Paper" property="og:type"/>
  <meta content="SpringerLink" property="og:site_name"/>
  <meta content="Playing for Data: Ground Truth from Computer Games" property="og:title"/>
  <meta content="Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an..." property="og:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-319-46475-6.jpg" property="og:image"/>
  <title>
   Playing for Data: Ground Truth from Computer Games | SpringerLink
  </title>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="shortcut icon"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="icon" sizes="16x16 32x32 48x48"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png" rel="icon" sizes="16x16" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png" rel="icon" sizes="32x32" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png" rel="icon" sizes="48x48" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png" rel="apple-touch-icon" sizes="72x72"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png" rel="apple-touch-icon" sizes="76x76"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png" rel="apple-touch-icon" sizes="114x114"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png" rel="apple-touch-icon" sizes="120x120"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png" rel="apple-touch-icon" sizes="144x144"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png" rel="apple-touch-icon" sizes="152x152"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon" sizes="180x180"/>
  <script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_SVG.js">
  </script>
  <script>
   (function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)
  </script>
  <script data-consent="link-springer-com.proxy.lib.ohio-state.edu" src="/static/js/lib/cookie-consent.min.js">
  </script>
  <style>
   @media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}body,img{max-width:100%}body{background:#fcfcfc;font-size:1.125rem;line-height:1.5;min-height:100%}main{display:block}h1{font-family:Georgia,Palatino,serif;font-size:2.25rem;font-style:normal;font-weight:400;line-height:1.4;margin:.67em 0}a{background-color:transparent;color:#004b83;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}sup{font-size:75%;line-height:0;position:relative;top:-.5em;vertical-align:baseline}img{border:0;height:auto;vertical-align:middle}button,input{font-family:inherit;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}*{margin:0}h2{font-family:Georgia,Palatino,serif;font-size:1.75rem;font-style:normal;font-weight:400;line-height:1.4}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input,p{margin:0;padding:0}a>img{vertical-align:middle}p{overflow-wrap:break-word;word-break:break-word}.c-app-header__theme{border-top-left-radius:2px;border-top-right-radius:2px;height:50px;margin:-16px -16px 0;overflow:hidden;position:relative}@media only screen and (min-width:1024px){.c-app-header__theme:after{background-color:hsla(0,0%,100%,.15);bottom:0;content:"";position:absolute;right:0;top:0;width:456px}}.c-app-header__content{padding-top:16px}@media only screen and (min-width:1024px){.c-app-header__content{display:flex}}.c-app-header__main{display:flex;flex:1 1 auto}.c-app-header__cover{margin-right:16px;margin-top:-50px;position:relative;z-index:5}.c-app-header__cover img{border:2px solid #fff;border-radius:4px;box-shadow:0 0 5px 2px hsla(0,0%,50%,.2);max-height:125px;max-width:96px}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-breadcrumbs>li{display:inline}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;list-style:none;margin:0;padding:16px}@media only screen and (min-width:540px){.c-pagination{justify-content:center}}.c-pagination__item{margin-bottom:8px;margin-right:16px}.c-pagination__item:last-child{margin-right:0}.c-pagination__link{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;min-width:30px;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link svg,.c-pagination__link--disabled svg{fill:currentcolor}.c-pagination__link:visited{color:#004b83}.c-pagination__link:focus,.c-pagination__link:hover{border:1px solid #666;text-decoration:none}.c-pagination__link:focus,.c-pagination__link:hover{background-color:#666;background-image:none;color:#fff}.c-pagination__link:focus svg path,.c-pagination__link:hover svg path{fill:#fff}.c-pagination__link--disabled{align-items:center;background-color:transparent;background-image:none;border-radius:2px;color:#333;cursor:default;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;opacity:.67;padding:8px;position:relative;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link--disabled:visited{color:#333}.c-pagination__link--disabled,.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{border:1px solid #ccc;text-decoration:none}.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{background-color:transparent;background-image:none;color:#333}.c-pagination__link--disabled:focus svg path,.c-pagination__link--disabled:hover svg path{fill:#333}.c-pagination__link--active{background-color:#666;background-image:none;border-color:#666;color:#fff;cursor:default}.c-pagination__ellipsis{background:0 0;border:0;min-width:auto;padding-left:0;padding-right:0}.c-pagination__icon{fill:#999;height:12px;width:16px}.c-pagination__icon--active{fill:#004b83}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#666;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-box{background-color:#fff;border:1px solid #ccc;border-radius:2px;line-height:1.3;padding:16px}.c-box--shadowed{box-shadow:0 0 5px 0 hsla(0,0%,50%,.1)}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #f2f2f2;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__section-item .c-article-section__title-number{display:none}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff}.c-pdf-download__link .u-icon{padding-top:2px}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}.u-button svg,.u-button--primary svg{fill:currentcolor}.app-elements .c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;line-height:1.4;padding:8px 0 0}.app-elements .c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav{border-top:2px solid #cedbe0;padding-top:4px;position:relative}.app-elements .c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav-container>:not(:last-child){margin-right:32px}.app-elements .c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.app-elements .c-header__list{list-style:none;margin:0;padding:0}.app-elements .c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.app-elements .c-header__list-item:not(:last-child){border-bottom:2px solid #cedbe0}.app-elements .c-header__item{color:inherit}@media only screen and (min-width:540px){.app-elements .c-header__item--menu{display:none;visibility:hidden}.app-elements .c-header__item--menu:first-child+*{margin-block-start:0}}.app-elements .c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:540px){.app-elements .c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.app-elements .c-header__item--divider:before{border-left:2px solid #cedbe0;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.app-elements .c-header__brand a{display:block;line-height:1;padding:16px 8px;text-decoration:none}.app-elements .c-header__brand img{height:24px;width:auto}.app-elements .c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-elements .c-header__link--static{flex:0 0 auto}.app-elements .c-header__icon{fill:currentcolor;display:inline-block;font-size:24px;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.app-elements .c-header__icon+*{margin-left:8px}.app-elements .c-header__expander{background-color:#ebf1f5}.app-elements .c-header__search{padding:24px 0}@media only screen and (min-width:540px){.app-elements .c-header__search{max-width:70%}}.app-elements .c-header__search-container{position:relative}.app-elements .c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.app-elements .c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.app-elements .c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.app-elements .has-tethered.c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:540px){.app-elements .has-tethered.c-header__expander--menu{display:none;visibility:hidden}}.app-elements .has-tethered .c-header__heading{display:none;visibility:hidden}.app-elements .has-tethered .c-header__heading:first-child+*{margin-block-start:0}.app-elements .has-tethered .c-header__search{margin:auto}.app-elements .c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-justify-content-space-between{justify-content:space-between}.u-flex-shrink{flex:0 1 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-mt-0{margin-top:0}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-float-left{float:left}.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-text-sm{font-size:1rem}.u-text-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-h3{font-family:Georgia,Palatino,serif;font-size:1.5rem;font-style:normal;font-weight:400;line-height:1.4}.c-article-section__content p{line-height:1.8}.c-pagination__input{border:1px solid #bfbfbf;border-radius:2px;box-shadow:inset 0 2px 6px 0 rgba(51,51,51,.2);box-sizing:initial;display:inline-block;height:28px;margin:0;max-width:64px;min-width:16px;padding:0 8px;text-align:center;transition:width .15s ease 0s}.c-pagination__input::-webkit-inner-spin-button,.c-pagination__input::-webkit-outer-spin-button{-webkit-appearance:none;margin:0}.c-article-associated-content__container .c-article-associated-content__collection-label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.063rem}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.063rem;font-weight:400}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-section__title,.c-article-title{font-weight:400}.c-chapter-book-series{font-size:1rem}.c-chapter-identifiers{margin:16px 0 8px}.c-chapter-book-details{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative}.c-chapter-book-details__title{font-weight:700}.c-chapter-book-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-chapter-book-details a{color:inherit}@media only screen and (max-width:539px){.c-chapter-book-details__meta{display:block}}.c-cover-image-lightbox{align-items:center;bottom:0;display:flex;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;transition:all .15s ease-in 0s;visibility:hidden;z-index:-1}.js-cover-image-lightbox--close{background:0 0;border:0;color:#fff;cursor:pointer;font-size:1.875rem;padding:13px;position:absolute;right:10px;top:0}.c-cover-image-lightbox__image{max-height:90vh;width:auto}.c-expand-overlay{background:#fff;color:#333;opacity:.5;padding:2px;position:absolute;right:3px;top:3px}.c-pdf-download__link{padding:13px 24px} }
  </style>
  <link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/enhanced-article-927ffe4eaf.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
  <script>
   window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/978-3-319-46475-6_7","Page":"chapter","page":{"attributes":{"environment":"live"}},"Country":"US","japan":false,"doi":"10.1007-978-3-319-46475-6_7","Keywords":"Association Rule, Association Rule Mining, Graphic Hardware, Visual Odometry, Semantic Label","kwrd":["Association_Rule","Association_Rule_Mining","Graphic_Hardware","Visual_Odometry","Semantic_Label"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate","cobranding","doNotAutoAssociate","cobranding"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3000266689","8200724141"],"businessPartnerIDString":"3000266689|8200724141"}},"Access Type":"permanently-free","Bpids":"3000266689, 8200724141","Bpnames":"OhioLINK Consortium, Ohio State University Libraries","BPID":["3000266689","8200724141"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-978-3-319-46475-6","Full HTML":"Y","session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1611-3349","pissn":"0302-9743"},"book":{"doi":"10.1007/978-3-319-46475-6","title":"Computer Vision – ECCV 2016","pisbn":"978-3-319-46474-9","eisbn":"978-3-319-46475-6","bookProductType":"Proceedings","seriesTitle":"Lecture Notes in Computer Science","seriesId":"558"},"chapter":{"doi":"10.1007/978-3-319-46475-6_7"},"type":"ConferencePaper","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"SCI","secondarySubjects":{"1":"Image Processing and Computer Vision","2":"Pattern Recognition","3":"Artificial Intelligence","4":"Computer Graphics"},"secondarySubjectCodes":{"1":"SCI22021","2":"SCI2203X","3":"SCI21000","4":"SCI22013"}},"sucode":"SUCO11645"},"attributes":{"deliveryPlatform":"oscar"},"country":"US","Has Preview":"N","subjectCodes":"SCI,SCI22021,SCI2203X,SCI21000,SCI22013","PMC":["SCI","SCI22021","SCI2203X","SCI21000","SCI22013"]},"Event Category":"Conference Paper","ConferenceSeriesId":"eccv, eccv","productId":"9783319464756"}];
  </script>
  <script>
   window.dataLayer.push({
        ga4MeasurementId: 'G-B3E4QL2TPR',
        ga360TrackingId: 'UA-26408784-1',
        twitterId: 'o47a7',
        ga4ServerUrl: 'https://collect-springer-com.proxy.lib.ohio-state.edu',
        imprint: 'springerlink'
    });
  </script>
  <script data-test="gtm-head">
   window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
  </script>
  <script>
   (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/lib/cookie-consent.min.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
  </script>
  <script>
   (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
  </script>
  <script>
   (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
  </script>
  <script class="js-entry">
   if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-f934ac6316.js', 'async': false},
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/app-es5-bundle-774ca0a0f5.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/app-es6-bundle-047cc3c848.js', 'async': false, 'module': true}
                
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-c14b406246.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
  </script>
  <script src="/oscar-static/js/airbrake-es5-bundle-f934ac6316.js">
  </script>
  <script src="/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js">
  </script>
  <link href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7" rel="canonical"/>
  <script type="application/ld+json">
   {"headline":"Playing for Data: Ground Truth from Computer Games","pageEnd":"118","pageStart":"102","image":"https://media-springernature-com.proxy.lib.ohio-state.edu/w153/springer-static/cover/book/978-3-319-46475-6.jpg","genre":["Computer Science","Computer Science (R0)"],"isPartOf":{"name":"Computer Vision – ECCV 2016","isbn":["978-3-319-46475-6","978-3-319-46474-9"],"@type":"Book"},"publisher":{"name":"Springer International Publishing","logo":{"url":"https://www-springernature-com.proxy.lib.ohio-state.edu/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Stephan R. Richter","affiliation":[{"name":"TU Darmstadt","address":{"name":"TU Darmstadt, Darmstadt, Germany","@type":"PostalAddress"},"@type":"Organization"}],"email":"stephan.richter@visinf.tu-darmstadt.de","@type":"Person"},{"name":"Vibhav Vineet","affiliation":[{"name":"Intel Labs","address":{"name":"Intel Labs, Santa Clara, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Stefan Roth","affiliation":[{"name":"TU Darmstadt","address":{"name":"TU Darmstadt, Darmstadt, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Vladlen Koltun","affiliation":[{"name":"Intel Labs","address":{"name":"Intel Labs, Santa Clara, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"keywords":"Association Rule, Association Rule Mining, Graphic Hardware, Visual Odometry, Semantic Label","description":"Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just \n                  \n                    \n                  \n                  $$\\tfrac{1}{3}$$\n                  \n                      \n                    \n                 of the CamVid training set outperform models trained on the complete CamVid training set.","datePublished":"2016","isAccessibleForFree":true,"@type":"ScholarlyArticle","@context":"https://schema.org"}
  </script>
  <style type="text/css">
   .c-cookie-banner {
			background-color: #01324b;
			color: white;
			font-size: 1rem;
			position: fixed;
			bottom: 0;
			left: 0;
			right: 0;
			padding: 16px 0;
			font-family: sans-serif;
			z-index: 100002;
			text-align: center;
		}
		.c-cookie-banner__container {
			margin: 0 auto;
			max-width: 1280px;
			padding: 0 16px;
		}
		.c-cookie-banner p {
			margin-bottom: 8px;
		}
		.c-cookie-banner p:last-child {
			margin-bottom: 0;
		}	
		.c-cookie-banner__dismiss {
			background-color: transparent;
			border: 0;
			padding: 0;
			margin-left: 4px;
			color: inherit;
			text-decoration: underline;
			font-size: inherit;
		}
		.c-cookie-banner__dismiss:hover {
			text-decoration: none;
		}
  </style>
  <style type="text/css">
   .MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
  </style>
  <style type="text/css">
   #MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
  </style>
  <style type="text/css">
   .MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
  </style>
  <style type="text/css">
   #MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
  </style>
  <style type="text/css">
   .MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
  </style>
  <style type="text/css">
   .MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <style type="text/css">
   .MathJax_SVG_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax_SVG .MJX-monospace {font-family: monospace}
.MathJax_SVG .MJX-sans-serif {font-family: sans-serif}
#MathJax_SVG_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax_SVG {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax_SVG * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_SVG > div {display: inline-block}
.mjx-svg-href {fill: blue; stroke: blue}
.MathJax_SVG_Processing {visibility: hidden; position: absolute; top: 0; left: 0; width: 0; height: 0; overflow: hidden; display: block!important}
.MathJax_SVG_Processed {display: none!important}
.MathJax_SVG_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_SVG_test.mjx-test-display {display: table!important}
.MathJax_SVG_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_SVG_test.mjx-test-default {display: block!important; clear: both}
.MathJax_SVG_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .MathJax_SVG_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_SVG_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_SVG_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax_SVG .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <script>
   window.dataLayer = window.dataLayer || [];
            window.dataLayer.push({
                recommendations: {
                    recommender: 'semantic',
                    model: 'specter',
                    policy_id: 'NA',
                    timestamp: 1698023164,
                    embedded_user: 'null'
                }
            });
  </script>
 </head>
 <body class="shared-article-renderer">
  <div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;">
   <div id="MathJax_SVG_Hidden">
    <div class="MathJax_SVG_Display MathJax_SVG_Processing">
     <span class="MathJax_SVG" id="MathJax-Element-19-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
     </span>
    </div>
    <br/>
   </div>
   <svg>
    <defs id="MathJax_SVG_glyphs">
     <path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" id="MJMAIN-31" stroke-width="1">
     </path>
     <path d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" id="MJMAIN-33" stroke-width="1">
     </path>
     <path d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" id="MJMAIN-74" stroke-width="1">
     </path>
     <path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="MJMAIN-68" stroke-width="1">
     </path>
     <path d="M333 -232Q332 -239 327 -244T313 -250Q303 -250 296 -240Q293 -233 202 6T110 250T201 494T296 740Q299 745 306 749L309 750Q312 750 313 750Q331 750 333 732Q333 727 243 489Q152 252 152 250T243 11Q333 -227 333 -232Z" id="MJMAIN-27E8" stroke-width="1">
     </path>
     <path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="MJMAIN-6D" stroke-width="1">
     </path>
     <path d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" id="MJMAIN-65" stroke-width="1">
     </path>
     <path d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" id="MJMAIN-73" stroke-width="1">
     </path>
     <path d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" id="MJMAIN-2C" stroke-width="1">
     </path>
     <path d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" id="MJMAIN-78" stroke-width="1">
     </path>
     <path d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" id="MJMAIN-75" stroke-width="1">
     </path>
     <path d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" id="MJMAIN-72" stroke-width="1">
     </path>
     <path d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" id="MJMAIN-61" stroke-width="1">
     </path>
     <path d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" id="MJMAIN-64" stroke-width="1">
     </path>
     <path d="M55 732Q56 739 61 744T75 750Q85 750 92 740Q95 733 186 494T278 250T187 6T92 -240Q85 -250 75 -250Q67 -250 62 -245T55 -232Q55 -227 145 11Q236 248 236 250T145 489Q55 727 55 732Z" id="MJMAIN-27E9" stroke-width="1">
     </path>
     <path d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" id="MJMAIN-39" stroke-width="1">
     </path>
     <path d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" id="MJMAIN-34" stroke-width="1">
     </path>
     <path d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" id="MJMAIN-D7" stroke-width="1">
     </path>
     <path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" id="MJMAIN-30" stroke-width="1">
     </path>
     <path d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" id="MJMAIN-35" stroke-width="1">
     </path>
     <path d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" id="MJMAIN-32" stroke-width="1">
     </path>
     <path d="M465 605Q428 605 394 614T340 632T319 641Q332 608 332 548Q332 458 293 403T202 347Q145 347 101 402T56 548Q56 637 101 693T202 750Q241 750 272 719Q359 642 464 642Q580 642 650 732Q662 748 668 749Q670 750 673 750Q682 750 688 743T693 726Q178 -47 170 -52Q166 -56 160 -56Q147 -56 142 -45Q137 -36 142 -27Q143 -24 363 304Q469 462 525 546T581 630Q528 605 465 605ZM207 385Q235 385 263 427T292 548Q292 617 267 664T200 712Q193 712 186 709T167 698T147 668T134 615Q132 595 132 548V527Q132 436 165 403Q183 385 203 385H207ZM500 146Q500 234 544 290T647 347Q699 347 737 292T776 146T737 0T646 -56Q590 -56 545 0T500 146ZM651 -18Q679 -18 707 24T736 146Q736 215 711 262T644 309Q637 309 630 306T611 295T591 265T578 212Q577 200 577 146V124Q577 -18 647 -18H651Z" id="MJMAIN-25" stroke-width="1">
     </path>
     <path d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z" id="MJMAIN-38" stroke-width="1">
     </path>
     <path d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" id="MJMAIN-2E" stroke-width="1">
     </path>
     <path d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z" id="MJMAIN-37" stroke-width="1">
     </path>
     <path d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" id="MJMAIN-36" stroke-width="1">
     </path>
     <path d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" id="MJMAIN-2212" stroke-width="1">
     </path>
    </defs>
   </svg>
  </div>
  <div id="MathJax_Message" style="">
   Processing math: 100%
  </div>
  <!-- Google Tag Manager (noscript) -->
  <noscript data-test="gtm-body">
   <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0">
   </iframe>
  </noscript>
  <!-- End Google Tag Manager (noscript) -->
  <div class="u-vh-full">
   <a class="c-skip-link" href="#main-content">
    Skip to main content
   </a>
   <div class="u-hide u-show-following-ad">
   </div>
   <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
    <div class="c-ad__inner">
     <p class="c-ad__label">
      Advertisement
     </p>
     <div data-gpt="" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;" data-gpt-unitpath="/270604982/springerlink/book/chapter" data-pa11y-ignore="" data-test="LB1-ad" id="div-gpt-ad-LB1" style="min-width:728px;min-height:90px">
     </div>
    </div>
   </aside>
   <div class="app-elements u-mb-24">
    <header class="c-header" data-header="">
     <div class="c-header__container" data-header-expander-anchor="">
      <div class="c-header__brand">
       <a data-test="logo" data-track="click" data-track-action="click logo link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu">
        <img alt="SpringerLink" src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg"/>
       </a>
      </div>
      <a class="c-header__link c-header__link--static" data-test="login-link" data-track="click" data-track-action="click log in link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-46475-6_7">
       <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
        <use xlink:href="#icon-eds-user-single">
        </use>
       </svg>
       <span>
        Log in
       </span>
      </a>
     </div>
     <nav aria-label="header navigation" class="c-header__nav">
      <div class="c-header__nav-container">
       <div class="c-header__item c-header__item--menu">
        <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
         <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
          <use xlink:href="#icon-eds-menu">
          </use>
         </svg>
         <span>
          Menu
         </span>
        </a>
       </div>
       <div class="c-header__item c-header__item--inline-links">
        <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
         Find a journal
        </a>
        <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
         Publish with us
        </a>
       </div>
       <div class="c-header__link-container">
        <div class="c-header__item c-header__item--divider">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
          <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span>
           Search
          </span>
         </a>
        </div>
        <div class="c-header__item">
         <div class="c-header__item ecommerce-cart" id="ecommerce-header-cart-icon-link" style="display:inline-block">
          <a class="c-header__link" href="https://order-springer-com.proxy.lib.ohio-state.edu/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
           <svg aria-hidden="true" focusable="false" height="24" id="eds-i-cart" style="vertical-align:bottom" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <path d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z" fill="currentColor" fill-rule="nonzero">
            </path>
           </svg>
           <span style="padding-left:10px">
            Cart
           </span>
           <span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center">
           </span>
          </a>
          <script>
           (function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    console.log("listen to updatedCart event");
    document.body.addEventListener("updatedCart", function () {
        console.log("updatedCart happened");
        updateCartIcon().then(function () { return console.log("Cart state update upon event"); });
    }, false);
    return updateCartIcon().then(function () { return console.log("Initial cart state update"); });
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function () { return console.log("Could not fetch cart info"); });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart-springer-com.proxy.lib.ohio-state.edu/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()
          </script>
         </div>
        </div>
       </div>
      </div>
     </nav>
    </header>
    <div class="c-header__expander has-tethered u-js-hide" hidden="" id="popup-search">
     <h2 class="c-header__heading">
      Search
     </h2>
     <div class="u-container">
      <div class="c-header__search">
       <form action="//link-springer-com.proxy.lib.ohio-state.edu/search" data-track="submit" data-track-action="submit search form" data-track-category="unified header" data-track-label="form" method="GET" role="search">
        <label class="c-header__search-label" for="header-search">
         Search by keyword or author
        </label>
        <div class="c-header__search-container">
         <input autocomplete="off" class="c-header__search-input" id="header-search" name="query" required="" type="text" value=""/>
         <button class="c-header__search-button" type="submit">
          <svg aria-hidden="true" class="c-header__icon" focusable="false">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span class="u-visually-hidden">
           Search
          </span>
         </button>
        </div>
       </form>
      </div>
     </div>
    </div>
    <div class="c-header__expander c-header__expander--menu has-tethered u-js-hide" hidden="" id="header-nav">
     <h2 class="c-header__heading">
      Navigation
     </h2>
     <ul class="c-header__list">
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
        Find a journal
       </a>
      </li>
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
        Publish with us
       </a>
      </li>
     </ul>
    </div>
   </div>
   <div class="u-container u-mb-32 u-clearfix" data-component="article-container" id="main-content">
    <div class="u-hide-at-lg js-context-bar-sticky-point-mobile">
     <div class="c-pdf-container">
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-319-46475-6.pdf?pdf=button" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book PDF
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-319-46475-6.epub" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book EPUB
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
     </div>
    </div>
    <header class="u-mb-24" data-test="chapter-information-header">
     <div class="c-box c-box--shadowed">
      <div class="c-app-header">
       <div class="c-app-header__theme" style="background-image: url('https://media-springernature-com.proxy.lib.ohio-state.edu/dominant-colour/springer-static/cover/book/978-3-319-46475-6.jpg')">
       </div>
       <div class="c-app-header__content">
        <div class="c-app-header__main">
         <div class="c-app-header__cover">
          <div class="c-app-expand-overlay-wrapper">
           <a data-component="cover-zoom" data-img-src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-319-46475-6" href="/chapter/10.1007/978-3-319-46475-6_7/cover" rel="nofollow">
            <picture>
             <source srcset="                                                         //media.springernature.com/w92/springer-static/cover/book/978-3-319-46475-6.jpg?as=webp 1x,                                                         //media.springernature.com/w184/springer-static/cover/book/978-3-319-46475-6.jpg?as=webp 2x" type="image/webp"/>
             <img alt="Book cover" height="130" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92/springer-static/cover/book/978-3-319-46475-6.jpg" width="90"/>
            </picture>
            <svg aria-hidden="true" class="c-expand-overlay u-icon" data-component="expand-icon" focusable="false" height="18" width="18">
             <use xlink:href="#icon-expand-image" xmlns:xlink="http://www.w3.org/1999/xlink">
             </use>
            </svg>
           </a>
          </div>
         </div>
         <div class="c-cover-image-lightbox u-hide" data-component="cover-lightbox">
          <button aria-label="Close expanded book cover" class="js-cover-image-lightbox--close" data-component="close-cover-lightbox" type="button">
           <span aria-hidden="true">
            ×
           </span>
          </button>
          <picture>
           <source srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-319-46475-6?as=webp" type="image/webp"/>
           <img alt="Book cover" class="c-cover-image-lightbox__image" height="1200" src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-319-46475-6" width="800"/>
          </picture>
         </div>
         <div class="u-flex-shrink">
          <p class="c-chapter-info-details u-mb-8">
           <a data-track="click" data-track-action="open conference" data-track-label="link" href="/conference/eccv eccv">
            European Conference on Computer Vision
           </a>
          </p>
          <p class="c-chapter-book-details right-arrow">
           ECCV 2016:
           <a class="c-chapter-book-details__title" data-test="book-link" data-track="click" data-track-action="open book series" data-track-label="link" href="/book/10.1007/978-3-319-46475-6">
            Computer Vision – ECCV 2016
           </a>
           pp
                                         102–118
           <a class="c-chapter-book-details__cite-as u-hide-print" data-track="click" data-track-action="cite this chapter" data-track-label="link" href="#citeas">
            Cite as
           </a>
          </p>
         </div>
        </div>
       </div>
      </div>
     </div>
    </header>
    <nav aria-label="breadcrumbs" class="u-mb-16" data-test="article-breadcrumbs">
     <ol class="c-breadcrumbs c-breadcrumbs--truncated" itemscope="" itemtype="https://schema.org/BreadcrumbList">
      <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb1" href="/" itemprop="item">
        <span itemprop="name">
         Home
        </span>
       </a>
       <meta content="1" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb2" href="/book/10.1007/978-3-319-46475-6" itemprop="item">
        <span itemprop="name">
         Computer Vision – ECCV 2016
        </span>
       </a>
       <meta content="2" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <span itemprop="name">
        Conference paper
       </span>
       <meta content="3" itemprop="position"/>
      </li>
     </ol>
    </nav>
    <main class="c-article-main-column u-float-left js-main-column u-text-sans-serif" data-track-component="conference paper">
     <div aria-hidden="true" class="c-context-bar u-hide" data-context-bar="" data-context-bar-with-recommendations="" data-test="context-bar">
      <div class="c-context-bar__container u-container">
       <div class="c-context-bar__title">
        Playing for Data: Ground Truth from Computer Games
       </div>
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-319-46475-6.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-319-46475-6.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div id="recommendations">
       <div class="c-recommendations__container u-container u-display-none" data-component-recommendations="">
        <aside class="c-status-message c-status-message--success u-display-none" data-component-status-msg="">
         <svg aria-label="success:" class="c-status-message__icon" focusable="false" height="24" role="img" width="24">
          <use xlink:href="#icon-success">
          </use>
         </svg>
         <div class="c-status-message__message" id="success-message" tabindex="-1">
          Your content has downloaded
         </div>
        </aside>
        <div class="c-recommendations-header u-display-flex u-justify-content-space-between">
         <h2 class="c-recommendations-title" id="recommendation-heading">
          Similar content being viewed by others
         </h2>
         <button aria-label="Close" class="c-recommendations-close u-flex-static" data-track="click" data-track-action="close recommendations" type="button">
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-close">
           </use>
          </svg>
         </button>
        </div>
        <section aria-labelledby="recommendation-heading" aria-roledescription="carousel">
         <p class="u-visually-hidden">
          Slider with three content items shown per slide. Use the Previous and Next buttons to navigate the slides or the slide controller buttons at the end to navigate through each slide.
         </p>
         <div class="c-recommendations-list-container">
          <div class="c-recommendations-list">
           <div aria-label="Recommendation 1 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-32040-9?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 1" data-track-label="10.1007/978-3-030-32040-9_41" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-32040-9_41" itemprop="url">
                  A Tool for Building Multi-purpose and Multi-pose Synthetic Data Sets
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2019
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 2 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58577-8?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 2" data-track-label="10.1007/978-3-030-58577-8_4" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58577-8_4" itemprop="url">
                  <span class="MathJax_Preview" style="">
                   S^3
                  </span>
                  <script id="MathJax-Element-19" type="math/tex; mode=display">
                   S^3
                  </script>
                  Net: Semantic-Aware Self-supervised Depth Estimation with Monocular Videos and Synthetic Data
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 3 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-92659-5?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 3" data-track-label="10.1007/978-3-030-92659-5_45" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-92659-5_45" itemprop="url">
                  Full-Glow: Fully Conditional Glow for More Realistic Image Generation
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2021
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 4 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-319-29060-7?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 4" data-track-label="10.1007/978-3-319-29060-7_17" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-319-29060-7_17" itemprop="url">
                  Serious Games for Large-Scale Image Sensing
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2016
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 5 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-319-46466-4?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 5" data-track-label="10.1007/978-3-319-46466-4_39" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-319-46466-4_39" itemprop="url">
                  Learning Dynamic Hierarchical Models for Anytime Scene Labeling
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2016
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 6 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-64559-5?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 6" data-track-label="10.1007/978-3-030-64559-5_41" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-64559-5_41" itemprop="url">
                  AI Playground: Unreal Engine-Based Data Ablation Tool for Deep Learning
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 7 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11042-022-13695-1/MediaObjects/11042_2022_13695_Fig1_HTML.png"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 7" data-track-label="10.1007/s11042-022-13695-1" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11042-022-13695-1" itemprop="url">
                  Exploiting semantic segmentation to boost reinforcement learning in video game environments
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  15 September 2022
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Javier Montalvo, Álvaro García-Martín &amp; Jesús Bescós
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 8 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-642-40602-7?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 8" data-track-label="10.1007/978-3-642-40602-7_35" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-642-40602-7_35" itemprop="url">
                  Framework for Generation of Synthetic Ground Truth Data for Driver Assistance Applications
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2013
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 9 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11263-019-01222-z/MediaObjects/11263_2019_1222_Fig1_HTML.jpg"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 9" data-track-label="10.1007/s11263-019-01222-z" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11263-019-01222-z" itemprop="url">
                  Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  23 October 2019
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                César Roberto de Souza, Adrien Gaidon, … Antonio Manuel López
               </p>
              </div>
             </div>
            </article>
           </div>
          </div>
         </div>
        </section>
       </div>
       <div class="js-greyout-page-background" data-component-grey-background="" style="display:none">
       </div>
      </div>
     </div>
     <article lang="en">
      <header data-test="chapter-detail-header">
       <div class="c-article-header">
        <h1 class="c-article-title" data-chapter-title="" data-test="chapter-title">
         Playing for Data: Ground Truth from Computer Games
        </h1>
        <ul class="c-article-author-list c-article-author-list--short js-no-scroll" data-component-authors-activator="authors-list" data-test="authors-list">
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Stephan_R_-Richter" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stephan_R_-Richter">
           Stephan R. Richter
           <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
            <use xlink:href="#icon-email" xmlns:xlink="http://www.w3.org/1999/xlink">
            </use>
           </svg>
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Vibhav-Vineet" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Vibhav-Vineet">
           Vibhav Vineet
          </a>
          <sup class="u-js-hide">
           <a href="#Aff18" tabindex="-1">
            18
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Stefan-Roth" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stefan-Roth">
           Stefan Roth
          </a>
          <sup class="u-js-hide">
           <a href="#Aff17" tabindex="-1">
            17
           </a>
          </sup>
          &amp;
         </li>
         <li aria-label="Show all 4 authors for this article" class="c-article-author-list__show-more" title="Show all 4 authors for this article">
          …
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Vladlen-Koltun" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Vladlen-Koltun">
           Vladlen Koltun
          </a>
          <sup class="u-js-hide">
           <a href="#Aff18" tabindex="-1">
            18
           </a>
          </sup>
         </li>
        </ul>
        <button aria-expanded="false" class="c-article-author-list__button">
         <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
          <use xlink:href="#icon-plus" xmlns:xlink="http://www.w3.org/1999/xlink">
          </use>
         </svg>
         <span>
          Show authors
         </span>
        </button>
        <ul class="c-article-identifiers c-chapter-identifiers">
         <li class="c-article-identifiers__item" data-test="article-category">
          Conference paper
         </li>
         <li class="c-article-identifiers__item">
          <a data-track="click" data-track-action="publication date" data-track-label="link" href="#chapter-info">
           First Online:
           <time datetime="2016-09-17">
            17 September 2016
           </time>
          </a>
         </li>
        </ul>
        <div data-test="article-metrics">
         <div id="altmetric-container">
          <div class="c-article-metrics-bar__wrapper u-clear-both">
           <ul class="c-article-metrics-bar u-list-reset">
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              25k
              <span class="c-article-metrics-bar__label">
               Accesses
              </span>
             </p>
            </li>
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              513
              <a class="c-article-metrics-bar__label" data-track="click" data-track-action="Citation count" data-track-label="link" href="http://citations.springer-com.proxy.lib.ohio-state.edu/item?doi=10.1007/978-3-319-46475-6_7" rel="noopener" target="_blank" title="Visit Springer Citations for full citation details">
               Citations
              </a>
             </p>
            </li>
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              2
              <a class="c-article-metrics-bar__label" data-track="click" data-track-action="Social mentions" data-track-label="link" href="https://link.altmetric.com/details/39290112" rel="noopener" target="_blank" title="Visit Altmetric for full social mention details">
               Altmetric
              </a>
             </p>
            </li>
           </ul>
          </div>
         </div>
        </div>
        <p class="c-chapter-book-series">
         Part of the
         <a data-track="click" data-track-action="open book series" data-track-label="link" href="/bookseries/558">
          Lecture Notes in Computer Science
         </a>
         book series (LNIP,volume 9906)
        </p>
       </div>
      </header>
      <div class="c-article-body" data-article-body="true">
       <section aria-labelledby="Abs1" data-title="Abstract" lang="en">
        <div class="c-article-section" id="Abs1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">
          <span class="c-article-section__title-number">
          </span>
          Abstract
         </h2>
         <div class="c-article-section__content" id="Abs1-content">
          <p>
           Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just
           <span class="mathjax-tex">
            <span class="MathJax_Preview" style="">
            </span>
            <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle displaystyle="false" scriptlevel="0"&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/math&gt;' id="MathJax-Element-1-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
             <svg aria-hidden="true" focusable="false" height="3.234ex" role="img" style="vertical-align: -1.041ex;" viewbox="0 -944.1 713.9 1392.4" width="1.658ex" xmlns:xlink="http://www.w3.org/1999/xlink">
              <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
               <g transform="translate(120,0)">
                <rect height="60" stroke="none" width="473" x="0" y="220">
                </rect>
                <use transform="scale(0.707)" x="84" xlink:href="#MJMAIN-31" y="570">
                </use>
                <use transform="scale(0.707)" x="84" xlink:href="#MJMAIN-33" y="-529">
                </use>
               </g>
              </g>
             </svg>
             <span class="MJX_Assistive_MathML" role="presentation">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <mstyle displaystyle="false" scriptlevel="0">
                <mfrac>
                 <mn>
                  1
                 </mn>
                 <mn>
                  3
                 </mn>
                </mfrac>
               </mstyle>
              </math>
             </span>
            </span>
            <script id="MathJax-Element-1" type="math/tex">
             \tfrac{1}{3}
            </script>
           </span>
           of the CamVid training set outperform models trained on the complete CamVid training set.
          </p>
          <h3 class="c-article__sub-heading">
           Keywords
          </h3>
          <ul class="c-article-subject-list">
           <li class="c-article-subject-list__subject">
            <span>
             Association Rule
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Association Rule Mining
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Graphic Hardware
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Visual Odometry
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Semantic Label
            </span>
           </li>
          </ul>
          <p>
           <i>
            These keywords were added by machine and not by the authors. This process is experimental and the keywords may be updated as the learning algorithm improves.
           </i>
          </p>
         </div>
        </div>
       </section>
       <div class="c-article-section__content c-article-section__content--separator">
        <p>
         S.R. Richter and V. Vineet—Authors contributed equally.
        </p>
       </div>
       <div data-test="chapter-cobranding-and-download">
        <div class="note test-pdf-link" id="cobranding-and-download-availability-text">
         <div class="c-article-access-provider" data-component="provided-by-box">
          <p class="c-article-access-provider__text">
           <a class="c-pdf-download__link" data-track="click" data-track-action="Pdf download" data-track-label="inline link" download="" href="/content/pdf/10.1007/978-3-319-46475-6_7.pdf?pdf=inline%20link" id="js-body-chapter-download" rel="noopener" style="display: inline; padding:0px!important;" target="_blank">
            Download
           </a>
           conference paper PDF
          </p>
         </div>
        </div>
       </div>
       <div class="main-content">
        <section data-title="Introduction">
         <div class="c-article-section" id="Sec1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">
           <span class="c-article-section__title-number">
            1
           </span>
           Introduction
          </h2>
          <div class="c-article-section__content" id="Sec1-content">
           <p>
            Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Image classification datasets with millions of labeled images support training deep and highly expressive models [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d172483776e940" title="Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classification with deep convolutional neural networks. In: NIPS (2012)">
             24
            </a>
            ]. Following their success in image classification, these models have recently been adapted for detailed scene understanding tasks such as semantic segmentation [
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d172483776e943" title="Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR (2015)">
             28
            </a>
            ]. Such semantic segmentation models are initially trained for image classification, for which large datasets are available, and then fine-tuned on semantic segmentation datasets, which have fewer images.
           </p>
           <p>
            We are therefore interested in creating very large datasets with pixel-accurate semantic labels. Such datasets may enable the design of more diverse model architectures that are not constrained by mandatory pre-training on image classification. They may also substantially increase the accuracy of semantic segmentation models, which at present appear to be limited by data rather than capacity. (For example, the top-performing semantic segmentation models on the PASCAL VOC leaderboard all use additional external sources of pixelwise labeled data for training.)
           </p>
           <p>
            Creating large datasets with pixelwise semantic labels is known to be very challenging due to the amount of human effort required to trace accurate object boundaries. High-quality semantic labeling was reported to require 60 min per image for the CamVid dataset [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d172483776e952" title="Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett. 30(2), 88–97 (2009)">
             8
            </a>
            ] and 90 min per image for the Cityscapes dataset [
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d172483776e955" title="Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR (2016)">
             11
            </a>
            ]. Due to the substantial manual effort involved in producing pixel-accurate annotations, semantic segmentation datasets with precise and comprehensive label maps are orders of magnitude smaller than image classification datasets. This has been referred to as the “curse of dataset annotation” [
            <a aria-label="Reference 50" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR50" id="ref-link-section-d172483776e958" title="Xie, J., Kiefel, M., Sun, M.T., Geiger, A.: Semantic instance annotation of street scenes by 3D to 2D label transfer. In: CVPR (2016)">
             50
            </a>
            ]: the more detailed the semantic labeling, the smaller the datasets.
           </p>
           <p>
            In this work, we explore the use of commercial video games for creating large-scale pixel-accurate ground truth data for training semantic segmentation systems. Modern open-world games such as Grand Theft Auto, Watch Dogs, and Hitman feature extensive and highly realistic worlds. Their realism is not only in the high fidelity of material appearance and light transport simulation. It is also in the content of the game worlds: the layout of objects and environments, the realistic textures, the motion of vehicles and autonomous characters, the presence of small objects that add detail, and the interaction between the player and the environment.
           </p>
           <p>
            The scale, appearance, and behavior of these game worlds are significant advantages over open-source sandboxes that lack this extensive content. However, detailed semantic annotation of images from off-the-shelf games is a challenge because the internal operation and content of the game are largely inaccessible. We show that this can be overcome by a technique known as detouring [
            <a aria-label="Reference 19" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR19" id="ref-link-section-d172483776e968" title="Hunt, G., Brubacher, D.: Detours: binary interception of Win32 functions. In: 3rd USENIX Windows NT Symposium (1999)">
             19
            </a>
            ]. We inject a wrapper between the game and the operating system, allowing us to record, modify, and reproduce rendering commands. By hashing distinct rendering resources – such as geometry, textures, and shaders – communicated by the game to the graphics hardware, we generate object signatures that persist across scenes and across gameplay sessions. This allows us to create pixel-accurate object labels without tracing boundaries. Crucially, it also enables propagating object labels across time and across instances that share distinctive resources.
           </p>
           <p>
            Using the presented approach, we have created pixel-level semantic segmentation ground truth for 25 thousand images extracted from the game Grand Theft Auto V. The labeling process was completed in only 49 h. Our labeling speed was thus roughly three orders of magnitude faster than for other semantic segmentation datasets with similar annotation density. The pixel-accurate propagation of label assignments through time and across instances led to a rapid acceleration of the labeling process: average annotation time per image decreased sharply during the process because new object labels were propagated across images. This is in contrast to prior labeling interfaces, in which annotation speed does not change significantly during the labeling process, and total labeling costs scale linearly with dataset size. Annotating the presented dataset with the approach used for CamVid or Cityscapes [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d172483776e974" title="Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett. 30(2), 88–97 (2009)">
             8
            </a>
            ,
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d172483776e977" title="Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR (2016)">
             11
            </a>
            ] would have taken at least 12 person-years. Three of the images we have labeled are shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            .
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 1." id="figure-1">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46475-6_7/figures/1" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig1_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 1" aria-describedby="Fig1" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig1_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc">
               <p>
                Images and ground-truth semantic label maps produced by the presented approach. Left: images extracted from the game Grand Theft Auto V. Right: semantic label maps. The color coding is defined in Fig.
                <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
                 4
                </a>
                . (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/figures/1" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            To evaluate the utility of using game data for training semantic segmentation systems, we used label definitions compatible with other datasets for urban scene understanding [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d172483776e1008" title="Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett. 30(2), 88–97 (2009)">
             8
            </a>
            ,
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d172483776e1011" title="Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR (2016)">
             11
            </a>
            ,
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR13" id="ref-link-section-d172483776e1014" title="Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI dataset. Int. J. Rob. Res. 32(11), 1231–1237 (2013)">
             13
            </a>
            ,
            <a aria-label="Reference 50" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR50" id="ref-link-section-d172483776e1017" title="Xie, J., Kiefel, M., Sun, M.T., Geiger, A.: Semantic instance annotation of street scenes by 3D to 2D label transfer. In: CVPR (2016)">
             50
            </a>
            ]. We conducted extensive experiments to evaluate the effectiveness of models trained with the acquired data. The experimental results show that using the acquired data to supplement the CamVid and KITTI training sets significantly increases accuracy on the respective datasets. In addition, the experiments demonstrate that the acquired data can reduce the need for expensive labeling of real-world images: models trained with game data and just
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle displaystyle="false" scriptlevel="0"&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/math&gt;' id="MathJax-Element-2-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="3.234ex" role="img" style="vertical-align: -1.041ex;" viewbox="0 -944.1 713.9 1392.4" width="1.658ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <g transform="translate(120,0)">
                 <rect height="60" stroke="none" width="473" x="0" y="220">
                 </rect>
                 <use transform="scale(0.707)" x="84" xlink:href="#MJMAIN-31" y="570">
                 </use>
                 <use transform="scale(0.707)" x="84" xlink:href="#MJMAIN-33" y="-529">
                 </use>
                </g>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mstyle displaystyle="false" scriptlevel="0">
                 <mfrac>
                  <mn>
                   1
                  </mn>
                  <mn>
                   3
                  </mn>
                 </mfrac>
                </mstyle>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-2" type="math/tex">
              \tfrac{1}{3}
             </script>
            </span>
            of the CamVid training set outperform models trained on the complete CamVid training set.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Related Work">
         <div class="c-article-section" id="Sec2-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">
           <span class="c-article-section__title-number">
            2
           </span>
           Related Work
          </h2>
          <div class="c-article-section__content" id="Sec2-content">
           <p>
            Synthetic data has been used for decades to benchmark the performance of computer vision algorithms. The use of synthetic data has been particularly significant in evaluating optical flow estimation due to the difficulty of obtaining accurate ground-truth flow measurements for real-world scenes [
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d172483776e1055" title="Baker, S., Scharstein, D., Lewis, J.P., Roth, S., Black, M.J., Szeliski, R.: A database and evaluation methodology for optical flow. IJCV 92(1), 1–31 (2011)">
             6
            </a>
            ,
            <a aria-label="Reference 7" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR7" id="ref-link-section-d172483776e1058" title="Barron, J.L., Fleet, D.J., Beauchemin, S.S.: Performance of optical flow techniques. IJCV 12(1), 43–77 (1994)">
             7
            </a>
            ,
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d172483776e1061" title="Horn, B.K.P., Schunck, B.G.: Determining optical flow. Artif. Intell. 17(1–3), 185–203 (1981)">
             18
            </a>
            ,
            <a aria-label="Reference 32" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR32" id="ref-link-section-d172483776e1064" title="McCane, B., Novins, K., Crannitch, D., Galvin, B.: On benchmarking optical flow. CVIU 84(1), 126–143 (2001)">
             32
            </a>
            ]. Most recently, the MPI-Sintel dataset has become a standard benchmark for optical flow algorithms [
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR9" id="ref-link-section-d172483776e1067" title="Butler, D.J., Wulff, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie for optical flow evaluation. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7577, pp. 611–625. Springer, Heidelberg (2012). doi:
                    10.1007/978-3-642-33783-3_44
                    
                  
        ">
             9
            </a>
            ] and has additionally yielded ground-truth data for depth estimation and bottom-up segmentation. Synthetic scenes have been used for evaluating the robustness of image features [
            <a aria-label="Reference 21" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR21" id="ref-link-section-d172483776e1071" title="Kaneva, B., Torralba, A., Freeman, W.T.: Evaluation of image features using a photorealistic virtual world. In: ICCV (2011)">
             21
            </a>
            ] and for benchmarking the accuracy of visual odometry [
            <a aria-label="Reference 16" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR16" id="ref-link-section-d172483776e1074" title="Handa, A., Whelan, T., McDonald, J., Davison, A.J.: A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM. In: ICRA (2014)">
             16
            </a>
            ]. Renderings of object models have been used to analyze the sensitivity of convolutional network features [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d172483776e1077" title="Aubry, M., Russell, B.C.: Understanding deep features with computer-generated imagery. In: ICCV (2015)">
             5
            </a>
            ]. In contrast to this line of work, we use synthetic data not for benchmarking but for training a vision system, and tackle the challenging problem of semantic segmentation.
           </p>
           <p>
            Rendered depth maps of parametric models have been used prominently in training leading systems for human pose estimation and hand tracking [
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d172483776e1083" title="Sharp, T., Keskin, C., Robertson, D.P., Taylor, J., Shotton, J., Kim, D., Rhemann, C., Leichter, I., Vinnikov, A., Wei, Y., Freedman, D., Kohli, P., Krupka, E., Fitzgibbon, A.W., Izadi, S.: Accurate, robust, and flexible real-time hand tracking. In: CHI (2015)">
             41
            </a>
            ,
            <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR42" id="ref-link-section-d172483776e1086" title="Shotton, J., Girshick, R.B., Fitzgibbon, A.W., Sharp, T., Cook, M., Finocchio, M., Moore, R., Kohli, P., Criminisi, A., Kipman, A., Blake, A.: Efficient human pose estimation from single depth images. PAMI 35(12), 2821–2840 (2013)">
             42
            </a>
            ]. 3D object models are also increasingly used for training representations for object detection and object pose estimation [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d172483776e1089" title="Aubry, M., Maturana, D., Efros, A.A., Russell, B.C., Sivic, J.: Seeing 3D chairs: exemplar part-based 2D–3D alignment using a large dataset of CAD models. In: CVPR (2014)">
             4
            </a>
            ,
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d172483776e1092" title="Liebelt, J., Schmid, C., Schertler, K.: Viewpoint-independent object class detection using 3D feature maps. In: CVPR (2008)">
             26
            </a>
            ,
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d172483776e1095" title="Massa, F., Russell, B.C., Aubry, M.: Deep exemplar 2D–3D detection by adapting from real to rendered views. In: CVPR (2016)">
             30
            </a>
            ,
            <a aria-label="Reference 34" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR34" id="ref-link-section-d172483776e1099" title="Peng, X., Sun, B., Ali, K., Saenko, K.: Learning deep object detectors from 3D models. In: ICCV (2015)">
             34
            </a>
            ,
            <a aria-label="Reference 35" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR35" id="ref-link-section-d172483776e1102" title="Pepik, B., Stark, M., Gehler, P.V., Schiele, B.: Multi-view and 3D deformable part models. PAMI 37(11), 2232–2245 (2015)">
             35
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d172483776e1105" title="Stark, M., Goesele, M., Schiele, B.: Back to the future: learning shape models from 3D CAD data. In: BMVC (2010)">
             44
            </a>
            ]. Renderings of 3D object models have been used to train shape-from-shading algorithms [
            <a aria-label="Reference 37" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR37" id="ref-link-section-d172483776e1108" title="Richter, S.R., Roth, S.: Discriminative shape from shading in uncalibrated illumination. In: CVPR (2015)">
             37
            </a>
            ] and convolutional networks for optical flow estimation [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d172483776e1111" title="Dosovitskiy, A., Fischer, P., Ilg, E., Häusser, P., Hazirbas, C., Golkov, V., van der Smagt, P., Cremers, D., Brox, T.: FlowNet: learning optical flow with convolutional networks. In: ICCV (2015)">
             12
            </a>
            ]. Renderings of entire synthetic environments have been proposed for training convolutional networks for stereo disparity and scene flow estimation [
            <a aria-label="Reference 31" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR31" id="ref-link-section-d172483776e1114" title="Mayer, N., Ilg, E., Häusser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox, T.: A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In: CVPR (2016)">
             31
            </a>
            ]. Our work is different in two ways. First, we tackle the problem of semantic segmentation, which involves both recognition and perceptual grouping [
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d172483776e1118" title="He, X., Zemel, R.S., Carreira-Perpiñán, M.: Multiscale conditional random fields for image labeling. In: CVPR (2004)">
             17
            </a>
            ,
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d172483776e1121" title="Krähenbühl, P., Koltun, V.: Efficient inference in fully connected CRFs with Gaussian edge potentials. In: NIPS (2011)">
             23
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d172483776e1124" title="Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR (2015)">
             28
            </a>
            ,
            <a aria-label="Reference 43" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR43" id="ref-link-section-d172483776e1127" title="Shotton, J., Winn, J.M., Rother, C., Criminisi, A.: Textonboost for image understanding: multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV 81(1), 2–23 (2009)">
             43
            </a>
            ]. Second, we obtain data not by rendering 3D object models or stylized environments, but by extracting photorealistic imagery from a modern open-world computer game with high-fidelity content.
           </p>
           <p>
            Computer games – and associated tools such as game engines and level editors – have been used a number of times in computer vision research. Development tools accompanying the game Half Life 2 were used for evaluating visual surveillance systems [
            <a aria-label="Reference 46" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR46" id="ref-link-section-d172483776e1133" title="Taylor, G.R., Chosak, A.J., Brewer, P.C.: OVVV: using virtual worlds to design and evaluate surveillance systems. In: CVPR (2007)">
             46
            </a>
            ]. These tools were subsequently used for creating an environment for training high-performing pedestrian detectors [
            <a aria-label="Reference 29" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR29" id="ref-link-section-d172483776e1136" title="Marín, J., Vázquez, D., Gerónimo, D., López, A.M.: Learning appearance in virtual scenarios for pedestrian detection. In: CVPR (2010)">
             29
            </a>
            ,
            <a aria-label="Reference 49" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR49" id="ref-link-section-d172483776e1139" title="Vázquez, D., López, A.M., Marín, J., Ponsa, D., Gomez, D.G.: Virtual and real world adaptation for pedestrian detection. PAMI 36(4), 797–809 (2014)">
             49
            </a>
            ,
            <a aria-label="Reference 51" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR51" id="ref-link-section-d172483776e1142" title="Xu, J., Vázquez, D., López, A.M., Marín, J., Ponsa, D.: Learning a part-based pedestrian detector in a virtual world. IEEE Trans. Intell. Transp. Syst. 15(5), 2121–2131 (2014)">
             51
            </a>
            ]. And an open-source driving simulator was used to learn mid-level cues for autonomous driving [
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d172483776e1145" title="Chen, C., Seff, A., Kornhauser, A.L., Xiao, J.: DeepDriving: learning affordance for direct perception in autonomous driving. In: ICCV (2015)">
             10
            </a>
            ]. In contrast to these works, we deal with the problem of semantic image segmentation and demonstrate that data extracted from an unmodified off-the-shelf computer game with no access to the source code or the content can be used to substantially improve the performance of semantic segmentation systems.
           </p>
           <p>
            Somewhat orthogonal to our work is the use of indoor scene models to train deep networks for semantic understanding of indoor environments from depth images [
            <a aria-label="Reference 15" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR15" id="ref-link-section-d172483776e1151" title="Handa, A., Pătrăucean, V., Badrinarayanan, V., Stent, S., Cipolla, R.: Understanding real world indoor scenes with synthetic data. In: CVPR (2016)">
             15
            </a>
            ,
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR33" id="ref-link-section-d172483776e1154" title="Papon, J., Schoeler, M.: Semantic pose using deep networks trained on synthetic RGB-D. In: ICCV (2015)">
             33
            </a>
            ]. These approaches compose synthetic indoor scenes from object models and synthesize depth maps with associated semantic labels. The training data synthesized in these works provides depth information but no appearance cues. The trained models are thus limited to analyzing depth maps. In contrast, we show that modern computer games can be used to increase the accuracy of state-of-the-art semantic segmentation models on challenging real-world benchmarks given regular color images only. Very recent concurrent work [
            <a aria-label="Reference 53" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR53" id="ref-link-section-d172483776e1157" title="Veeravasarapu, V.S.R., Rothkopf, C., Ramesh, V.: Model-driven simulations for deep convolutional neural networks (2016). arXiv preprint 
                    arXiv:1605.09582
                    
                  
        ">
             53
            </a>
            ,
            <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d172483776e1160" title="Shafaei, A., Little, J.J., Schmidt, M.: Play and learn: using video Games to train computer vision models. In: BMVC (2016)">
             54
            </a>
            ] demonstrates the heightened interest in using virtual worlds to train algorithms for scene understanding.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Breaking the Curse of Dataset Annotation">
         <div class="c-article-section" id="Sec3-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">
           <span class="c-article-section__title-number">
            3
           </span>
           Breaking the Curse of Dataset Annotation
          </h2>
          <div class="c-article-section__content" id="Sec3-content">
           <p>
            Extracting images and metadata from a game is easy if the source code and content are available [
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d172483776e1171" title="Chen, C., Seff, A., Kornhauser, A.L., Xiao, J.: DeepDriving: learning affordance for direct perception in autonomous driving. In: ICCV (2015)">
             10
            </a>
            ,
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR14" id="ref-link-section-d172483776e1174" title="Haltakov, V., Unger, C., Ilic, S.: Framework for generation of synthetic ground truth data for driver assistance applications. In: Weickert, J., Hein, M., Schiele, B. (eds.) GCPR 2013. LNCS, vol. 8142, pp. 323–332. Springer, Heidelberg (2013). doi:
                    10.1007/978-3-642-40602-7_35
                    
                  
        ">
             14
            </a>
            ]. Open-source games, however, lack the extensive, detailed, and realistic content of commercial productions. In this section, we show that rapid semantic labeling can be performed on images generated by off-the-shelf games, without access to their source code or content. We then demonstrate the presented approach by producing pixel-accurate semantic annotations for 25 thousand images from the game Grand Theft Auto V. (The publisher of Grand Theft Auto V allows non-commercial use of footage from the game as long as certain conditions are met, such as not distributing spoilers [
            <a aria-label="Reference 38" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR38" id="ref-link-section-d172483776e1177" title="Rockstar Games: Policy on posting copyrighted Rockstar Games material. 
                    http://tinyurl-com.proxy.lib.ohio-state.edu/pjfoqo5
                    
                  
        ">
             38
            </a>
            ].)
           </p>
           <h3 class="c-article__sub-heading" id="Sec4">
            <span class="c-article-section__title-number">
             3.1
            </span>
            Data Acquisition
           </h3>
           <p>
            <b>
             A Brief Introduction to Real-Time Rendering.
            </b>
            To present our approach to data acquisition, we first need to review some relevant aspects of the rendering pipeline used in modern computer games [
            <a aria-label="Reference 2" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR2" id="ref-link-section-d172483776e1190" title="Akenine-Möller, T., Haines, E., Hoffman, N.: Real-Time Rendering, 3rd edn. A K Peters, Natick (2008)">
             2
            </a>
            ]. Modern real-time rendering systems are commonly based on deferred shading. Geometric resources are communicated to the GPU to create a depth buffer and a normal buffer. Maps that specify the diffuse and specular components of surface reflectance are communicated to create the diffuse and specular buffers. Buffers that collect such intermediate products of the rendering pipeline are called G-buffers. Illumination is applied to these G-buffers, rather than to the original scene components [
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d172483776e1193" title="Saito, T., Takahashi, T.: Comprehensible rendering of 3-D shapes. In: SIGGRAPH (1990)">
             40
            </a>
            ]. This decoupled processing of geometry, reflectance properties, and illumination significantly accelerates the rendering process. First, shading does not need to be performed on elements that are subsequently culled by the geometry pipeline. Second, shading can be performed much more efficiently on G-buffers than on an unstructured stream of objects. For these reasons, deferred shading has been widely adopted in high-performance game engines.
           </p>
           <p>
            <b>
             Extracting Information from the Rendering Pipeline.
            </b>
            How can this pipeline be employed in creating ground-truth semantic labels if we don’t have access to the game’s code or content? The key lies in the game’s communication with the graphics hardware. This communication is structured. Specifically, the game communicates resources of different types, including geometric meshes, texture maps, and shaders. The game then specifies how these resources should be combined to compose the scene. The content of these resources persists through time and across gameplay sessions. By tracking the application of resources to different scene elements, we can establish associations between these scene elements.
           </p>
           <p>
            Our basic approach is to intercept the communication between the game and the graphics hardware. Games communicate with the hardware through APIs such as OpenGL, Direct3D, or Vulkan, which are provided via dynamically loaded libraries. To initiate the use of the hardware, a game loads the library into its application memory. By posing as the graphics library during this loading process, a wrapper to the library can be injected and all subsequent communication between the game and the graphics API can be monitored and modified. This injection method is known as detouring [
            <a aria-label="Reference 19" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR19" id="ref-link-section-d172483776e1205" title="Hunt, G., Brubacher, D.: Detours: binary interception of Win32 functions. In: 3rd USENIX Windows NT Symposium (1999)">
             19
            </a>
            ] and is commonly used for patching software binaries and achieving program interoperability. It is also used by screen-capturing programs and off-the-shelf graphics debugging tools such as RenderDoc [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d172483776e1208" title="Karlsson, B.: RenderDoc. 
                    https://renderdoc.org
                    
                  
        ">
             22
            </a>
            ] and Intel Graphics Performance Analyzers [
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d172483776e1211" title="Intel: Intel Graphics Performance Analyzers. 
                    https://software-intel-com.proxy.lib.ohio-state.edu/en-us/gpa
                    
                  
        ">
             20
            </a>
            ]. To perform detouring, a wrapper needs to implement all relevant interfaces and forward calls to the original library. We implemented a wrapper for the DirectX 9 API and used RenderDoc for wrapping Direct3D 11. We successfully tested these two implementations on three different rendering engines used in AAA computer games. By intercepting all communication with the graphics hardware, we are able to monitor the creation, modification, and deletion of resources used to specify the scene and synthesize an image.
           </p>
           <p>
            We now focus on the application of our approach to the game Grand Theft Auto V (GTA5), although much of the discussion applies more broadly. To collect data, we used RenderDoc to record every 40
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mrow class="MJX-TeXAtom-ORD"&gt;&lt;mi mathvariant="normal"&gt;t&lt;/mi&gt;&lt;mi mathvariant="normal"&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;' id="MathJax-Element-3-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.313ex" role="img" style="vertical-align: -0.12ex;" viewbox="0 -944.1 768.9 995.7" width="1.786ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <g transform="translate(0,362)">
                 <use transform="scale(0.707)" x="0" xlink:href="#MJMAIN-74" y="0">
                 </use>
                 <use transform="scale(0.707)" x="389" xlink:href="#MJMAIN-68" y="0">
                 </use>
                </g>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <msup>
                 <mi>
                 </mi>
                 <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="normal">
                   t
                  </mi>
                  <mi mathvariant="normal">
                   h
                  </mi>
                 </mrow>
                </msup>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-3" type="math/tex">
              ^\mathrm{th}
             </script>
            </span>
            frame during GTA5 gameplay. Being a debugger for applications, RenderDoc can be configured to record all calls of an application to the graphics API with their respective arguments and allows detailed inspection of the arguments. Since RenderDoc is scriptable and its source code is available as well, we modified it to automatically transform recorded data into a format that is suitable for annotation.
           </p>
           <p>
            Specifically, the wrapper saves all information needed to reproduce a frame. The frames are then processed in batch after a gameplay session to extract all information needed for annotation. (This separate processing requires about 30 s per frame.) Annotation of large batches of collected and processed frames is performed later in an interactive interface that uses the extracted information to support highly efficient annotation (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            ). In the following paragraphs, we discuss several challenges that had to be addressed:
           </p>
           <ol class="u-list-style-none">
            <li>
             <span class="u-custom-list-number">
              1.
             </span>
             <p>
              Identify function calls that are relevant for rendering objects into the set of G-buffers that we are interested in.
             </p>
            </li>
            <li>
             <span class="u-custom-list-number">
              2.
             </span>
             <p>
              Create persistent identities for resources that link their use across frames and across gameplay sessions.
             </p>
            </li>
            <li>
             <span class="u-custom-list-number">
              3.
             </span>
             <p>
              Organize and store resource identities to support rapid annotation in a separate interactive interface.
             </p>
            </li>
           </ol>
           <p>
            <b>
             Identifying Relevant Function Calls.
            </b>
            To identify rendering passes, RenderDoc groups function calls into common rendering passes based on predefined heuristics. We found that strictly grouping the calls by the G-buffers that are assigned as render targets works more reliably. That way, we identify the main pass that processes the scene geometry and updates the albedo, surface normal, stencil, and depth buffers as well as the rendering passes that draw the head-up display on top of the scene image. GTA5 applies post-processing effects such as camera distortion to the rendered image before displaying it. To preserve the association of object information extracted from the main pass with pixels in the final image and to bypass drawing the head-up display (HUD), we omit the camera distortion and subsequent HUD passes.
           </p>
           <p>
            <b>
             Identifying Resources.
            </b>
            To propagate labels across images, we need to reliably identify resources used to specify different scene elements. When the same mesh is used in two different frames to specify the shape of a scene element, we want to reliably recognize that it is the same mesh. During a single gameplay session, a resource can be recognized by its location in memory or by the ID used by the application to address this resource. However, the next time the game is launched, memory locations and IDs associated with resources will be different. To recognize resources across different gameplay sessions, we instead hash the associated memory content. We use a non-cryptographic hash function [
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR3" id="ref-link-section-d172483776e1294" title="Appleby, A.: Murmurhash. 
                    https://github.com/aappleby/smhasher
                    
                  
        ">
             3
            </a>
            ] to create a 128-bit key for the content of the memory occupied by the resource. This key is stored and is used to identify the resource in different frames. Thus, for each recorded frame, we create a lookup table to map the volatile resource IDs to persistent hash keys.
           </p>
           <p>
            <b>
             Formatting for Annotation.
            </b>
            Although we can now identify and associate resources that are being used to create different frames, we have not yet associated these resources with pixels in the rendered images. We want to associate each mesh, texture, and shader with their footprint in each rendered image. One way to do this would be to step through the rendering calls and perform pixel-level comparisons of the content of each G-buffer before and after each call. However, this is unreliable and computationally expensive. Instead, we perform two complete rendering passes instead of one and produce two distinct images. The first rendering pass produces the color image and the associated buffers as described above: this is the conventional rendering pass performed by the game. The second rendering pass is used to encode IDs into pixels, such that after this pass each pixel stores the resource IDs for the mesh, texture, and shader that specify the scene element imaged at that pixel. For this second rendering pass, we replace all the shaders with our own custom shader that encodes the resource IDs of textures, meshes, and original shaders into colors and writes them into four render targets. Four render targets with three 8-bit color channels each provide us with 96 bits per pixel, which we use to store three 32-bit resource IDs: one for the mesh, one for the texture, one for the shader. In a subsequent processing stage, we read off these 32-bit IDs, which do not persist across frames, and map them to the persistent 128-bit hash keys created earlier.
           </p>
           <h3 class="c-article__sub-heading" id="Sec5">
            <span class="c-article-section__title-number">
             3.2
            </span>
            Semantic Labeling
           </h3>
           <p>
            For each image extracted from the game, the pipeline described in Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec4">
             3.1
            </a>
            produces a corresponding resource ID map. For each pixel, this ID map identifies the mesh, texture, and shader that were used by the surface imaged at that pixel. These IDs are persistent: the same resource is identified by the same ID in different frames. This is the data used by the annotation process.
           </p>
           <p>
            <b>
             Patch Decomposition.
            </b>
            We begin by automatically decomposing each image into patches of pixels that share a common
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mo fence="false" stretchy="false"&gt;&amp;#x27E8;&lt;/mo&gt;&lt;mtext&gt;mesh&lt;/mtext&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mtext&gt;texture&lt;/mtext&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mtext&gt;shader&lt;/mtext&gt;&lt;mo fence="false" stretchy="false"&gt;&amp;#x27E9;&lt;/mo&gt;&lt;/math&gt;' id="MathJax-Element-4-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 9888.8 1194" width="22.968ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAIN-27E8" y="0">
                </use>
                <g transform="translate(389,0)">
                 <use xlink:href="#MJMAIN-6D">
                 </use>
                 <use x="833" xlink:href="#MJMAIN-65" y="0">
                 </use>
                 <use x="1278" xlink:href="#MJMAIN-73" y="0">
                 </use>
                 <use x="1672" xlink:href="#MJMAIN-68" y="0">
                 </use>
                </g>
                <use x="2618" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(3063,0)">
                 <use xlink:href="#MJMAIN-74">
                 </use>
                 <use x="389" xlink:href="#MJMAIN-65" y="0">
                 </use>
                 <use x="834" xlink:href="#MJMAIN-78" y="0">
                 </use>
                 <use x="1362" xlink:href="#MJMAIN-74" y="0">
                 </use>
                 <use x="1752" xlink:href="#MJMAIN-75" y="0">
                 </use>
                 <use x="2308" xlink:href="#MJMAIN-72" y="0">
                 </use>
                 <use x="2701" xlink:href="#MJMAIN-65" y="0">
                 </use>
                </g>
                <use x="6209" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(6654,0)">
                 <use xlink:href="#MJMAIN-73">
                 </use>
                 <use x="394" xlink:href="#MJMAIN-68" y="0">
                 </use>
                 <use x="951" xlink:href="#MJMAIN-61" y="0">
                 </use>
                 <use x="1451" xlink:href="#MJMAIN-64" y="0">
                 </use>
                 <use x="2008" xlink:href="#MJMAIN-65" y="0">
                 </use>
                 <use x="2452" xlink:href="#MJMAIN-72" y="0">
                 </use>
                </g>
                <use x="9499" xlink:href="#MJMAIN-27E9" y="0">
                </use>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mo fence="false" stretchy="false">
                 ⟨
                </mo>
                <mtext>
                 mesh
                </mtext>
                <mo>
                 ,
                </mo>
                <mtext>
                 texture
                </mtext>
                <mo>
                 ,
                </mo>
                <mtext>
                 shader
                </mtext>
                <mo fence="false" stretchy="false">
                 ⟩
                </mo>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-4" type="math/tex">
              \langle \text {mesh}, \text {texture}, \text {shader}\rangle
             </script>
            </span>
            combination (henceforth, MTS). Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            shows an image from the game and the resulting patches. The patches are fine-grained and each object is typically decomposed into multiple patches. Furthermore, a given patch is likely to be contained within a single object. A given mesh may contain multiple objects (a building and an adjacent sidewalk), a texture may be used on objects from different semantic classes (car and truck), and a shader may likewise be applied on semantically distinct objects, but an MTS combination is almost always used within a single object type.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 2." id="figure-2">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46475-6_7/figures/2" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig2_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 2" aria-describedby="Fig2" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig2_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc">
               <p>
                Illustration of the patches used as atomic units during the annotation process. Top left: one of the images in our dataset. Top right: patches constructed by grouping pixels that share a common MTS combination. Different patches are filled with different colors for illustration. Bottom row: partial magnifications of the top images. (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/figures/2" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            The identified patches are thus akin to superpixels [
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d172483776e1381" title="Ren, X., Malik, J.: Learning a classification model for segmentation. In: ICCV (2003)">
             36
            </a>
            ], but have significant advantages over superpixels generated by a bottom-up segmentation algorithm. First, they are associated with the underlying surfaces in the scene, and patches that depict the same surface in different images are linked. Second, boundaries of semantic classes in an image coincide with patch boundaries. There is no need for a human annotator to delineate object boundaries: pixel-accurate label maps can be created simply by grouping patches. Third, as we shall see next, the metadata associated with each patch can be used to propagate labels even across object instances that do not share the same MTS.
           </p>
           <p>
            <b>
             Association Rule Mining.
            </b>
            So far we have required that two patches share a complete MTS combination to be linked. However, requiring that the mesh, texture, and shader all be identical is sometimes too conservative: there are many cases in which just one or two resources are sufficient to uniquely identify the semantic class of a patch. For example, a car mesh is highly unlikely to be used for anything but a car. Instead of specifying such cases by hand, we discover them automatically during the labeling process via association rule mining [
            <a aria-label="Reference 1" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR1" id="ref-link-section-d172483776e1390" title="Agrawal, R., Srikant, R.: Fast algorithms for mining association rules in large databases. In: VLDB (1994)">
             1
            </a>
            ].
           </p>
           <p>
            During the annotation process, statistical regularities in associations between resources and semantic labels are detected. When sufficient evidence is available for a clear association between a resource and a semantic class, a rule is automatically created that labels other patches that use this resource by the associated class. This further speeds up annotation by propagating labels not just to observations of the same surface in the scene at different times, but also across different objects that use a distinctive resource that clearly identifies their semantic class.
           </p>
           <p>
            <b>
             Annotation Process.
            </b>
            We use a simple interactive interface that supports labeling patches by clicking. The interface is shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            . Labeled areas are tinted by the color of their semantic class. (The color definitions are shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
             4
            </a>
            .) The annotator selects a semantic class and then clicks on a patch that has not yet been labeled. In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            (left), four patches are unlabeled: part of a sidewalk, a fire hydrant, and two patches on the building. They are labeled in 14 s to yield the complete label map shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            (right).
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 3." id="figure-3">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46475-6_7/figures/3" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig3_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 3" aria-describedby="Fig3" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig3_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc">
               <p>
                Annotation interface. Labeled patches are tinted by the color of their semantic class. The annotator selects a semantic class and applies it to a patch with a single click. Left: an intermediate state with four patches yet unlabeled. Right: a complete labeling produced 14 s later. (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/figures/3" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            Labeling the very first image still takes time due to the granularity of the patches. However, the annotation tool automatically propagates labels to all patches that share the same MTS in all images, as well as other patches that are linked by distinctive resources identified by association rule mining. As the annotation progresses, more and more patches in all images are pre-labeled. The annotation time per image decreases during the process: the more images have been annotated, the faster the labeling of each new image is. Our annotation tool only presents an image for annotation if more than 3 % of the image area has not been pre-labeled automatically by propagating labels from other frames. In this way, only a fraction of the images has to be explicitly handled by the annotator.
           </p>
           <p>
            Labeling each pixel in every image directly would be difficult even with our single-click interface because distant or heavily occluded objects are often hard to recognize. The label propagation ensures that even if a patch is left unlabeled in a given image because it is small or far away, it will likely be labeled eventually when the underlying surface is seen more closely and its assigned label is propagated back.
           </p>
           <h3 class="c-article__sub-heading" id="Sec6">
            <span class="c-article-section__title-number">
             3.3
            </span>
            Dataset and Analysis
           </h3>
           <p>
            We extracted 24,966 frames from GTA5. Each frame has a resolution of
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;1914&lt;/mn&gt;&lt;mrow class="MJX-TeXAtom-BIN"&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;1052&lt;/mn&gt;&lt;/math&gt;' id="MathJax-Element-5-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 5226.9 846.9" width="12.14ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-31">
                </use>
                <use x="500" xlink:href="#MJMAIN-39" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-31" y="0">
                </use>
                <use x="1501" xlink:href="#MJMAIN-34" y="0">
                </use>
                <use x="2224" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <g transform="translate(3224,0)">
                 <use xlink:href="#MJMAIN-31">
                 </use>
                 <use x="500" xlink:href="#MJMAIN-30" y="0">
                 </use>
                 <use x="1001" xlink:href="#MJMAIN-35" y="0">
                 </use>
                 <use x="1501" xlink:href="#MJMAIN-32" y="0">
                 </use>
                </g>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 1914
                </mn>
                <mrow class="MJX-TeXAtom-BIN">
                 <mo>
                  ×
                 </mo>
                </mrow>
                <mn>
                 1052
                </mn>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-5" type="math/tex">
              1914 \mathbin {\!\times \!}1052
             </script>
            </span>
            pixels. The frames were then semantically labeled using the interface described in Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec5">
             3.2
            </a>
            . The labeling process was completed in 49 h. In this time, 98.3 % of the pixel area of the extracted images was labeled with corresponding semantic classes. Classes were defined to be compatible with other semantic segmentation datasets for outdoor scenes [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d172483776e1484" title="Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett. 30(2), 88–97 (2009)">
             8
            </a>
            ,
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d172483776e1487" title="Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR (2016)">
             11
            </a>
            ,
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d172483776e1490" title="Ros, G., Ramos, S., Granados, M., Bakhtiary, A., Vázquez, D., López, A.M.: Vision-based offline-online perception paradigm for autonomous driving. In: WACV (2015)">
             39
            </a>
            ,
            <a aria-label="Reference 50" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR50" id="ref-link-section-d172483776e1494" title="Xie, J., Kiefel, M., Sun, M.T., Geiger, A.: Semantic instance annotation of street scenes by 3D to 2D label transfer. In: CVPR (2016)">
             50
            </a>
            ]. The distribution of classes in our dataset is shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
             4
            </a>
            .
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 4." id="figure-4">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46475-6_7/figures/4" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig4_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 4" aria-describedby="Fig4" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig4_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc">
               <p>
                Number of annotated pixels per class in our dataset. Note the logarithmic scale. (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/figures/4" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            General statistics for the dataset are summarized in Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab1">
             1
            </a>
            . Our dataset is roughly two orders of magnitude larger than CamVid [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d172483776e1524" title="Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett. 30(2), 88–97 (2009)">
             8
            </a>
            ] and three orders of magnitude larger than semantic annotations created for the KITTI dataset [
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR13" id="ref-link-section-d172483776e1527" title="Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI dataset. Int. J. Rob. Res. 32(11), 1231–1237 (2013)">
             13
            </a>
            ,
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d172483776e1530" title="Ros, G., Ramos, S., Granados, M., Bakhtiary, A., Vázquez, D., López, A.M.: Vision-based offline-online perception paradigm for autonomous driving. In: WACV (2015)">
             39
            </a>
            ]. The average annotation time for our dataset was 7 s per image: 514 times faster than the per-image annotation time reported for CamVid [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d172483776e1533" title="Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett. 30(2), 88–97 (2009)">
             8
            </a>
            ] and 771 times faster than the per-image annotation time for Cityscapes [
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d172483776e1537" title="Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR (2016)">
             11
            </a>
            ].
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab1">
               Table 1. Comparison of densely labeled semantic segmentation datasets for outdoor scenes. We achieve a three order of magnitude speed-up in annotation time, enabling us to densely label tens of thousands of high-resolution images.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/tables/1" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             Label Propagation.
            </b>
            The label propagation mechanisms significantly accelerated annotation time. Specific MTS combinations labeled during the process cover 73 % of the cumulative pixel area in the dataset. Only a fraction of that area was directly labeled by the annotator; most labels were propagated from other images. Patches covered by learned association rules account for 43 % of the dataset. During the annotation process, 2,295 association rules were automatically constructed. The union of the areas covered by labeled MTS combinations and learned association rules accounts for the 98.3 % annotation density of our dataset.
           </p>
           <p>
            Define the pre-annotated area of an image to be the set of patches that are pre-labeled before the annotator reaches that image. The patches in the pre-annotated area are pre-labeled by label propagation across patches that share the same MTS and via learned association rules. For each image, we can measure the size of the pre-annotated area relative to the whole frame. This size is
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-6-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.313ex" role="img" style="vertical-align: -0.35ex;" viewbox="0 -844.9 1500.7 995.7" width="3.485ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAIN-30" y="0">
                </use>
                <use x="667" xlink:href="#MJMAIN-25" y="0">
                </use>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 0
                </mn>
                <mspace width="thinmathspace">
                </mspace>
                <mi mathvariant="normal">
                 %
                </mi>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-6" type="math/tex">
              0\,\%
             </script>
            </span>
            if none of the image area is pre-annotated (e.g., for the very first image processed by the annotator) and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;100&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-7-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.313ex" role="img" style="vertical-align: -0.35ex;" viewbox="0 -844.9 2501.7 995.7" width="5.81ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-31">
                </use>
                <use x="500" xlink:href="#MJMAIN-30" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-30" y="0">
                </use>
                <use x="1668" xlink:href="#MJMAIN-25" y="0">
                </use>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 100
                </mn>
                <mspace width="thinmathspace">
                </mspace>
                <mi mathvariant="normal">
                 %
                </mi>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-7" type="math/tex">
              100\,\%
             </script>
            </span>
            if the entirety of the image area is already annotated by the time the annotator reaches this image. In a conventional annotation process used for datasets such as CamVid and Cityscapes, the pre-annotated area is a constant
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-8-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.313ex" role="img" style="vertical-align: -0.35ex;" viewbox="0 -844.9 1500.7 995.7" width="3.485ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAIN-30" y="0">
                </use>
                <use x="667" xlink:href="#MJMAIN-25" y="0">
                </use>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 0
                </mn>
                <mspace width="thinmathspace">
                </mspace>
                <mi mathvariant="normal">
                 %
                </mi>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-8" type="math/tex">
              0\,\%
             </script>
            </span>
            for all images. The pre-annotated area for images handled during our labeling process is plotted in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            .
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;98.7&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-9-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.313ex" role="img" style="vertical-align: -0.35ex;" viewbox="0 -844.9 2780.2 995.7" width="6.457ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-39">
                </use>
                <use x="500" xlink:href="#MJMAIN-38" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-2E" y="0">
                </use>
                <use x="1279" xlink:href="#MJMAIN-37" y="0">
                </use>
                <use x="1946" xlink:href="#MJMAIN-25" y="0">
                </use>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 98.7
                </mn>
                <mspace width="thinmathspace">
                </mspace>
                <mi mathvariant="normal">
                 %
                </mi>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-9" type="math/tex">
              98.7\,\%
             </script>
            </span>
            of the frames are more than
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;90&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-10-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.313ex" role="img" style="vertical-align: -0.35ex;" viewbox="0 -844.9 2001.2 995.7" width="4.648ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-39">
                </use>
                <use x="500" xlink:href="#MJMAIN-30" y="0">
                </use>
                <use x="1167" xlink:href="#MJMAIN-25" y="0">
                </use>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 90
                </mn>
                <mspace width="thinmathspace">
                </mspace>
                <mi mathvariant="normal">
                 %
                </mi>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-10" type="math/tex">
              90\,\%
             </script>
            </span>
            pre-annotated by the time they are reached by the human annotator.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 5." id="figure-5">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46475-6_7/figures/5" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig5_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 5" aria-describedby="Fig5" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig5_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc">
               <p>
                Effect of label propagation. For each frame, the plots show the fraction of image area that is pre-annotated by the time the human annotator reaches that frame. On the left, the frames are arranged in the order they are processed; on the right, the frames are sorted by magnitude of pre-annotated area. Only 333 frames (
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                 </span>
                 <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;1.3&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-11-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
                  <svg aria-hidden="true" focusable="false" height="2.186ex" role="img" style="vertical-align: -0.277ex;" viewbox="0 -822.1 2279.7 941.3" width="5.295ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use xlink:href="#MJMAIN-31">
                    </use>
                    <use x="500" xlink:href="#MJMAIN-2E" y="0">
                    </use>
                    <use x="779" xlink:href="#MJMAIN-33" y="0">
                    </use>
                    <use x="1446" xlink:href="#MJMAIN-25" y="0">
                    </use>
                   </g>
                  </svg>
                  <span class="MJX_Assistive_MathML" role="presentation">
                   <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mn>
                     1.3
                    </mn>
                    <mspace width="thinmathspace">
                    </mspace>
                    <mi mathvariant="normal">
                     %
                    </mi>
                   </math>
                  </span>
                 </span>
                 <script id="MathJax-Element-11" type="math/tex">
                  1.3\,\%
                 </script>
                </span>
                of the total) are less than
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                 </span>
                 <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;90&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-12-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
                  <svg aria-hidden="true" focusable="false" height="2.186ex" role="img" style="vertical-align: -0.277ex;" viewbox="0 -822.1 2001.2 941.3" width="4.648ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use xlink:href="#MJMAIN-39">
                    </use>
                    <use x="500" xlink:href="#MJMAIN-30" y="0">
                    </use>
                    <use x="1167" xlink:href="#MJMAIN-25" y="0">
                    </use>
                   </g>
                  </svg>
                  <span class="MJX_Assistive_MathML" role="presentation">
                   <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mn>
                     90
                    </mn>
                    <mspace width="thinmathspace">
                    </mspace>
                    <mi mathvariant="normal">
                     %
                    </mi>
                   </math>
                  </span>
                 </span>
                 <script id="MathJax-Element-12" type="math/tex">
                  90\,\%
                 </script>
                </span>
                pre-annotated by the time they are reached by the human annotator.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 5" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure5 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/figures/5" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             Diversity of the Collected Data.
            </b>
            We also analyze the diversity of the images extracted from the game world. The effectiveness of label propagation may suggest that the collected images are visually uniform. This is not the case. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
             6
            </a>
            shows the distribution of the number of frames in which MTS combinations in the dataset occur. As shown in the figure,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;26.5&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-13-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.313ex" role="img" style="vertical-align: -0.35ex;" viewbox="0 -844.9 2780.2 995.7" width="6.457ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-32">
                </use>
                <use x="500" xlink:href="#MJMAIN-36" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-2E" y="0">
                </use>
                <use x="1279" xlink:href="#MJMAIN-35" y="0">
                </use>
                <use x="1946" xlink:href="#MJMAIN-25" y="0">
                </use>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 26.5
                </mn>
                <mspace width="thinmathspace">
                </mspace>
                <mi mathvariant="normal">
                 %
                </mi>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-13" type="math/tex">
              26.5\,\%
             </script>
            </span>
            of the MTS combinations only occur in a single image in the collected dataset. That is, more than a quarter of the MTS combinations observed in the 25 thousand collected images are only observed in a single image each. The median number of frames in which an MTS occurs is 4: that is, most of the MTS combinations are only seen in 4 images or less out of the 25 thousand collected images. This indicates the high degree of variability in the collected dataset.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 6." id="figure-6">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46475-6_7/figures/6" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig6_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 6" aria-describedby="Fig6" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig6_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc">
               <p>
                Distribution of the number of frames in which an MTS combination occurs.
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                 </span>
                 <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;26.5&lt;/mn&gt;&lt;mspace width="thinmathspace" /&gt;&lt;mi mathvariant="normal"&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;' id="MathJax-Element-14-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
                  <svg aria-hidden="true" focusable="false" height="2.186ex" role="img" style="vertical-align: -0.277ex;" viewbox="0 -822.1 2780.2 941.3" width="6.457ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use xlink:href="#MJMAIN-32">
                    </use>
                    <use x="500" xlink:href="#MJMAIN-36" y="0">
                    </use>
                    <use x="1001" xlink:href="#MJMAIN-2E" y="0">
                    </use>
                    <use x="1279" xlink:href="#MJMAIN-35" y="0">
                    </use>
                    <use x="1946" xlink:href="#MJMAIN-25" y="0">
                    </use>
                   </g>
                  </svg>
                  <span class="MJX_Assistive_MathML" role="presentation">
                   <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mn>
                     26.5
                    </mn>
                    <mspace width="thinmathspace">
                    </mspace>
                    <mi mathvariant="normal">
                     %
                    </mi>
                   </math>
                  </span>
                 </span>
                 <script id="MathJax-Element-14" type="math/tex">
                  26.5\,\%
                 </script>
                </span>
                of the MTS combinations only occur in a single image each and most of the MTS combinations are only seen in 4 images or less.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 6" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure6 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/figures/6" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            In addition, Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            shows 20 randomly sampled images from our dataset. As the figure demonstrates, the images in the collected dataset are highly variable in their content and layout.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 7." id="figure-7">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-319-46475-6_7/figures/7" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig7_HTML.gif?as=webp" type="image/webp"/>
                 <img alt="figure 7" aria-describedby="Fig7" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig7_HTML.gif"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc">
               <p>
                Randomly sampled images from the collected dataset, illustrating its diversity.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 7" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure7 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/figures/7" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
          </div>
         </div>
        </section>
        <section data-title="Semantic Segmentation">
         <div class="c-article-section" id="Sec7-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">
           <span class="c-article-section__title-number">
            4
           </span>
           Semantic Segmentation
          </h2>
          <div class="c-article-section__content" id="Sec7-content">
           <p>
            We now evaluate the effectiveness of using the acquired data for training semantic segmentation models. We evaluate on two datasets for semantic segmentation of outdoor scenes: CamVid [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d172483776e2132" title="Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett. 30(2), 88–97 (2009)">
             8
            </a>
            ] and KITTI [
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR13" id="ref-link-section-d172483776e2135" title="Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI dataset. Int. J. Rob. Res. 32(11), 1231–1237 (2013)">
             13
            </a>
            ,
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d172483776e2138" title="Ros, G., Ramos, S., Granados, M., Bakhtiary, A., Vázquez, D., López, A.M.: Vision-based offline-online perception paradigm for autonomous driving. In: WACV (2015)">
             39
            </a>
            ]. As our semantic segmentation model, we use the front-end prediction module of Yu and Koltun [
            <a aria-label="Reference 52" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR52" id="ref-link-section-d172483776e2141" title="Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: ICLR (2016)">
             52
            </a>
            ]. Our training procedure consists of two stages. In the first stage, we jointly train on real and synthetic data using mini-batch stochastic gradient descent with mini-batches of 8 images: 4 real and 4 synthetic. 50 K iterations are performed with a learning rate of
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;msup&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mrow class="MJX-TeXAtom-ORD"&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;' id="MathJax-Element-15-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="2.428ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -944.1 2005.4 1045.3" width="4.658ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-31">
                </use>
                <use x="500" xlink:href="#MJMAIN-30" y="0">
                </use>
                <g transform="translate(1001,393)">
                 <use transform="scale(0.707)" x="0" xlink:href="#MJMAIN-2212" y="0">
                 </use>
                 <use transform="scale(0.707)" x="778" xlink:href="#MJMAIN-34" y="0">
                 </use>
                </g>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <msup>
                 <mn>
                  10
                 </mn>
                 <mrow class="MJX-TeXAtom-ORD">
                  <mo>
                   −
                  </mo>
                  <mn>
                   4
                  </mn>
                 </mrow>
                </msup>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-15" type="math/tex">
              10^{-4}
             </script>
            </span>
            and momentum 0.99. The crop size is
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;628&lt;/mn&gt;&lt;mrow class="MJX-TeXAtom-BIN"&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;628&lt;/mn&gt;&lt;/math&gt;' id="MathJax-Element-16-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 4225.9 846.9" width="9.815ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-36">
                </use>
                <use x="500" xlink:href="#MJMAIN-32" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-38" y="0">
                </use>
                <use x="1723" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <g transform="translate(2724,0)">
                 <use xlink:href="#MJMAIN-36">
                 </use>
                 <use x="500" xlink:href="#MJMAIN-32" y="0">
                 </use>
                 <use x="1001" xlink:href="#MJMAIN-38" y="0">
                 </use>
                </g>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 628
                </mn>
                <mrow class="MJX-TeXAtom-BIN">
                 <mo>
                  ×
                 </mo>
                </mrow>
                <mn>
                 628
                </mn>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-16" type="math/tex">
              628 \mathbin {\!\times \!}628
             </script>
            </span>
            and the receptive field is
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mn&gt;373&lt;/mn&gt;&lt;mrow class="MJX-TeXAtom-BIN"&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;373&lt;/mn&gt;&lt;/math&gt;' id="MathJax-Element-17-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 4225.9 846.9" width="9.815ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-33">
                </use>
                <use x="500" xlink:href="#MJMAIN-37" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-33" y="0">
                </use>
                <use x="1723" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <g transform="translate(2724,0)">
                 <use xlink:href="#MJMAIN-33">
                 </use>
                 <use x="500" xlink:href="#MJMAIN-37" y="0">
                 </use>
                 <use x="1001" xlink:href="#MJMAIN-33" y="0">
                 </use>
                </g>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mn>
                 373
                </mn>
                <mrow class="MJX-TeXAtom-BIN">
                 <mo>
                  ×
                 </mo>
                </mrow>
                <mn>
                 373
                </mn>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-17" type="math/tex">
              373\mathbin {\!\times \!}373
             </script>
            </span>
            pixels. In the second stage, we fine-tune for 4 K iterations on real data only, using the same parameters as in the first stage.
           </p>
           <h3 class="c-article__sub-heading" id="Sec8">
            <span class="c-article-section__title-number">
             4.1
            </span>
            CamVid Dataset
           </h3>
           <p>
            We begin with experiments on the CamVid dataset. For ease of comparison to prior work, we adopted the training and test setup of Sturgess et al. [
            <a aria-label="Reference 45" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR45" id="ref-link-section-d172483776e2247" title="Sturgess, P., Alahari, K., Ladicky, L., Torr, P.H.S.: Combining appearance and structure from motion features for road scene understanding. In: BMVC (2009)">
             45
            </a>
            ], which has become standard for the CamVid dataset. This setup has 11 semantic classes and a split of the dataset into 367 training, 100 validation, and 233 test images.
           </p>
           <p>
            The main results are summarized in Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab2">
             2
            </a>
            . The table shows that using the synthetic data during training increases the mean IoU by 3.9 % points. In addition, we used the full set of synthetic images and varied the proportion of real images in the training set. The results show that when we train on
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" data-mathml='&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle displaystyle="false" scriptlevel="0"&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/math&gt;' id="MathJax-Element-18-Frame" role="presentation" style="font-size: 100%; display: inline-block; position: relative;" tabindex="0">
              <svg aria-hidden="true" focusable="false" height="3.234ex" role="img" style="vertical-align: -1.041ex;" viewbox="0 -944.1 713.9 1392.4" width="1.658ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <g transform="translate(120,0)">
                 <rect height="60" stroke="none" width="473" x="0" y="220">
                 </rect>
                 <use transform="scale(0.707)" x="84" xlink:href="#MJMAIN-31" y="570">
                 </use>
                 <use transform="scale(0.707)" x="84" xlink:href="#MJMAIN-33" y="-529">
                 </use>
                </g>
               </g>
              </svg>
              <span class="MJX_Assistive_MathML" role="presentation">
               <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mstyle displaystyle="false" scriptlevel="0">
                 <mfrac>
                  <mn>
                   1
                  </mn>
                  <mn>
                   3
                  </mn>
                 </mfrac>
                </mstyle>
               </math>
              </span>
             </span>
             <script id="MathJax-Element-18" type="math/tex">
              \tfrac{1}{3}
             </script>
            </span>
            of the CamVid training set along with the game data, we surpass the accuracy achieved when training on the full CamVid training set without game data. This suggests that the presented approach to acquiring and labeling synthetic data can significantly reduce the amount of hand-labeled real-world images required for training semantic segmentation models.
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-2">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab2">
               Table 2. Controlled experiments on the CamVid dataset. Training with the full CamVid training set augmented by the synthetic images increases the mean IoU by 3.9 % points. Synthetic images also allow reducing the amount of labeled real-world training data by a factor of 3.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 2" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/tables/2" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab3">
             3
            </a>
            provides a comparison of our results to prior work on semantic segmentation on the CamVid dataset. Our strongest baseline is the state-of-the-art system of Kundu et al. [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d172483776e2528" title="Kundu, A., Vineet, V., Koltun, V.: Feature space optimization for semantic video segmentation. In: CVPR (2016)">
             25
            </a>
            ], which used a bigger ConvNet and analyzed whole video sequences. By using synthetic data during training, we outperform this baseline by 2.8 % points, while considering individual frames only.
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-3">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab3">
               Table 3. Comparison to prior work on the CamVid dataset. We outperform the state-of-the-art system of Kundu et al. by 2.8 % points without utilizing temporal cues.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 3" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/tables/3" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec9">
            <span class="c-article-section__title-number">
             4.2
            </span>
            KITTI Dataset
           </h3>
           <p>
            We have also performed an evaluation on the KITTI semantic segmentation dataset. The results are reported in Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab4">
             4
            </a>
            . We use the split of Ros et al. [
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d172483776e3171" title="Ros, G., Ramos, S., Granados, M., Bakhtiary, A., Vázquez, D., López, A.M.: Vision-based offline-online perception paradigm for autonomous driving. In: WACV (2015)">
             39
            </a>
            ], which consists of 100 training images and 46 test images. We compare against several baselines for which the authors have either provided results on this dataset or released their code. The model trained with game data outperforms the model trained without game data by 2.6 % points.
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-4">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab4">
               Table 4. Results on the KITTI dataset. Training with game data yields a 2.6 % point improvement over training without game data.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 4" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-319-46475-6_7/tables/4" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
          </div>
         </div>
        </section>
        <section data-title="Discussion">
         <div class="c-article-section" id="Sec10-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">
           <span class="c-article-section__title-number">
            5
           </span>
           Discussion
          </h2>
          <div class="c-article-section__content" id="Sec10-content">
           <p>
            We presented an approach to rapidly producing pixel-accurate semantic label maps for images synthesized by modern computer games. We have demonstrated the approach by creating dense pixel-level semantic annotations for 25 thousand images extracted from a realistic open-world game. Our experiments have shown that data created with the presented approach can increase the performance of semantic segmentation models on real-world images and can reduce the need for expensive conventional labeling.
           </p>
           <p>
            There are many extensions that would be interesting. First, the presented ideas can be extended to produce continuous video streams in which each frame is densely annotated. Second, the presented approach can be applied to produce ground-truth data for many dense prediction problems, including optical flow, scene flow, depth estimation, boundary detection, stereo reconstruction, intrinsic image decomposition, visual odometry, and more. Third, our ideas can be extended to produce instance-level – rather than class-level – segmentations. There are many other exciting opportunities and we believe that modern game worlds can play a significant role in training artificial vision systems.
           </p>
          </div>
         </div>
        </section>
       </div>
       <div id="MagazineFulltextChapterBodySuffix">
        <section aria-labelledby="Bib1" data-title="References">
         <div class="c-article-section" id="Bib1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">
           <span class="c-article-section__title-number">
           </span>
           References
          </h2>
          <div class="c-article-section__content" id="Bib1-content">
           <div data-container-section="references">
            <ol class="c-article-references" data-track-component="outbound reference">
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1.">
              <p class="c-article-references__text" id="ref-CR1">
               Agrawal, R., Srikant, R.: Fast algorithms for mining association rules in large databases. In: VLDB (1994)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR1-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Agrawal%2C%20R.%2C%20Srikant%2C%20R.%3A%20Fast%20algorithms%20for%20mining%20association%20rules%20in%20large%20databases.%20In%3A%20VLDB%20%281994%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2.">
              <p class="c-article-references__text" id="ref-CR2">
               Akenine-Möller, T., Haines, E., Hoffman, N.: Real-Time Rendering, 3rd edn. A K Peters, Natick (2008)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR2-links">
               <a aria-label="CrossRef reference 2" data-doi="10.1201/b10644" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1201/b10644" href="https://doi-org.proxy.lib.ohio-state.edu/10.1201%2Fb10644" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 2" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Real-Time%20Rendering&amp;publication_year=2008&amp;author=Akenine-M%C3%B6ller%2CT&amp;author=Haines%2CE&amp;author=Hoffman%2CN" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3.">
              <p class="c-article-references__text" id="ref-CR3">
               Appleby, A.: Murmurhash.
               <a data-track="click" data-track-action="external reference" data-track-label="https://github.com/aappleby/smhasher" href="https://github.com/aappleby/smhasher">
                https://github.com/aappleby/smhasher
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4.">
              <p class="c-article-references__text" id="ref-CR4">
               Aubry, M., Maturana, D., Efros, A.A., Russell, B.C., Sivic, J.: Seeing 3D chairs: exemplar part-based 2D–3D alignment using a large dataset of CAD models. In: CVPR (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR4-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Aubry%2C%20M.%2C%20Maturana%2C%20D.%2C%20Efros%2C%20A.A.%2C%20Russell%2C%20B.C.%2C%20Sivic%2C%20J.%3A%20Seeing%203D%20chairs%3A%20exemplar%20part-based%202D%E2%80%933D%20alignment%20using%20a%20large%20dataset%20of%20CAD%20models.%20In%3A%20CVPR%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5.">
              <p class="c-article-references__text" id="ref-CR5">
               Aubry, M., Russell, B.C.: Understanding deep features with computer-generated imagery. In: ICCV (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR5-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Aubry%2C%20M.%2C%20Russell%2C%20B.C.%3A%20Understanding%20deep%20features%20with%20computer-generated%20imagery.%20In%3A%20ICCV%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6.">
              <p class="c-article-references__text" id="ref-CR6">
               Baker, S., Scharstein, D., Lewis, J.P., Roth, S., Black, M.J., Szeliski, R.: A database and evaluation methodology for optical flow. IJCV
               <b>
                92
               </b>
               (1), 1–31 (2011)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR6-links">
               <a aria-label="CrossRef reference 6" data-doi="10.1007/s11263-010-0390-2" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/s11263-010-0390-2" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-010-0390-2" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 6" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=A%20database%20and%20evaluation%20methodology%20for%20optical%20flow&amp;journal=IJCV&amp;volume=92&amp;issue=1&amp;pages=1-31&amp;publication_year=2011&amp;author=Baker%2CS&amp;author=Scharstein%2CD&amp;author=Lewis%2CJP&amp;author=Roth%2CS&amp;author=Black%2CMJ&amp;author=Szeliski%2CR" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7.">
              <p class="c-article-references__text" id="ref-CR7">
               Barron, J.L., Fleet, D.J., Beauchemin, S.S.: Performance of optical flow techniques. IJCV
               <b>
                12
               </b>
               (1), 43–77 (1994)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR7-links">
               <a aria-label="CrossRef reference 7" data-doi="10.1007/BF01420984" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/BF01420984" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2FBF01420984" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 7" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Performance%20of%20optical%20flow%20techniques&amp;journal=IJCV&amp;volume=12&amp;issue=1&amp;pages=43-77&amp;publication_year=1994&amp;author=Barron%2CJL&amp;author=Fleet%2CDJ&amp;author=Beauchemin%2CSS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8.">
              <p class="c-article-references__text" id="ref-CR8">
               Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett.
               <b>
                30
               </b>
               (2), 88–97 (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR8-links">
               <a aria-label="CrossRef reference 8" data-doi="10.1016/j.patrec.2008.04.005" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1016/j.patrec.2008.04.005" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.patrec.2008.04.005" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 8" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Semantic%20object%20classes%20in%20video%3A%20a%20high-definition%20ground%20truth%20database&amp;journal=Pattern%20Recogn.%20Lett.&amp;volume=30&amp;issue=2&amp;pages=88-97&amp;publication_year=2009&amp;author=Brostow%2CGJ&amp;author=Fauqueur%2CJ&amp;author=Cipolla%2CR" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9.">
              <p class="c-article-references__text" id="ref-CR9">
               Butler, D.J., Wulff, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie for optical flow evaluation. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7577, pp. 611–625. Springer, Heidelberg (2012). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-33783-3_44" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-33783-3_44">
                10.1007/978-3-642-33783-3_44
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR9-links">
               <a aria-label="Google Scholar reference 9" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=A%20naturalistic%20open%20source%20movie%20for%20optical%20flow%20evaluation&amp;pages=611-625&amp;publication_year=2012&amp;author=Butler%2CDJ&amp;author=Wulff%2CJ&amp;author=Stanley%2CGB&amp;author=Black%2CMJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10.">
              <p class="c-article-references__text" id="ref-CR10">
               Chen, C., Seff, A., Kornhauser, A.L., Xiao, J.: DeepDriving: learning affordance for direct perception in autonomous driving. In: ICCV (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR10-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Chen%2C%20C.%2C%20Seff%2C%20A.%2C%20Kornhauser%2C%20A.L.%2C%20Xiao%2C%20J.%3A%20DeepDriving%3A%20learning%20affordance%20for%20direct%20perception%20in%20autonomous%20driving.%20In%3A%20ICCV%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11.">
              <p class="c-article-references__text" id="ref-CR11">
               Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR11-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Cordts%2C%20M.%2C%20Omran%2C%20M.%2C%20Ramos%2C%20S.%2C%20Rehfeld%2C%20T.%2C%20Enzweiler%2C%20M.%2C%20Benenson%2C%20R.%2C%20Franke%2C%20U.%2C%20Roth%2C%20S.%2C%20Schiele%2C%20B.%3A%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12.">
              <p class="c-article-references__text" id="ref-CR12">
               Dosovitskiy, A., Fischer, P., Ilg, E., Häusser, P., Hazirbas, C., Golkov, V., van der Smagt, P., Cremers, D., Brox, T.: FlowNet: learning optical flow with convolutional networks. In: ICCV (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR12-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Dosovitskiy%2C%20A.%2C%20Fischer%2C%20P.%2C%20Ilg%2C%20E.%2C%20H%C3%A4usser%2C%20P.%2C%20Hazirbas%2C%20C.%2C%20Golkov%2C%20V.%2C%20van%20der%20Smagt%2C%20P.%2C%20Cremers%2C%20D.%2C%20Brox%2C%20T.%3A%20FlowNet%3A%20learning%20optical%20flow%20with%20convolutional%20networks.%20In%3A%20ICCV%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13.">
              <p class="c-article-references__text" id="ref-CR13">
               Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI dataset. Int. J. Rob. Res.
               <b>
                32
               </b>
               (11), 1231–1237 (2013)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR13-links">
               <a aria-label="CrossRef reference 13" data-doi="10.1177/0278364913491297" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1177/0278364913491297" href="https://doi-org.proxy.lib.ohio-state.edu/10.1177%2F0278364913491297" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 13" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Vision%20meets%20robotics%3A%20the%20KITTI%20dataset&amp;journal=Int.%20J.%20Rob.%20Res.&amp;volume=32&amp;issue=11&amp;pages=1231-1237&amp;publication_year=2013&amp;author=Geiger%2CA&amp;author=Lenz%2CP&amp;author=Stiller%2CC&amp;author=Urtasun%2CR" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14.">
              <p class="c-article-references__text" id="ref-CR14">
               Haltakov, V., Unger, C., Ilic, S.: Framework for generation of synthetic ground truth data for driver assistance applications. In: Weickert, J., Hein, M., Schiele, B. (eds.) GCPR 2013. LNCS, vol. 8142, pp. 323–332. Springer, Heidelberg (2013). doi:
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-40602-7_35" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-40602-7_35">
                10.1007/978-3-642-40602-7_35
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR14-links">
               <a aria-label="CrossRef reference 14" data-doi="10.1007/978-3-642-40602-7_35" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-642-40602-7_35" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-642-40602-7_35" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 14" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Framework%20for%20generation%20of%20synthetic%20ground%20truth%20data%20for%20driver%20assistance%20applications&amp;pages=323-332&amp;publication_year=2013&amp;author=Haltakov%2CV&amp;author=Unger%2CC&amp;author=Ilic%2CS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15.">
              <p class="c-article-references__text" id="ref-CR15">
               Handa, A., Pătrăucean, V., Badrinarayanan, V., Stent, S., Cipolla, R.: Understanding real world indoor scenes with synthetic data. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR15-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Handa%2C%20A.%2C%20P%C4%83tr%C4%83ucean%2C%20V.%2C%20Badrinarayanan%2C%20V.%2C%20Stent%2C%20S.%2C%20Cipolla%2C%20R.%3A%20Understanding%20real%20world%20indoor%20scenes%20with%20synthetic%20data.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16.">
              <p class="c-article-references__text" id="ref-CR16">
               Handa, A., Whelan, T., McDonald, J., Davison, A.J.: A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM. In: ICRA (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR16-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Handa%2C%20A.%2C%20Whelan%2C%20T.%2C%20McDonald%2C%20J.%2C%20Davison%2C%20A.J.%3A%20A%20benchmark%20for%20RGB-D%20visual%20odometry%2C%203D%20reconstruction%20and%20SLAM.%20In%3A%20ICRA%20%282014%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17.">
              <p class="c-article-references__text" id="ref-CR17">
               He, X., Zemel, R.S., Carreira-Perpiñán, M.: Multiscale conditional random fields for image labeling. In: CVPR (2004)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR17-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=He%2C%20X.%2C%20Zemel%2C%20R.S.%2C%20Carreira-Perpi%C3%B1%C3%A1n%2C%20M.%3A%20Multiscale%20conditional%20random%20fields%20for%20image%20labeling.%20In%3A%20CVPR%20%282004%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18.">
              <p class="c-article-references__text" id="ref-CR18">
               Horn, B.K.P., Schunck, B.G.: Determining optical flow. Artif. Intell.
               <b>
                17
               </b>
               (1–3), 185–203 (1981)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR18-links">
               <a aria-label="CrossRef reference 18" data-doi="10.1016/0004-3702(81)90024-2" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1016/0004-3702(81)90024-2" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2F0004-3702%2881%2990024-2" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 18" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Determining%20optical%20flow&amp;journal=Artif.%20Intell.&amp;volume=17&amp;issue=1%E2%80%933&amp;pages=185-203&amp;publication_year=1981&amp;author=Horn%2CBKP&amp;author=Schunck%2CBG" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19.">
              <p class="c-article-references__text" id="ref-CR19">
               Hunt, G., Brubacher, D.: Detours: binary interception of Win32 functions. In: 3rd USENIX Windows NT Symposium (1999)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR19-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hunt%2C%20G.%2C%20Brubacher%2C%20D.%3A%20Detours%3A%20binary%20interception%20of%20Win32%20functions.%20In%3A%203rd%20USENIX%20Windows%20NT%20Symposium%20%281999%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20.">
              <p class="c-article-references__text" id="ref-CR20">
               Intel: Intel Graphics Performance Analyzers.
               <a data-track="click" data-track-action="external reference" data-track-label="https://software-intel-com.proxy.lib.ohio-state.edu/en-us/gpa" href="https://software-intel-com.proxy.lib.ohio-state.edu/en-us/gpa">
                https://software-intel-com.proxy.lib.ohio-state.edu/en-us/gpa
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21.">
              <p class="c-article-references__text" id="ref-CR21">
               Kaneva, B., Torralba, A., Freeman, W.T.: Evaluation of image features using a photorealistic virtual world. In: ICCV (2011)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR21-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kaneva%2C%20B.%2C%20Torralba%2C%20A.%2C%20Freeman%2C%20W.T.%3A%20Evaluation%20of%20image%20features%20using%20a%20photorealistic%20virtual%20world.%20In%3A%20ICCV%20%282011%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22.">
              <p class="c-article-references__text" id="ref-CR22">
               Karlsson, B.: RenderDoc.
               <a data-track="click" data-track-action="external reference" data-track-label="https://renderdoc.org" href="https://renderdoc.org">
                https://renderdoc.org
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23.">
              <p class="c-article-references__text" id="ref-CR23">
               Krähenbühl, P., Koltun, V.: Efficient inference in fully connected CRFs with Gaussian edge potentials. In: NIPS (2011)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR23-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kr%C3%A4henb%C3%BChl%2C%20P.%2C%20Koltun%2C%20V.%3A%20Efficient%20inference%20in%20fully%20connected%20CRFs%20with%20Gaussian%20edge%20potentials.%20In%3A%20NIPS%20%282011%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24.">
              <p class="c-article-references__text" id="ref-CR24">
               Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classification with deep convolutional neural networks. In: NIPS (2012)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR24-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Krizhevsky%2C%20A.%2C%20Sutskever%2C%20I.%2C%20Hinton%2C%20G.E.%3A%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks.%20In%3A%20NIPS%20%282012%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25.">
              <p class="c-article-references__text" id="ref-CR25">
               Kundu, A., Vineet, V., Koltun, V.: Feature space optimization for semantic video segmentation. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR25-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kundu%2C%20A.%2C%20Vineet%2C%20V.%2C%20Koltun%2C%20V.%3A%20Feature%20space%20optimization%20for%20semantic%20video%20segmentation.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26.">
              <p class="c-article-references__text" id="ref-CR26">
               Liebelt, J., Schmid, C., Schertler, K.: Viewpoint-independent object class detection using 3D feature maps. In: CVPR (2008)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR26-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liebelt%2C%20J.%2C%20Schmid%2C%20C.%2C%20Schertler%2C%20K.%3A%20Viewpoint-independent%20object%20class%20detection%20using%203D%20feature%20maps.%20In%3A%20CVPR%20%282008%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27.">
              <p class="c-article-references__text" id="ref-CR27">
               Liu, B., He, X.: Multiclass semantic video segmentation with object-level active inference. In: CVPR (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR27-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liu%2C%20B.%2C%20He%2C%20X.%3A%20Multiclass%20semantic%20video%20segmentation%20with%20object-level%20active%20inference.%20In%3A%20CVPR%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28.">
              <p class="c-article-references__text" id="ref-CR28">
               Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR28-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Long%2C%20J.%2C%20Shelhamer%2C%20E.%2C%20Darrell%2C%20T.%3A%20Fully%20convolutional%20networks%20for%20semantic%20segmentation.%20In%3A%20CVPR%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29.">
              <p class="c-article-references__text" id="ref-CR29">
               Marín, J., Vázquez, D., Gerónimo, D., López, A.M.: Learning appearance in virtual scenarios for pedestrian detection. In: CVPR (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR29-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Mar%C3%ADn%2C%20J.%2C%20V%C3%A1zquez%2C%20D.%2C%20Ger%C3%B3nimo%2C%20D.%2C%20L%C3%B3pez%2C%20A.M.%3A%20Learning%20appearance%20in%20virtual%20scenarios%20for%20pedestrian%20detection.%20In%3A%20CVPR%20%282010%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30.">
              <p class="c-article-references__text" id="ref-CR30">
               Massa, F., Russell, B.C., Aubry, M.: Deep exemplar 2D–3D detection by adapting from real to rendered views. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR30-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Massa%2C%20F.%2C%20Russell%2C%20B.C.%2C%20Aubry%2C%20M.%3A%20Deep%20exemplar%202D%E2%80%933D%20detection%20by%20adapting%20from%20real%20to%20rendered%20views.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31.">
              <p class="c-article-references__text" id="ref-CR31">
               Mayer, N., Ilg, E., Häusser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox, T.: A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR31-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Mayer%2C%20N.%2C%20Ilg%2C%20E.%2C%20H%C3%A4usser%2C%20P.%2C%20Fischer%2C%20P.%2C%20Cremers%2C%20D.%2C%20Dosovitskiy%2C%20A.%2C%20Brox%2C%20T.%3A%20A%20large%20dataset%20to%20train%20convolutional%20networks%20for%20disparity%2C%20optical%20flow%2C%20and%20scene%20flow%20estimation.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32.">
              <p class="c-article-references__text" id="ref-CR32">
               McCane, B., Novins, K., Crannitch, D., Galvin, B.: On benchmarking optical flow. CVIU
               <b>
                84
               </b>
               (1), 126–143 (2001)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR32-links">
               <a aria-label="MATH reference 32" data-track="click" data-track-action="MATH reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1021.68733" rel="nofollow noopener">
                MATH
               </a>
               <a aria-label="Google Scholar reference 32" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=On%20benchmarking%20optical%20flow&amp;journal=CVIU&amp;volume=84&amp;issue=1&amp;pages=126-143&amp;publication_year=2001&amp;author=McCane%2CB&amp;author=Novins%2CK&amp;author=Crannitch%2CD&amp;author=Galvin%2CB" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33.">
              <p class="c-article-references__text" id="ref-CR33">
               Papon, J., Schoeler, M.: Semantic pose using deep networks trained on synthetic RGB-D. In: ICCV (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR33-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Papon%2C%20J.%2C%20Schoeler%2C%20M.%3A%20Semantic%20pose%20using%20deep%20networks%20trained%20on%20synthetic%20RGB-D.%20In%3A%20ICCV%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34.">
              <p class="c-article-references__text" id="ref-CR34">
               Peng, X., Sun, B., Ali, K., Saenko, K.: Learning deep object detectors from 3D models. In: ICCV (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR34-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Peng%2C%20X.%2C%20Sun%2C%20B.%2C%20Ali%2C%20K.%2C%20Saenko%2C%20K.%3A%20Learning%20deep%20object%20detectors%20from%203D%20models.%20In%3A%20ICCV%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35.">
              <p class="c-article-references__text" id="ref-CR35">
               Pepik, B., Stark, M., Gehler, P.V., Schiele, B.: Multi-view and 3D deformable part models. PAMI
               <b>
                37
               </b>
               (11), 2232–2245 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR35-links">
               <a aria-label="CrossRef reference 35" data-doi="10.1109/TPAMI.2015.2408347" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1109/TPAMI.2015.2408347" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2015.2408347" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 35" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Multi-view%20and%203D%20deformable%20part%20models&amp;journal=PAMI&amp;volume=37&amp;issue=11&amp;pages=2232-2245&amp;publication_year=2015&amp;author=Pepik%2CB&amp;author=Stark%2CM&amp;author=Gehler%2CPV&amp;author=Schiele%2CB" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36.">
              <p class="c-article-references__text" id="ref-CR36">
               Ren, X., Malik, J.: Learning a classification model for segmentation. In: ICCV (2003)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR36-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ren%2C%20X.%2C%20Malik%2C%20J.%3A%20Learning%20a%20classification%20model%20for%20segmentation.%20In%3A%20ICCV%20%282003%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37.">
              <p class="c-article-references__text" id="ref-CR37">
               Richter, S.R., Roth, S.: Discriminative shape from shading in uncalibrated illumination. In: CVPR (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR37-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Richter%2C%20S.R.%2C%20Roth%2C%20S.%3A%20Discriminative%20shape%20from%20shading%20in%20uncalibrated%20illumination.%20In%3A%20CVPR%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38.">
              <p class="c-article-references__text" id="ref-CR38">
               Rockstar Games: Policy on posting copyrighted Rockstar Games material.
               <a data-track="click" data-track-action="external reference" data-track-label="http://tinyurl-com.proxy.lib.ohio-state.edu/pjfoqo5" href="http://tinyurl-com.proxy.lib.ohio-state.edu/pjfoqo5">
                http://tinyurl-com.proxy.lib.ohio-state.edu/pjfoqo5
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39.">
              <p class="c-article-references__text" id="ref-CR39">
               Ros, G., Ramos, S., Granados, M., Bakhtiary, A., Vázquez, D., López, A.M.: Vision-based offline-online perception paradigm for autonomous driving. In: WACV (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR39-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ros%2C%20G.%2C%20Ramos%2C%20S.%2C%20Granados%2C%20M.%2C%20Bakhtiary%2C%20A.%2C%20V%C3%A1zquez%2C%20D.%2C%20L%C3%B3pez%2C%20A.M.%3A%20Vision-based%20offline-online%20perception%20paradigm%20for%20autonomous%20driving.%20In%3A%20WACV%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40.">
              <p class="c-article-references__text" id="ref-CR40">
               Saito, T., Takahashi, T.: Comprehensible rendering of 3-D shapes. In: SIGGRAPH (1990)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR40-links">
               <a aria-label="Google Scholar reference 40" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Comprehensible%20rendering%20of%203-D%20shapes&amp;journal=ACM%20SIGGRAPH%20Computer%20Graphics&amp;volume=24&amp;issue=4&amp;pages=197-206&amp;publication_year=1990&amp;author=Saito%2CTakafumi&amp;author=Takahashi%2CTokiichiro" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41.">
              <p class="c-article-references__text" id="ref-CR41">
               Sharp, T., Keskin, C., Robertson, D.P., Taylor, J., Shotton, J., Kim, D., Rhemann, C., Leichter, I., Vinnikov, A., Wei, Y., Freedman, D., Kohli, P., Krupka, E., Fitzgibbon, A.W., Izadi, S.: Accurate, robust, and flexible real-time hand tracking. In: CHI (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR41-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sharp%2C%20T.%2C%20Keskin%2C%20C.%2C%20Robertson%2C%20D.P.%2C%20Taylor%2C%20J.%2C%20Shotton%2C%20J.%2C%20Kim%2C%20D.%2C%20Rhemann%2C%20C.%2C%20Leichter%2C%20I.%2C%20Vinnikov%2C%20A.%2C%20Wei%2C%20Y.%2C%20Freedman%2C%20D.%2C%20Kohli%2C%20P.%2C%20Krupka%2C%20E.%2C%20Fitzgibbon%2C%20A.W.%2C%20Izadi%2C%20S.%3A%20Accurate%2C%20robust%2C%20and%20flexible%20real-time%20hand%20tracking.%20In%3A%20CHI%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42.">
              <p class="c-article-references__text" id="ref-CR42">
               Shotton, J., Girshick, R.B., Fitzgibbon, A.W., Sharp, T., Cook, M., Finocchio, M., Moore, R., Kohli, P., Criminisi, A., Kipman, A., Blake, A.: Efficient human pose estimation from single depth images. PAMI
               <b>
                35
               </b>
               (12), 2821–2840 (2013)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR42-links">
               <a aria-label="CrossRef reference 42" data-doi="10.1109/TPAMI.2012.241" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1109/TPAMI.2012.241" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2012.241" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 42" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Efficient%20human%20pose%20estimation%20from%20single%20depth%20images&amp;journal=PAMI&amp;volume=35&amp;issue=12&amp;pages=2821-2840&amp;publication_year=2013&amp;author=Shotton%2CJ&amp;author=Girshick%2CRB&amp;author=Fitzgibbon%2CAW&amp;author=Sharp%2CT&amp;author=Cook%2CM&amp;author=Finocchio%2CM&amp;author=Moore%2CR&amp;author=Kohli%2CP&amp;author=Criminisi%2CA&amp;author=Kipman%2CA&amp;author=Blake%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43.">
              <p class="c-article-references__text" id="ref-CR43">
               Shotton, J., Winn, J.M., Rother, C., Criminisi, A.: Textonboost for image understanding: multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV
               <b>
                81
               </b>
               (1), 2–23 (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR43-links">
               <a aria-label="CrossRef reference 43" data-doi="10.1007/s11263-007-0109-1" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/s11263-007-0109-1" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-007-0109-1" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 43" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Textonboost%20for%20image%20understanding%3A%20multi-class%20object%20recognition%20and%20segmentation%20by%20jointly%20modeling%20texture%2C%20layout%2C%20and%20context&amp;journal=IJCV&amp;volume=81&amp;issue=1&amp;pages=2-23&amp;publication_year=2009&amp;author=Shotton%2CJ&amp;author=Winn%2CJM&amp;author=Rother%2CC&amp;author=Criminisi%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44.">
              <p class="c-article-references__text" id="ref-CR44">
               Stark, M., Goesele, M., Schiele, B.: Back to the future: learning shape models from 3D CAD data. In: BMVC (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR44-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Stark%2C%20M.%2C%20Goesele%2C%20M.%2C%20Schiele%2C%20B.%3A%20Back%20to%20the%20future%3A%20learning%20shape%20models%20from%203D%20CAD%20data.%20In%3A%20BMVC%20%282010%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45.">
              <p class="c-article-references__text" id="ref-CR45">
               Sturgess, P., Alahari, K., Ladicky, L., Torr, P.H.S.: Combining appearance and structure from motion features for road scene understanding. In: BMVC (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR45-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sturgess%2C%20P.%2C%20Alahari%2C%20K.%2C%20Ladicky%2C%20L.%2C%20Torr%2C%20P.H.S.%3A%20Combining%20appearance%20and%20structure%20from%20motion%20features%20for%20road%20scene%20understanding.%20In%3A%20BMVC%20%282009%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46.">
              <p class="c-article-references__text" id="ref-CR46">
               Taylor, G.R., Chosak, A.J., Brewer, P.C.: OVVV: using virtual worlds to design and evaluate surveillance systems. In: CVPR (2007)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR46-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Taylor%2C%20G.R.%2C%20Chosak%2C%20A.J.%2C%20Brewer%2C%20P.C.%3A%20OVVV%3A%20using%20virtual%20worlds%20to%20design%20and%20evaluate%20surveillance%20systems.%20In%3A%20CVPR%20%282007%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47.">
              <p class="c-article-references__text" id="ref-CR47">
               Tighe, J., Lazebnik, S.: Superparsing – scalable nonparametric image parsing with superpixels. IJCV
               <b>
                101
               </b>
               (2), 329–349 (2013)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR47-links">
               <a aria-label="CrossRef reference 47" data-doi="10.1007/s11263-012-0574-z" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/s11263-012-0574-z" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-012-0574-z" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="MathSciNet reference 47" data-track="click" data-track-action="MathSciNet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=3021066" rel="nofollow noopener">
                MathSciNet
               </a>
               <a aria-label="Google Scholar reference 47" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Superparsing%20%E2%80%93%20scalable%20nonparametric%20image%20parsing%20with%20superpixels&amp;journal=IJCV&amp;volume=101&amp;issue=2&amp;pages=329-349&amp;publication_year=2013&amp;author=Tighe%2CJ&amp;author=Lazebnik%2CS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48.">
              <p class="c-article-references__text" id="ref-CR48">
               Tripathi, S., Belongie, S., Hwang, Y., Nguyen, T.Q.: Semantic video segmentation: exploring inference efficiency. In: ISOCC (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR48-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tripathi%2C%20S.%2C%20Belongie%2C%20S.%2C%20Hwang%2C%20Y.%2C%20Nguyen%2C%20T.Q.%3A%20Semantic%20video%20segmentation%3A%20exploring%20inference%20efficiency.%20In%3A%20ISOCC%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49.">
              <p class="c-article-references__text" id="ref-CR49">
               Vázquez, D., López, A.M., Marín, J., Ponsa, D., Gomez, D.G.: Virtual and real world adaptation for pedestrian detection. PAMI
               <b>
                36
               </b>
               (4), 797–809 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR49-links">
               <a aria-label="CrossRef reference 49" data-doi="10.1109/TPAMI.2013.163" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1109/TPAMI.2013.163" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2013.163" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 49" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Virtual%20and%20real%20world%20adaptation%20for%20pedestrian%20detection&amp;journal=PAMI&amp;volume=36&amp;issue=4&amp;pages=797-809&amp;publication_year=2014&amp;author=V%C3%A1zquez%2CD&amp;author=L%C3%B3pez%2CAM&amp;author=Mar%C3%ADn%2CJ&amp;author=Ponsa%2CD&amp;author=Gomez%2CDG" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50.">
              <p class="c-article-references__text" id="ref-CR50">
               Xie, J., Kiefel, M., Sun, M.T., Geiger, A.: Semantic instance annotation of street scenes by 3D to 2D label transfer. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR50-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Xie%2C%20J.%2C%20Kiefel%2C%20M.%2C%20Sun%2C%20M.T.%2C%20Geiger%2C%20A.%3A%20Semantic%20instance%20annotation%20of%20street%20scenes%20by%203D%20to%202D%20label%20transfer.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51.">
              <p class="c-article-references__text" id="ref-CR51">
               Xu, J., Vázquez, D., López, A.M., Marín, J., Ponsa, D.: Learning a part-based pedestrian detector in a virtual world. IEEE Trans. Intell. Transp. Syst.
               <b>
                15
               </b>
               (5), 2121–2131 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR51-links">
               <a aria-label="CrossRef reference 51" data-doi="10.1109/TITS.2014.2310138" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1109/TITS.2014.2310138" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTITS.2014.2310138" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 51" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Learning%20a%20part-based%20pedestrian%20detector%20in%20a%20virtual%20world&amp;journal=IEEE%20Trans.%20Intell.%20Transp.%20Syst.&amp;volume=15&amp;issue=5&amp;pages=2121-2131&amp;publication_year=2014&amp;author=Xu%2CJ&amp;author=V%C3%A1zquez%2CD&amp;author=L%C3%B3pez%2CAM&amp;author=Mar%C3%ADn%2CJ&amp;author=Ponsa%2CD" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52.">
              <p class="c-article-references__text" id="ref-CR52">
               Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: ICLR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR52-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yu%2C%20F.%2C%20Koltun%2C%20V.%3A%20Multi-scale%20context%20aggregation%20by%20dilated%20convolutions.%20In%3A%20ICLR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53.">
              <p class="c-article-references__text" id="ref-CR53">
               Veeravasarapu, V.S.R., Rothkopf, C., Ramesh, V.: Model-driven simulations for deep convolutional neural networks (2016). arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/arxiv:1605.09582" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/arxiv:1605.09582">
                arXiv:1605.09582
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54.">
              <p class="c-article-references__text" id="ref-CR54">
               Shafaei, A., Little, J.J., Schmidt, M.: Play and learn: using video Games to train computer vision models. In: BMVC (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR54-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Shafaei%2C%20A.%2C%20Little%2C%20J.J.%2C%20Schmidt%2C%20M.%3A%20Play%20and%20learn%3A%20using%20video%20Games%20to%20train%20computer%20vision%20models.%20In%3A%20BMVC%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
            </ol>
            <p class="c-article-references__download u-hide-print">
             <a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-319-46475-6_7?format=refman&amp;flavour=references" rel="nofollow">
              Download references
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </p>
           </div>
          </div>
         </div>
        </section>
       </div>
       <section data-title="Acknowledgements" lang="en">
        <div class="c-article-section" id="Ack1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">
          <span class="c-article-section__title-number">
          </span>
          Acknowledgements
         </h2>
         <div class="c-article-section__content" id="Ack1-content">
          <p>
           SRR was supported in part by the German Research Foundation (DFG) within the GRK 1362. Additionally, SRR and SR were supported in part by the European Research Council under the European Union’s Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement No. 307942. Work on this project was conducted in part while SRR was an intern at Intel Labs.
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="author-information" data-title="Author information">
        <div class="c-article-section" id="author-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">
          <span class="c-article-section__title-number">
          </span>
          Author information
         </h2>
         <div class="c-article-section__content" id="author-information-content">
          <h3 class="c-article__sub-heading" id="affiliations">
           Authors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff17">
            <p class="c-article-author-affiliation__address">
             TU Darmstadt, Darmstadt, Germany
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Stephan R. Richter &amp; Stefan Roth
            </p>
           </li>
           <li id="Aff18">
            <p class="c-article-author-affiliation__address">
             Intel Labs, Santa Clara, USA
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Vibhav Vineet &amp; Vladlen Koltun
            </p>
           </li>
          </ol>
          <div class="u-js-hide u-hide-print" data-test="author-info">
           <span class="c-article__sub-heading">
            Authors
           </span>
           <ol class="c-article-authors-search u-list-reset">
            <li id="auth-Stephan_R_-Richter">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Stephan R. Richter
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Stephan%20R.%20Richter" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Stephan%20R.%20Richter" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stephan%20R.%20Richter%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Vibhav-Vineet">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Vibhav Vineet
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Vibhav%20Vineet" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Vibhav%20Vineet" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Vibhav%20Vineet%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Stefan-Roth">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Stefan Roth
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Stefan%20Roth" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Stefan%20Roth" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stefan%20Roth%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Vladlen-Koltun">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Vladlen Koltun
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Vladlen%20Koltun" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Vladlen%20Koltun" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Vladlen%20Koltun%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
           </ol>
          </div>
          <h3 class="c-article__sub-heading" id="corresponding-author">
           Corresponding author
          </h3>
          <p id="corresponding-author-list">
           Correspondence to
           <a href="mailto:stephan.richter@visinf.tu-darmstadt.de" id="corresp-c1">
            Stephan R. Richter
           </a>
           .
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="editor-information" data-title="Editor information">
        <div class="c-article-section" id="editor-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="editor-information">
          <span class="c-article-section__title-number">
          </span>
          Editor information
         </h2>
         <div class="c-article-section__content" id="editor-information-content">
          <h3 class="c-article__sub-heading" id="editor-affiliations">
           Editors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff13">
            <p class="c-article-author-affiliation__address">
             RWTH Aachen, Aachen, Germany
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Bastian Leibe
            </p>
           </li>
           <li id="Aff14">
            <p class="c-article-author-affiliation__address">
             Czech Technical University, Prague 2, Czech Republic
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Jiri Matas
            </p>
           </li>
           <li id="Aff15">
            <p class="c-article-author-affiliation__address">
             University of Trento, Povo -  Trento, Italy
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Nicu Sebe
            </p>
           </li>
           <li id="Aff16">
            <p class="c-article-author-affiliation__address">
             University of Amsterdam, Amsterdam, The Netherlands
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Max Welling
            </p>
           </li>
          </ol>
         </div>
        </div>
       </section>
       <section data-title="Rights and permissions" lang="en">
        <div class="c-article-section" id="rightslink-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">
          <span class="c-article-section__title-number">
          </span>
          Rights and permissions
         </h2>
         <div class="c-article-section__content" id="rightslink-content">
          <p class="c-article-rights" data-test="rightslink-content">
           <a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?publisherName=SpringerNature&amp;orderBeanReset=true&amp;orderSource=SpringerLink&amp;title=Playing%20for%20Data%3A%20Ground%20Truth%20from%20Computer%20Games&amp;author=Stephan%20R.%20Richter%2C%20Vibhav%20Vineet%2C%20Stefan%20Roth%20et%20al&amp;contentID=10.1007%2F978-3-319-46475-6_7&amp;copyright=Springer%20International%20Publishing%20AG&amp;publication=eBook&amp;publicationDate=2016&amp;startPage=102&amp;endPage=118&amp;imprint=Springer%20International%20Publishing%20AG">
            Reprints and Permissions
           </a>
          </p>
         </div>
        </div>
       </section>
       <section data-title="Copyright information">
        <div class="c-article-section" id="copyright-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="copyright-information">
          <span class="c-article-section__title-number">
          </span>
          Copyright information
         </h2>
         <div class="c-article-section__content" id="copyright-information-content">
          <p>
           © 2016 Springer International Publishing AG
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="chapter-info" data-title="About this paper" lang="en">
        <div class="c-article-section" id="chapter-info-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="chapter-info">
          <span class="c-article-section__title-number">
          </span>
          About this paper
         </h2>
         <div class="c-article-section__content" id="chapter-info-content">
          <div class="c-bibliographic-information">
           <div class="c-bibliographic-information__column">
            <h3 class="c-article__sub-heading" id="citeas">
             Cite this paper
            </h3>
            <p class="c-bibliographic-information__citation" data-test="bibliographic-information__cite_this_chapter">
             Richter, S.R., Vineet, V., Roth, S., Koltun, V. (2016).  Playing for Data: Ground Truth from Computer Games.

                     In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds) Computer Vision – ECCV 2016. ECCV 2016. Lecture Notes in Computer Science(), vol 9906. Springer, Cham. https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46475-6_7
            </p>
            <h3 class="c-bibliographic-information__download-citation u-mb-8 u-mt-16 u-hide-print">
             Download citation
            </h3>
            <ul class="c-bibliographic-information__download-citation-list">
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-319-46475-6_7?format=refman&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .RIS file">
               .RIS
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-319-46475-6_7?format=endnote&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .ENW file">
               .ENW
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-319-46475-6_7?format=bibtex&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .BIB file">
               .BIB
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
            </ul>
            <ul class="c-bibliographic-information__list u-mb-24" data-test="publication-history">
             <li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--chapter-doi">
              <p data-test="bibliographic-information__doi">
               <abbr title="Digital Object Identifier">
                DOI
               </abbr>
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                https://doi.org/10.1007/978-3-319-46475-6_7
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p>
               Published
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2016-09-17">
                 17 September 2016
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__publisher-name">
               Publisher Name
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                Springer, Cham
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__pisbn">
               Print ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-319-46474-9
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__eisbn">
               Online ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-319-46475-6
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__package">
               eBook Packages
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=11645&amp;facet-start-year=2016&amp;facet-end-year=2016">
                 Computer Science
                </a>
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=43710&amp;facet-start-year=2016&amp;facet-end-year=2016">
                 Computer Science (R0)
                </a>
               </span>
              </p>
             </li>
            </ul>
            <div data-component="share-box">
             <div class="c-article-share-box u-display-block">
              <h3 class="c-article__sub-heading">
               Share this paper
              </h3>
              <p class="c-article-share-box__description">
               Anyone you share the following link with will be able to read this content:
              </p>
              <button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url">
               Get shareable link
              </button>
              <div class="js-no-share-url-container u-display-none" hidden="">
               <p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">
                Sorry, a shareable link is not currently available for this article.
               </p>
              </div>
              <div class="js-share-url-container u-display-none" hidden="">
               <p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url">
               </p>
               <button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url">
                Copy to clipboard
               </button>
              </div>
              <p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
               Provided by the Springer Nature SharedIt content-sharing initiative
              </p>
             </div>
            </div>
            <div data-component="chapter-info-list">
            </div>
           </div>
          </div>
         </div>
        </div>
       </section>
      </div>
     </article>
    </main>
    <div class="c-article-extras u-text-sm u-hide-print" data-container-type="reading-companion" data-track-component="conference paper" id="sidebar">
     <aside>
      <div class="js-context-bar-sticky-point-desktop" data-test="download-article-link-wrapper">
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-319-46475-6.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-319-46475-6.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div data-test="editorial-summary">
      </div>
      <div class="c-reading-companion">
       <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky" style="top: 40px;">
        <ul class="c-reading-companion__tabs" role="tablist">
         <li role="presentation">
          <button aria-controls="tabpanel-sections" aria-selected="true" class="c-reading-companion__tab c-reading-companion__tab--active" data-tab-target="sections" data-track="click" data-track-action="sections tab" data-track-label="tab" id="tab-sections" role="tab">
           Sections
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-figures" aria-selected="false" class="c-reading-companion__tab" data-tab-target="figures" data-track="click" data-track-action="figures tab" data-track-label="tab" id="tab-figures" role="tab" tabindex="-1">
           Figures
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-references" aria-selected="false" class="c-reading-companion__tab" data-tab-target="references" data-track="click" data-track-action="references tab" data-track-label="tab" id="tab-references" role="tab" tabindex="-1">
           References
          </button>
         </li>
        </ul>
        <div aria-labelledby="tab-sections" class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections" role="tabpanel">
         <div class="c-reading-companion__scroll-pane" style="max-height: 4544px;">
          <ul class="c-reading-companion__sections-list">
           <li class="c-reading-companion__section-item c-reading-companion__section-item--active" id="rc-sec-Abs1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Abstract" href="#Abs1">
             <span class="c-article-section__title-number">
             </span>
             Abstract
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Introduction" href="#Sec1">
             <span class="c-article-section__title-number">
              1
             </span>
             Introduction
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec2">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Related Work" href="#Sec2">
             <span class="c-article-section__title-number">
              2
             </span>
             Related Work
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec3">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Breaking the Curse of Dataset Annotation" href="#Sec3">
             <span class="c-article-section__title-number">
              3
             </span>
             Breaking the Curse of Dataset Annotation
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec7">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Semantic Segmentation" href="#Sec7">
             <span class="c-article-section__title-number">
              4
             </span>
             Semantic Segmentation
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec10">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Discussion" href="#Sec10">
             <span class="c-article-section__title-number">
              5
             </span>
             Discussion
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Bib1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: References" href="#Bib1">
             <span class="c-article-section__title-number">
             </span>
             References
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Ack1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Acknowledgements" href="#Ack1">
             <span class="c-article-section__title-number">
             </span>
             Acknowledgements
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-author-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Author information" href="#author-information">
             <span class="c-article-section__title-number">
             </span>
             Author information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-editor-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Editor information" href="#editor-information">
             <span class="c-article-section__title-number">
             </span>
             Editor information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-rightslink">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Rights and permissions" href="#rightslink">
             <span class="c-article-section__title-number">
             </span>
             Rights and permissions
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-copyright-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Copyright information" href="#copyright-information">
             <span class="c-article-section__title-number">
             </span>
             Copyright information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-chapter-info">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: About this paper" href="#chapter-info">
             <span class="c-article-section__title-number">
             </span>
             About this paper
            </a>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-figures" class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ul class="c-reading-companion__figures-list">
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig1_HTML.gif?"/>
              <img alt="figure 1" aria-describedby="rc-Fig1" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig1_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7/figures/1" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig2_HTML.gif?"/>
              <img alt="figure 2" aria-describedby="rc-Fig2" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig2_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7/figures/2" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig3_HTML.gif?"/>
              <img alt="figure 3" aria-describedby="rc-Fig3" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig3_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7/figures/3" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig4_HTML.gif?"/>
              <img alt="figure 4" aria-describedby="rc-Fig4" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig4_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7/figures/4" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig5_HTML.gif?"/>
              <img alt="figure 5" aria-describedby="rc-Fig5" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig5_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7/figures/5" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig6_HTML.gif?"/>
              <img alt="figure 6" aria-describedby="rc-Fig6" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig6_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7/figures/6" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig7_HTML.gif?"/>
              <img alt="figure 7" aria-describedby="rc-Fig7" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-46475-6_7/MediaObjects/419974_1_En_7_Fig7_HTML.gif"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-319-46475-6_7/figures/7" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-references" class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ol class="c-reading-companion__references-list c-reading-companion__references-list--numeric">
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR1">
             Agrawal, R., Srikant, R.: Fast algorithms for mining association rules in large databases. In: VLDB (1994)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Agrawal%2C%20R.%2C%20Srikant%2C%20R.%3A%20Fast%20algorithms%20for%20mining%20association%20rules%20in%20large%20databases.%20In%3A%20VLDB%20%281994%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR2">
             Akenine-Möller, T., Haines, E., Hoffman, N.: Real-Time Rendering, 3rd edn. A K Peters, Natick (2008)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1201/b10644" href="https://doi-org.proxy.lib.ohio-state.edu/10.1201%2Fb10644">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Real-Time%20Rendering&amp;publication_year=2008&amp;author=Akenine-M%C3%B6ller%2CT&amp;author=Haines%2CE&amp;author=Hoffman%2CN">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR3">
             Appleby, A.: Murmurhash.
             <a data-track="click" data-track-action="external reference" data-track-label="https://github.com/aappleby/smhasher" href="https://github.com/aappleby/smhasher">
              https://github.com/aappleby/smhasher
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR4">
             Aubry, M., Maturana, D., Efros, A.A., Russell, B.C., Sivic, J.: Seeing 3D chairs: exemplar part-based 2D–3D alignment using a large dataset of CAD models. In: CVPR (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Aubry%2C%20M.%2C%20Maturana%2C%20D.%2C%20Efros%2C%20A.A.%2C%20Russell%2C%20B.C.%2C%20Sivic%2C%20J.%3A%20Seeing%203D%20chairs%3A%20exemplar%20part-based%202D%E2%80%933D%20alignment%20using%20a%20large%20dataset%20of%20CAD%20models.%20In%3A%20CVPR%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR5">
             Aubry, M., Russell, B.C.: Understanding deep features with computer-generated imagery. In: ICCV (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Aubry%2C%20M.%2C%20Russell%2C%20B.C.%3A%20Understanding%20deep%20features%20with%20computer-generated%20imagery.%20In%3A%20ICCV%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR6">
             Baker, S., Scharstein, D., Lewis, J.P., Roth, S., Black, M.J., Szeliski, R.: A database and evaluation methodology for optical flow. IJCV
             <b>
              92
             </b>
             (1), 1–31 (2011)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/s11263-010-0390-2" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-010-0390-2">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=A%20database%20and%20evaluation%20methodology%20for%20optical%20flow&amp;journal=IJCV&amp;volume=92&amp;issue=1&amp;pages=1-31&amp;publication_year=2011&amp;author=Baker%2CS&amp;author=Scharstein%2CD&amp;author=Lewis%2CJP&amp;author=Roth%2CS&amp;author=Black%2CMJ&amp;author=Szeliski%2CR">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR7">
             Barron, J.L., Fleet, D.J., Beauchemin, S.S.: Performance of optical flow techniques. IJCV
             <b>
              12
             </b>
             (1), 43–77 (1994)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/BF01420984" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2FBF01420984">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Performance%20of%20optical%20flow%20techniques&amp;journal=IJCV&amp;volume=12&amp;issue=1&amp;pages=43-77&amp;publication_year=1994&amp;author=Barron%2CJL&amp;author=Fleet%2CDJ&amp;author=Beauchemin%2CSS">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR8">
             Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: a high-definition ground truth database. Pattern Recogn. Lett.
             <b>
              30
             </b>
             (2), 88–97 (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1016/j.patrec.2008.04.005" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.patrec.2008.04.005">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Semantic%20object%20classes%20in%20video%3A%20a%20high-definition%20ground%20truth%20database&amp;journal=Pattern%20Recogn.%20Lett.&amp;volume=30&amp;issue=2&amp;pages=88-97&amp;publication_year=2009&amp;author=Brostow%2CGJ&amp;author=Fauqueur%2CJ&amp;author=Cipolla%2CR">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR9">
             Butler, D.J., Wulff, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie for optical flow evaluation. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7577, pp. 611–625. Springer, Heidelberg (2012). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-33783-3_44" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-33783-3_44">
              10.1007/978-3-642-33783-3_44
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=A%20naturalistic%20open%20source%20movie%20for%20optical%20flow%20evaluation&amp;pages=611-625&amp;publication_year=2012&amp;author=Butler%2CDJ&amp;author=Wulff%2CJ&amp;author=Stanley%2CGB&amp;author=Black%2CMJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR10">
             Chen, C., Seff, A., Kornhauser, A.L., Xiao, J.: DeepDriving: learning affordance for direct perception in autonomous driving. In: ICCV (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Chen%2C%20C.%2C%20Seff%2C%20A.%2C%20Kornhauser%2C%20A.L.%2C%20Xiao%2C%20J.%3A%20DeepDriving%3A%20learning%20affordance%20for%20direct%20perception%20in%20autonomous%20driving.%20In%3A%20ICCV%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR11">
             Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Cordts%2C%20M.%2C%20Omran%2C%20M.%2C%20Ramos%2C%20S.%2C%20Rehfeld%2C%20T.%2C%20Enzweiler%2C%20M.%2C%20Benenson%2C%20R.%2C%20Franke%2C%20U.%2C%20Roth%2C%20S.%2C%20Schiele%2C%20B.%3A%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR12">
             Dosovitskiy, A., Fischer, P., Ilg, E., Häusser, P., Hazirbas, C., Golkov, V., van der Smagt, P., Cremers, D., Brox, T.: FlowNet: learning optical flow with convolutional networks. In: ICCV (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Dosovitskiy%2C%20A.%2C%20Fischer%2C%20P.%2C%20Ilg%2C%20E.%2C%20H%C3%A4usser%2C%20P.%2C%20Hazirbas%2C%20C.%2C%20Golkov%2C%20V.%2C%20van%20der%20Smagt%2C%20P.%2C%20Cremers%2C%20D.%2C%20Brox%2C%20T.%3A%20FlowNet%3A%20learning%20optical%20flow%20with%20convolutional%20networks.%20In%3A%20ICCV%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR13">
             Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI dataset. Int. J. Rob. Res.
             <b>
              32
             </b>
             (11), 1231–1237 (2013)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1177/0278364913491297" href="https://doi-org.proxy.lib.ohio-state.edu/10.1177%2F0278364913491297">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Vision%20meets%20robotics%3A%20the%20KITTI%20dataset&amp;journal=Int.%20J.%20Rob.%20Res.&amp;volume=32&amp;issue=11&amp;pages=1231-1237&amp;publication_year=2013&amp;author=Geiger%2CA&amp;author=Lenz%2CP&amp;author=Stiller%2CC&amp;author=Urtasun%2CR">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR14">
             Haltakov, V., Unger, C., Ilic, S.: Framework for generation of synthetic ground truth data for driver assistance applications. In: Weickert, J., Hein, M., Schiele, B. (eds.) GCPR 2013. LNCS, vol. 8142, pp. 323–332. Springer, Heidelberg (2013). doi:
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-40602-7_35" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-40602-7_35">
              10.1007/978-3-642-40602-7_35
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-642-40602-7_35" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-642-40602-7_35">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Framework%20for%20generation%20of%20synthetic%20ground%20truth%20data%20for%20driver%20assistance%20applications&amp;pages=323-332&amp;publication_year=2013&amp;author=Haltakov%2CV&amp;author=Unger%2CC&amp;author=Ilic%2CS">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR15">
             Handa, A., Pătrăucean, V., Badrinarayanan, V., Stent, S., Cipolla, R.: Understanding real world indoor scenes with synthetic data. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Handa%2C%20A.%2C%20P%C4%83tr%C4%83ucean%2C%20V.%2C%20Badrinarayanan%2C%20V.%2C%20Stent%2C%20S.%2C%20Cipolla%2C%20R.%3A%20Understanding%20real%20world%20indoor%20scenes%20with%20synthetic%20data.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR16">
             Handa, A., Whelan, T., McDonald, J., Davison, A.J.: A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM. In: ICRA (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Handa%2C%20A.%2C%20Whelan%2C%20T.%2C%20McDonald%2C%20J.%2C%20Davison%2C%20A.J.%3A%20A%20benchmark%20for%20RGB-D%20visual%20odometry%2C%203D%20reconstruction%20and%20SLAM.%20In%3A%20ICRA%20%282014%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR17">
             He, X., Zemel, R.S., Carreira-Perpiñán, M.: Multiscale conditional random fields for image labeling. In: CVPR (2004)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=He%2C%20X.%2C%20Zemel%2C%20R.S.%2C%20Carreira-Perpi%C3%B1%C3%A1n%2C%20M.%3A%20Multiscale%20conditional%20random%20fields%20for%20image%20labeling.%20In%3A%20CVPR%20%282004%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR18">
             Horn, B.K.P., Schunck, B.G.: Determining optical flow. Artif. Intell.
             <b>
              17
             </b>
             (1–3), 185–203 (1981)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1016/0004-3702(81)90024-2" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2F0004-3702%2881%2990024-2">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Determining%20optical%20flow&amp;journal=Artif.%20Intell.&amp;volume=17&amp;issue=1%E2%80%933&amp;pages=185-203&amp;publication_year=1981&amp;author=Horn%2CBKP&amp;author=Schunck%2CBG">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR19">
             Hunt, G., Brubacher, D.: Detours: binary interception of Win32 functions. In: 3rd USENIX Windows NT Symposium (1999)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hunt%2C%20G.%2C%20Brubacher%2C%20D.%3A%20Detours%3A%20binary%20interception%20of%20Win32%20functions.%20In%3A%203rd%20USENIX%20Windows%20NT%20Symposium%20%281999%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR20">
             Intel: Intel Graphics Performance Analyzers.
             <a data-track="click" data-track-action="external reference" data-track-label="https://software-intel-com.proxy.lib.ohio-state.edu/en-us/gpa" href="https://software-intel-com.proxy.lib.ohio-state.edu/en-us/gpa">
              https://software-intel-com.proxy.lib.ohio-state.edu/en-us/gpa
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR21">
             Kaneva, B., Torralba, A., Freeman, W.T.: Evaluation of image features using a photorealistic virtual world. In: ICCV (2011)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kaneva%2C%20B.%2C%20Torralba%2C%20A.%2C%20Freeman%2C%20W.T.%3A%20Evaluation%20of%20image%20features%20using%20a%20photorealistic%20virtual%20world.%20In%3A%20ICCV%20%282011%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR22">
             Karlsson, B.: RenderDoc.
             <a data-track="click" data-track-action="external reference" data-track-label="https://renderdoc.org" href="https://renderdoc.org">
              https://renderdoc.org
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR23">
             Krähenbühl, P., Koltun, V.: Efficient inference in fully connected CRFs with Gaussian edge potentials. In: NIPS (2011)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kr%C3%A4henb%C3%BChl%2C%20P.%2C%20Koltun%2C%20V.%3A%20Efficient%20inference%20in%20fully%20connected%20CRFs%20with%20Gaussian%20edge%20potentials.%20In%3A%20NIPS%20%282011%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR24">
             Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classification with deep convolutional neural networks. In: NIPS (2012)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Krizhevsky%2C%20A.%2C%20Sutskever%2C%20I.%2C%20Hinton%2C%20G.E.%3A%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks.%20In%3A%20NIPS%20%282012%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR25">
             Kundu, A., Vineet, V., Koltun, V.: Feature space optimization for semantic video segmentation. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kundu%2C%20A.%2C%20Vineet%2C%20V.%2C%20Koltun%2C%20V.%3A%20Feature%20space%20optimization%20for%20semantic%20video%20segmentation.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR26">
             Liebelt, J., Schmid, C., Schertler, K.: Viewpoint-independent object class detection using 3D feature maps. In: CVPR (2008)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liebelt%2C%20J.%2C%20Schmid%2C%20C.%2C%20Schertler%2C%20K.%3A%20Viewpoint-independent%20object%20class%20detection%20using%203D%20feature%20maps.%20In%3A%20CVPR%20%282008%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR27">
             Liu, B., He, X.: Multiclass semantic video segmentation with object-level active inference. In: CVPR (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liu%2C%20B.%2C%20He%2C%20X.%3A%20Multiclass%20semantic%20video%20segmentation%20with%20object-level%20active%20inference.%20In%3A%20CVPR%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR28">
             Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Long%2C%20J.%2C%20Shelhamer%2C%20E.%2C%20Darrell%2C%20T.%3A%20Fully%20convolutional%20networks%20for%20semantic%20segmentation.%20In%3A%20CVPR%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR29">
             Marín, J., Vázquez, D., Gerónimo, D., López, A.M.: Learning appearance in virtual scenarios for pedestrian detection. In: CVPR (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Mar%C3%ADn%2C%20J.%2C%20V%C3%A1zquez%2C%20D.%2C%20Ger%C3%B3nimo%2C%20D.%2C%20L%C3%B3pez%2C%20A.M.%3A%20Learning%20appearance%20in%20virtual%20scenarios%20for%20pedestrian%20detection.%20In%3A%20CVPR%20%282010%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR30">
             Massa, F., Russell, B.C., Aubry, M.: Deep exemplar 2D–3D detection by adapting from real to rendered views. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Massa%2C%20F.%2C%20Russell%2C%20B.C.%2C%20Aubry%2C%20M.%3A%20Deep%20exemplar%202D%E2%80%933D%20detection%20by%20adapting%20from%20real%20to%20rendered%20views.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR31">
             Mayer, N., Ilg, E., Häusser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox, T.: A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Mayer%2C%20N.%2C%20Ilg%2C%20E.%2C%20H%C3%A4usser%2C%20P.%2C%20Fischer%2C%20P.%2C%20Cremers%2C%20D.%2C%20Dosovitskiy%2C%20A.%2C%20Brox%2C%20T.%3A%20A%20large%20dataset%20to%20train%20convolutional%20networks%20for%20disparity%2C%20optical%20flow%2C%20and%20scene%20flow%20estimation.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR32">
             McCane, B., Novins, K., Crannitch, D., Galvin, B.: On benchmarking optical flow. CVIU
             <b>
              84
             </b>
             (1), 126–143 (2001)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="math reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?1021.68733">
              MATH
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=On%20benchmarking%20optical%20flow&amp;journal=CVIU&amp;volume=84&amp;issue=1&amp;pages=126-143&amp;publication_year=2001&amp;author=McCane%2CB&amp;author=Novins%2CK&amp;author=Crannitch%2CD&amp;author=Galvin%2CB">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR33">
             Papon, J., Schoeler, M.: Semantic pose using deep networks trained on synthetic RGB-D. In: ICCV (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Papon%2C%20J.%2C%20Schoeler%2C%20M.%3A%20Semantic%20pose%20using%20deep%20networks%20trained%20on%20synthetic%20RGB-D.%20In%3A%20ICCV%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR34">
             Peng, X., Sun, B., Ali, K., Saenko, K.: Learning deep object detectors from 3D models. In: ICCV (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Peng%2C%20X.%2C%20Sun%2C%20B.%2C%20Ali%2C%20K.%2C%20Saenko%2C%20K.%3A%20Learning%20deep%20object%20detectors%20from%203D%20models.%20In%3A%20ICCV%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR35">
             Pepik, B., Stark, M., Gehler, P.V., Schiele, B.: Multi-view and 3D deformable part models. PAMI
             <b>
              37
             </b>
             (11), 2232–2245 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1109/TPAMI.2015.2408347" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2015.2408347">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Multi-view%20and%203D%20deformable%20part%20models&amp;journal=PAMI&amp;volume=37&amp;issue=11&amp;pages=2232-2245&amp;publication_year=2015&amp;author=Pepik%2CB&amp;author=Stark%2CM&amp;author=Gehler%2CPV&amp;author=Schiele%2CB">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR36">
             Ren, X., Malik, J.: Learning a classification model for segmentation. In: ICCV (2003)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ren%2C%20X.%2C%20Malik%2C%20J.%3A%20Learning%20a%20classification%20model%20for%20segmentation.%20In%3A%20ICCV%20%282003%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR37">
             Richter, S.R., Roth, S.: Discriminative shape from shading in uncalibrated illumination. In: CVPR (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Richter%2C%20S.R.%2C%20Roth%2C%20S.%3A%20Discriminative%20shape%20from%20shading%20in%20uncalibrated%20illumination.%20In%3A%20CVPR%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR38">
             Rockstar Games: Policy on posting copyrighted Rockstar Games material.
             <a data-track="click" data-track-action="external reference" data-track-label="http://tinyurl-com.proxy.lib.ohio-state.edu/pjfoqo5" href="http://tinyurl-com.proxy.lib.ohio-state.edu/pjfoqo5">
              http://tinyurl-com.proxy.lib.ohio-state.edu/pjfoqo5
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR39">
             Ros, G., Ramos, S., Granados, M., Bakhtiary, A., Vázquez, D., López, A.M.: Vision-based offline-online perception paradigm for autonomous driving. In: WACV (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Ros%2C%20G.%2C%20Ramos%2C%20S.%2C%20Granados%2C%20M.%2C%20Bakhtiary%2C%20A.%2C%20V%C3%A1zquez%2C%20D.%2C%20L%C3%B3pez%2C%20A.M.%3A%20Vision-based%20offline-online%20perception%20paradigm%20for%20autonomous%20driving.%20In%3A%20WACV%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR40">
             Saito, T., Takahashi, T.: Comprehensible rendering of 3-D shapes. In: SIGGRAPH (1990)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Comprehensible%20rendering%20of%203-D%20shapes&amp;journal=ACM%20SIGGRAPH%20Computer%20Graphics&amp;volume=24&amp;issue=4&amp;pages=197-206&amp;publication_year=1990&amp;author=Saito%2CTakafumi&amp;author=Takahashi%2CTokiichiro">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR41">
             Sharp, T., Keskin, C., Robertson, D.P., Taylor, J., Shotton, J., Kim, D., Rhemann, C., Leichter, I., Vinnikov, A., Wei, Y., Freedman, D., Kohli, P., Krupka, E., Fitzgibbon, A.W., Izadi, S.: Accurate, robust, and flexible real-time hand tracking. In: CHI (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sharp%2C%20T.%2C%20Keskin%2C%20C.%2C%20Robertson%2C%20D.P.%2C%20Taylor%2C%20J.%2C%20Shotton%2C%20J.%2C%20Kim%2C%20D.%2C%20Rhemann%2C%20C.%2C%20Leichter%2C%20I.%2C%20Vinnikov%2C%20A.%2C%20Wei%2C%20Y.%2C%20Freedman%2C%20D.%2C%20Kohli%2C%20P.%2C%20Krupka%2C%20E.%2C%20Fitzgibbon%2C%20A.W.%2C%20Izadi%2C%20S.%3A%20Accurate%2C%20robust%2C%20and%20flexible%20real-time%20hand%20tracking.%20In%3A%20CHI%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR42">
             Shotton, J., Girshick, R.B., Fitzgibbon, A.W., Sharp, T., Cook, M., Finocchio, M., Moore, R., Kohli, P., Criminisi, A., Kipman, A., Blake, A.: Efficient human pose estimation from single depth images. PAMI
             <b>
              35
             </b>
             (12), 2821–2840 (2013)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1109/TPAMI.2012.241" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2012.241">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Efficient%20human%20pose%20estimation%20from%20single%20depth%20images&amp;journal=PAMI&amp;volume=35&amp;issue=12&amp;pages=2821-2840&amp;publication_year=2013&amp;author=Shotton%2CJ&amp;author=Girshick%2CRB&amp;author=Fitzgibbon%2CAW&amp;author=Sharp%2CT&amp;author=Cook%2CM&amp;author=Finocchio%2CM&amp;author=Moore%2CR&amp;author=Kohli%2CP&amp;author=Criminisi%2CA&amp;author=Kipman%2CA&amp;author=Blake%2CA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR43">
             Shotton, J., Winn, J.M., Rother, C., Criminisi, A.: Textonboost for image understanding: multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV
             <b>
              81
             </b>
             (1), 2–23 (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/s11263-007-0109-1" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-007-0109-1">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Textonboost%20for%20image%20understanding%3A%20multi-class%20object%20recognition%20and%20segmentation%20by%20jointly%20modeling%20texture%2C%20layout%2C%20and%20context&amp;journal=IJCV&amp;volume=81&amp;issue=1&amp;pages=2-23&amp;publication_year=2009&amp;author=Shotton%2CJ&amp;author=Winn%2CJM&amp;author=Rother%2CC&amp;author=Criminisi%2CA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR44">
             Stark, M., Goesele, M., Schiele, B.: Back to the future: learning shape models from 3D CAD data. In: BMVC (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Stark%2C%20M.%2C%20Goesele%2C%20M.%2C%20Schiele%2C%20B.%3A%20Back%20to%20the%20future%3A%20learning%20shape%20models%20from%203D%20CAD%20data.%20In%3A%20BMVC%20%282010%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR45">
             Sturgess, P., Alahari, K., Ladicky, L., Torr, P.H.S.: Combining appearance and structure from motion features for road scene understanding. In: BMVC (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sturgess%2C%20P.%2C%20Alahari%2C%20K.%2C%20Ladicky%2C%20L.%2C%20Torr%2C%20P.H.S.%3A%20Combining%20appearance%20and%20structure%20from%20motion%20features%20for%20road%20scene%20understanding.%20In%3A%20BMVC%20%282009%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR46">
             Taylor, G.R., Chosak, A.J., Brewer, P.C.: OVVV: using virtual worlds to design and evaluate surveillance systems. In: CVPR (2007)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Taylor%2C%20G.R.%2C%20Chosak%2C%20A.J.%2C%20Brewer%2C%20P.C.%3A%20OVVV%3A%20using%20virtual%20worlds%20to%20design%20and%20evaluate%20surveillance%20systems.%20In%3A%20CVPR%20%282007%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR47">
             Tighe, J., Lazebnik, S.: Superparsing – scalable nonparametric image parsing with superpixels. IJCV
             <b>
              101
             </b>
             (2), 329–349 (2013)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/s11263-012-0574-z" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs11263-012-0574-z">
              CrossRef
             </a>
             <a data-track="click" data-track-action="mathscinet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=3021066">
              MathSciNet
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Superparsing%20%E2%80%93%20scalable%20nonparametric%20image%20parsing%20with%20superpixels&amp;journal=IJCV&amp;volume=101&amp;issue=2&amp;pages=329-349&amp;publication_year=2013&amp;author=Tighe%2CJ&amp;author=Lazebnik%2CS">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR48">
             Tripathi, S., Belongie, S., Hwang, Y., Nguyen, T.Q.: Semantic video segmentation: exploring inference efficiency. In: ISOCC (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tripathi%2C%20S.%2C%20Belongie%2C%20S.%2C%20Hwang%2C%20Y.%2C%20Nguyen%2C%20T.Q.%3A%20Semantic%20video%20segmentation%3A%20exploring%20inference%20efficiency.%20In%3A%20ISOCC%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR49">
             Vázquez, D., López, A.M., Marín, J., Ponsa, D., Gomez, D.G.: Virtual and real world adaptation for pedestrian detection. PAMI
             <b>
              36
             </b>
             (4), 797–809 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1109/TPAMI.2013.163" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2013.163">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Virtual%20and%20real%20world%20adaptation%20for%20pedestrian%20detection&amp;journal=PAMI&amp;volume=36&amp;issue=4&amp;pages=797-809&amp;publication_year=2014&amp;author=V%C3%A1zquez%2CD&amp;author=L%C3%B3pez%2CAM&amp;author=Mar%C3%ADn%2CJ&amp;author=Ponsa%2CD&amp;author=Gomez%2CDG">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR50">
             Xie, J., Kiefel, M., Sun, M.T., Geiger, A.: Semantic instance annotation of street scenes by 3D to 2D label transfer. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Xie%2C%20J.%2C%20Kiefel%2C%20M.%2C%20Sun%2C%20M.T.%2C%20Geiger%2C%20A.%3A%20Semantic%20instance%20annotation%20of%20street%20scenes%20by%203D%20to%202D%20label%20transfer.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR51">
             Xu, J., Vázquez, D., López, A.M., Marín, J., Ponsa, D.: Learning a part-based pedestrian detector in a virtual world. IEEE Trans. Intell. Transp. Syst.
             <b>
              15
             </b>
             (5), 2121–2131 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1109/TITS.2014.2310138" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTITS.2014.2310138">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Learning%20a%20part-based%20pedestrian%20detector%20in%20a%20virtual%20world&amp;journal=IEEE%20Trans.%20Intell.%20Transp.%20Syst.&amp;volume=15&amp;issue=5&amp;pages=2121-2131&amp;publication_year=2014&amp;author=Xu%2CJ&amp;author=V%C3%A1zquez%2CD&amp;author=L%C3%B3pez%2CAM&amp;author=Mar%C3%ADn%2CJ&amp;author=Ponsa%2CD">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR52">
             Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: ICLR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yu%2C%20F.%2C%20Koltun%2C%20V.%3A%20Multi-scale%20context%20aggregation%20by%20dilated%20convolutions.%20In%3A%20ICLR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR53">
             Veeravasarapu, V.S.R., Rothkopf, C., Ramesh, V.: Model-driven simulations for deep convolutional neural networks (2016). arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/arxiv:1605.09582" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/arxiv:1605.09582">
              arXiv:1605.09582
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR54">
             Shafaei, A., Little, J.J., Schmidt, M.: Play and learn: using video Games to train computer vision models. In: BMVC (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Shafaei%2C%20A.%2C%20Little%2C%20J.J.%2C%20Schmidt%2C%20M.%3A%20Play%20and%20learn%3A%20using%20video%20Games%20to%20train%20computer%20vision%20models.%20In%3A%20BMVC%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
          </ol>
         </div>
        </div>
       </div>
      </div>
     </aside>
    </div>
   </div>
   <div class="app-elements">
    <footer data-test="universal-footer">
     <div class="c-footer" data-track-component="unified-footer">
      <div class="c-footer__container">
       <div class="c-footer__grid c-footer__group--separator">
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Discover content
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="journals a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
            Journals A-Z
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="books a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/books/a/1">
            Books A-Z
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Publish with us
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="publish your research" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
            Publish your research
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="open access publishing" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/open-research/about/the-fundamentals-of-open-access-and-open-research">
            Open access publishing
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Products and services
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="our products" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/products">
            Our products
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="librarians" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/librarians">
            Librarians
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="societies" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/societies">
            Societies
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="partners and advertisers" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/partners">
            Partners and advertisers
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Our imprints
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Springer" data-track-label="link" href="https://www-springer-com.proxy.lib.ohio-state.edu/">
            Springer
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Nature Portfolio" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/">
            Nature Portfolio
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="BMC" data-track-label="link" href="https://www-biomedcentral-com.proxy.lib.ohio-state.edu/">
            BMC
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Palgrave Macmillan" data-track-label="link" href="https://www.palgrave.com/">
            Palgrave Macmillan
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Apress" data-track-label="link" href="https://www.apress.com/">
            Apress
           </a>
          </li>
         </ul>
        </div>
       </div>
      </div>
      <div class="c-footer__container">
       <nav aria-label="footer navigation">
        <ul class="c-footer__links">
         <li class="c-footer__item">
          <button class="c-footer__link" data-cc-action="preferences" data-track="click" data-track-action="Manage cookies" data-track-label="link">
           <span class="c-footer__button-text">
            Your privacy choices/Manage cookies
           </span>
          </button>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="california privacy statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/legal/ccpa">
           Your US state privacy rights
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="accessibility statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/info/accessibility">
           Accessibility statement
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="terms and conditions" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/termsandconditions">
           Terms and conditions
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="privacy policy" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/privacystatement">
           Privacy policy
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="help and support" data-track-label="link" href="https://support-springernature-com.proxy.lib.ohio-state.edu/en/support/home">
           Help and support
          </a>
         </li>
        </ul>
       </nav>
       <div class="c-footer__user">
        <p class="c-footer__user-info">
         <span data-test="footer-user-ip">
          3.128.143.42
         </span>
        </p>
        <p class="c-footer__user-info" data-test="footer-business-partners">
         OhioLINK Consortium (3000266689)  - Ohio State University Libraries (8200724141)
        </p>
       </div>
       <a class="c-footer__link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/">
        <img alt="Springer Nature" height="20" loading="lazy" src="/oscar-static/images/darwin/footer/img/logo-springernature_white-64dbfad7d8.svg" width="200"/>
       </a>
       <p class="c-footer__legal" data-test="copyright">
        © 2023 Springer Nature
       </p>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <div aria-hidden="true" class="u-visually-hidden">
   <!--?xml version="1.0" encoding="UTF-8"?-->
   <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <defs>
     <path d="M0 .74h56.72v55.24H0z" id="a">
     </path>
    </defs>
    <symbol id="icon-access" viewbox="0 0 18 18">
     <path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-account" viewbox="0 0 18 18">
     <path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-alert" viewbox="0 0 18 18">
     <path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-broad" viewbox="0 0 16 16">
     <path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)">
     </path>
    </symbol>
    <symbol id="icon-arrow-down" viewbox="0 0 16 16">
     <path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-left" viewbox="0 0 16 16">
     <path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-right" viewbox="0 0 16 16">
     <path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-sub" viewbox="0 0 16 16">
     <path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-up" viewbox="0 0 16 16">
     <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-article" viewbox="0 0 18 18">
     <path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-audio" viewbox="0 0 18 18">
     <path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-block" viewbox="0 0 24 24">
     <path d="m0 0h24v24h-24z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-book" viewbox="0 0 18 18">
     <path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-broad" viewbox="0 0 24 24">
     <path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)">
     </path>
    </symbol>
    <symbol id="icon-calendar" viewbox="0 0 18 18">
     <path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-cart" viewbox="0 0 18 18">
     <path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z">
     </path>
    </symbol>
    <symbol id="icon-chevron-less" viewbox="0 0 10 10">
     <path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)">
     </path>
    </symbol>
    <symbol id="icon-chevron-more" viewbox="0 0 10 10">
     <path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-chevron-right" viewbox="0 0 10 10">
     <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
     </path>
    </symbol>
    <symbol id="icon-circle-fill" viewbox="0 0 16 16">
     <path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-circle" viewbox="0 0 16 16">
     <path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-citation" viewbox="0 0 18 18">
     <path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-close" viewbox="0 0 16 16">
     <path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-collections" viewbox="0 0 18 18">
     <path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-compare" viewbox="0 0 18 18">
     <path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download-file" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download" viewbox="0 0 16 16">
     <path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-editors" viewbox="0 0 18 18">
     <path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-email" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-error" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-ethics" viewbox="0 0 18 18">
     <path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-expand">
     <path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-explore" viewbox="0 0 18 18">
     <path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-filter" viewbox="0 0 16 16">
     <path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z">
     </path>
    </symbol>
    <symbol id="icon-home" viewbox="0 0 18 18">
     <path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-image" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-info" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-institution" viewbox="0 0 18 18">
     <path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-location" viewbox="0 0 18 18">
     <path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-minus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-newsletter" viewbox="0 0 18 18">
     <path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-orcid" viewbox="0 0 18 18">
     <path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-plus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-print" viewbox="0 0 18 18">
     <path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-search" viewbox="0 0 22 22">
     <path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-facebook" viewbox="0 0 24 24">
     <path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-twitter" viewbox="0 0 24 24">
     <path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-youtube" viewbox="0 0 24 24">
     <path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-subject-medicine" viewbox="0 0 18 18">
     <path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-success" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-table" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick-circle" viewbox="0 0 24 24">
     <path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick" viewbox="0 0 16 16">
     <path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-update" viewbox="0 0 18 18">
     <path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-upload" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-video" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-warning" viewbox="0 0 18 18">
     <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-altmetric">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm-1.886 9.684-1.101 1.845a1 1 0 0 1-.728.479l-.13.008H3.056a9.001 9.001 0 0 0 17.886 0l-4.564-.001-2.779 4.156c-.454.68-1.467.55-1.758-.179l-.038-.113-1.69-6.195ZM12 3a9.001 9.001 0 0 0-8.947 8.016h4.533l2.017-3.375c.452-.757 1.592-.6 1.824.25l1.73 6.345 1.858-2.777a1 1 0 0 1 .707-.436l.124-.008h5.1A9.001 9.001 0 0 0 12 3Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-checklist-banner" viewbox="0 0 56.69 56.69">
     <path d="M0 0h56.69v56.69H0z" style="fill:none">
     </path>
     <clippath id="b">
      <use style="overflow:visible" xlink:href="#a">
      </use>
     </clippath>
     <path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round">
     </path>
     <path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round">
     </path>
    </symbol>
    <symbol id="icon-chevron-down" viewbox="0 0 16 16">
     <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-citations">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-eds-checklist" viewbox="0 0 32 32">
     <path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-external-link-medium" viewbox="0 0 24 24">
     <path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-info-filled-medium" viewbox="0 0 24 24">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z">
     </path>
    </symbol>
    <symbol id="icon-eds-menu" viewbox="0 0 24 24">
     <path d="M21.09 5c.503 0 .91.448.91 1s-.407 1-.91 1H2.91C2.406 7 2 6.552 2 6s.407-1 .91-1h18.18Zm-3.817 6c.401 0 .727.448.727 1s-.326 1-.727 1H2.727C2.326 13 2 12.552 2 12s.326-1 .727-1h14.546Zm3.818 6c.502 0 .909.448.909 1s-.407 1-.91 1H2.91c-.503 0-.91-.448-.91-1s.407-1 .91-1h18.18Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-search" viewbox="0 0 24 24">
     <path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-small-arrow-right" viewbox="0 0 16 16">
     <g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035">
      </path>
     </g>
    </symbol>
    <symbol id="icon-eds-user-single" viewbox="0 0 24 24">
     <path d="M12 12c5.498 0 10 4.001 10 9a1 1 0 0 1-2 0c0-3.838-3.557-7-8-7s-8 3.162-8 7a1 1 0 0 1-2 0c0-4.999 4.502-9 10-9Zm0-11a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm0 2a3 3 0 1 1 0 6 3 3 0 0 1 0-6Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-email-new" viewbox="0 0 24 24">
     <path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z">
     </path>
    </symbol>
    <symbol id="icon-expand-image" viewbox="0 0 18 18">
     <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-github" viewbox="0 0 100 100">
     <path clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-mentions">
     <g fill-rule="evenodd" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M22 15.255A9.373 9.373 0 0 1 8.745 2L22 15.255ZM15.477 8.523l4.215-4.215">
      </path>
      <path d="m7 13-5 9h10l-1-5">
      </path>
     </g>
    </symbol>
    <symbol id="icon-metrics-accesses">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM7.708 13.308c2.004 0 3.969 1.198 5.802 2.995l.23.23a2.285 2.285 0 0 1 .009 3.233C11.853 21.693 9.799 23 7.707 23c-2.091 0-4.14-1.305-6.033-3.226a2.285 2.285 0 0 1-.007-3.233c1.9-1.93 3.949-3.233 6.04-3.233Zm0 2c-1.396 0-3.064 1.062-4.623 2.644a.285.285 0 0 0 .007.41C4.642 19.938 6.311 21 7.707 21c1.397 0 3.069-1.065 4.623-2.644a.285.285 0 0 0 0-.404l-.23-.229c-1.487-1.451-3.064-2.415-4.393-2.415Zm-.036 1.077a1.77 1.77 0 1 1 .126 3.537 1.77 1.77 0 0 1-.126-3.537Zm.072 1.538a.23.23 0 1 0-.017.461.23.23 0 0 0 .017-.46Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-metrics">
     <path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-left">
     <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-right">
     <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z">
     </path>
    </symbol>
    <symbol id="icon-submit-open" viewbox="0 0 16 17">
     <path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero">
     </path>
    </symbol>
   </svg>
  </div>
  <script nomodule="true" src="/oscar-static/js/app-es5-bundle-774ca0a0f5.js">
  </script>
  <script src="/oscar-static/js/app-es6-bundle-047cc3c848.js" type="module">
  </script>
  <script nomodule="true" src="/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js">
  </script>
  <script src="/oscar-static/js/global-article-es6-bundle-c14b406246.js" type="module">
  </script>
  <div class="c-cookie-banner">
   <div class="c-cookie-banner__container">
    <p>
     This website sets only cookies which are necessary for it to function. They are used to enable core functionality such as security, network management and accessibility. These cookies cannot be switched off in our systems. You may disable these by changing your browser settings, but this may affect how the website functions. Please view our privacy policy for further details on how we process your information.
     <button class="c-cookie-banner__dismiss">
      Dismiss
     </button>
    </p>
   </div>
  </div>
 </body>
</html>
