<html class="js" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta content="pc,mobile" name="applicable-device"/>
  <meta content="Yes" name="access"/>
  <meta content="SpringerLink" name="twitter:site"/>
  <meta content="summary" name="twitter:card"/>
  <meta content="Content cover image" name="twitter:image:alt"/>
  <meta content="3D Face Reconstruction with Dense Landmarks" name="twitter:title"/>
  <meta content="Landmarks often play a key role in face analysis, but many aspects of identity or expression cannot be represented by sparse landmarks alone. Thus, in order to reconstruct faces more accurately, landmarks are often combined with additional signals like depth images..." name="twitter:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-031-19778-9.jpg" name="twitter:image"/>
  <meta content="10.1007/978-3-031-19778-9_10" name="dc.identifier"/>
  <meta content="10.1007/978-3-031-19778-9_10" name="DOI"/>
  <meta content="Landmarks often play a key role in face analysis, but many aspects of identity or expression cannot be represented by sparse landmarks alone. Thus, in order to reconstruct faces more accurately, landmarks are often combined with additional signals like depth images..." name="dc.description"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/content/pdf/10.1007/978-3-031-19778-9_10.pdf" name="citation_pdf_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10" name="citation_fulltext_html_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10" name="citation_abstract_html_url"/>
  <meta content="Computer Vision – ECCV 2022" name="citation_inbook_title"/>
  <meta content="3D Face Reconstruction with Dense Landmarks" name="citation_title"/>
  <meta content="2022" name="citation_publication_date"/>
  <meta content="160" name="citation_firstpage"/>
  <meta content="177" name="citation_lastpage"/>
  <meta content="en" name="citation_language"/>
  <meta content="10.1007/978-3-031-19778-9_10" name="citation_doi"/>
  <meta content="springer/eccv, dblp/eccv" name="citation_conference_series_id"/>
  <meta content="European Conference on Computer Vision" name="citation_conference_title"/>
  <meta content="ECCV" name="citation_conference_abbrev"/>
  <meta content="228082" name="size"/>
  <meta content="Landmarks often play a key role in face analysis, but many aspects of identity or expression cannot be represented by sparse landmarks alone. Thus, in order to reconstruct faces more accurately, landmarks are often combined with additional signals like depth images..." name="description"/>
  <meta content="Wood, Erroll" name="citation_author"/>
  <meta content="errollw@gmail.com" name="citation_author_email"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Baltrušaitis, Tadas" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Hewitt, Charlie" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Johnson, Matthew" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Shen, Jingjing" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Milosavljević, Nikola" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Wilde, Daniel" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Garbin, Stephan" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Sharp, Toby" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Stojiljković, Ivan" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Cashman, Tom" name="citation_author"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Valentin, Julien" name="citation_author"/>
  <meta content="valentin.julien@microsoft.com" name="citation_author_email"/>
  <meta content="Microsoft" name="citation_author_institution"/>
  <meta content="Springer, Cham" name="citation_publisher"/>
  <meta content="http://api.springer-com.proxy.lib.ohio-state.edu/xmldata/jats?q=doi:10.1007/978-3-031-19778-9_10&amp;api_key=" name="citation_springer_api_url"/>
  <meta content="telephone=no" name="format-detection"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10" property="og:url"/>
  <meta content="Paper" property="og:type"/>
  <meta content="SpringerLink" property="og:site_name"/>
  <meta content="3D Face Reconstruction with Dense Landmarks" property="og:title"/>
  <meta content="Landmarks often play a key role in face analysis, but many aspects of identity or expression cannot be represented by sparse landmarks alone. Thus, in order to reconstruct faces more accurately, landmarks are often combined with additional signals like depth images..." property="og:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-031-19778-9.jpg" property="og:image"/>
  <title>
   3D Face Reconstruction with Dense Landmarks | SpringerLink
  </title>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="shortcut icon"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="icon" sizes="16x16 32x32 48x48"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png" rel="icon" sizes="16x16" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png" rel="icon" sizes="32x32" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png" rel="icon" sizes="48x48" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png" rel="apple-touch-icon" sizes="72x72"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png" rel="apple-touch-icon" sizes="76x76"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png" rel="apple-touch-icon" sizes="114x114"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png" rel="apple-touch-icon" sizes="120x120"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png" rel="apple-touch-icon" sizes="144x144"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png" rel="apple-touch-icon" sizes="152x152"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon" sizes="180x180"/>
  <script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_SVG.js">
  </script>
  <script>
   (function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)
  </script>
  <script data-consent="link-springer-com.proxy.lib.ohio-state.edu" src="/static/js/lib/cookie-consent.min.js">
  </script>
  <style>
   @media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}body,img{max-width:100%}body{background:#fcfcfc;font-size:1.125rem;line-height:1.5;min-height:100%}main{display:block}h1{font-family:Georgia,Palatino,serif;font-size:2.25rem;font-style:normal;font-weight:400;line-height:1.4;margin:.67em 0}a{background-color:transparent;color:#004b83;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}sup{font-size:75%;line-height:0;position:relative;top:-.5em;vertical-align:baseline}img{border:0;height:auto;vertical-align:middle}button,input{font-family:inherit;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}*{margin:0}h2{font-family:Georgia,Palatino,serif;font-size:1.75rem;font-style:normal;font-weight:400;line-height:1.4}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input,p{margin:0;padding:0}a>img{vertical-align:middle}p{overflow-wrap:break-word;word-break:break-word}.c-app-header__theme{border-top-left-radius:2px;border-top-right-radius:2px;height:50px;margin:-16px -16px 0;overflow:hidden;position:relative}@media only screen and (min-width:1024px){.c-app-header__theme:after{background-color:hsla(0,0%,100%,.15);bottom:0;content:"";position:absolute;right:0;top:0;width:456px}}.c-app-header__content{padding-top:16px}@media only screen and (min-width:1024px){.c-app-header__content{display:flex}}.c-app-header__main{display:flex;flex:1 1 auto}.c-app-header__cover{margin-right:16px;margin-top:-50px;position:relative;z-index:5}.c-app-header__cover img{border:2px solid #fff;border-radius:4px;box-shadow:0 0 5px 2px hsla(0,0%,50%,.2);max-height:125px;max-width:96px}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-breadcrumbs>li{display:inline}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;list-style:none;margin:0;padding:16px}@media only screen and (min-width:540px){.c-pagination{justify-content:center}}.c-pagination__item{margin-bottom:8px;margin-right:16px}.c-pagination__item:last-child{margin-right:0}.c-pagination__link{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;min-width:30px;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link svg,.c-pagination__link--disabled svg{fill:currentcolor}.c-pagination__link:visited{color:#004b83}.c-pagination__link:focus,.c-pagination__link:hover{border:1px solid #666;text-decoration:none}.c-pagination__link:focus,.c-pagination__link:hover{background-color:#666;background-image:none;color:#fff}.c-pagination__link:focus svg path,.c-pagination__link:hover svg path{fill:#fff}.c-pagination__link--disabled{align-items:center;background-color:transparent;background-image:none;border-radius:2px;color:#333;cursor:default;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;opacity:.67;padding:8px;position:relative;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link--disabled:visited{color:#333}.c-pagination__link--disabled,.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{border:1px solid #ccc;text-decoration:none}.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{background-color:transparent;background-image:none;color:#333}.c-pagination__link--disabled:focus svg path,.c-pagination__link--disabled:hover svg path{fill:#333}.c-pagination__link--active{background-color:#666;background-image:none;border-color:#666;color:#fff;cursor:default}.c-pagination__ellipsis{background:0 0;border:0;min-width:auto;padding-left:0;padding-right:0}.c-pagination__icon{fill:#999;height:12px;width:16px}.c-pagination__icon--active{fill:#004b83}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#666;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-box{background-color:#fff;border:1px solid #ccc;border-radius:2px;line-height:1.3;padding:16px}.c-box--shadowed{box-shadow:0 0 5px 0 hsla(0,0%,50%,.1)}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #f2f2f2;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__section-item .c-article-section__title-number{display:none}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff}.c-pdf-download__link .u-icon{padding-top:2px}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}.u-button svg,.u-button--primary svg{fill:currentcolor}.app-elements .c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;line-height:1.4;padding:8px 0 0}.app-elements .c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav{border-top:2px solid #cedbe0;padding-top:4px;position:relative}.app-elements .c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav-container>:not(:last-child){margin-right:32px}.app-elements .c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.app-elements .c-header__list{list-style:none;margin:0;padding:0}.app-elements .c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.app-elements .c-header__list-item:not(:last-child){border-bottom:2px solid #cedbe0}.app-elements .c-header__item{color:inherit}@media only screen and (min-width:540px){.app-elements .c-header__item--menu{display:none;visibility:hidden}.app-elements .c-header__item--menu:first-child+*{margin-block-start:0}}.app-elements .c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:540px){.app-elements .c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.app-elements .c-header__item--divider:before{border-left:2px solid #cedbe0;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.app-elements .c-header__brand a{display:block;line-height:1;padding:16px 8px;text-decoration:none}.app-elements .c-header__brand img{height:24px;width:auto}.app-elements .c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-elements .c-header__link--static{flex:0 0 auto}.app-elements .c-header__icon{fill:currentcolor;display:inline-block;font-size:24px;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.app-elements .c-header__icon+*{margin-left:8px}.app-elements .c-header__expander{background-color:#ebf1f5}.app-elements .c-header__search{padding:24px 0}@media only screen and (min-width:540px){.app-elements .c-header__search{max-width:70%}}.app-elements .c-header__search-container{position:relative}.app-elements .c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.app-elements .c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.app-elements .c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.app-elements .has-tethered.c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:540px){.app-elements .has-tethered.c-header__expander--menu{display:none;visibility:hidden}}.app-elements .has-tethered .c-header__heading{display:none;visibility:hidden}.app-elements .has-tethered .c-header__heading:first-child+*{margin-block-start:0}.app-elements .has-tethered .c-header__search{margin:auto}.app-elements .c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-justify-content-space-between{justify-content:space-between}.u-flex-shrink{flex:0 1 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-mt-0{margin-top:0}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-float-left{float:left}.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-text-sm{font-size:1rem}.u-text-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-h3{font-family:Georgia,Palatino,serif;font-size:1.5rem;font-style:normal;font-weight:400;line-height:1.4}.c-article-section__content p{line-height:1.8}.c-pagination__input{border:1px solid #bfbfbf;border-radius:2px;box-shadow:inset 0 2px 6px 0 rgba(51,51,51,.2);box-sizing:initial;display:inline-block;height:28px;margin:0;max-width:64px;min-width:16px;padding:0 8px;text-align:center;transition:width .15s ease 0s}.c-pagination__input::-webkit-inner-spin-button,.c-pagination__input::-webkit-outer-spin-button{-webkit-appearance:none;margin:0}.c-article-associated-content__container .c-article-associated-content__collection-label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.063rem}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.063rem;font-weight:400}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-section__title,.c-article-title{font-weight:400}.c-chapter-book-series{font-size:1rem}.c-chapter-identifiers{margin:16px 0 8px}.c-chapter-book-details{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative}.c-chapter-book-details__title{font-weight:700}.c-chapter-book-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-chapter-book-details a{color:inherit}@media only screen and (max-width:539px){.c-chapter-book-details__meta{display:block}}.c-cover-image-lightbox{align-items:center;bottom:0;display:flex;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;transition:all .15s ease-in 0s;visibility:hidden;z-index:-1}.js-cover-image-lightbox--close{background:0 0;border:0;color:#fff;cursor:pointer;font-size:1.875rem;padding:13px;position:absolute;right:10px;top:0}.c-cover-image-lightbox__image{max-height:90vh;width:auto}.c-expand-overlay{background:#fff;color:#333;opacity:.5;padding:2px;position:absolute;right:3px;top:3px}.c-pdf-download__link{padding:13px 24px} }
  </style>
  <link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/enhanced-article-927ffe4eaf.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
  <script>
   window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/978-3-031-19778-9_10","Page":"chapter","page":{"attributes":{"environment":"live"}},"Country":"US","japan":false,"doi":"10.1007-978-3-031-19778-9_10","Keywords":"Dense correspondences, 3D Morphable model, Face alignment, Landmarks, Synthetic data","kwrd":["Dense_correspondences","3D_Morphable_model","Face_alignment","Landmarks","Synthetic_data"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate","cobranding","doNotAutoAssociate","cobranding"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3000266689","8200724141"],"businessPartnerIDString":"3000266689|8200724141"}},"Access Type":"subscription","Bpids":"3000266689, 8200724141","Bpnames":"OhioLINK Consortium, Ohio State University Libraries","BPID":["3000266689","8200724141"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-978-3-031-19778-9","Full HTML":"Y","session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1611-3349","pissn":"0302-9743"},"book":{"doi":"10.1007/978-3-031-19778-9","title":"Computer Vision – ECCV 2022","pisbn":"978-3-031-19777-2","eisbn":"978-3-031-19778-9","bookProductType":"Proceedings","seriesTitle":"Lecture Notes in Computer Science","seriesId":"558"},"chapter":{"doi":"10.1007/978-3-031-19778-9_10"},"type":"ConferencePaper","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"SCI","secondarySubjects":{"1":"Image Processing and Computer Vision"},"secondarySubjectCodes":{"1":"SCI22021"}},"sucode":"SUCO11645"},"attributes":{"deliveryPlatform":"oscar"},"country":"US","Has Preview":"N","subjectCodes":"SCI,SCI22021","PMC":["SCI","SCI22021"]},"Event Category":"Conference Paper","ConferenceSeriesId":"eccv, eccv","productId":"9783031197789"}];
  </script>
  <script>
   window.dataLayer.push({
        ga4MeasurementId: 'G-B3E4QL2TPR',
        ga360TrackingId: 'UA-26408784-1',
        twitterId: 'o47a7',
        ga4ServerUrl: 'https://collect-springer-com.proxy.lib.ohio-state.edu',
        imprint: 'springerlink'
    });
  </script>
  <script data-test="gtm-head">
   window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
  </script>
  <script>
   (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/lib/cookie-consent.min.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
  </script>
  <script>
   (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
  </script>
  <script>
   (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
  </script>
  <script class="js-entry">
   if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-f934ac6316.js', 'async': false},
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/app-es5-bundle-774ca0a0f5.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/app-es6-bundle-047cc3c848.js', 'async': false, 'module': true}
                
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-c14b406246.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
  </script>
  <script src="/oscar-static/js/airbrake-es5-bundle-f934ac6316.js">
  </script>
  <script src="/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js">
  </script>
  <link href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10" rel="canonical"/>
  <script type="application/ld+json">
   {"headline":"3D Face Reconstruction with Dense Landmarks","pageEnd":"177","pageStart":"160","image":"https://media-springernature-com.proxy.lib.ohio-state.edu/w153/springer-static/cover/book/978-3-031-19778-9.jpg","genre":["Computer Science","Computer Science (R0)"],"isPartOf":{"name":"Computer Vision – ECCV 2022","isbn":["978-3-031-19778-9","978-3-031-19777-2"],"@type":"Book"},"publisher":{"name":"Springer Nature Switzerland","logo":{"url":"https://www-springernature-com.proxy.lib.ohio-state.edu/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Erroll Wood","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"email":"errollw@gmail.com","@type":"Person"},{"name":"Tadas Baltrušaitis","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Charlie Hewitt","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Matthew Johnson","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Jingjing Shen","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Nikola Milosavljević","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Belgrade, Serbia","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Daniel Wilde","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Stephan Garbin","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Toby Sharp","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Ivan Stojiljković","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Belgrade, Serbia","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Tom Cashman","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Julien Valentin","affiliation":[{"name":"Microsoft","address":{"name":"Microsoft, Zurich, Switzerland","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"keywords":"Dense correspondences, 3D Morphable model, Face alignment, Landmarks, Synthetic data","description":"Landmarks often play a key role in face analysis, but many aspects of identity or expression cannot be represented by sparse landmarks alone. Thus, in order to reconstruct faces more accurately, landmarks are often combined with additional signals like depth images or techniques like differentiable rendering. Can we keep things simple by just using more landmarks? In answer, we present the first method that accurately predicts 10\n              \n                \n              \n              $$\\times $$\n              \n             as many landmarks as usual, covering the whole head, including the eyes and teeth. This is accomplished using synthetic training data, which guarantees perfect landmark annotations. By fitting a morphable model to these dense landmarks, we achieve state-of-the-art results for monocular 3D face reconstruction in the wild. We show that dense landmarks are an ideal signal for integrating face shape information across frames by demonstrating accurate and expressive facial performance capture in both monocular and multi-view scenarios. Finally, our method is highly efficient: we can predict dense landmarks and fit our 3D face model at over 150FPS on a single CPU thread. Please see our website: \n              https://microsoft.github.io/DenseLandmarks/\n              \n            .","datePublished":"2022","isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle","@context":"https://schema.org"}
  </script>
  <style type="text/css">
   .c-cookie-banner {
			background-color: #01324b;
			color: white;
			font-size: 1rem;
			position: fixed;
			bottom: 0;
			left: 0;
			right: 0;
			padding: 16px 0;
			font-family: sans-serif;
			z-index: 100002;
			text-align: center;
		}
		.c-cookie-banner__container {
			margin: 0 auto;
			max-width: 1280px;
			padding: 0 16px;
		}
		.c-cookie-banner p {
			margin-bottom: 8px;
		}
		.c-cookie-banner p:last-child {
			margin-bottom: 0;
		}	
		.c-cookie-banner__dismiss {
			background-color: transparent;
			border: 0;
			padding: 0;
			margin-left: 4px;
			color: inherit;
			text-decoration: underline;
			font-size: inherit;
		}
		.c-cookie-banner__dismiss:hover {
			text-decoration: none;
		}
  </style>
  <style type="text/css">
   .MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
  </style>
  <style type="text/css">
   #MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
  </style>
  <style type="text/css">
   .MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
  </style>
  <style type="text/css">
   #MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
  </style>
  <style type="text/css">
   .MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
  </style>
  <style type="text/css">
   .MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <script>
   window.dataLayer = window.dataLayer || [];
            window.dataLayer.push({
                recommendations: {
                    recommender: 'semantic',
                    model: 'specter',
                    policy_id: 'NA',
                    timestamp: 1698026576,
                    embedded_user: 'null'
                }
            });
  </script>
  <style type="text/css">
   .MathJax_SVG_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax_SVG .MJX-monospace {font-family: monospace}
.MathJax_SVG .MJX-sans-serif {font-family: sans-serif}
#MathJax_SVG_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax_SVG {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax_SVG * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_SVG > div {display: inline-block}
.mjx-svg-href {fill: blue; stroke: blue}
.MathJax_SVG_Processing {visibility: hidden; position: absolute; top: 0; left: 0; width: 0; height: 0; overflow: hidden; display: block!important}
.MathJax_SVG_Processed {display: none!important}
.MathJax_SVG_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_SVG_test.mjx-test-display {display: table!important}
.MathJax_SVG_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_SVG_test.mjx-test-default {display: block!important; clear: both}
.MathJax_SVG_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .MathJax_SVG_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_SVG_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_SVG_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax_SVG .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/jax/output/SVG/fonts/TeX/Size1/Regular/Main.js?V=2.7.5" type="text/javascript">
  </script>
 </head>
 <body class="shared-article-renderer">
  <div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;">
   <div id="MathJax_SVG_Hidden">
   </div>
   <svg>
    <defs id="MathJax_SVG_glyphs">
     <path d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" id="MJMAIN-D7" stroke-width="1">
     </path>
     <path d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z" id="MJMATHI-3BC" stroke-width="1">
     </path>
     <path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z" id="MJMATHI-3C3" stroke-width="1">
     </path>
     <path d="M609 0Q582 3 415 3T221 0H207V62H342V168L328 169Q193 180 117 241Q64 286 64 343T117 445Q193 506 328 517L342 518V624H207V686H221Q248 683 415 683T609 686H623V624H488V518L502 517Q637 506 713 445Q766 400 766 343T713 241Q637 180 502 169L488 168V62H623V0H609ZM342 219T342 343T340 467Q328 467 304 459Q277 451 261 439T237 409T228 378T226 343Q226 314 229 296T250 259T301 228Q331 219 341 219Q342 219 342 343ZM604 343Q604 365 602 379T591 413T560 446T503 464L489 467Q488 467 488 343T489 219Q499 219 529 228Q554 236 570 248T593 277T602 308T604 343Z" id="MJMAINB-3A6" stroke-width="1">
     </path>
     <path d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" id="MJMATHI-45" stroke-width="1">
     </path>
     <path d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" id="MJMAIN-28" stroke-width="1">
     </path>
     <path d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z" id="MJMAIN-3B" stroke-width="1">
     </path>
     <path d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" id="MJMATHI-4C" stroke-width="1">
     </path>
     <path d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" id="MJMAIN-29" stroke-width="1">
     </path>
     <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" id="MJMATHI-69" stroke-width="1">
     </path>
     <path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" id="MJMAIN-3D" stroke-width="1">
     </path>
     <path d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z" id="MJMAIN-7B" stroke-width="1">
     </path>
     <path d="M294 -8Q265 -8 244 -5T213 1T201 4Q200 4 192 -32T172 -111T155 -168Q134 -211 86 -211Q62 -211 48 -196T34 -158Q37 -144 103 123T174 404Q182 424 201 438T244 452Q271 452 284 436T298 404Q298 392 267 269T235 114Q235 43 305 43Q342 43 375 68T418 110Q420 112 455 253T492 397Q514 444 562 444Q587 444 601 429T615 397Q615 387 599 320T563 178T542 93Q540 81 540 72Q540 42 558 42Q580 42 596 75Q606 94 616 134Q621 155 624 158T646 162H651H662Q682 162 682 148Q681 142 679 132T665 94T641 47T602 9T548 -8Q523 -8 502 -3T468 11T446 27T432 40L429 46Q367 -8 294 -8Z" id="MJMATHBI-3BC" stroke-width="1">
     </path>
     <path d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" id="MJMAIN-2C" stroke-width="1">
     </path>
     <path d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z" id="MJMAIN-7D" stroke-width="1">
     </path>
     <path d="M118 -250V750H255V710H158V-210H255V-250H118Z" id="MJMAIN-5B" stroke-width="1">
     </path>
     <path d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" id="MJMATHI-78" stroke-width="1">
     </path>
     <path d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" id="MJMATHI-79" stroke-width="1">
     </path>
     <path d="M22 710V750H159V-250H22V-210H119V710H22Z" id="MJMAIN-5D" stroke-width="1">
     </path>
     <path d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" id="MJMAIN-2032" stroke-width="1">
     </path>
     <path d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" id="MJMAIN-4C" stroke-width="1">
     </path>
     <path d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" id="MJMAIN-6F" stroke-width="1">
     </path>
     <path d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" id="MJMAIN-73" stroke-width="1">
     </path>
     <path d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z" id="MJSZ2-2211" stroke-width="1">
     </path>
     <path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" id="MJMAIN-31" stroke-width="1">
     </path>
     <path d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" id="MJMAIN-7C" stroke-width="1">
     </path>
     <path d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z" id="MJMATHI-3BB" stroke-width="1">
     </path>
     <path d="M758 -1237T758 -1240T752 -1249H736Q718 -1249 717 -1248Q711 -1245 672 -1199Q237 -706 237 251T672 1700Q697 1730 716 1749Q718 1750 735 1750H752Q758 1744 758 1741Q758 1737 740 1713T689 1644T619 1537T540 1380T463 1176Q348 802 348 251Q348 -242 441 -599T744 -1218Q758 -1237 758 -1240Z" id="MJSZ4-28" stroke-width="1">
     </path>
     <path d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" id="MJMAIN-6C" stroke-width="1">
     </path>
     <path d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" id="MJMAIN-67" stroke-width="1">
     </path>
     <path d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" id="MJMAIN-32" stroke-width="1">
     </path>
    </defs>
   </svg>
  </div>
  <div id="MathJax_Message" style="">
   Loading [MathJax]/jax/output/SVG/fonts/TeX/Size1/Regular/Main.js
  </div>
  <!-- Google Tag Manager (noscript) -->
  <noscript data-test="gtm-body">
   <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0">
   </iframe>
  </noscript>
  <!-- End Google Tag Manager (noscript) -->
  <div class="u-vh-full">
   <a class="c-skip-link" href="#main-content">
    Skip to main content
   </a>
   <div class="u-hide u-show-following-ad">
   </div>
   <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
    <div class="c-ad__inner">
     <p class="c-ad__label">
      Advertisement
     </p>
     <div data-gpt="" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;" data-gpt-unitpath="/270604982/springerlink/book/chapter" data-pa11y-ignore="" data-test="LB1-ad" id="div-gpt-ad-LB1" style="min-width:728px;min-height:90px">
     </div>
    </div>
   </aside>
   <div class="app-elements u-mb-24">
    <header class="c-header" data-header="">
     <div class="c-header__container" data-header-expander-anchor="">
      <div class="c-header__brand">
       <a data-test="logo" data-track="click" data-track-action="click logo link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu">
        <img alt="SpringerLink" src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg"/>
       </a>
      </div>
      <a class="c-header__link c-header__link--static" data-test="login-link" data-track="click" data-track-action="click log in link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-031-19778-9_10">
       <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
        <use xlink:href="#icon-eds-user-single">
        </use>
       </svg>
       <span>
        Log in
       </span>
      </a>
     </div>
     <nav aria-label="header navigation" class="c-header__nav">
      <div class="c-header__nav-container">
       <div class="c-header__item c-header__item--menu">
        <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
         <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
          <use xlink:href="#icon-eds-menu">
          </use>
         </svg>
         <span>
          Menu
         </span>
        </a>
       </div>
       <div class="c-header__item c-header__item--inline-links">
        <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
         Find a journal
        </a>
        <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
         Publish with us
        </a>
       </div>
       <div class="c-header__link-container">
        <div class="c-header__item c-header__item--divider">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
          <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span>
           Search
          </span>
         </a>
        </div>
        <div class="c-header__item">
         <div class="c-header__item ecommerce-cart" id="ecommerce-header-cart-icon-link" style="display:inline-block">
          <a class="c-header__link" href="https://order-springer-com.proxy.lib.ohio-state.edu/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
           <svg aria-hidden="true" focusable="false" height="24" id="eds-i-cart" style="vertical-align:bottom" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <path d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z" fill="currentColor" fill-rule="nonzero">
            </path>
           </svg>
           <span style="padding-left:10px">
            Cart
           </span>
           <span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center">
           </span>
          </a>
          <script>
           (function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    console.log("listen to updatedCart event");
    document.body.addEventListener("updatedCart", function () {
        console.log("updatedCart happened");
        updateCartIcon().then(function () { return console.log("Cart state update upon event"); });
    }, false);
    return updateCartIcon().then(function () { return console.log("Initial cart state update"); });
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function () { return console.log("Could not fetch cart info"); });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart-springer-com.proxy.lib.ohio-state.edu/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()
          </script>
         </div>
        </div>
       </div>
      </div>
     </nav>
    </header>
    <div class="c-header__expander has-tethered u-js-hide" hidden="" id="popup-search">
     <h2 class="c-header__heading">
      Search
     </h2>
     <div class="u-container">
      <div class="c-header__search">
       <form action="//link-springer-com.proxy.lib.ohio-state.edu/search" data-track="submit" data-track-action="submit search form" data-track-category="unified header" data-track-label="form" method="GET" role="search">
        <label class="c-header__search-label" for="header-search">
         Search by keyword or author
        </label>
        <div class="c-header__search-container">
         <input autocomplete="off" class="c-header__search-input" id="header-search" name="query" required="" type="text" value=""/>
         <button class="c-header__search-button" type="submit">
          <svg aria-hidden="true" class="c-header__icon" focusable="false">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span class="u-visually-hidden">
           Search
          </span>
         </button>
        </div>
       </form>
      </div>
     </div>
    </div>
    <div class="c-header__expander c-header__expander--menu has-tethered u-js-hide" hidden="" id="header-nav">
     <h2 class="c-header__heading">
      Navigation
     </h2>
     <ul class="c-header__list">
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
        Find a journal
       </a>
      </li>
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
        Publish with us
       </a>
      </li>
     </ul>
    </div>
   </div>
   <div class="u-container u-mb-32 u-clearfix" data-component="article-container" id="main-content">
    <div class="u-hide-at-lg js-context-bar-sticky-point-mobile">
     <div class="c-pdf-container">
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-031-19778-9.pdf?pdf=button" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book PDF
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-031-19778-9.epub" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book EPUB
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
     </div>
    </div>
    <header class="u-mb-24" data-test="chapter-information-header">
     <div class="c-box c-box--shadowed">
      <div class="c-app-header">
       <div class="c-app-header__theme" style="background-image: url('https://media-springernature-com.proxy.lib.ohio-state.edu/dominant-colour/springer-static/cover/book/978-3-031-19778-9.jpg')">
       </div>
       <div class="c-app-header__content">
        <div class="c-app-header__main">
         <div class="c-app-header__cover">
          <div class="c-app-expand-overlay-wrapper">
           <a data-component="cover-zoom" data-img-src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-031-19778-9" href="/chapter/10.1007/978-3-031-19778-9_10/cover" rel="nofollow">
            <picture>
             <source srcset="                                                         //media.springernature.com/w92/springer-static/cover/book/978-3-031-19778-9.jpg?as=webp 1x,                                                         //media.springernature.com/w184/springer-static/cover/book/978-3-031-19778-9.jpg?as=webp 2x" type="image/webp"/>
             <img alt="Book cover" height="130" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92/springer-static/cover/book/978-3-031-19778-9.jpg" width="90"/>
            </picture>
            <svg aria-hidden="true" class="c-expand-overlay u-icon" data-component="expand-icon" focusable="false" height="18" width="18">
             <use xlink:href="#icon-expand-image" xmlns:xlink="http://www.w3.org/1999/xlink">
             </use>
            </svg>
           </a>
          </div>
         </div>
         <div class="c-cover-image-lightbox u-hide" data-component="cover-lightbox">
          <button aria-label="Close expanded book cover" class="js-cover-image-lightbox--close" data-component="close-cover-lightbox" type="button">
           <span aria-hidden="true">
            ×
           </span>
          </button>
          <picture>
           <source srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-031-19778-9?as=webp" type="image/webp"/>
           <img alt="Book cover" class="c-cover-image-lightbox__image" height="1200" src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-031-19778-9" width="800"/>
          </picture>
         </div>
         <div class="u-flex-shrink">
          <p class="c-chapter-info-details u-mb-8">
           <a data-track="click" data-track-action="open conference" data-track-label="link" href="/conference/eccv eccv">
            European Conference on Computer Vision
           </a>
          </p>
          <p class="c-chapter-book-details right-arrow">
           ECCV 2022:
           <a class="c-chapter-book-details__title" data-test="book-link" data-track="click" data-track-action="open book series" data-track-label="link" href="/book/10.1007/978-3-031-19778-9">
            Computer Vision – ECCV 2022
           </a>
           pp
                                         160–177
           <a class="c-chapter-book-details__cite-as u-hide-print" data-track="click" data-track-action="cite this chapter" data-track-label="link" href="#citeas">
            Cite as
           </a>
          </p>
         </div>
        </div>
       </div>
      </div>
     </div>
    </header>
    <nav aria-label="breadcrumbs" class="u-mb-16" data-test="article-breadcrumbs">
     <ol class="c-breadcrumbs c-breadcrumbs--truncated" itemscope="" itemtype="https://schema.org/BreadcrumbList">
      <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb1" href="/" itemprop="item">
        <span itemprop="name">
         Home
        </span>
       </a>
       <meta content="1" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb2" href="/book/10.1007/978-3-031-19778-9" itemprop="item">
        <span itemprop="name">
         Computer Vision – ECCV 2022
        </span>
       </a>
       <meta content="2" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <span itemprop="name">
        Conference paper
       </span>
       <meta content="3" itemprop="position"/>
      </li>
     </ol>
    </nav>
    <main class="c-article-main-column u-float-left js-main-column u-text-sans-serif" data-track-component="conference paper">
     <div aria-hidden="true" class="c-context-bar u-hide" data-context-bar="" data-context-bar-with-recommendations="" data-test="context-bar">
      <div class="c-context-bar__container u-container">
       <div class="c-context-bar__title">
        3D Face Reconstruction with Dense Landmarks
       </div>
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-031-19778-9.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-031-19778-9.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div id="recommendations">
       <div class="c-recommendations__container u-container u-display-none" data-component-recommendations="">
        <aside class="c-status-message c-status-message--success u-display-none" data-component-status-msg="">
         <svg aria-label="success:" class="c-status-message__icon" focusable="false" height="24" role="img" width="24">
          <use xlink:href="#icon-success">
          </use>
         </svg>
         <div class="c-status-message__message" id="success-message" tabindex="-1">
          Your content has downloaded
         </div>
        </aside>
        <div class="c-recommendations-header u-display-flex u-justify-content-space-between">
         <h2 class="c-recommendations-title" id="recommendation-heading">
          Similar content being viewed by others
         </h2>
         <button aria-label="Close" class="c-recommendations-close u-flex-static" data-track="click" data-track-action="close recommendations" type="button">
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-close">
           </use>
          </svg>
         </button>
        </div>
        <section aria-labelledby="recommendation-heading" aria-roledescription="carousel">
         <p class="u-visually-hidden">
          Slider with three content items shown per slide. Use the Previous and Next buttons to navigate the slides or the slide controller buttons at the end to navigate through each slide.
         </p>
         <div class="c-recommendations-list-container">
          <div class="c-recommendations-list">
           <div aria-label="Recommendation 1 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58536-5?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 1" data-track-label="10.1007/978-3-030-58536-5_41" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58536-5_41" itemprop="url">
                  “Look Ma, No Landmarks!” – Unsupervised, Model-Based Dense Face Alignment
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 2 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-68793-9?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 2" data-track-label="10.1007/978-3-030-68793-9_25" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-68793-9_25" itemprop="url">
                  The $$2^\mathrm{nd}$$ 106-Point Lightweight Facial Landmark Localization Grand Challenge
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2021
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 3 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-25072-9?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 3" data-track-label="10.1007/978-3-031-25072-9_23" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-25072-9_23" itemprop="url">
                  Perspective Reconstruction of Human Faces by Joint Mesh and Landmark Regression
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2023
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 4 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-319-48881-3?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 4" data-track-label="10.1007/978-3-319-48881-3_41" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-319-48881-3_41" itemprop="url">
                  Fast and Precise Face Alignment and 3D Shape Reconstruction from a Single 2D Image
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2016
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 5 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11042-023-14770-x/MediaObjects/11042_2023_14770_Fig1_HTML.png"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 5" data-track-label="10.1007/s11042-023-14770-x" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11042-023-14770-x" itemprop="url">
                  68 landmarks are efficient for 3D face alignment: what about more?
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  01 April 2023
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Marwa Jabberi, Ali Wali, … Adel M. Alimi
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 6 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11263-017-1009-7/MediaObjects/11263_2017_1009_Fig1_HTML.jpg"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 6" data-track-label="10.1007/s11263-017-1009-7" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11263-017-1009-7" itemprop="url">
                  Large Scale 3D Morphable Models
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  08 April 2017
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                James Booth, Anastasios Roussos, … Stefanos Zafeiriou
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 7 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-25072-9?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 7" data-track-label="10.1007/978-3-031-25072-9_28" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-25072-9_28" itemprop="url">
                  End to End Face Reconstruction via Differentiable PnP
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2023
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 8 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-319-41778-3?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 8" data-track-label="10.1007/978-3-319-41778-3_19" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-319-41778-3_19" itemprop="url">
                  3D Morphable Face Models and Their Applications
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2016
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 9 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11042-018-5895-7/MediaObjects/11042_2018_5895_Fig1_HTML.gif"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 9" data-track-label="10.1007/s11042-018-5895-7" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11042-018-5895-7" itemprop="url">
                  3D facial feature and expression computing from Internet image or video
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  21 March 2018
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Shan Wang, Xukun Shen &amp; Yan Zhang
               </p>
              </div>
             </div>
            </article>
           </div>
          </div>
         </div>
        </section>
       </div>
       <div class="js-greyout-page-background" data-component-grey-background="" style="display:none">
       </div>
      </div>
     </div>
     <article lang="en">
      <header data-test="chapter-detail-header">
       <div class="c-article-header">
        <h1 class="c-article-title" data-chapter-title="" data-test="chapter-title">
         3D Face Reconstruction with Dense Landmarks
        </h1>
        <ul class="c-article-author-list c-article-author-list--short js-no-scroll" data-component-authors-activator="authors-list" data-test="authors-list">
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Erroll-Wood" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Erroll-Wood">
           Erroll Wood
           <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
            <use xlink:href="#icon-email" xmlns:xlink="http://www.w3.org/1999/xlink">
            </use>
           </svg>
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Tadas-Baltru_aitis" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tadas-Baltru_aitis">
           Tadas Baltrušaitis
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Charlie-Hewitt" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Charlie-Hewitt">
           Charlie Hewitt
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Matthew-Johnson" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Matthew-Johnson">
           Matthew Johnson
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Jingjing-Shen" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jingjing-Shen">
           Jingjing Shen
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Nikola-Milosavljevi_" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Nikola-Milosavljevi_">
           Nikola Milosavljević
          </a>
          <sup class="u-js-hide">
           <a href="#Aff13" tabindex="-1">
            13
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Daniel-Wilde" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daniel-Wilde">
           Daniel Wilde
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Stephan-Garbin" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stephan-Garbin">
           Stephan Garbin
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Toby-Sharp" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Toby-Sharp">
           Toby Sharp
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Ivan-Stojiljkovi_" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ivan-Stojiljkovi_">
           Ivan Stojiljković
          </a>
          <sup class="u-js-hide">
           <a href="#Aff13" tabindex="-1">
            13
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Tom-Cashman" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tom-Cashman">
           Tom Cashman
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          &amp;
         </li>
         <li aria-label="Show all 12 authors for this article" class="c-article-author-list__show-more" title="Show all 12 authors for this article">
          …
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Julien-Valentin" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Julien-Valentin">
           Julien Valentin
          </a>
          <sup class="u-js-hide">
           <a href="#Aff14" tabindex="-1">
            14
           </a>
          </sup>
         </li>
        </ul>
        <button aria-expanded="false" class="c-article-author-list__button">
         <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
          <use xlink:href="#icon-plus" xmlns:xlink="http://www.w3.org/1999/xlink">
          </use>
         </svg>
         <span>
          Show authors
         </span>
        </button>
        <ul class="c-article-identifiers c-chapter-identifiers">
         <li class="c-article-identifiers__item" data-test="article-category">
          Conference paper
         </li>
         <li class="c-article-identifiers__item">
          <a data-track="click" data-track-action="publication date" data-track-label="link" href="#chapter-info">
           First Online:
           <time datetime="2022-11-03">
            03 November 2022
           </time>
          </a>
         </li>
        </ul>
        <div data-test="article-metrics">
         <div id="altmetric-container">
          <div class="c-article-metrics-bar__wrapper u-clear-both">
           <ul class="c-article-metrics-bar u-list-reset">
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              2337
              <span class="c-article-metrics-bar__label">
               Accesses
              </span>
             </p>
            </li>
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              16
              <a class="c-article-metrics-bar__label" data-track="click" data-track-action="Citation count" data-track-label="link" href="http://citations.springer-com.proxy.lib.ohio-state.edu/item?doi=10.1007/978-3-031-19778-9_10" rel="noopener" target="_blank" title="Visit Springer Citations for full citation details">
               Citations
              </a>
             </p>
            </li>
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              3
              <a class="c-article-metrics-bar__label" data-track="click" data-track-action="Social mentions" data-track-label="link" href="https://link.altmetric.com/details/143317436" rel="noopener" target="_blank" title="Visit Altmetric for full social mention details">
               Altmetric
              </a>
             </p>
            </li>
           </ul>
          </div>
         </div>
        </div>
        <p class="c-chapter-book-series">
         Part of the
         <a data-track="click" data-track-action="open book series" data-track-label="link" href="/bookseries/558">
          Lecture Notes in Computer Science
         </a>
         book series (LNCS,volume 13673)
        </p>
       </div>
      </header>
      <div class="c-article-body" data-article-body="true">
       <section aria-labelledby="Abs1" data-title="Abstract" lang="en">
        <div class="c-article-section" id="Abs1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">
          <span class="c-article-section__title-number">
          </span>
          Abstract
         </h2>
         <div class="c-article-section__content" id="Abs1-content">
          <p>
           Landmarks often play a key role in face analysis, but many aspects of identity or expression cannot be represented by sparse landmarks alone. Thus, in order to reconstruct faces more accurately, landmarks are often combined with additional signals like depth images or techniques like differentiable rendering. Can we keep things simple by just using more landmarks? In answer, we present the first method that accurately predicts 10
           <span class="mathjax-tex">
            <span class="MathJax_Preview" style="">
             \times
            </span>
            <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             <svg focusable="false" height="1.391ex" role="img" style="vertical-align: 0.019ex; margin-bottom: -0.138ex;" viewbox="0 -547.4 778.5 599" width="1.808ex" xmlns:xlink="http://www.w3.org/1999/xlink">
              <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
               <use x="0" xlink:href="#MJMAIN-D7" y="0">
               </use>
              </g>
             </svg>
            </span>
            <script id="MathJax-Element-1" type="math/tex">
             \times
            </script>
           </span>
           as many landmarks as usual, covering the whole head, including the eyes and teeth. This is accomplished using synthetic training data, which guarantees perfect landmark annotations. By fitting a morphable model to these dense landmarks, we achieve state-of-the-art results for monocular 3D face reconstruction in the wild. We show that dense landmarks are an ideal signal for integrating face shape information across frames by demonstrating accurate and expressive facial performance capture in both monocular and multi-view scenarios. Finally, our method is highly efficient: we can predict dense landmarks and fit our 3D face model at over 150FPS on a single CPU thread. Please see our website:
           <a href="https://microsoft.github.io/DenseLandmarks/">
            https://microsoft.github.io/DenseLandmarks/
           </a>
           .
          </p>
          <h3 class="c-article__sub-heading">
           Keywords
          </h3>
          <ul class="c-article-subject-list">
           <li class="c-article-subject-list__subject">
            <span>
             Dense correspondences
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             3D Morphable model
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Face alignment
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Landmarks
            </span>
           </li>
           <li class="c-article-subject-list__subject">
            <span>
             Synthetic data
            </span>
           </li>
          </ul>
         </div>
        </div>
       </section>
       <div data-test="chapter-cobranding-and-download">
        <div class="note test-pdf-link" id="cobranding-and-download-availability-text">
         <div class="c-article-access-provider" data-component="provided-by-box">
          <p class="c-article-access-provider__text">
           Access provided by
           <span class="js-institution-name">
            Ohio State University Libraries
           </span>
          </p>
          <p class="c-article-access-provider__text">
           <a class="c-pdf-download__link" data-track="click" data-track-action="Pdf download" data-track-label="inline link" download="" href="/content/pdf/10.1007/978-3-031-19778-9_10.pdf?pdf=inline%20link" id="js-body-chapter-download" rel="noopener" style="display: inline; padding:0px!important;" target="_blank">
            Download
           </a>
           conference paper PDF
          </p>
         </div>
        </div>
       </div>
       <div class="main-content">
        <section data-title="Introduction">
         <div class="c-article-section" id="Sec1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">
           <span class="c-article-section__title-number">
            1
           </span>
           Introduction
          </h2>
          <div class="c-article-section__content" id="Sec1-content">
           <p>
            Landmarks are points in correspondence across all faces, like the tip of the nose or the corner of the eye. They often play a role in face-related computer vision, e.g., being used to extract facial regions of interest [
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR33" id="ref-link-section-d85345740e838" title="Hassner, T., Harel, S., Paz, E., Enbar, R.: Effective face frontalization in unconstrained images. In: CVPR (2015)">
             33
            </a>
            ], or helping to constrain 3D model fitting [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d85345740e841" title="Garrido, P., et al.: Reconstruction of personalized 3d face rigs from monocular video. ACM Trans. Graph. 35(3), 1–15 (2016)">
             25
            </a>
            ,
            <a aria-label="Reference 78" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR78" id="ref-link-section-d85345740e844" title="Zollhöfer, M., et al.: State of the art on monocular 3d face reconstruction, tracking, and applications. Comput. Graph. Forum 37(2), 523–550 (2018)">
             78
            </a>
            ]. Unfortunately, many aspects of facial identity or expression cannot be encoded by a typical sparse set of 68 landmarks alone. For example, without landmarks on the cheeks, we cannot tell whether or not someone has high cheek-bones. Likewise, without landmarks around the outer eye region, we cannot tell if someone is softly closing their eyes, or scrunching up their face.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 1." id="figure-1">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/1" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig1_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 1" aria-describedby="Fig1" height="304" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig1_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc">
               <p>
                Given a single image (top), we first robustly and accurately predict 703 landmarks (middle). To aid visualization, we draw lines between landmarks. We then fit our 3D morphable face model to these landmarks to reconstruct faces in 3D (bottom).
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/1" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            In order to reconstruct faces more accurately, previous work has therefore used additional signals beyond color images, such as depth images [
            <a aria-label="Reference 63" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR63" id="ref-link-section-d85345740e868" title="Thies, J., Zollhöfer, M., Nießner, M., Valgaerts, L., Stamminger, M., Theobalt, C.: Real-time expression transfer for facial reenactment. ACM Trans. Graph. 34(6), 1–183 (2015)">
             63
            </a>
            ] or optical flow [
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR13" id="ref-link-section-d85345740e871" title="Cao, C., Chai, M., Woodford, O., Luo, L.: Stabilized real-time face tracking via a learned dynamic rigidity prior. ACM Trans. Graph. 37(6), 1–11 (2018)">
             13
            </a>
            ]. However, these signals may not be available or reliable to compute. Instead, given color images alone, others have approached the problem using analysis-by-synthesis: minimizing a photometric error [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d85345740e874" title="Garrido, P., et al.: Reconstruction of personalized 3d face rigs from monocular video. ACM Trans. Graph. 35(3), 1–15 (2016)">
             25
            </a>
            ] between a generative 3D face model and an observed image using differentiable rendering [
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d85345740e877" title="Dib, A., et al.: Practical face reconstruction via differentiable ray tracing. Comput. Graph. Forum 40(2), 153–164 (2021)">
             18
            </a>
            ,
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d85345740e880" title="Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., Freeman, W.T.: Unsupervised training for 3d morphable model regression. In: CVPR (2018)">
             26
            </a>
            ]. Unfortunately, these approaches are limited by the approximations that must be made in order for differentiable rendering to be computationally feasible. In reality, faces are not purely Lambertian [
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d85345740e884" title="Feng, Y., Feng, H., Black, M.J., Bolkart, T.: Learning an animatable detailed 3D face model from in-the-wild images. ACM Trans. Graph. (ToG) 40(4), 1–13 (2021)">
             23
            </a>
            ], and many important illumination effects are not explained using spherical harmonics alone [
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d85345740e887" title="Dib, A., et al.: Practical face reconstruction via differentiable ray tracing. Comput. Graph. Forum 40(2), 153–164 (2021)">
             18
            </a>
            ], e.g., ambient occlusion or shadows cast by the nose.
           </p>
           <p>
            Faced with this complexity, wouldn’t it be great if we could just use more landmarks? We present the first method that predicts over 700 landmarks both accurately and robustly. Instead of only the frontal “hockey-mask” portion of the face, our landmarks cover the entire head, including the ears, eyeballs, and teeth. As shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            , these landmarks provide a rich signal for both facial identity and expression. Even with as few as 68, it is hard for humans to precisely annotate landmarks that are not aligned with a salient image feature. That is why we use synthetic training data which guarantees consistent annotations. Furthermore, instead of representing each landmark as just a 2D coordinate, we predict each one as a random variable: a 2D circular Gaussian with position and uncertainty [
            <a aria-label="Reference 37" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR37" id="ref-link-section-d85345740e896" title="Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for computer vision? In: Advances in Neural Information Processing Systems, vol. 30 (2017)">
             37
            </a>
            ]. This allows our predictor to express uncertainty about certain landmarks, e.g., occluded landmarks on the back of the head.
           </p>
           <p>
            Since our dense landmarks represent points of correspondence across all faces, we can perform 3D face reconstruction by fitting a morphable face model [
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d85345740e902" title="Blanz, V., Vetter, T.: A morphable model for the synthesis of 3D faces. In: Computer Graphics and Interactive Techniques (1999)">
             6
            </a>
            ] to them. Although previous approaches have fit models to landmarks in a similar way [
            <a aria-label="Reference 76" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR76" id="ref-link-section-d85345740e905" title="Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: a 3d solution. In: CVPR (2016)">
             76
            </a>
            ], we are the first to show that landmarks are the only signal required to achieve state-of-the-art results for monocular face reconstruction in the wild.
           </p>
           <p>
            The probabilistic nature of our predictions also makes them ideal for fitting a 3D model over a temporal sequence, or across multiple views. An optimizer can discount uncertain landmarks and rely on more certain ones. We demonstrate this with accurate and expressive results for both multi-view and monocular facial performance capture. Finally, we show that predicting dense landmarks and then fitting a model can be highly efficient by demonstrating real-time facial performance capture at over 150FPS on a single CPU thread.
           </p>
           <p>
            In summary, our main contribution is to show that you can achieve more with less. You don’t need parametric appearance models, illumination models, or differentiable rendering for accurate 3D face reconstruction. All you need is a sufficiently large quantity of accurate 2D landmarks and a 3D model to fit to them. In addition, we show that combining probabilistic landmarks and model fitting lets us intelligently aggregate face shape information across multiple images by demonstrating robust and expressive results for both multi-view and monocular facial performance capture.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 2." id="figure-2">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/2" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig2_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 2" aria-describedby="Fig2" height="215" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig2_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc">
               <p>
                Compared to a typical sparse set of 68 facial landmarks (a), our dense landmarks (b) cover the entire head in great detail, including ears, eyes, and teeth. These dense landmarks are better at encoding facial identity and subtle expressions.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/2" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
          </div>
         </div>
        </section>
        <section data-title="Related Work">
         <div class="c-article-section" id="Sec2-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">
           <span class="c-article-section__title-number">
            2
           </span>
           Related Work
          </h2>
          <div class="c-article-section__content" id="Sec2-content">
           <p>
            Reconstructing faces in 3D from images is a mature field at the intersection of vision and graphics. We focus our literature review on methods that are closer to our own, and refer the reader to Morales et al. [
            <a aria-label="Reference 46" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR46" id="ref-link-section-d85345740e941" title="Morales, A., Piella, G., Sukno, F.M.: Survey on 3d face reconstruction from uncalibrated images. Comput. Sci. Rev. 40, 100400 (2021)">
             46
            </a>
            ] for an extensive survey.
           </p>
           <p>
            <b>
             Regression-Based 3D Face Reconstruction.
            </b>
            DNN-based regression has been extensively used as a tool for 3D face reconstruction. Techniques fall into two broad categories: supervised, and self-supervised. Approaches either use 3D Morphable Models (3DMMs) [
            <a aria-label="Reference 7" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR7" id="ref-link-section-d85345740e949" title="Blanz, V., Vetter, T.: Face recognition based on fitting a 3d morphable model. TPAMI 25(9), 1063–1074 (2003)">
             7
            </a>
            ,
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d85345740e952" title="Gerig, T., et al.: Morphable face models-an open framework. In: Automatic Face &amp; Gesture Recognition (FG). IEEE (2018)">
             27
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d85345740e955" title="Li, T., Bolkart, T., Black, M.J., Li, H., Romero, J.: Learning a model of facial shape and expression from 4D scans. In: ACM Transactions on Graphics, (Proceedings SIGGRAPH Asia) (2017)">
             40
            </a>
            ], or eschew linear models and instead learn a non-linear one as part of the training process [
            <a aria-label="Reference 65" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR65" id="ref-link-section-d85345740e958" title="Tran, L., Liu, F., Liu, X.: Towards high-fidelity nonlinear 3D face morphable model. In: CVPR (2019)">
             65
            </a>
            ].
           </p>
           <p>
            Fully supervised techniques either use parameter values from a 3DMM that is fit to the data via optimization as labels [
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR14" id="ref-link-section-d85345740e964" title="Chandran, P., Bradley, D., Gross, M., Beeler, T.: Semantic deep face models. In: International Conference on 3D Vision (3DV) (2020)">
             14
            </a>
            ,
            <a aria-label="Reference 67" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR67" id="ref-link-section-d85345740e967" title="Tuan Tran, A., Hassner, T., Masi, I., Medioni, G.: Regressing robust and discriminative 3D morphable models with a very deep neural network. In: CVPR (2017)">
             67
            </a>
            ,
            <a aria-label="Reference 72" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR72" id="ref-link-section-d85345740e970" title="Yi, H., et al.: MMFace: a multi-metric regression network for unconstrained face reconstruction. In: CVPR (2019)">
             72
            </a>
            ], or known face geometry is posed by sampling from a 3DMM and rendered to create synthetic datasets [
            <a aria-label="Reference 21" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR21" id="ref-link-section-d85345740e973" title="Dou, P., Shah, S.K., Kakadiaris, I.A.: End-to-end 3D face reconstruction with deep neural networks. In: CVPR (2017)">
             21
            </a>
            ,
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d85345740e976" title="Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., Freeman, W.T.: Unsupervised training for 3d morphable model regression. In: CVPR (2018)">
             26
            </a>
            ,
            <a aria-label="Reference 50" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR50" id="ref-link-section-d85345740e980" title="Richardson, E., Sela, M., Kimmel, R.: 3D face reconstruction by learning from synthetic data. In: 3DV. IEEE (2016)">
             50
            </a>
            ,
            <a aria-label="Reference 56" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR56" id="ref-link-section-d85345740e983" title="Sela, M., Richardson, E., Kimmel, R.: Unrestricted facial geometry reconstruction using image-to-image translation. In: ICCV (2017)">
             56
            </a>
            ]. Self-supervised approaches commonly use landmark reprojection error and/or perceptual loss via differentiable rendering [
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d85345740e986" title="Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face reconstruction with weakly-supervised learning: from single image to image set. In: CVPR Workshops (2019)">
             17
            </a>
            ,
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d85345740e989" title="Feng, Y., Feng, H., Black, M.J., Bolkart, T.: Learning an animatable detailed 3D face model from in-the-wild images. ACM Trans. Graph. (ToG) 40(4), 1–13 (2021)">
             23
            </a>
            ,
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d85345740e992" title="Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., Freeman, W.T.: Unsupervised training for 3d morphable model regression. In: CVPR (2018)">
             26
            </a>
            ,
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d85345740e995" title="Guo, J., Zhu, X., Yang, Y., Yang, F., Lei, Z., Li, S.Z.: Towards fast, accurate and stable 3d dense face alignment. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12364, pp. 152–168. Springer, Cham (2020). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58529-7_10
                
              ">
             30
            </a>
            ,
            <a aria-label="Reference 31" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR31" id="ref-link-section-d85345740e999" title="Guo, Y., Cai, J., Jiang, B., Zheng, J., et al.: Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images. TPAMI 41(6), 1294–1307 (2018)">
             31
            </a>
            ,
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d85345740e1002" title="Liu, Y., Jourabloo, A., Ren, W., Liu, X.: Dense face alignment. In: ICCV Workshops (2017)">
             44
            </a>
            ,
            <a aria-label="Reference 51" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR51" id="ref-link-section-d85345740e1005" title="Richardson, E., Sela, M., Or-El, R., Kimmel, R.: Learning detailed face reconstruction from a single image. In: CVPR (2017)">
             51
            </a>
            ,
            <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d85345740e1008" title="Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3d face shape and expression from an image without 3d supervision. In: CVPR (2019)">
             54
            </a>
            ,
            <a aria-label="Reference 61" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR61" id="ref-link-section-d85345740e1011" title="Tewari, A., et al: Self-supervised multi-level face model learning for monocular reconstruction at over 250 Hz. In: CVPR (2018)">
             61
            </a>
            ,
            <a aria-label="Reference 62" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR62" id="ref-link-section-d85345740e1014" title="Tewari, A., et al.: Mofa: model-based deep convolutional face autoencoder for unsupervised monocular reconstruction. In: ICCV Workshops (2017)">
             62
            </a>
            ,
            <a aria-label="Reference 65" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR65" id="ref-link-section-d85345740e1018" title="Tran, L., Liu, F., Liu, X.: Towards high-fidelity nonlinear 3D face morphable model. In: CVPR (2019)">
             65
            </a>
            ,
            <a aria-label="Reference 66" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR66" id="ref-link-section-d85345740e1021" title="Tran, L., Liu, X.: Nonlinear 3d face morphable model. In: CVPR (2018)">
             66
            </a>
            ]. Other techniques augment this with 3D or multiview constraints [
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d85345740e1024" title="Dou, P., Kakadiaris, I.A.: Multi-view 3D face reconstruction with deep recurrent neural networks. Image Vis. Comput. 80, 80–91 (2018)">
             20
            </a>
            ,
            <a aria-label="Reference 43" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR43" id="ref-link-section-d85345740e1027" title="Liu, F., Zhu, R., Zeng, D., Zhao, Q., Liu, X.: Disentangling features in 3D face shapes for joint face reconstruction and recognition. In: CVPR (2018)">
             43
            </a>
            ,
            <a aria-label="Reference 57" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR57" id="ref-link-section-d85345740e1030" title="Shang, J.: Self-supervised monocular 3d face reconstruction by occlusion-aware multi-view geometry consistency. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12360, pp. 53–70. Springer, Cham (2020). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58555-6_4
                
              ">
             57
            </a>
            ,
            <a aria-label="Reference 60" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR60" id="ref-link-section-d85345740e1033" title="Tewari, A., et al.: FML: face model learning from videos. In: CVPR (2019)">
             60
            </a>
            ,
            <a aria-label="Reference 73" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR73" id="ref-link-section-d85345740e1037" title="Yoon, J.S., Shiratori, T., Yu, S.I., Park, H.S.: Self-supervised adaptation of high-fidelity face models for monocular performance tracking. In: CVPR (2019)">
             73
            </a>
            ,
            <a aria-label="Reference 74" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR74" id="ref-link-section-d85345740e1040" title="Zhou, Y., Deng, J., Kotsia, I., Zafeiriou, S.: Dense 3d face decoding over 2500fps: joint texture &amp; shape convolutional mesh decoders. In: CVPR (2019)">
             74
            </a>
            ]. While this is similar to our technique, we only use a DNN to regress landmark positions which are then used to optimize 3DMM parameters, as in the large body of hybrid model-fitting methods [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d85345740e1043" title="Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep It SMPL: automatic estimation of 3d human pose and shape from a single image. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9909, pp. 561–578. Springer, Cham (2016). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46454-1_34
                
              ">
             8
            </a>
            ,
            <a aria-label="Reference 32" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR32" id="ref-link-section-d85345740e1046" title="Han, S., et al.: Megatrack: monochrome egocentric articulated hand-tracking for virtual reality. ACM Trans. Graph. (TOG) 39(4), 1–87 (2020)">
             32
            </a>
            ].
           </p>
           <p>
            <b>
             Optimization-Based 3D Face Reconstruction.
            </b>
            Traditionally, markerless reconstruction of face geometry is achieved with multi-view stereo [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d85345740e1054" title="Beeler, T., Bickel, B., Beardsley, P., Sumner, B., Gross, M.: High-quality single-shot capture of facial geometry. In: ACM Transactions on Graphics (2010)">
             4
            </a>
            ,
            <a aria-label="Reference 55" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR55" id="ref-link-section-d85345740e1057" title="Seitz, S.M., Curless, B., Diebel, J., Scharstein, D., Szeliski, R.: A comparison and evaluation of multi-view stereo reconstruction algorithms. In: CVPR (2006)">
             55
            </a>
            ], followed by optical flow based alignment, and then optimisation using geometric and temporal priors [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d85345740e1060" title="Beeler, T., et al.: High-quality passive facial performance capture using anchor frames. In: ACM Transactions on Graphics (2011)">
             5
            </a>
            ,
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR9" id="ref-link-section-d85345740e1063" title="Bradley, D., Heidrich, W., Popa, T., Sheffer, A.: High resolution passive facial performance capture. In: ACM Transactions on Graphics, vol. 29, no. 4 (2010)">
             9
            </a>
            ,
            <a aria-label="Reference 49" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR49" id="ref-link-section-d85345740e1066" title="Popa, T., South-Dickinson, I., Bradley, D., Sheffer, A., Heidrich, W.: Globally consistent space-time reconstruction. Comput. Graph. Forum 29(5), 1633–1642 (2010)">
             49
            </a>
            ]. While such methods produce detailed results, each step takes hours to complete. They also suffer from drift and other issues due to their reliance on optical flow and multi-view stereo [
            <a aria-label="Reference 15" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR15" id="ref-link-section-d85345740e1070" title="Cong, M., Lan, L., Fedkiw, R.: Local geometric indexing of high resolution data for facial reconstruction from sparse markers. CoRR abs/1903.00119 (2019). 
                www.arxiv.org/abs/1903.00119
                
              ">
             15
            </a>
            ]. While our method cannot reconstruct faces in such fine detail, it accurately recovers the low-frequency shape of the face, and aligns it with a common topology. This enriches the raw data with semantics, making it useful for other tasks.
           </p>
           <p>
            If only a single image is available, dense photometric [
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d85345740e1077" title="Dib, A., et al.: Practical face reconstruction via differentiable ray tracing. Comput. Graph. Forum 40(2), 153–164 (2021)">
             18
            </a>
            ,
            <a aria-label="Reference 64" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR64" id="ref-link-section-d85345740e1080" title="Thies, J., Zollhöfer, M., Stamminger, M., Theobalt, C., Nießner, M.: Face2Face: real-time face capture and reenactment of RGB videos. In: CVPR (2016)">
             64
            </a>
            ], depth [
            <a aria-label="Reference 63" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR63" id="ref-link-section-d85345740e1083" title="Thies, J., Zollhöfer, M., Nießner, M., Valgaerts, L., Stamminger, M., Theobalt, C.: Real-time expression transfer for facial reenactment. ACM Trans. Graph. 34(6), 1–183 (2015)">
             63
            </a>
            ], or optical flow [
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR13" id="ref-link-section-d85345740e1086" title="Cao, C., Chai, M., Woodford, O., Luo, L.: Stabilized real-time face tracking via a learned dynamic rigidity prior. ACM Trans. Graph. 37(6), 1–11 (2018)">
             13
            </a>
            ] constraints are commonly used to recover face shape and motion. However, these methods still rely on sparse landmarks for initializing the optimization close to the dense constraint’s basin of convergence, and coping with fast head motion [
            <a aria-label="Reference 78" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR78" id="ref-link-section-d85345740e1089" title="Zollhöfer, M., et al.: State of the art on monocular 3d face reconstruction, tracking, and applications. Comput. Graph. Forum 37(2), 523–550 (2018)">
             78
            </a>
            ]. In contrast, we argue that dense landmarks alone are sufficient for accurately recovering the overall shape of the face.
           </p>
           <p>
            <b>
             Dense Landmark Prediction.
            </b>
           </p>
           <p>
            While sparse landmark prediction is a mainstay of the field [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d85345740e1101" title="Bulat, A., Tzimiropoulos, G.: How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3D facial landmarks). In: ICCV (2017)">
             12
            </a>
            ], few methods directly predict dense landmarks or correspondences. This is because annotating a face with dense landmarks is a highly ambiguous task, so either synthetic data [
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d85345740e1104" title="Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)">
             70
            </a>
            ], pseudo-labels made with model-fitting [
            <a aria-label="Reference 16" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR16" id="ref-link-section-d85345740e1107" title="Deng, J., Guo, J., Ververas, E., Kotsia, I., Zafeiriou, S.: RetinaFace: single-shot multi-level face localisation in the wild. In: CVPR (2020)">
             16
            </a>
            ,
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d85345740e1110" title="Feng, Y., Wu, F., Shao, X., Wang, Y., Zhou, X.: Joint 3d face reconstruction and dense alignment with position map regression network. In: ECCV (2018)">
             24
            </a>
            ,
            <a aria-label="Reference 77" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR77" id="ref-link-section-d85345740e1113" title="Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: a 3d solution. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 146–155 (2016)">
             77
            </a>
            ], or semi-automatic refinement of training data [
            <a aria-label="Reference 35" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR35" id="ref-link-section-d85345740e1117" title="Jeni, L.A., Cohn, J.F., Kanade, T.: Dense 3D face alignment from 2D videos in real-time. In: Automatic Face and Gesture Recognition (FG). IEEE (2015)">
             35
            </a>
            ,
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d85345740e1120" title="Kartynnik, Y., Ablavatski, A., Grishchenko, I., Grundmann, M.: Real-time facial surface geometry from monocular video on mobile GPUs. In: CVPR Workshops (2019)">
             36
            </a>
            ] are used. Another issue with predicting dense landmarks is that heatmaps, the
            <i>
             de facto
            </i>
            technique for predicting landmarks [
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d85345740e1126" title="Bulat, A., Sanchez, E., Tzimiropoulos, G.: Subpixel heatmap regression for facial landmark Localization. In: BMVC (2021)">
             11
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d85345740e1129" title="Bulat, A., Tzimiropoulos, G.: How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3D facial landmarks). In: ICCV (2017)">
             12
            </a>
            ], rise in computational complexity with the number of landmarks. While a few previous methods have predicted dense frontal-face landmarks via cascade regression [
            <a aria-label="Reference 35" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR35" id="ref-link-section-d85345740e1132" title="Jeni, L.A., Cohn, J.F., Kanade, T.: Dense 3D face alignment from 2D videos in real-time. In: Automatic Face and Gesture Recognition (FG). IEEE (2015)">
             35
            </a>
            ] or direct regression [
            <a aria-label="Reference 16" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR16" id="ref-link-section-d85345740e1136" title="Deng, J., Guo, J., Ververas, E., Kotsia, I., Zafeiriou, S.: RetinaFace: single-shot multi-level face localisation in the wild. In: CVPR (2020)">
             16
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d85345740e1139" title="Grishchenko, I., Ablavatski, A., Kartynnik, Y., Raveendran, K., Grundmann, M.: Attention mesh: high-fidelity face mesh prediction in real-time. In: CVPR Workshops (2020)">
             28
            </a>
            ,
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d85345740e1142" title="Kartynnik, Y., Ablavatski, A., Grishchenko, I., Grundmann, M.: Real-time facial surface geometry from monocular video on mobile GPUs. In: CVPR Workshops (2019)">
             36
            </a>
            ], we are the first to accurately and robustly predict over 700 landmarks covering the whole head, including eyes and teeth.
           </p>
           <p>
            Some methods choose to predict dense correspondences as an image instead, where each pixel corresponds to a fixed point in a UV-unwrapping of the face [
            <a aria-label="Reference 1" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR1" id="ref-link-section-d85345740e1148" title="Alp Güler, R., Trigeorgis, G., Antonakos, E., Snape, P., Zafeiriou, S., Kokkinos, I.: DenseReg: fully convolutional dense shape regression in-the-wild. In: CVPR (2017)">
             1
            </a>
            ,
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d85345740e1151" title="Feng, Y., Wu, F., Shao, X., Wang, Y., Zhou, X.: Joint 3d face reconstruction and dense alignment with position map regression network. In: ECCV (2018)">
             24
            </a>
            ] or body [
            <a aria-label="Reference 29" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR29" id="ref-link-section-d85345740e1154" title="Güler, R.A., Neverova, N., Kokkinos, I.: Densepose: dense human pose estimation in the wild. In: CVPR (2018)">
             29
            </a>
            ,
            <a aria-label="Reference 59" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR59" id="ref-link-section-d85345740e1157" title="Taylor, J., Shotton, J., Sharp, T., Fitzgibbon, A.: The vitruvian manifold: inferring dense correspondences for one-shot human pose estimation. In: CVPR (2012)">
             59
            </a>
            ]. Such parameterization suffers from several drawbacks. How does one handle self-occluded portions of the face, e.g., the back of the head? Furthermore, what occurs at UV-island boundaries? If a pixel is half-nose and half-cheek, to which does it correspond? Instead, we choose to discretize the face into dense landmarks. This lets us predict parts of the face that are self-occluded, or lie outside image bounds. Having a fixed set of correspondences also benefits the model-fitter, making it more amenable to running in real-time.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 3." id="figure-3">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/3" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig3_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 3" aria-describedby="Fig3" height="178" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig3_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc">
               <p>
                Given an image, we first predict probabilistic dense landmarks
                <i>
                 L
                </i>
                , each with position
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                  \mu
                 </span>
                 <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-2-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.914ex" role="img" style="vertical-align: -0.685ex;" viewbox="0 -529.2 603.5 824.1" width="1.402ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMATHI-3BC" y="0">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-2" type="math/tex">
                  \mu
                 </script>
                </span>
                and certainty
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                  \sigma
                 </span>
                 <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-3-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.506ex" role="img" style="vertical-align: -0.277ex;" viewbox="0 -529.2 572.5 648.4" width="1.33ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMATHI-3C3" y="0">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-3" type="math/tex">
                  \sigma
                 </script>
                </span>
                . Then, we fit our 3D face model to
                <i>
                 L
                </i>
                , minimizing an energy
                <i>
                 E
                </i>
                by optimizing model parameters
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                  \boldsymbol{\mathbf {\Phi }}
                 </span>
                 <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-4-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="2.05ex" role="img" style="vertical-align: -0.277ex;" viewbox="0 -763.5 831.5 882.7" width="1.931ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMAINB-3A6" y="0">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-4" type="math/tex">
                  \boldsymbol{\mathbf {\Phi }}
                 </script>
                </span>
                .
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/3" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
          </div>
         </div>
        </section>
        <section data-title="Method">
         <div class="c-article-section" id="Sec3-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">
           <span class="c-article-section__title-number">
            3
           </span>
           Method
          </h2>
          <div class="c-article-section__content" id="Sec3-content">
           <p>
            In recent years, methods for 3D face reconstruction have become more and more complicated, involving differentiable rendering and complex neural network training strategies. We show instead that success can be found by keeping things simple. Our approach consists of two stages: First we predict probabilistic dense 2D landmarks
            <i>
             L
            </i>
            using a traditional convolutional neural network (CNN). Then, we fit a 3D face model, parameterized by
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\Phi }}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-5-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 831.5 846.9" width="1.931ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAINB-3A6" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-5" type="math/tex">
              \boldsymbol{\mathbf {\Phi }}
             </script>
            </span>
            , to the 2D landmarks by minimizing an energy function
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E(\boldsymbol{\mathbf {\Phi }}; L)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-6-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 3501.7 1194" width="8.133ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-45" y="0">
                </use>
                <use x="764" xlink:href="#MJMAIN-28" y="0">
                </use>
                <use x="1154" xlink:href="#MJMAINB-3A6" y="0">
                </use>
                <use x="1985" xlink:href="#MJMAIN-3B" y="0">
                </use>
                <use x="2430" xlink:href="#MJMATHI-4C" y="0">
                </use>
                <use x="3112" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-6" type="math/tex">
              E(\boldsymbol{\mathbf {\Phi }}; L)
             </script>
            </span>
            . Images themselves are not part of this optimization; the only data used are 2D landmarks.
           </p>
           <p>
            The main difference between our work and previous approaches is the number and quality of landmarks. No one before has predicted so many 2D landmarks, so accurately. This lets us achieve accurate 3D face reconstruction results by fitting a 3D model to these landmarks alone.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 4." id="figure-4">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/4" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig4_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 4" aria-describedby="Fig4" height="205" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig4_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc">
               <p>
                Examples of our synthetic training data. Without the perfectly consistent annotations provided by synthetic data, dense landmark prediction would not be possible.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/4" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec4">
            <span class="c-article-section__title-number">
             3.1
            </span>
            Landmark Prediction
           </h3>
           <p>
            <b>
             Synthetic Training Data.
            </b>
            Our results are only possible because we use synthetic training data. While a human can consistently label face images with e.g., 68 landmarks, it would be almost impossible for them to annotate an image with dense landmarks. How would it be possible to consistently annotate occluded landmarks on the back of the head, or multiple landmarks over a largely featureless patch of skin e.g., the forehead? In previous work, pseudo-labelled real images with dense correspondences are obtained by fitting a 3DMM to images [
            <a aria-label="Reference 1" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR1" id="ref-link-section-d85345740e1343" title="Alp Güler, R., Trigeorgis, G., Antonakos, E., Snape, P., Zafeiriou, S., Kokkinos, I.: DenseReg: fully convolutional dense shape regression in-the-wild. In: CVPR (2017)">
             1
            </a>
            ], but the resulting label consistency heavily depends on the quality of the 3D fitting. Using synthetic data has the advantage of guaranteeing perfectly consistent labels. We rendered a training dataset of 100 k images using the method of Wood et al. [
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d85345740e1346" title="Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)">
             70
            </a>
            ] with some minor modifications: we include expression-dependent wrinkle texture maps for more realistic skin appearance, and additional clothing, accessory, and hair assets. See Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
             4
            </a>
            for some examples.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 5." id="figure-5">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/5" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig5_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 5" aria-describedby="Fig5" height="112" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig5_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc">
               <p>
                When parts of the face are occluded by e.g. hair or clothing, the corresponding landmarks are predicted with high uncertainty (red), compared to those visible (green). (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 5" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure5 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/5" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             Probabilistic Landmark Regression.
            </b>
            We predict each landmark as a random variable with the probability density function of a circular 2D Gaussian. So
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              L_i = \{\boldsymbol{\mathbf {\mu }}_i, \sigma _i\}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-7-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 5774.6 1194" width="13.412ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-4C" y="0">
                </use>
                <use transform="scale(0.707)" x="963" xlink:href="#MJMATHI-69" y="-213">
                </use>
                <use x="1303" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <use x="2359" xlink:href="#MJMAIN-7B" y="0">
                </use>
                <g transform="translate(2860,0)">
                 <use x="0" xlink:href="#MJMATHBI-3BC" y="0">
                 </use>
                 <use transform="scale(0.707)" x="1001" xlink:href="#MJMATHI-69" y="-350">
                 </use>
                </g>
                <use x="3913" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(4358,0)">
                 <use x="0" xlink:href="#MJMATHI-3C3" y="0">
                 </use>
                 <use transform="scale(0.707)" x="808" xlink:href="#MJMATHI-69" y="-213">
                 </use>
                </g>
                <use x="5274" xlink:href="#MJMAIN-7D" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-7" type="math/tex">
              L_i = \{\boldsymbol{\mathbf {\mu }}_i, \sigma _i\}
             </script>
            </span>
            , where
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\mu }}_i = [x_i, y_i]
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-8-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 5140.6 1194" width="11.94ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHBI-3BC" y="0">
                </use>
                <use transform="scale(0.707)" x="1001" xlink:href="#MJMATHI-69" y="-350">
                </use>
                <use x="1330" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <use x="2386" xlink:href="#MJMAIN-5B" y="0">
                </use>
                <g transform="translate(2665,0)">
                 <use x="0" xlink:href="#MJMATHI-78" y="0">
                 </use>
                 <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-69" y="-213">
                 </use>
                </g>
                <use x="3582" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(4027,0)">
                 <use x="0" xlink:href="#MJMATHI-79" y="0">
                 </use>
                 <use transform="scale(0.707)" x="693" xlink:href="#MJMATHI-69" y="-213">
                 </use>
                </g>
                <use x="4862" xlink:href="#MJMAIN-5D" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-8" type="math/tex">
              \boldsymbol{\mathbf {\mu }}_i = [x_i, y_i]
             </script>
            </span>
            is the expected position of that landmark, and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma _i
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-9-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 915.8 747.8" width="2.127ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-3C3" y="0">
                </use>
                <use transform="scale(0.707)" x="808" xlink:href="#MJMATHI-69" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-9" type="math/tex">
              \sigma _i
             </script>
            </span>
            (the standard deviation) is a measure of uncertainty. Our training data includes labels for landmark positions
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\mu }}_i^\prime = [x_i^\prime , y_i^\prime ]
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-10-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.888ex" role="img" style="vertical-align: -0.926ex;" viewbox="0 -844.9 5140.6 1243.6" width="11.94ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHBI-3BC" y="0">
                </use>
                <use transform="scale(0.707)" x="1001" xlink:href="#MJMAIN-2032" y="445">
                </use>
                <use transform="scale(0.707)" x="1001" xlink:href="#MJMATHI-69" y="-430">
                </use>
                <use x="1330" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <use x="2386" xlink:href="#MJMAIN-5B" y="0">
                </use>
                <g transform="translate(2665,0)">
                 <use x="0" xlink:href="#MJMATHI-78" y="0">
                 </use>
                 <use transform="scale(0.707)" x="809" xlink:href="#MJMAIN-2032" y="445">
                 </use>
                 <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-69" y="-430">
                 </use>
                </g>
                <use x="3582" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(4027,0)">
                 <use x="0" xlink:href="#MJMATHI-79" y="0">
                 </use>
                 <use transform="scale(0.707)" x="706" xlink:href="#MJMAIN-2032" y="445">
                 </use>
                 <use transform="scale(0.707)" x="693" xlink:href="#MJMATHI-69" y="-430">
                 </use>
                </g>
                <use x="4862" xlink:href="#MJMAIN-5D" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-10" type="math/tex">
              \boldsymbol{\mathbf {\mu }}_i^\prime = [x_i^\prime , y_i^\prime ]
             </script>
            </span>
            , but not for
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-11-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.391ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -497.8 572.5 599" width="1.33ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-3C3" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-11" type="math/tex">
              \sigma
             </script>
            </span>
            . The network learns to output
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-12-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.391ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -497.8 572.5 599" width="1.33ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-3C3" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-12" type="math/tex">
              \sigma
             </script>
            </span>
            in an unsupervised fashion to show that it is certain about some landmarks, e.g., visible landmarks on the front of the face, and uncertain about others, e.g., landmarks hidden behind hair (see Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            ). This is achieved by training the network with a Gaussian negative log likelihood (GNLL) loss [
            <a aria-label="Reference 37" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR37" id="ref-link-section-d85345740e1609" title="Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for computer vision? In: Advances in Neural Information Processing Systems, vol. 30 (2017)">
             37
            </a>
            ]:
           </p>
           <div class="c-article-equation" id="Equ1">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              <span class="MathJax_Preview" style="">
               \begin{aligned} \text {Loss}(L) = \sum _{i=1}^{|L|} \lambda _i \Bigg (\underbrace{\log \left( \sigma _i^2\right) }_{\text {Loss}_{\sigma }} + \underbrace{\frac{\Vert \boldsymbol{\mathbf {\mu }}_{i} - \boldsymbol{\mathbf {\mu }}_{i}^\prime \Vert ^2}{2 \sigma _{i}^2}}_{\text {Loss}_{\mu }}\Bigg ) \end{aligned}
              </span>
              <div class="MathJax_SVG_Display MathJax_SVG_Processing">
               <span class="MathJax_SVG" id="MathJax-Element-13-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
               </span>
              </div>
              <script id="MathJax-Element-13" type="math/tex; mode=display">
               \begin{aligned} \text {Loss}(L) = \sum _{i=1}^{|L|} \lambda _i \Bigg (\underbrace{\log \left( \sigma _i^2\right) }_{\text {Loss}_{\sigma }} + \underbrace{\frac{\Vert \boldsymbol{\mathbf {\mu }}_{i} - \boldsymbol{\mathbf {\mu }}_{i}^\prime \Vert ^2}{2 \sigma _{i}^2}}_{\text {Loss}_{\mu }}\Bigg ) \end{aligned}
              </script>
             </span>
            </div>
            <div class="c-article-equation__number">
             (1)
            </div>
           </div>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \text {Loss}_{\sigma }
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-14-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-14" type="math/tex">
              \text {Loss}_{\sigma }
             </script>
            </span>
            penalizes the network for being too uncertain, and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \text {Loss}_{\mu }
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-15-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-15" type="math/tex">
              \text {Loss}_{\mu }
             </script>
            </span>
            penalizes the network for being inaccurate.
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \lambda _i
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-16-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-16" type="math/tex">
              \lambda _i
             </script>
            </span>
            is a per-landmark weight that focuses the loss on certain parts of the face. This is the only loss used during training.
           </p>
           <p>
            The probabilistic nature of our landmark predictions is important for accuracy. A network trained with the GNLL loss is more accurate than a network trained with L2 loss on positions only. Perhaps this is the result of the CNN being able to discount challenging landmarks (e.g., fully occluded ones), and spend more capacity on making precise predictions about visible landmarks.
           </p>
           <p>
            Landmarks are commonly predicted via heatmaps [
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d85345740e1894" title="Bulat, A., Sanchez, E., Tzimiropoulos, G.: Subpixel heatmap regression for facial landmark Localization. In: BMVC (2021)">
             11
            </a>
            ]. However, generating heatmaps is computationally expensive [
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d85345740e1897" title="Li, Y., Yang, S., Zhang, S., Wang, Z., Yang, W., Xia, S.T., Zhou, E.: Is 2d heatmap representation even necessary for human pose estimation? (2021)">
             41
            </a>
            ]; it would not be feasible to output over 700 heatmaps in real-time. Heatmaps also prevent us predicting landmarks outside image bounds. Instead, we keep things simple, and directly regress position and uncertainty using a traditional CNN. We are able to take any off-the-shelf architecture, and alter the final fully-connected layer to output three values per-landmark: two for position and one for uncertainty. Since this final layer represents a small percentage of total CNN compute, our method scales well with landmark quantity.
           </p>
           <p>
            <b>
             Training Details.
            </b>
            Landmark coordinates are normalized from [0,
            <i>
             S
            </i>
            ] to
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              [-1, 1]
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-17-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-17" type="math/tex">
              [-1, 1]
             </script>
            </span>
            , for a square image of size
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              S \! \times \! S
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-18-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-18" type="math/tex">
              S \! \times \! S
             </script>
            </span>
            . Rather than directly outputting
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-19-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-19" type="math/tex">
              \sigma
             </script>
            </span>
            , we predict
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \log \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-20-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-20" type="math/tex">
              \log \sigma
             </script>
            </span>
            , and take its exponential to ensure
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-21-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-21" type="math/tex">
              \sigma
             </script>
            </span>
            is positive. Using PyTorch [
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d85345740e2027" title="Paszke, A., et al.: Pytorch: an imperative style, high-performance deep learning library. In: NeurIPS (2019)">
             47
            </a>
            ], we train ResNet [
            <a aria-label="Reference 34" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR34" id="ref-link-section-d85345740e2030" title="He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)">
             34
            </a>
            ] and MobileNet V2 [
            <a aria-label="Reference 53" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR53" id="ref-link-section-d85345740e2033" title="Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenet V2: Inverted residuals and linear bottlenecks. In: CVPR (2018)">
             53
            </a>
            ] models from the timm [
            <a aria-label="Reference 69" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR69" id="ref-link-section-d85345740e2036" title="Wightman, R.: Pytorch image models (2019). 
                https://www.github.com/rwightman/pytorch-image-models
                
              , 
                https://doi-org.proxy.lib.ohio-state.edu/10.5281/zenodo.4414861
                
              ">
             69
            </a>
            ] library using AdamW [
            <a aria-label="Reference 45" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR45" id="ref-link-section-d85345740e2039" title="Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019)">
             45
            </a>
            ] with automatically determined learning rate [
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d85345740e2043" title="Falcon, W., et al.: Pytorch lightning 3(6) (2019). GitHub. Note. 
                https://github.com/PyTorchLightning/pytorch-lightning
                
              ">
             22
            </a>
            ]. We use data augmentation to help our synthetic data cross the domain gap [
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d85345740e2046" title="Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)">
             70
            </a>
            ].
           </p>
           <h3 class="c-article__sub-heading" id="Sec5">
            <span class="c-article-section__title-number">
             3.2
            </span>
            3D Model Fitting
           </h3>
           <p>
            Given probabilistic dense 2D landmarks
            <i>
             L
            </i>
            , our goal is to find optimal model parameters
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\Phi }}^*
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-22-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-22" type="math/tex">
              \boldsymbol{\mathbf {\Phi }}^*
             </script>
            </span>
            that minimize the following energy:
           </p>
           <div class="c-article-equation" id="Equ3">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              <span class="MathJax_Preview" style="">
               E(\boldsymbol{\mathbf {\Phi }}; L) = \underbrace{ E_{\text {landmarks} } }_{ \text {Data term} } + \underbrace{ E_{\text {identity}} + E_{\text {expression}} + E_{\text {joints}} + E_{\text {temporal}} + E_{\text {intersect}} }_{ \text {Regularizers} }
              </span>
              <div class="MathJax_SVG_Display MathJax_SVG_Processing">
               <span class="MathJax_SVG" id="MathJax-Element-23-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
               </span>
              </div>
              <script id="MathJax-Element-23" type="math/tex; mode=display">
               E(\boldsymbol{\mathbf {\Phi }}; L) = \underbrace{ E_{\text {landmarks} } }_{ \text {Data term} } + \underbrace{ E_{\text {identity}} + E_{\text {expression}} + E_{\text {joints}} + E_{\text {temporal}} + E_{\text {intersect}} }_{ \text {Regularizers} }
              </script>
             </span>
            </div>
           </div>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E_{\text {landmarks}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-24-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-24" type="math/tex">
              E_{\text {landmarks}}
             </script>
            </span>
            is the only term that encourages the 3D model to explain the observed 2D landmarks. The other terms use prior knowledge to regularize the fit.
           </p>
           <p>
            Part of the beauty of our approach is how naturally it scales to multiple images and cameras. In this section we present the general form of our method, suitable for
            <i>
             F
            </i>
            frames over
            <i>
             C
            </i>
            cameras, i.e., multi-view performance capture.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 6." id="figure-6">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/6" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig6_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 6" aria-describedby="Fig6" height="204" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig6_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc">
               <p>
                We implemented two versions of our approach: one for processing multi-view recordings offline (a), and one for real-time facial performance capture (b).
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 6" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure6 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/6" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             3D Face Model.
            </b>
            We use the face model described in [
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d85345740e2260" title="Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)">
             70
            </a>
            ], comprising
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              N\!=\!7,\!667
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-25-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-25" type="math/tex">
              N\!=\!7,\!667
             </script>
            </span>
            vertices and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              K\!=\!4
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-26-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-26" type="math/tex">
              K\!=\!4
             </script>
            </span>
            skeletal joints (the head, neck, and two eyes). Vertex positions are determined by the mesh-generating function
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}, \boldsymbol{\mathbf {\theta }}) \!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|+|\boldsymbol{\mathbf {\psi }}|+|\boldsymbol{\mathbf {\theta }}|}\!\rightarrow \!\mathbb {R}^{3N}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-27-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-27" type="math/tex">
              \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}, \boldsymbol{\mathbf {\theta }}) \!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|+|\boldsymbol{\mathbf {\psi }}|+|\boldsymbol{\mathbf {\theta }}|}\!\rightarrow \!\mathbb {R}^{3N}
             </script>
            </span>
            which takes parameters
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\beta }}\in \mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-28-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-28" type="math/tex">
              \boldsymbol{\mathbf {\beta }}\in \mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|}
             </script>
            </span>
            for identity,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\psi }}\in \mathbb {R}^{|\boldsymbol{\mathbf {\psi }}|}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-29-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-29" type="math/tex">
              \boldsymbol{\mathbf {\psi }}\in \mathbb {R}^{|\boldsymbol{\mathbf {\psi }}|}
             </script>
            </span>
            for expression, and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\theta }}\in \mathbb {R}^{3K + 3}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-30-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-30" type="math/tex">
              \boldsymbol{\mathbf {\theta }}\in \mathbb {R}^{3K + 3}
             </script>
            </span>
            for skeletal pose (including root joint translation).
           </p>
           <div class="c-article-equation" id="Equ4">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              <span class="MathJax_Preview" style="">
               \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}, \boldsymbol{\mathbf {\theta }}) = \mathcal {L}(\mathcal {T}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}), \boldsymbol{\mathbf {\theta }}, \mathcal {J}(\boldsymbol{\mathbf {\beta }}); \textbf{W})
              </span>
              <div class="MathJax_SVG_Display MathJax_SVG_Processing">
               <span class="MathJax_SVG" id="MathJax-Element-31-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
               </span>
              </div>
              <script id="MathJax-Element-31" type="math/tex; mode=display">
               \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}, \boldsymbol{\mathbf {\theta }}) = \mathcal {L}(\mathcal {T}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}), \boldsymbol{\mathbf {\theta }}, \mathcal {J}(\boldsymbol{\mathbf {\beta }}); \textbf{W})
              </script>
             </span>
            </div>
           </div>
           <p>
            where
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \mathcal {L}(\textbf{V}, \boldsymbol{\mathbf {\theta }}, \textbf{J}; \textbf{W})
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-32-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-32" type="math/tex">
              \mathcal {L}(\textbf{V}, \boldsymbol{\mathbf {\theta }}, \textbf{J}; \textbf{W})
             </script>
            </span>
            is a standard linear blend skinning (LBS) function [
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d85345740e2775" title="Lewis, J.P., Cordner, M., Fong, N.: Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation. In: SIGGRAPH (2000)">
             39
            </a>
            ] that rotates vertex positions
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \textbf{V}\in \mathbb {R}^{3N}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-33-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-33" type="math/tex">
              \textbf{V}\in \mathbb {R}^{3N}
             </script>
            </span>
            about joint locations
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \textbf{J}\in \mathbb {R}^{3K}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-34-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-34" type="math/tex">
              \textbf{J}\in \mathbb {R}^{3K}
             </script>
            </span>
            by local joint rotations in
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\theta }}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-35-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-35" type="math/tex">
              \boldsymbol{\mathbf {\theta }}
             </script>
            </span>
            , with per-vertex weights
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \textbf{W}\in \mathbb {R}^{K\times N}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-36-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-36" type="math/tex">
              \textbf{W}\in \mathbb {R}^{K\times N}
             </script>
            </span>
            . The face mesh and joint locations in the bind pose are determined by
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \mathcal {T}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }})\!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}| + |\boldsymbol{\mathbf {\psi }}|}\rightarrow \mathbb {R}^{3N}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-37-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-37" type="math/tex">
              \mathcal {T}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }})\!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}| + |\boldsymbol{\mathbf {\psi }}|}\rightarrow \mathbb {R}^{3N}
             </script>
            </span>
            and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \mathcal {J}(\boldsymbol{\mathbf {\beta }})\!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|}\rightarrow \mathbb {R}^{3K}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-38-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-38" type="math/tex">
              \mathcal {J}(\boldsymbol{\mathbf {\beta }})\!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|}\rightarrow \mathbb {R}^{3K}
             </script>
            </span>
            respectively. See Wood et al. [
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d85345740e3114" title="Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)">
             70
            </a>
            ] for more details.
           </p>
           <p>
            <b>
             Cameras
            </b>
            are described by a world-to-camera rigid transform
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{X}} \in \mathbb {R}^{3 \times 4} = \left[ \boldsymbol{\textbf{R}} | \boldsymbol{\textbf{T}} \right]
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-39-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-39" type="math/tex">
              \boldsymbol{\textbf{X}} \in \mathbb {R}^{3 \times 4} = \left[ \boldsymbol{\textbf{R}} | \boldsymbol{\textbf{T}} \right]
             </script>
            </span>
            comprising rotation and translation, and a pinhole camera projection matrix
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\Pi }} \in \mathbb {R}^{3 \times 3}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-40-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-40" type="math/tex">
              \boldsymbol{\mathbf {\Pi }} \in \mathbb {R}^{3 \times 3}
             </script>
            </span>
            . Thus, the image-space projection of the
            <i>
             j
            </i>
            <sup>
             th
            </sup>
            landmark in the
            <i>
             i
            </i>
            <sup>
             th
            </sup>
            camera is
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{x}}_{i,j} = \boldsymbol{\mathbf {\Pi }}_i \boldsymbol{\textbf{X}}_i \mathcal {M}_j
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-41-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-41" type="math/tex">
              \boldsymbol{\textbf{x}}_{i,j} = \boldsymbol{\mathbf {\Pi }}_i \boldsymbol{\textbf{X}}_i \mathcal {M}_j
             </script>
            </span>
            . In the monocular case,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{X}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-42-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-42" type="math/tex">
              \boldsymbol{\textbf{X}}
             </script>
            </span>
            can be ignored.
           </p>
           <p>
            <b>
             Parameters
            </b>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\Phi }}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-43-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-43" type="math/tex">
              \boldsymbol{\mathbf {\Phi }}
             </script>
            </span>
            are optimized to minimize
            <i>
             E
            </i>
            . The main parameters of interest control the face, but we also optimize camera parameters if they are unknown.
           </p>
           <div class="c-article-equation" id="Equ5">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              <span class="MathJax_Preview" style="">
               \begin{aligned} \boldsymbol{\mathbf {\Phi }} = \{ \underbrace{ \boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\Psi }}_{F \times |\boldsymbol{\mathbf {\psi }}|}, \boldsymbol{\mathbf {\Theta }}_{F \times |\boldsymbol{\mathbf {\theta }}|} }_{\text {Face}} ;\, \underbrace{ \boldsymbol{\textbf{R}}_{C \times 3}, \boldsymbol{\textbf{T}}_{C \times 3}, \boldsymbol{\textbf{f}}_{C} }_{\text {Camera(s)}} \} \end{aligned}
              </span>
              <div class="MathJax_SVG_Display MathJax_SVG_Processing">
               <span class="MathJax_SVG" id="MathJax-Element-44-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
               </span>
              </div>
              <script id="MathJax-Element-44" type="math/tex; mode=display">
               \begin{aligned} \boldsymbol{\mathbf {\Phi }} = \{ \underbrace{ \boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\Psi }}_{F \times |\boldsymbol{\mathbf {\psi }}|}, \boldsymbol{\mathbf {\Theta }}_{F \times |\boldsymbol{\mathbf {\theta }}|} }_{\text {Face}} ;\, \underbrace{ \boldsymbol{\textbf{R}}_{C \times 3}, \boldsymbol{\textbf{T}}_{C \times 3}, \boldsymbol{\textbf{f}}_{C} }_{\text {Camera(s)}} \} \end{aligned}
              </script>
             </span>
            </div>
           </div>
           <p>
            Facial identity
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\beta }}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-45-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-45" type="math/tex">
              \boldsymbol{\mathbf {\beta }}
             </script>
            </span>
            is shared over a sequence of
            <i>
             F
            </i>
            frames, but expression
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\Psi }}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-46-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-46" type="math/tex">
              \boldsymbol{\mathbf {\Psi }}
             </script>
            </span>
            and pose
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\Theta }}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-47-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-47" type="math/tex">
              \boldsymbol{\mathbf {\Theta }}
             </script>
            </span>
            vary per frame. For each of our
            <i>
             C
            </i>
            cameras we have six degrees of freedom for rotation
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{R}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-48-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-48" type="math/tex">
              \boldsymbol{\textbf{R}}
             </script>
            </span>
            and translation
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{T}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-49-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-49" type="math/tex">
              \boldsymbol{\textbf{T}}
             </script>
            </span>
            , and a single focal length parameter
            <i>
             f
            </i>
            . In the monocular case, we only optimize focal length.
           </p>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E_{\text {landmarks}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-50-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-50" type="math/tex">
              E_{\text {landmarks}}
             </script>
            </span>
            encourages the 3D model to explain the predicted 2D landmarks:
           </p>
           <div class="c-article-equation" id="Equ2">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              <span class="MathJax_Preview" style="">
               \begin{aligned} E_{\text {landmarks}} = \sum _{i, j, k}^{F, C, |L|} \frac{\Vert \boldsymbol{\textbf{x}}_{ijk} - \boldsymbol{\mathbf {\mu }}_{ijk} \Vert ^2}{2 \sigma _{ijk}^2} \end{aligned}
              </span>
              <div class="MathJax_SVG_Display MathJax_SVG_Processing">
               <span class="MathJax_SVG" id="MathJax-Element-51-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
               </span>
              </div>
              <script id="MathJax-Element-51" type="math/tex; mode=display">
               \begin{aligned} E_{\text {landmarks}} = \sum _{i, j, k}^{F, C, |L|} \frac{\Vert \boldsymbol{\textbf{x}}_{ijk} - \boldsymbol{\mathbf {\mu }}_{ijk} \Vert ^2}{2 \sigma _{ijk}^2} \end{aligned}
              </script>
             </span>
            </div>
            <div class="c-article-equation__number">
             (2)
            </div>
           </div>
           <p>
            where, for the
            <i>
             k
            </i>
            <sup>
             th
            </sup>
            landmark seen by the
            <i>
             j
            </i>
            <sup>
             th
            </sup>
            camera in the
            <i>
             i
            </i>
            <sup>
             th
            </sup>
            frame,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              [\boldsymbol{\mathbf {\mu }}_{ijk}, \sigma _{ijk}]
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-52-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-52" type="math/tex">
              [\boldsymbol{\mathbf {\mu }}_{ijk}, \sigma _{ijk}]
             </script>
            </span>
            is the 2D location and uncertainty predicted by our dense landmark CNN, and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{x}}_{ijk} = \boldsymbol{\mathbf {\Pi }}_j \boldsymbol{\textbf{X}}_j \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}_i, \boldsymbol{\mathbf {\theta }}_i)_k
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-53-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-53" type="math/tex">
              \boldsymbol{\textbf{x}}_{ijk} = \boldsymbol{\mathbf {\Pi }}_j \boldsymbol{\textbf{X}}_j \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}_i, \boldsymbol{\mathbf {\theta }}_i)_k
             </script>
            </span>
            is the 2D projection of that landmark on our 3D model. The similarity of Eq.
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ2">
             2
            </a>
            to
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \text {Loss}_{\mu }
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-54-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-54" type="math/tex">
              \text {Loss}_{\mu }
             </script>
            </span>
            in Eq.
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            is no accident: treating landmarks as 2D random variables during both prediction and model-fitting allows our approach to elegantly handle uncertainty, taking advantage of landmarks the CNN is confident in, and discounting those it is uncertain about.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 7." id="figure-7">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/7" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig7_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 7" aria-describedby="Fig7" height="205" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig7_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc">
               <p>
                We encourage the optimizer to avoid face mesh self-intersections by penalizing skin vertices that enter the convex hulls of the eyeballs or teeth parts.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 7" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure7 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/7" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E_{\text {identity}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-55-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-55" type="math/tex">
              E_{\text {identity}}
             </script>
            </span>
            penalizes unlikely face shape by maximizing the relative log-likelihood of shape parameters
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\beta }}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-56-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-56" type="math/tex">
              \boldsymbol{\mathbf {\beta }}
             </script>
            </span>
            under a multivariate Gaussian Mixture Model (GMM) of
            <i>
             G
            </i>
            components fit to a library of 3D head scans [
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d85345740e4109" title="Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)">
             70
            </a>
            ].
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E_{\text {identity}} = -\log \left( p(\boldsymbol{\mathbf {\beta }})\right)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-57-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-57" type="math/tex">
              E_{\text {identity}} = -\log \left( p(\boldsymbol{\mathbf {\beta }})\right)
             </script>
            </span>
            where
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              p(\boldsymbol{\mathbf {\beta }}) = \sum _{i=1}^{G} \gamma _i\; \mathcal {N}\! \left( \boldsymbol{\mathbf {\beta }} | \boldsymbol{\mathbf {\nu }}_i,\, \boldsymbol{\mathbf {\Sigma }}_i\right)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-58-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-58" type="math/tex">
              p(\boldsymbol{\mathbf {\beta }}) = \sum _{i=1}^{G} \gamma _i\; \mathcal {N}\! \left( \boldsymbol{\mathbf {\beta }} | \boldsymbol{\mathbf {\nu }}_i,\, \boldsymbol{\mathbf {\Sigma }}_i\right)
             </script>
            </span>
            .
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\nu }}_i
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-59-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-59" type="math/tex">
              \boldsymbol{\mathbf {\nu }}_i
             </script>
            </span>
            and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\Sigma }}_i
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-60-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-60" type="math/tex">
              \boldsymbol{\mathbf {\Sigma }}_i
             </script>
            </span>
            are the mean and covariance matrix of the
            <i>
             i
            </i>
            <sup>
             th
            </sup>
            component, and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \gamma _i
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-61-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-61" type="math/tex">
              \gamma _i
             </script>
            </span>
            is the weight of that component.
           </p>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E_{\text {expression}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-62-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-62" type="math/tex">
              E_{\text {expression}}
             </script>
            </span>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              = \Vert \boldsymbol{\mathbf {\psi }} \Vert ^2
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-63-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-63" type="math/tex">
              = \Vert \boldsymbol{\mathbf {\psi }} \Vert ^2
             </script>
            </span>
            and
           </p>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E_{\text {joints}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-64-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-64" type="math/tex">
              E_{\text {joints}}
             </script>
            </span>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              = \Vert \boldsymbol{\mathbf {\theta }}_{i:i\in [2,K]} \Vert ^2
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-65-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-65" type="math/tex">
              = \Vert \boldsymbol{\mathbf {\theta }}_{i:i\in [2,K]} \Vert ^2
             </script>
            </span>
            encourage the optimizer to explain the data with as little expression and joint rotation as possible. We do not penalize global translation or rotation by ignoring the root joint
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\theta }}_1
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-66-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-66" type="math/tex">
              \boldsymbol{\mathbf {\theta }}_1
             </script>
            </span>
            .
           </p>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E_{\text {temporal}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-67-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-67" type="math/tex">
              E_{\text {temporal}}
             </script>
            </span>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              = \sum _{i=2, j, k}^{F, C, |L|} \Vert \boldsymbol{\textbf{x}}_{i,j,k} - \boldsymbol{\textbf{x}}_{i-1,j,k} \Vert ^2
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-68-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-68" type="math/tex">
              = \sum _{i=2, j, k}^{F, C, |L|} \Vert \boldsymbol{\textbf{x}}_{i,j,k} - \boldsymbol{\textbf{x}}_{i-1,j,k} \Vert ^2
             </script>
            </span>
            reduces jitter by encouraging face mesh vertices
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{x}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-69-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-69" type="math/tex">
              \boldsymbol{\textbf{x}}
             </script>
            </span>
            to remain still between neighboring frames
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              i-1
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-70-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-70" type="math/tex">
              i-1
             </script>
            </span>
            and
            <i>
             i
            </i>
            .
           </p>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E_{\text {intersect}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-71-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-71" type="math/tex">
              E_{\text {intersect}}
             </script>
            </span>
            encourages the optimizer to find solutions without intersections between the skin and eyeballs or teeth (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            ). Please refer to the supplementary material for further details.
           </p>
           <h3 class="c-article__sub-heading" id="Sec6">
            <span class="c-article-section__title-number">
             3.3
            </span>
            Implementation
           </h3>
           <p>
            We implemented two versions of our system: one for processing multi-camera recordings offline, and one for real-time facial performance capture.
           </p>
           <p>
            Our
            <b>
             offline
            </b>
            system produces the best quality results without constraints on compute. We predict 703 landmarks with a ResNet 101 [
            <a aria-label="Reference 34" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR34" id="ref-link-section-d85345740e4775" title="He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)">
             34
            </a>
            ]. To extract a facial Region-of-Interest (ROI) from an image we run a full-head probabilistic landmark CNN on multi-scale sliding windows, and select the window with the lowest uncertainty. When fitting our 3DMM, we use PyTorch [
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d85345740e4778" title="Paszke, A., et al.: Pytorch: an imperative style, high-performance deep learning library. In: NeurIPS (2019)">
             47
            </a>
            ] to minimize
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E(\boldsymbol{\mathbf {\Phi }})
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-72-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-72" type="math/tex">
              E(\boldsymbol{\mathbf {\Phi }})
             </script>
            </span>
            with L-BFGS [
            <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR42" id="ref-link-section-d85345740e4813" title="Liu, D.C., Nocedal, J.: On the limited memory BFGS method for large scale optimization. Math. Program. 45(1), 503–528 (1989). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/BF01589116
                
              ">
             42
            </a>
            ], optimizing all parameters across all frames simultaneously.
           </p>
           <p>
            For our
            <b>
             real-time
            </b>
            system, we trained a lightweight dense landmark model with MobileNet V2 architecture [
            <a aria-label="Reference 53" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR53" id="ref-link-section-d85345740e4822" title="Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenet V2: Inverted residuals and linear bottlenecks. In: CVPR (2018)">
             53
            </a>
            ]. To compensate for a reduction in network capacity, we predict 320 landmarks rather than 703, and modify the ROI strategy: aligning the face so it appears upright with the eyes a fixed distance apart. This makes the CNN’s job easier for frontal faces at the expense of profile ones.
           </p>
           <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec7">
            <span class="c-article-section__title-number">
             3.3.1
            </span>
            Real-Time Model Fitting.
           </h4>
           <p>
            We use the Levenberg-Marquardt algorithm to optimize our model-fitting energy. Camera and identity parameters are only fit occasionally. For the majority of frames we fit pose and expression parameters only. We rewrite the energy
            <i>
             E
            </i>
            in terms of the vector of residuals,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{r}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-73-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-73" type="math/tex">
              \boldsymbol{\textbf{r}}
             </script>
            </span>
            , as
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              E(\boldsymbol{\mathbf {\Phi }}) = \Vert \boldsymbol{\mathbf {r(\boldsymbol{\mathbf {\Phi }})}}\Vert ^2 = \sum _i r_i(\boldsymbol{\mathbf {\Phi }})^2
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-74-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-74" type="math/tex">
              E(\boldsymbol{\mathbf {\Phi }}) = \Vert \boldsymbol{\mathbf {r(\boldsymbol{\mathbf {\Phi }})}}\Vert ^2 = \sum _i r_i(\boldsymbol{\mathbf {\Phi }})^2
             </script>
            </span>
            . Then at each iteration
            <i>
             k
            </i>
            of our optimization, we can compute
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{r}}(\boldsymbol{\mathbf {\Phi }}_k)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-75-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-75" type="math/tex">
              \boldsymbol{\textbf{r}}(\boldsymbol{\mathbf {\Phi }}_k)
             </script>
            </span>
            and the Jacobian,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              J(\boldsymbol{\mathbf {\Phi }}_k) = \frac{\partial \boldsymbol{\textbf{r}}(\boldsymbol{\mathbf {\Phi }})}{\partial \boldsymbol{\mathbf {\Phi }}} |^{\boldsymbol{\mathbf {\Phi }}=\boldsymbol{\mathbf {\Phi }}_k}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-76-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-76" type="math/tex">
              J(\boldsymbol{\mathbf {\Phi }}_k) = \frac{\partial \boldsymbol{\textbf{r}}(\boldsymbol{\mathbf {\Phi }})}{\partial \boldsymbol{\mathbf {\Phi }}} |^{\boldsymbol{\mathbf {\Phi }}=\boldsymbol{\mathbf {\Phi }}_k}
             </script>
            </span>
            , and use these to solve the symmetric, positive-semi-definite linear system,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              (J^T J + \lambda \text {diag}(J^T J))\boldsymbol{\mathbf {\delta }}_k = -J^T\boldsymbol{\textbf{r}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-77-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-77" type="math/tex">
              (J^T J + \lambda \text {diag}(J^T J))\boldsymbol{\mathbf {\delta }}_k = -J^T\boldsymbol{\textbf{r}}
             </script>
            </span>
            via Cholesky decomposition. We then apply the update rule,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\Phi }}_{k+1} = \boldsymbol{\mathbf {\Phi }}_k + \boldsymbol{\mathbf {\delta }}_k
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-78-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-78" type="math/tex">
              \boldsymbol{\mathbf {\Phi }}_{k+1} = \boldsymbol{\mathbf {\Phi }}_k + \boldsymbol{\mathbf {\delta }}_k
             </script>
            </span>
            .
           </p>
           <p>
            In practice we do not actually form the residual vector
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\textbf{r}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-79-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-79" type="math/tex">
              \boldsymbol{\textbf{r}}
             </script>
            </span>
            nor the Jacobian matrix
            <i>
             J
            </i>
            . Instead, for performance reasons, we directly compute the quantities
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              J^T J
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-80-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-80" type="math/tex">
              J^T J
             </script>
            </span>
            and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              J^T \boldsymbol{\textbf{r}}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-81-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-81" type="math/tex">
              J^T \boldsymbol{\textbf{r}}
             </script>
            </span>
            as we visit each term
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              r_i(\boldsymbol{\mathbf {\Phi }}_k)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-82-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-82" type="math/tex">
              r_i(\boldsymbol{\mathbf {\Phi }}_k)
             </script>
            </span>
            of the energy. Most of the computational cost is incurred in evaluating these products for the landmark data term, as expected. However, the Jacobian of landmark term residuals is not fully dense. Each individual landmark depends on its own subset of expression parameters, and is invariant to other expression parameters. We performed a static analysis of the sparsity of each landmark term with respect to parameters,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \partial r_i / \partial \Phi _j
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-83-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-83" type="math/tex">
              \partial r_i / \partial \Phi _j
             </script>
            </span>
            , and we use this set of
            <i>
             i
            </i>
            ,
            <i>
             j
            </i>
            indices to reduce the cost of our outer products from
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              O(|\boldsymbol{\mathbf {\Phi }}|^2)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-84-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-84" type="math/tex">
              O(|\boldsymbol{\mathbf {\Phi }}|^2)
             </script>
            </span>
            to
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              O(m_i^2)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-85-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-85" type="math/tex">
              O(m_i^2)
             </script>
            </span>
            , where
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              m_i
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-86-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-86" type="math/tex">
              m_i
             </script>
            </span>
            is the sparsified dimensionality of
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \partial r_i / \partial \boldsymbol{\mathbf {\Phi }}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-87-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-87" type="math/tex">
              \partial r_i / \partial \boldsymbol{\mathbf {\Phi }}
             </script>
            </span>
            . We further enhance the sparsity by ignoring any components of the Jacobian with an absolute value below a certain empirically-determined threshold.
           </p>
           <p>
            By exploiting sparsity in this way, the landmark term residuals and their derivatives become very cheap to evaluate. This formulation avoids the correspondence problem usually seen with depth images [
            <a aria-label="Reference 58" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR58" id="ref-link-section-d85345740e5592" title="Taylor, J., et al.: Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences. ACM Trans. Graph. (ToG) 35(4), 1–12 (2016)">
             58
            </a>
            ], which requires a more expensive optimization. In addition, adding more landmarks does not significantly increase the cost of optimization. It therefore becomes possible to implement a very detailed and well-regularized fitter with a relatively small compute burden, simply by adding a sufficient number of landmarks. The cost of the Cholesky solve for the update
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \boldsymbol{\mathbf {\delta }}_k
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-88-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-88" type="math/tex">
              \boldsymbol{\mathbf {\delta }}_k
             </script>
            </span>
            is independent of the number of landmarks.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Evaluation">
         <div class="c-article-section" id="Sec8-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">
           <span class="c-article-section__title-number">
            4
           </span>
           Evaluation
          </h2>
          <div class="c-article-section__content" id="Sec8-content">
           <h3 class="c-article__sub-heading" id="Sec9">
            <span class="c-article-section__title-number">
             4.1
            </span>
            Landmark Accuracy
           </h3>
           <p>
            We measure the accuracy of a ResNet 101 dense landmark model on the
            <b>
             300W
            </b>
            [
            <a aria-label="Reference 52" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR52" id="ref-link-section-d85345740e5638" title="Sagonas, C., Antonakos, E., Tzimiropoulos, G., Zafeiriou, S., Pantic, M.: 300 faces in-the-wild challenge: database and results. Image Vis. Computi. (IMAVIS) 47, 3–18 (2016)">
             52
            </a>
            ] dataset. For benchmark purposes only, we employ label translation [
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d85345740e5641" title="Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)">
             70
            </a>
            ] to deal with systematic inconsistencies between our 703 predicted dense landmarks and the 68 sparse landmarks labelled as ground truth (see Table
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig8">
             8
            </a>
            ). While previous work [
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d85345740e5647" title="Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)">
             70
            </a>
            ] used label translation to evaluate a synthetically-trained sparse landmark predictor, we use it to evaluate a dense landmark predictor.
           </p>
           <p>
            We use the standard normalized mean error (NME) and failure rate (FR
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              _{10\%}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-89-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-89" type="math/tex">
              _{10\%}
             </script>
            </span>
            ) error metrics [
            <a aria-label="Reference 52" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR52" id="ref-link-section-d85345740e5680" title="Sagonas, C., Antonakos, E., Tzimiropoulos, G., Zafeiriou, S., Pantic, M.: 300 faces in-the-wild challenge: database and results. Image Vis. Computi. (IMAVIS) 47, 3–18 (2016)">
             52
            </a>
            ]. Our model’s results in Table
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig8">
             8
            </a>
            are competitive with the state of the art, despite being trained with synthetic data alone. Note: these results provide a conservative estimate of our method’s accuracy as the translation network may introduce error, especially for rarely seen expressions.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 8." id="figure-8">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig8">
               Fig. 8.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/8" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig8_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 8" aria-describedby="Fig8" height="211" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig8_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc">
               <p>
                Left: results on 300W dataset, lower is better. Note competitive performance of our model (despite being evaluated across-dataset) and importance of GNLL loss. Right: sample predictions (top row) with label-translated results (bottom row).
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 8" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure8 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/8" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             Ablation Study.
            </b>
            We measured the importance of predicting each landmark as a random variable rather than as a 2D coordinate. We trained two landmark prediction models, one with our proposed GNLL loss (Eq.
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            ), and one with a simpler L2 loss on landmark coordinate only. Results in Table
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig8">
             8
            </a>
            confirm that including uncertainty in landmark regression results in better accuracy.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 9." id="figure-9">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig9">
               Fig. 9.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/9" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig9_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 9" aria-describedby="Fig9" height="371" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig9_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc">
               <p>
                We compare our real-time landmark CNN (MobileNet V2) with MediaPipe Attention Mesh [
                <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d85345740e5725" title="Grishchenko, I., Ablavatski, A., Kartynnik, Y., Raveendran, K., Grundmann, M.: Attention mesh: high-fidelity face mesh prediction in real-time. In: CVPR Workshops (2020)">
                 28
                </a>
                ], a publicly available method for dense landmark prediction. Our approach is more robust to challenging expressions and illumination.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 9" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure9 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/9" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             Qualitative Comparisons
            </b>
            are shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig9">
             9
            </a>
            between our real-time dense landmark model (MobileNet V2) and MediaPipe Attention Mesh [
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d85345740e5745" title="Grishchenko, I., Ablavatski, A., Kartynnik, Y., Raveendran, K., Grundmann, M.: Attention mesh: high-fidelity face mesh prediction in real-time. In: CVPR Workshops (2020)">
             28
            </a>
            ], a publicly available dense landmark method designed for mobile devices. Our method is more robust, perhaps due to the consistency and diversity of our synthetic training data. See the supplementary material for additional qualitative results, including landmark predictions on the Challenging subset of 300W.
           </p>
           <h3 class="c-article__sub-heading" id="Sec10">
            <span class="c-article-section__title-number">
             4.2
            </span>
            3D Face Reconstruction
           </h3>
           <p>
            Quantitatively, we compare our offline approach with recent methods on two benchmarks: the NoW Challenge [
            <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d85345740e5756" title="Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3d face shape and expression from an image without 3d supervision. In: CVPR (2019)">
             54
            </a>
            ] and the MICC dataset [
            <a aria-label="Reference 2" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR2" id="ref-link-section-d85345740e5759" title="Bagdanov, A.D., Del Bimbo, A., Masi, I.: The Florence 2D/3D hybrid face dataset. In: Workshop on Human Gesture and Behavior Understanding. ACM (2011)">
             2
            </a>
            ].
           </p>
           <p>
            <b>
             The Now Challenge
            </b>
            [
            <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d85345740e5767" title="Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3d face shape and expression from an image without 3d supervision. In: CVPR (2019)">
             54
            </a>
            ] provides a standard evaluation protocol for measuring the accuracy and robustness of 3D face reconstruction in the wild. It consists of 2054 face images of 100 subjects along with a 3D head scan for each subject which serves as ground truth. We undertake the challenge in two ways:
            <i>
             single view
            </i>
            , where we fit our face model to each image separately, and
            <i>
             multi-view
            </i>
            , where we fit a per-subject face model to all image of a particular subject. As shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig10">
             10
            </a>
            , we achieve state of the art results.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 10." id="figure-10">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig10">
               Fig. 10.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/10" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig10_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 10" aria-describedby="Fig10" height="247" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig10_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc">
               <p>
                Results for the now challenge [
                <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d85345740e5789" title="Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3d face shape and expression from an image without 3d supervision. In: CVPR (2019)">
                 54
                </a>
                ]. We outperform the state of the art on both single- and multi-view 3D face reconstruction.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 10" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure10 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/10" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <b>
             The MICC Dataset
            </b>
            [
            <a aria-label="Reference 2" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR2" id="ref-link-section-d85345740e5806" title="Bagdanov, A.D., Del Bimbo, A., Masi, I.: The Florence 2D/3D hybrid face dataset. In: Workshop on Human Gesture and Behavior Understanding. ACM (2011)">
             2
            </a>
            ] consists of 3D face scans and videos of 53 subjects. The videos were recorded in three environments: a “cooperative” laboratory environment, an indoor environment, and an outdoor environment. We follow Deng et al. [
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d85345740e5809" title="Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face reconstruction with weakly-supervised learning: from single image to image set. In: CVPR Workshops (2019)">
             17
            </a>
            ], and evaluate our method in two ways:
            <i>
             single view
            </i>
            , where we estimate one face shape per frame in a video, and average the resulting face meshes, and
            <i>
             multi-view
            </i>
            , where we fit a single face model to all frames in a video jointly. As shown in Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab1">
             1
            </a>
            , we achieve state of the art results.
           </p>
           <p>
            Note that many previous methods are incapable of aggregating face shape information across multiple views. The fact ours can benefit from multiple views highlights the flexibility of our hybrid model-fitting approach.
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab1">
               Table 1. Results on the MICC dataset [
               <a aria-label="Reference 2" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR2" id="ref-link-section-d85345740e5834" title="Bagdanov, A.D., Del Bimbo, A., Masi, I.: The Florence 2D/3D hybrid face dataset. In: Workshop on Human Gesture and Behavior Understanding. ACM (2011)">
                2
               </a>
               ], following the single and multi-frame evaluation protocol of Deng et al. [
               <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d85345740e5837" title="Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face reconstruction with weakly-supervised learning: from single image to image set. In: CVPR Workshops (2019)">
                17
               </a>
               ]. We achieve state-of-the-art results.
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/tables/1" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec11">
            <span class="c-article-section__title-number">
             4.2.1
            </span>
            Ablation Studies.
           </h4>
           <p>
            We conducted an experiment to measure the importance of landmark quantity for 3D face reconstruction. We trained three landmark CNNs, predicting 703, 320, and 68 landmarks respectively, and used these on the NoW Challenge (validation set). As shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig11">
             11
            </a>
            , fitting with more landmarks results in more accurate 3D face reconstruction.
           </p>
           <p>
            In addition, we investigated the importance of using landmark uncertainty
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-90-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-90" type="math/tex">
              \sigma
             </script>
            </span>
            in model fitting. We fit our model to 703 landmark predictions on the NoW validation set, but using fixed rather than predicted
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-91-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-91" type="math/tex">
              \sigma
             </script>
            </span>
            . Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig11">
             11
            </a>
            (bottom row of table) shows that fitting without
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-92-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-92" type="math/tex">
              \sigma
             </script>
            </span>
            leads to worse results.
           </p>
           <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec12">
            <span class="c-article-section__title-number">
             4.2.2
            </span>
            Qualitative Comparisons
           </h4>
           <p>
            between our work and several publicly available methods [
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d85345740e5925" title="Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face reconstruction with weakly-supervised learning: from single image to image set. In: CVPR Workshops (2019)">
             17
            </a>
            ,
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d85345740e5928" title="Feng, Y., Feng, H., Black, M.J., Bolkart, T.: Learning an animatable detailed 3D face model from in-the-wild images. ACM Trans. Graph. (ToG) 40(4), 1–13 (2021)">
             23
            </a>
            ,
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d85345740e5931" title="Guo, J., Zhu, X., Yang, Y., Yang, F., Lei, Z., Li, S.Z.: Towards fast, accurate and stable 3d dense face alignment. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12364, pp. 152–168. Springer, Cham (2020). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58529-7_10
                
              ">
             30
            </a>
            ,
            <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d85345740e5934" title="Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3d face shape and expression from an image without 3d supervision. In: CVPR (2019)">
             54
            </a>
            ,
            <a aria-label="Reference 57" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR57" id="ref-link-section-d85345740e5937" title="Shang, J.: Self-supervised monocular 3d face reconstruction by occlusion-aware multi-view geometry consistency. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12360, pp. 53–70. Springer, Cham (2020). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58555-6_4
                
              ">
             57
            </a>
            ] can be found in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig13">
             13
            </a>
            .
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 11." id="figure-11">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig11">
               Fig. 11.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/11" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig11_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 11" aria-describedby="Fig11" height="128" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig11_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc">
               <p>
                Ablation studies on the NoW [
                <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d85345740e5954" title="Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3d face shape and expression from an image without 3d supervision. In: CVPR (2019)">
                 54
                </a>
                ] validation set confirm that denser is better: model fitting with more landmarks leads to more accurate results. In addition, we see that fitting without using
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                  \sigma
                 </span>
                 <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-93-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                 </span>
                 <script id="MathJax-Element-93" type="math/tex">
                  \sigma
                 </script>
                </span>
                leads to worse results.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 11" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure11 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/11" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h3 class="c-article__sub-heading" id="Sec13">
            <span class="c-article-section__title-number">
             4.3
            </span>
            Facial Performance Capture
           </h3>
           <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec14">
            <span class="c-article-section__title-number">
             4.3.1
            </span>
            Multi-view.
           </h4>
           <p>
            Good synthetic training data requires a database of facial expression parameters from which to sample. We acquired such a database by conducting markerless facial performance capture for 108 subjects. We recorded each subject in our 17-camera studio, and processed each recording with our offline multi-view model fitter. For a 520 frame sequence it takes 3 min to predict dense landmarks for all images, and a further 9 min to optimize face model parameters. See Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig12">
             12
            </a>
            for some of the 125,000 frames of expression data captured with our system. As the system which is used to create the database is then subsequently re-trained with it, we produced several databases in this manner until no further improvement was seen. We do not reconstruct faces in fine detail like previous multi-view stereo approaches [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d85345740e5999" title="Beeler, T., et al.: High-quality passive facial performance capture using anchor frames. In: ACM Transactions on Graphics (2011)">
             5
            </a>
            ,
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR9" id="ref-link-section-d85345740e6002" title="Bradley, D., Heidrich, W., Popa, T., Sheffer, A.: High resolution passive facial performance capture. In: ACM Transactions on Graphics, vol. 29, no. 4 (2010)">
             9
            </a>
            ,
            <a aria-label="Reference 49" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR49" id="ref-link-section-d85345740e6005" title="Popa, T., South-Dickinson, I., Bradley, D., Sheffer, A., Heidrich, W.: Globally consistent space-time reconstruction. Comput. Graph. Forum 29(5), 1633–1642 (2010)">
             49
            </a>
            ]. However, while previous work can track a detailed 3D mesh over a performance, our approach reconstructs the performance with richer semantics: identity and expression parameters for our generative model. In many cases it is sufficient to reconstruct the low-frequency shape of the face accurately, without fine details.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 12." id="figure-12">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig12">
               Fig. 12.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/12" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig12_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 12" aria-describedby="Fig12" height="171" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig12_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc">
               <p>
                We demonstrate the robustness and reliability of our method by using it to collect a massive database of 125,000 facial expressions, fully automatically.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 12" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure12 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/12" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec15">
            <span class="c-article-section__title-number">
             4.3.2
            </span>
            Real-Time Monocular.
           </h4>
           <p>
            See the last two columns of Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig13">
             13
            </a>
            for a comparison between our offline and real-time systems for monocular 3D model-fitting. While our offline system produces the best possible results by using a large CNN and optimizing over all frames simultaneously, our real-time system can still produce accurate and expressive results fitting frame-to-frame. Please refer to the supplementary material for more results. Running on a single CPU thread (i5-11600K), our real-time system spends 6.5 ms processing a frame (150FPS), of which 4.1 ms is spent predicting dense landmarks and 2.3 ms is spent fitting our face model.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 13." id="figure-13">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig13">
               Fig. 13.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/13" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig13_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 13" aria-describedby="Fig13" height="441" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig13_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc">
               <p>
                Compared to previous recent monocular 3D face reconstruction methods, ours better captures gaze, expressions like winks and sneers, and subtleties of facial identity. In addition, our method can run in real time with only a minor loss of fidelity.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 13" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure13 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/13" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 14." id="figure-14">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig14">
               Fig. 14.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19778-9_10/figures/14" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig14_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 14" aria-describedby="Fig14" height="104" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig14_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc">
               <p>
                Bad landmarks result in bad fits, and we are incapable of tracking the tongue.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 14" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure14 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19778-9_10/figures/14" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
          </div>
         </div>
        </section>
        <section data-title="Limitations and Future Work">
         <div class="c-article-section" id="Sec16-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">
           <span class="c-article-section__title-number">
            5
           </span>
           Limitations and Future Work
          </h2>
          <div class="c-article-section__content" id="Sec16-content">
           <p>
            Our method depends entirely on accurate landmarks. As shown in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig14">
             14
            </a>
            , if landmarks are poorly predicted, the resulting model fit suffers. We plan to address this by improving our synthetic training data. Additionally, since our model does not include tongue articulation we cannot recover tongue movement.
           </p>
           <p>
            Heatmaps have dominated landmark prediction for some time [
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d85345740e6089" title="Bulat, A., Sanchez, E., Tzimiropoulos, G.: Subpixel heatmap regression for facial landmark Localization. In: BMVC (2021)">
             11
            </a>
            ,
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d85345740e6092" title="Bulat, A., Tzimiropoulos, G.: How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3D facial landmarks). In: ICCV (2017)">
             12
            </a>
            ]. We were pleasantly surprised to find that directly regressing 2D landmark coordinates with unspecialized architectures works well and eliminates the need for computationally-costly heatmap generation. In addition, we were surprised that predicting
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \sigma
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-94-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
             </span>
             <script id="MathJax-Element-94" type="math/tex">
              \sigma
             </script>
            </span>
            helps accuracy. We look forward to further investigating direct probabilistic landmark regression as an alternative to heatmaps in future work.
           </p>
           <p>
            In conclusion, we have demonstrated that dense landmarks are an ideal signal for 3D face reconstruction. Quantitative and qualitative evaluations have shown that our approach outperforms those previous by a significant margin, and excels at multi-view and monocular facial performance capture. Finally, our approach is highly efficient, and runs at over 150FPS on a single CPU thread.
           </p>
          </div>
         </div>
        </section>
       </div>
       <div id="MagazineFulltextChapterBodySuffix">
        <section aria-labelledby="Bib1" data-title="References">
         <div class="c-article-section" id="Bib1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">
           <span class="c-article-section__title-number">
           </span>
           References
          </h2>
          <div class="c-article-section__content" id="Bib1-content">
           <div data-container-section="references">
            <ol class="c-article-references" data-track-component="outbound reference">
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1.">
              <p class="c-article-references__text" id="ref-CR1">
               Alp Güler, R., Trigeorgis, G., Antonakos, E., Snape, P., Zafeiriou, S., Kokkinos, I.: DenseReg: fully convolutional dense shape regression in-the-wild. In: CVPR (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR1-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Alp%20G%C3%BCler%2C%20R.%2C%20Trigeorgis%2C%20G.%2C%20Antonakos%2C%20E.%2C%20Snape%2C%20P.%2C%20Zafeiriou%2C%20S.%2C%20Kokkinos%2C%20I.%3A%20DenseReg%3A%20fully%20convolutional%20dense%20shape%20regression%20in-the-wild.%20In%3A%20CVPR%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2.">
              <p class="c-article-references__text" id="ref-CR2">
               Bagdanov, A.D., Del Bimbo, A., Masi, I.: The Florence 2D/3D hybrid face dataset. In: Workshop on Human Gesture and Behavior Understanding. ACM (2011)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR2-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bagdanov%2C%20A.D.%2C%20Del%20Bimbo%2C%20A.%2C%20Masi%2C%20I.%3A%20The%20Florence%202D%2F3D%20hybrid%20face%20dataset.%20In%3A%20Workshop%20on%20Human%20Gesture%20and%20Behavior%20Understanding.%20ACM%20%282011%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3.">
              <p class="c-article-references__text" id="ref-CR3">
               Bai, Z., Cui, Z., Liu, X., Tan, P.: Riggable 3D face reconstruction via in-network optimization. In: CVPR (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR3-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bai%2C%20Z.%2C%20Cui%2C%20Z.%2C%20Liu%2C%20X.%2C%20Tan%2C%20P.%3A%20Riggable%203D%20face%20reconstruction%20via%20in-network%20optimization.%20In%3A%20CVPR%20%282021%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4.">
              <p class="c-article-references__text" id="ref-CR4">
               Beeler, T., Bickel, B., Beardsley, P., Sumner, B., Gross, M.: High-quality single-shot capture of facial geometry. In: ACM Transactions on Graphics (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR4-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Beeler%2C%20T.%2C%20Bickel%2C%20B.%2C%20Beardsley%2C%20P.%2C%20Sumner%2C%20B.%2C%20Gross%2C%20M.%3A%20High-quality%20single-shot%20capture%20of%20facial%20geometry.%20In%3A%20ACM%20Transactions%20on%20Graphics%20%282010%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5.">
              <p class="c-article-references__text" id="ref-CR5">
               Beeler, T., et al.: High-quality passive facial performance capture using anchor frames. In: ACM Transactions on Graphics (2011)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR5-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Beeler%2C%20T.%2C%20et%20al.%3A%20High-quality%20passive%20facial%20performance%20capture%20using%20anchor%20frames.%20In%3A%20ACM%20Transactions%20on%20Graphics%20%282011%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6.">
              <p class="c-article-references__text" id="ref-CR6">
               Blanz, V., Vetter, T.: A morphable model for the synthesis of 3D faces. In: Computer Graphics and Interactive Techniques (1999)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR6-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Blanz%2C%20V.%2C%20Vetter%2C%20T.%3A%20A%20morphable%20model%20for%20the%20synthesis%20of%203D%20faces.%20In%3A%20Computer%20Graphics%20and%20Interactive%20Techniques%20%281999%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7.">
              <p class="c-article-references__text" id="ref-CR7">
               Blanz, V., Vetter, T.: Face recognition based on fitting a 3d morphable model. TPAMI
               <b>
                25
               </b>
               (9), 1063–1074 (2003)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR7-links">
               <a aria-label="CrossRef reference 7" data-doi="10.1109/TPAMI.2003.1227983" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1109/TPAMI.2003.1227983" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2003.1227983" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 7" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Face%20recognition%20based%20on%20fitting%20a%203d%20morphable%20model&amp;journal=TPAMI&amp;volume=25&amp;issue=9&amp;pages=1063-1074&amp;publication_year=2003&amp;author=Blanz%2CV&amp;author=Vetter%2CT" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8.">
              <p class="c-article-references__text" id="ref-CR8">
               Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep It SMPL: automatic estimation of 3d human pose and shape from a single image. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9909, pp. 561–578. Springer, Cham (2016).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-46454-1_34" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46454-1_34">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46454-1_34
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR8-links">
               <a aria-label="CrossRef reference 8" data-doi="10.1007/978-3-319-46454-1_34" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-319-46454-1_34" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-319-46454-1_34" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 8" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Keep%20It%20SMPL%3A%20automatic%20estimation%20of%203d%20human%20pose%20and%20shape%20from%20a%20single%20image&amp;pages=561-578&amp;publication_year=2016 2016 2016&amp;author=Bogo%2CF&amp;author=Kanazawa%2CA&amp;author=Lassner%2CC&amp;author=Gehler%2CP&amp;author=Romero%2CJ&amp;author=Black%2CMJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9.">
              <p class="c-article-references__text" id="ref-CR9">
               Bradley, D., Heidrich, W., Popa, T., Sheffer, A.: High resolution passive facial performance capture. In: ACM Transactions on Graphics, vol. 29, no. 4 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR9-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bradley%2C%20D.%2C%20Heidrich%2C%20W.%2C%20Popa%2C%20T.%2C%20Sheffer%2C%20A.%3A%20High%20resolution%20passive%20facial%20performance%20capture.%20In%3A%20ACM%20Transactions%20on%20Graphics%2C%20vol.%2029%2C%20no.%204%20%282010%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10.">
              <p class="c-article-references__text" id="ref-CR10">
               Browatzki, B., Wallraven, C.: 3FabRec: Fast Few-shot Face alignment by Reconstruction. In: CVPR (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR10-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Browatzki%2C%20B.%2C%20Wallraven%2C%20C.%3A%203FabRec%3A%20Fast%20Few-shot%20Face%20alignment%20by%20Reconstruction.%20In%3A%20CVPR%20%282020%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11.">
              <p class="c-article-references__text" id="ref-CR11">
               Bulat, A., Sanchez, E., Tzimiropoulos, G.: Subpixel heatmap regression for facial landmark Localization. In: BMVC (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR11-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bulat%2C%20A.%2C%20Sanchez%2C%20E.%2C%20Tzimiropoulos%2C%20G.%3A%20Subpixel%20heatmap%20regression%20for%20facial%20landmark%20Localization.%20In%3A%20BMVC%20%282021%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12.">
              <p class="c-article-references__text" id="ref-CR12">
               Bulat, A., Tzimiropoulos, G.: How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3D facial landmarks). In: ICCV (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR12-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bulat%2C%20A.%2C%20Tzimiropoulos%2C%20G.%3A%20How%20far%20are%20we%20from%20solving%20the%202d%20%26%203d%20face%20alignment%20problem%3F%20%28and%20a%20dataset%20of%20230%2C000%203D%20facial%20landmarks%29.%20In%3A%20ICCV%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13.">
              <p class="c-article-references__text" id="ref-CR13">
               Cao, C., Chai, M., Woodford, O., Luo, L.: Stabilized real-time face tracking via a learned dynamic rigidity prior. ACM Trans. Graph.
               <b>
                37
               </b>
               (6), 1–11 (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR13-links">
               <a aria-label="Google Scholar reference 13" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Stabilized%20real-time%20face%20tracking%20via%20a%20learned%20dynamic%20rigidity%20prior&amp;journal=ACM%20Trans.%20Graph.&amp;volume=37&amp;issue=6&amp;pages=1-11&amp;publication_year=2018&amp;author=Cao%2CC&amp;author=Chai%2CM&amp;author=Woodford%2CO&amp;author=Luo%2CL" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14.">
              <p class="c-article-references__text" id="ref-CR14">
               Chandran, P., Bradley, D., Gross, M., Beeler, T.: Semantic deep face models. In: International Conference on 3D Vision (3DV) (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR14-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Chandran%2C%20P.%2C%20Bradley%2C%20D.%2C%20Gross%2C%20M.%2C%20Beeler%2C%20T.%3A%20Semantic%20deep%20face%20models.%20In%3A%20International%20Conference%20on%203D%20Vision%20%283DV%29%20%282020%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15.">
              <p class="c-article-references__text" id="ref-CR15">
               Cong, M., Lan, L., Fedkiw, R.: Local geometric indexing of high resolution data for facial reconstruction from sparse markers. CoRR abs/1903.00119 (2019).
               <a data-track="click" data-track-action="external reference" data-track-label="http://www.arxiv-org.proxy.lib.ohio-state.edu/abs/1903.00119" href="http://www.arxiv-org.proxy.lib.ohio-state.edu/abs/1903.00119">
                www.arxiv.org/abs/1903.00119
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16.">
              <p class="c-article-references__text" id="ref-CR16">
               Deng, J., Guo, J., Ververas, E., Kotsia, I., Zafeiriou, S.: RetinaFace: single-shot multi-level face localisation in the wild. In: CVPR (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR16-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Deng%2C%20J.%2C%20Guo%2C%20J.%2C%20Ververas%2C%20E.%2C%20Kotsia%2C%20I.%2C%20Zafeiriou%2C%20S.%3A%20RetinaFace%3A%20single-shot%20multi-level%20face%20localisation%20in%20the%20wild.%20In%3A%20CVPR%20%282020%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17.">
              <p class="c-article-references__text" id="ref-CR17">
               Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face reconstruction with weakly-supervised learning: from single image to image set. In: CVPR Workshops (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR17-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Deng%2C%20Y.%2C%20Yang%2C%20J.%2C%20Xu%2C%20S.%2C%20Chen%2C%20D.%2C%20Jia%2C%20Y.%2C%20Tong%2C%20X.%3A%20Accurate%203d%20face%20reconstruction%20with%20weakly-supervised%20learning%3A%20from%20single%20image%20to%20image%20set.%20In%3A%20CVPR%20Workshops%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18.">
              <p class="c-article-references__text" id="ref-CR18">
               Dib, A., et al.: Practical face reconstruction via differentiable ray tracing. Comput. Graph. Forum
               <b>
                40
               </b>
               (2), 153–164 (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR18-links">
               <a aria-label="CrossRef reference 18" data-doi="10.1111/cgf.142622" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1111/cgf.142622" href="https://doi-org.proxy.lib.ohio-state.edu/10.1111%2Fcgf.142622" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 18" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Practical%20face%20reconstruction%20via%20differentiable%20ray%20tracing&amp;journal=Comput.%20Graph.%20Forum&amp;volume=40&amp;issue=2&amp;pages=153-164&amp;publication_year=2021&amp;author=Dib%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19.">
              <p class="c-article-references__text" id="ref-CR19">
               Dib, A., Thebault, C., Ahn, J., Gosselin, P.H., Theobalt, C., Chevallier, L.: Towards high fidelity monocular face reconstruction with rich reflectance using self-supervised learning and ray tracing. In: CVPR (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR19-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Dib%2C%20A.%2C%20Thebault%2C%20C.%2C%20Ahn%2C%20J.%2C%20Gosselin%2C%20P.H.%2C%20Theobalt%2C%20C.%2C%20Chevallier%2C%20L.%3A%20Towards%20high%20fidelity%20monocular%20face%20reconstruction%20with%20rich%20reflectance%20using%20self-supervised%20learning%20and%20ray%20tracing.%20In%3A%20CVPR%20%282021%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20.">
              <p class="c-article-references__text" id="ref-CR20">
               Dou, P., Kakadiaris, I.A.: Multi-view 3D face reconstruction with deep recurrent neural networks. Image Vis. Comput.
               <b>
                80
               </b>
               , 80–91 (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR20-links">
               <a aria-label="CrossRef reference 20" data-doi="10.1016/j.imavis.2018.09.004" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1016/j.imavis.2018.09.004" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.imavis.2018.09.004" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 20" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Multi-view%203D%20face%20reconstruction%20with%20deep%20recurrent%20neural%20networks&amp;journal=Image%20Vis.%20Comput.&amp;volume=80&amp;pages=80-91&amp;publication_year=2018&amp;author=Dou%2CP&amp;author=Kakadiaris%2CIA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21.">
              <p class="c-article-references__text" id="ref-CR21">
               Dou, P., Shah, S.K., Kakadiaris, I.A.: End-to-end 3D face reconstruction with deep neural networks. In: CVPR (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR21-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Dou%2C%20P.%2C%20Shah%2C%20S.K.%2C%20Kakadiaris%2C%20I.A.%3A%20End-to-end%203D%20face%20reconstruction%20with%20deep%20neural%20networks.%20In%3A%20CVPR%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22.">
              <p class="c-article-references__text" id="ref-CR22">
               Falcon, W., et al.: Pytorch lightning
               <b>
                3
               </b>
               (6) (2019). GitHub. Note.
               <a data-track="click" data-track-action="external reference" data-track-label="https://github.com/PyTorchLightning/pytorch-lightning" href="https://github.com/PyTorchLightning/pytorch-lightning">
                https://github.com/PyTorchLightning/pytorch-lightning
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23.">
              <p class="c-article-references__text" id="ref-CR23">
               Feng, Y., Feng, H., Black, M.J., Bolkart, T.: Learning an animatable detailed 3D face model from in-the-wild images. ACM Trans. Graph. (ToG)
               <b>
                40
               </b>
               (4), 1–13 (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR23-links">
               <a aria-label="CrossRef reference 23" data-doi="10.1145/3450626.3459936" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/3450626.3459936" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3450626.3459936" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 23" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Learning%20an%20animatable%20detailed%203D%20face%20model%20from%20in-the-wild%20images&amp;journal=ACM%20Trans.%20Graph.%20%28ToG%29&amp;volume=40&amp;issue=4&amp;pages=1-13&amp;publication_year=2021&amp;author=Feng%2CY&amp;author=Feng%2CH&amp;author=Black%2CMJ&amp;author=Bolkart%2CT" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24.">
              <p class="c-article-references__text" id="ref-CR24">
               Feng, Y., Wu, F., Shao, X., Wang, Y., Zhou, X.: Joint 3d face reconstruction and dense alignment with position map regression network. In: ECCV (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR24-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Feng%2C%20Y.%2C%20Wu%2C%20F.%2C%20Shao%2C%20X.%2C%20Wang%2C%20Y.%2C%20Zhou%2C%20X.%3A%20Joint%203d%20face%20reconstruction%20and%20dense%20alignment%20with%20position%20map%20regression%20network.%20In%3A%20ECCV%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25.">
              <p class="c-article-references__text" id="ref-CR25">
               Garrido, P., et al.: Reconstruction of personalized 3d face rigs from monocular video. ACM Trans. Graph.
               <b>
                35
               </b>
               (3), 1–15 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR25-links">
               <a aria-label="CrossRef reference 25" data-doi="10.1145/2890493" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/2890493" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2890493" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 25" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Reconstruction%20of%20personalized%203d%20face%20rigs%20from%20monocular%20video&amp;journal=ACM%20Trans.%20Graph.&amp;volume=35&amp;issue=3&amp;pages=1-15&amp;publication_year=2016&amp;author=Garrido%2CP" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26.">
              <p class="c-article-references__text" id="ref-CR26">
               Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., Freeman, W.T.: Unsupervised training for 3d morphable model regression. In: CVPR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR26-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Genova%2C%20K.%2C%20Cole%2C%20F.%2C%20Maschinot%2C%20A.%2C%20Sarna%2C%20A.%2C%20Vlasic%2C%20D.%2C%20Freeman%2C%20W.T.%3A%20Unsupervised%20training%20for%203d%20morphable%20model%20regression.%20In%3A%20CVPR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27.">
              <p class="c-article-references__text" id="ref-CR27">
               Gerig, T., et al.: Morphable face models-an open framework. In: Automatic Face &amp; Gesture Recognition (FG). IEEE (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR27-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Gerig%2C%20T.%2C%20et%20al.%3A%20Morphable%20face%20models-an%20open%20framework.%20In%3A%20Automatic%20Face%20%26%20Gesture%20Recognition%20%28FG%29.%20IEEE%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28.">
              <p class="c-article-references__text" id="ref-CR28">
               Grishchenko, I., Ablavatski, A., Kartynnik, Y., Raveendran, K., Grundmann, M.: Attention mesh: high-fidelity face mesh prediction in real-time. In: CVPR Workshops (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR28-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Grishchenko%2C%20I.%2C%20Ablavatski%2C%20A.%2C%20Kartynnik%2C%20Y.%2C%20Raveendran%2C%20K.%2C%20Grundmann%2C%20M.%3A%20Attention%20mesh%3A%20high-fidelity%20face%20mesh%20prediction%20in%20real-time.%20In%3A%20CVPR%20Workshops%20%282020%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29.">
              <p class="c-article-references__text" id="ref-CR29">
               Güler, R.A., Neverova, N., Kokkinos, I.: Densepose: dense human pose estimation in the wild. In: CVPR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR29-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=G%C3%BCler%2C%20R.A.%2C%20Neverova%2C%20N.%2C%20Kokkinos%2C%20I.%3A%20Densepose%3A%20dense%20human%20pose%20estimation%20in%20the%20wild.%20In%3A%20CVPR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30.">
              <p class="c-article-references__text" id="ref-CR30">
               Guo, J., Zhu, X., Yang, Y., Yang, F., Lei, Z., Li, S.Z.: Towards fast, accurate and stable 3d dense face alignment. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12364, pp. 152–168. Springer, Cham (2020).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-030-58529-7_10" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58529-7_10">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58529-7_10
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR30-links">
               <a aria-label="CrossRef reference 30" data-doi="10.1007/978-3-030-58529-7_10" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-030-58529-7_10" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-030-58529-7_10" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 30" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Towards%20fast%2C%20accurate%20and%20stable%203d%20dense%20face%20alignment&amp;pages=152-168&amp;publication_year=2020 2020 2020&amp;author=Guo%2CJ&amp;author=Zhu%2CX&amp;author=Yang%2CY&amp;author=Yang%2CF&amp;author=Lei%2CZ&amp;author=Li%2CSZ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31.">
              <p class="c-article-references__text" id="ref-CR31">
               Guo, Y., Cai, J., Jiang, B., Zheng, J., et al.: Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images. TPAMI
               <b>
                41
               </b>
               (6), 1294–1307 (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR31-links">
               <a aria-label="CrossRef reference 31" data-doi="10.1109/TPAMI.2018.2837742" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1109/TPAMI.2018.2837742" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2018.2837742" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 31" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Cnn-based%20real-time%20dense%20face%20reconstruction%20with%20inverse-rendered%20photo-realistic%20face%20images&amp;journal=TPAMI&amp;volume=41&amp;issue=6&amp;pages=1294-1307&amp;publication_year=2018&amp;author=Guo%2CY&amp;author=Cai%2CJ&amp;author=Jiang%2CB&amp;author=Zheng%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32.">
              <p class="c-article-references__text" id="ref-CR32">
               Han, S., et al.: Megatrack: monochrome egocentric articulated hand-tracking for virtual reality. ACM Trans. Graph. (TOG)
               <b>
                39
               </b>
               (4), 1–87 (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR32-links">
               <a aria-label="CrossRef reference 32" data-doi="10.1145/3386569.3392452" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/3386569.3392452" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3386569.3392452" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 32" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Megatrack%3A%20monochrome%20egocentric%20articulated%20hand-tracking%20for%20virtual%20reality&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=39&amp;issue=4&amp;pages=1-87&amp;publication_year=2020&amp;author=Han%2CS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33.">
              <p class="c-article-references__text" id="ref-CR33">
               Hassner, T., Harel, S., Paz, E., Enbar, R.: Effective face frontalization in unconstrained images. In: CVPR (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR33-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hassner%2C%20T.%2C%20Harel%2C%20S.%2C%20Paz%2C%20E.%2C%20Enbar%2C%20R.%3A%20Effective%20face%20frontalization%20in%20unconstrained%20images.%20In%3A%20CVPR%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34.">
              <p class="c-article-references__text" id="ref-CR34">
               He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR34-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=He%2C%20K.%2C%20Zhang%2C%20X.%2C%20Ren%2C%20S.%2C%20Sun%2C%20J.%3A%20Deep%20residual%20learning%20for%20image%20recognition.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35.">
              <p class="c-article-references__text" id="ref-CR35">
               Jeni, L.A., Cohn, J.F., Kanade, T.: Dense 3D face alignment from 2D videos in real-time. In: Automatic Face and Gesture Recognition (FG). IEEE (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR35-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Jeni%2C%20L.A.%2C%20Cohn%2C%20J.F.%2C%20Kanade%2C%20T.%3A%20Dense%203D%20face%20alignment%20from%202D%20videos%20in%20real-time.%20In%3A%20Automatic%20Face%20and%20Gesture%20Recognition%20%28FG%29.%20IEEE%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36.">
              <p class="c-article-references__text" id="ref-CR36">
               Kartynnik, Y., Ablavatski, A., Grishchenko, I., Grundmann, M.: Real-time facial surface geometry from monocular video on mobile GPUs. In: CVPR Workshops (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR36-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kartynnik%2C%20Y.%2C%20Ablavatski%2C%20A.%2C%20Grishchenko%2C%20I.%2C%20Grundmann%2C%20M.%3A%20Real-time%20facial%20surface%20geometry%20from%20monocular%20video%20on%20mobile%20GPUs.%20In%3A%20CVPR%20Workshops%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37.">
              <p class="c-article-references__text" id="ref-CR37">
               Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for computer vision? In: Advances in Neural Information Processing Systems, vol. 30 (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR37-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kendall%2C%20A.%2C%20Gal%2C%20Y.%3A%20What%20uncertainties%20do%20we%20need%20in%20bayesian%20deep%20learning%20for%20computer%20vision%3F%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20vol.%2030%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38.">
              <p class="c-article-references__text" id="ref-CR38">
               Kumar, A., et al.: Luvli face alignment: estimating landmarks’ location, uncertainty, and visibility likelihood. In: CVPR (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR38-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kumar%2C%20A.%2C%20et%20al.%3A%20Luvli%20face%20alignment%3A%20estimating%20landmarks%E2%80%99%20location%2C%20uncertainty%2C%20and%20visibility%20likelihood.%20In%3A%20CVPR%20%282020%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39.">
              <p class="c-article-references__text" id="ref-CR39">
               Lewis, J.P., Cordner, M., Fong, N.: Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation. In: SIGGRAPH (2000)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR39-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Lewis%2C%20J.P.%2C%20Cordner%2C%20M.%2C%20Fong%2C%20N.%3A%20Pose%20space%20deformation%3A%20a%20unified%20approach%20to%20shape%20interpolation%20and%20skeleton-driven%20deformation.%20In%3A%20SIGGRAPH%20%282000%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40.">
              <p class="c-article-references__text" id="ref-CR40">
               Li, T., Bolkart, T., Black, M.J., Li, H., Romero, J.: Learning a model of facial shape and expression from 4D scans. In: ACM Transactions on Graphics, (Proceedings SIGGRAPH Asia) (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR40-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Li%2C%20T.%2C%20Bolkart%2C%20T.%2C%20Black%2C%20M.J.%2C%20Li%2C%20H.%2C%20Romero%2C%20J.%3A%20Learning%20a%20model%20of%20facial%20shape%20and%20expression%20from%204D%20scans.%20In%3A%20ACM%20Transactions%20on%20Graphics%2C%20%28Proceedings%20SIGGRAPH%20Asia%29%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41.">
              <p class="c-article-references__text" id="ref-CR41">
               Li, Y., Yang, S., Zhang, S., Wang, Z., Yang, W., Xia, S.T., Zhou, E.: Is 2d heatmap representation even necessary for human pose estimation? (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR41-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Li%2C%20Y.%2C%20Yang%2C%20S.%2C%20Zhang%2C%20S.%2C%20Wang%2C%20Z.%2C%20Yang%2C%20W.%2C%20Xia%2C%20S.T.%2C%20Zhou%2C%20E.%3A%20Is%202d%20heatmap%20representation%20even%20necessary%20for%20human%20pose%20estimation%3F%20%282021%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42.">
              <p class="c-article-references__text" id="ref-CR42">
               Liu, D.C., Nocedal, J.: On the limited memory BFGS method for large scale optimization. Math. Program.
               <b>
                45
               </b>
               (1), 503–528 (1989).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/BF01589116" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/BF01589116">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/BF01589116
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR42-links">
               <a aria-label="CrossRef reference 42" data-doi="10.1007/BF01589116" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/BF01589116" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2FBF01589116" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="MathSciNet reference 42" data-track="click" data-track-action="MathSciNet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=1038245" rel="nofollow noopener">
                MathSciNet
               </a>
               <a aria-label="MATH reference 42" data-track="click" data-track-action="MATH reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?0696.90048" rel="nofollow noopener">
                MATH
               </a>
               <a aria-label="Google Scholar reference 42" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=On%20the%20limited%20memory%20BFGS%20method%20for%20large%20scale%20optimization&amp;journal=Math.%20Program.&amp;doi=10.1007%2FBF01589116&amp;volume=45&amp;issue=1&amp;pages=503-528&amp;publication_year=1989&amp;author=Liu%2CDC&amp;author=Nocedal%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43.">
              <p class="c-article-references__text" id="ref-CR43">
               Liu, F., Zhu, R., Zeng, D., Zhao, Q., Liu, X.: Disentangling features in 3D face shapes for joint face reconstruction and recognition. In: CVPR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR43-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liu%2C%20F.%2C%20Zhu%2C%20R.%2C%20Zeng%2C%20D.%2C%20Zhao%2C%20Q.%2C%20Liu%2C%20X.%3A%20Disentangling%20features%20in%203D%20face%20shapes%20for%20joint%20face%20reconstruction%20and%20recognition.%20In%3A%20CVPR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44.">
              <p class="c-article-references__text" id="ref-CR44">
               Liu, Y., Jourabloo, A., Ren, W., Liu, X.: Dense face alignment. In: ICCV Workshops (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR44-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liu%2C%20Y.%2C%20Jourabloo%2C%20A.%2C%20Ren%2C%20W.%2C%20Liu%2C%20X.%3A%20Dense%20face%20alignment.%20In%3A%20ICCV%20Workshops%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45.">
              <p class="c-article-references__text" id="ref-CR45">
               Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR45-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Loshchilov%2C%20I.%2C%20Hutter%2C%20F.%3A%20Decoupled%20weight%20decay%20regularization.%20In%3A%20ICLR%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46.">
              <p class="c-article-references__text" id="ref-CR46">
               Morales, A., Piella, G., Sukno, F.M.: Survey on 3d face reconstruction from uncalibrated images. Comput. Sci. Rev.
               <b>
                40
               </b>
               , 100400 (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR46-links">
               <a aria-label="CrossRef reference 46" data-doi="10.1016/j.cosrev.2021.100400" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1016/j.cosrev.2021.100400" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.cosrev.2021.100400" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 46" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Survey%20on%203d%20face%20reconstruction%20from%20uncalibrated%20images&amp;journal=Comput.%20Sci.%20Rev.&amp;volume=40&amp;publication_year=2021&amp;author=Morales%2CA&amp;author=Piella%2CG&amp;author=Sukno%2CFM" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47.">
              <p class="c-article-references__text" id="ref-CR47">
               Paszke, A., et al.: Pytorch: an imperative style, high-performance deep learning library. In: NeurIPS (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR47-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Paszke%2C%20A.%2C%20et%20al.%3A%20Pytorch%3A%20an%20imperative%20style%2C%20high-performance%20deep%20learning%20library.%20In%3A%20NeurIPS%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48.">
              <p class="c-article-references__text" id="ref-CR48">
               Piotraschke, M., Blanz, V.: Automated 3D face reconstruction from multiple images using quality measures. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR48-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Piotraschke%2C%20M.%2C%20Blanz%2C%20V.%3A%20Automated%203D%20face%20reconstruction%20from%20multiple%20images%20using%20quality%20measures.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49.">
              <p class="c-article-references__text" id="ref-CR49">
               Popa, T., South-Dickinson, I., Bradley, D., Sheffer, A., Heidrich, W.: Globally consistent space-time reconstruction. Comput. Graph. Forum
               <b>
                29
               </b>
               (5), 1633–1642 (2010)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR49-links">
               <a aria-label="CrossRef reference 49" data-doi="10.1111/j.1467-8659.2010.01772.x" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1111/j.1467-8659.2010.01772.x" href="https://doi-org.proxy.lib.ohio-state.edu/10.1111%2Fj.1467-8659.2010.01772.x" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 49" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Globally%20consistent%20space-time%20reconstruction&amp;journal=Comput.%20Graph.%20Forum&amp;volume=29&amp;issue=5&amp;pages=1633-1642&amp;publication_year=2010&amp;author=Popa%2CT&amp;author=South-Dickinson%2CI&amp;author=Bradley%2CD&amp;author=Sheffer%2CA&amp;author=Heidrich%2CW" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50.">
              <p class="c-article-references__text" id="ref-CR50">
               Richardson, E., Sela, M., Kimmel, R.: 3D face reconstruction by learning from synthetic data. In: 3DV. IEEE (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR50-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Richardson%2C%20E.%2C%20Sela%2C%20M.%2C%20Kimmel%2C%20R.%3A%203D%20face%20reconstruction%20by%20learning%20from%20synthetic%20data.%20In%3A%203DV.%20IEEE%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51.">
              <p class="c-article-references__text" id="ref-CR51">
               Richardson, E., Sela, M., Or-El, R., Kimmel, R.: Learning detailed face reconstruction from a single image. In: CVPR (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR51-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Richardson%2C%20E.%2C%20Sela%2C%20M.%2C%20Or-El%2C%20R.%2C%20Kimmel%2C%20R.%3A%20Learning%20detailed%20face%20reconstruction%20from%20a%20single%20image.%20In%3A%20CVPR%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52.">
              <p class="c-article-references__text" id="ref-CR52">
               Sagonas, C., Antonakos, E., Tzimiropoulos, G., Zafeiriou, S., Pantic, M.: 300 faces in-the-wild challenge: database and results. Image Vis. Computi. (IMAVIS)
               <b>
                47
               </b>
               , 3–18 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR52-links">
               <a aria-label="CrossRef reference 52" data-doi="10.1016/j.imavis.2016.01.002" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1016/j.imavis.2016.01.002" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.imavis.2016.01.002" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 52" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=300%20faces%20in-the-wild%20challenge%3A%20database%20and%20results&amp;journal=Image%20Vis.%20Computi.%20%28IMAVIS%29&amp;volume=47&amp;pages=3-18&amp;publication_year=2016&amp;author=Sagonas%2CC&amp;author=Antonakos%2CE&amp;author=Tzimiropoulos%2CG&amp;author=Zafeiriou%2CS&amp;author=Pantic%2CM" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53.">
              <p class="c-article-references__text" id="ref-CR53">
               Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenet V2: Inverted residuals and linear bottlenecks. In: CVPR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR53-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sandler%2C%20M.%2C%20Howard%2C%20A.%2C%20Zhu%2C%20M.%2C%20Zhmoginov%2C%20A.%2C%20Chen%2C%20L.C.%3A%20Mobilenet%20V2%3A%20Inverted%20residuals%20and%20linear%20bottlenecks.%20In%3A%20CVPR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54.">
              <p class="c-article-references__text" id="ref-CR54">
               Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3d face shape and expression from an image without 3d supervision. In: CVPR (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR54-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sanyal%2C%20S.%2C%20Bolkart%2C%20T.%2C%20Feng%2C%20H.%2C%20Black%2C%20M.%3A%20Learning%20to%20regress%203d%20face%20shape%20and%20expression%20from%20an%20image%20without%203d%20supervision.%20In%3A%20CVPR%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55.">
              <p class="c-article-references__text" id="ref-CR55">
               Seitz, S.M., Curless, B., Diebel, J., Scharstein, D., Szeliski, R.: A comparison and evaluation of multi-view stereo reconstruction algorithms. In: CVPR (2006)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR55-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Seitz%2C%20S.M.%2C%20Curless%2C%20B.%2C%20Diebel%2C%20J.%2C%20Scharstein%2C%20D.%2C%20Szeliski%2C%20R.%3A%20A%20comparison%20and%20evaluation%20of%20multi-view%20stereo%20reconstruction%20algorithms.%20In%3A%20CVPR%20%282006%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56.">
              <p class="c-article-references__text" id="ref-CR56">
               Sela, M., Richardson, E., Kimmel, R.: Unrestricted facial geometry reconstruction using image-to-image translation. In: ICCV (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR56-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sela%2C%20M.%2C%20Richardson%2C%20E.%2C%20Kimmel%2C%20R.%3A%20Unrestricted%20facial%20geometry%20reconstruction%20using%20image-to-image%20translation.%20In%3A%20ICCV%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57.">
              <p class="c-article-references__text" id="ref-CR57">
               Shang, J.: Self-supervised monocular 3d face reconstruction by occlusion-aware multi-view geometry consistency. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12360, pp. 53–70. Springer, Cham (2020).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-030-58555-6_4" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58555-6_4">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58555-6_4
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR57-links">
               <a aria-label="CrossRef reference 57" data-doi="10.1007/978-3-030-58555-6_4" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-030-58555-6_4" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-030-58555-6_4" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 57" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Self-supervised%20monocular%203d%20face%20reconstruction%20by%20occlusion-aware%20multi-view%20geometry%20consistency&amp;pages=53-70&amp;publication_year=2020 2020 2020&amp;author=Shang%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58.">
              <p class="c-article-references__text" id="ref-CR58">
               Taylor, J., et al.: Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences. ACM Trans. Graph. (ToG)
               <b>
                35
               </b>
               (4), 1–12 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR58-links">
               <a aria-label="CrossRef reference 58" data-doi="10.1145/2897824.2925965" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/2897824.2925965" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2897824.2925965" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 58" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Efficient%20and%20precise%20interactive%20hand%20tracking%20through%20joint%2C%20continuous%20optimization%20of%20pose%20and%20correspondences&amp;journal=ACM%20Trans.%20Graph.%20%28ToG%29&amp;volume=35&amp;issue=4&amp;pages=1-12&amp;publication_year=2016&amp;author=Taylor%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59.">
              <p class="c-article-references__text" id="ref-CR59">
               Taylor, J., Shotton, J., Sharp, T., Fitzgibbon, A.: The vitruvian manifold: inferring dense correspondences for one-shot human pose estimation. In: CVPR (2012)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR59-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Taylor%2C%20J.%2C%20Shotton%2C%20J.%2C%20Sharp%2C%20T.%2C%20Fitzgibbon%2C%20A.%3A%20The%20vitruvian%20manifold%3A%20inferring%20dense%20correspondences%20for%20one-shot%20human%20pose%20estimation.%20In%3A%20CVPR%20%282012%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60.">
              <p class="c-article-references__text" id="ref-CR60">
               Tewari, A., et al.: FML: face model learning from videos. In: CVPR (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR60-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tewari%2C%20A.%2C%20et%20al.%3A%20FML%3A%20face%20model%20learning%20from%20videos.%20In%3A%20CVPR%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61.">
              <p class="c-article-references__text" id="ref-CR61">
               Tewari, A., et al: Self-supervised multi-level face model learning for monocular reconstruction at over 250 Hz. In: CVPR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR61-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tewari%2C%20A.%2C%20et%20al%3A%20Self-supervised%20multi-level%20face%20model%20learning%20for%20monocular%20reconstruction%20at%20over%20250%20Hz.%20In%3A%20CVPR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62.">
              <p class="c-article-references__text" id="ref-CR62">
               Tewari, A., et al.: Mofa: model-based deep convolutional face autoencoder for unsupervised monocular reconstruction. In: ICCV Workshops (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR62-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tewari%2C%20A.%2C%20et%20al.%3A%20Mofa%3A%20model-based%20deep%20convolutional%20face%20autoencoder%20for%20unsupervised%20monocular%20reconstruction.%20In%3A%20ICCV%20Workshops%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63.">
              <p class="c-article-references__text" id="ref-CR63">
               Thies, J., Zollhöfer, M., Nießner, M., Valgaerts, L., Stamminger, M., Theobalt, C.: Real-time expression transfer for facial reenactment. ACM Trans. Graph.
               <b>
                34
               </b>
               (6), 1–183 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR63-links">
               <a aria-label="CrossRef reference 63" data-doi="10.1145/2816795.2818056" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/2816795.2818056" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2816795.2818056" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 63" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Real-time%20expression%20transfer%20for%20facial%20reenactment&amp;journal=ACM%20Trans.%20Graph.&amp;volume=34&amp;issue=6&amp;pages=1-183&amp;publication_year=2015&amp;author=Thies%2CJ&amp;author=Zollh%C3%B6fer%2CM&amp;author=Nie%C3%9Fner%2CM&amp;author=Valgaerts%2CL&amp;author=Stamminger%2CM&amp;author=Theobalt%2CC" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64.">
              <p class="c-article-references__text" id="ref-CR64">
               Thies, J., Zollhöfer, M., Stamminger, M., Theobalt, C., Nießner, M.: Face2Face: real-time face capture and reenactment of RGB videos. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR64-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Thies%2C%20J.%2C%20Zollh%C3%B6fer%2C%20M.%2C%20Stamminger%2C%20M.%2C%20Theobalt%2C%20C.%2C%20Nie%C3%9Fner%2C%20M.%3A%20Face2Face%3A%20real-time%20face%20capture%20and%20reenactment%20of%20RGB%20videos.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65.">
              <p class="c-article-references__text" id="ref-CR65">
               Tran, L., Liu, F., Liu, X.: Towards high-fidelity nonlinear 3D face morphable model. In: CVPR (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR65-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tran%2C%20L.%2C%20Liu%2C%20F.%2C%20Liu%2C%20X.%3A%20Towards%20high-fidelity%20nonlinear%203D%20face%20morphable%20model.%20In%3A%20CVPR%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66.">
              <p class="c-article-references__text" id="ref-CR66">
               Tran, L., Liu, X.: Nonlinear 3d face morphable model. In: CVPR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR66-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tran%2C%20L.%2C%20Liu%2C%20X.%3A%20Nonlinear%203d%20face%20morphable%20model.%20In%3A%20CVPR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67.">
              <p class="c-article-references__text" id="ref-CR67">
               Tuan Tran, A., Hassner, T., Masi, I., Medioni, G.: Regressing robust and discriminative 3D morphable models with a very deep neural network. In: CVPR (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR67-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tuan%20Tran%2C%20A.%2C%20Hassner%2C%20T.%2C%20Masi%2C%20I.%2C%20Medioni%2C%20G.%3A%20Regressing%20robust%20and%20discriminative%203D%20morphable%20models%20with%20a%20very%20deep%20neural%20network.%20In%3A%20CVPR%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68.">
              <p class="c-article-references__text" id="ref-CR68">
               Wang, X., Bo, L., Fuxin, L.: Adaptive wing loss for robust face alignment via heatmap regression. In: ICCV (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR68-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wang%2C%20X.%2C%20Bo%2C%20L.%2C%20Fuxin%2C%20L.%3A%20Adaptive%20wing%20loss%20for%20robust%20face%20alignment%20via%20heatmap%20regression.%20In%3A%20ICCV%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69.">
              <p class="c-article-references__text" id="ref-CR69">
               Wightman, R.: Pytorch image models (2019).
               <a data-track="click" data-track-action="external reference" data-track-label="https://www.github.com/rwightman/pytorch-image-models" href="https://www.github.com/rwightman/pytorch-image-models">
                https://www.github.com/rwightman/pytorch-image-models
               </a>
               ,
               <a data-track="click" data-track-action="external reference" data-track-label="10.5281/zenodo.4414861" href="https://doi-org.proxy.lib.ohio-state.edu/10.5281/zenodo.4414861">
                https://doi-org.proxy.lib.ohio-state.edu/10.5281/zenodo.4414861
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70.">
              <p class="c-article-references__text" id="ref-CR70">
               Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR70-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wood%2C%20E.%2C%20et%20al.%3A%20Fake%20it%20till%20you%20make%20it%3A%20Face%20analysis%20in%20the%20wild%20using%20synthetic%20data%20alone%20%282021%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71.">
              <p class="c-article-references__text" id="ref-CR71">
               Wu, W., Qian, C., Yang, S., Wang, Q., Cai, Y., Zhou, Q.: Look at boundary: a boundary-aware face alignment algorithm. In: CVPR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR71-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wu%2C%20W.%2C%20Qian%2C%20C.%2C%20Yang%2C%20S.%2C%20Wang%2C%20Q.%2C%20Cai%2C%20Y.%2C%20Zhou%2C%20Q.%3A%20Look%20at%20boundary%3A%20a%20boundary-aware%20face%20alignment%20algorithm.%20In%3A%20CVPR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="72.">
              <p class="c-article-references__text" id="ref-CR72">
               Yi, H., et al.: MMFace: a multi-metric regression network for unconstrained face reconstruction. In: CVPR (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR72-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yi%2C%20H.%2C%20et%20al.%3A%20MMFace%3A%20a%20multi-metric%20regression%20network%20for%20unconstrained%20face%20reconstruction.%20In%3A%20CVPR%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="73.">
              <p class="c-article-references__text" id="ref-CR73">
               Yoon, J.S., Shiratori, T., Yu, S.I., Park, H.S.: Self-supervised adaptation of high-fidelity face models for monocular performance tracking. In: CVPR (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR73-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yoon%2C%20J.S.%2C%20Shiratori%2C%20T.%2C%20Yu%2C%20S.I.%2C%20Park%2C%20H.S.%3A%20Self-supervised%20adaptation%20of%20high-fidelity%20face%20models%20for%20monocular%20performance%20tracking.%20In%3A%20CVPR%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="74.">
              <p class="c-article-references__text" id="ref-CR74">
               Zhou, Y., Deng, J., Kotsia, I., Zafeiriou, S.: Dense 3d face decoding over 2500fps: joint texture &amp; shape convolutional mesh decoders. In: CVPR (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR74-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhou%2C%20Y.%2C%20Deng%2C%20J.%2C%20Kotsia%2C%20I.%2C%20Zafeiriou%2C%20S.%3A%20Dense%203d%20face%20decoding%20over%202500fps%3A%20joint%20texture%20%26%20shape%20convolutional%20mesh%20decoders.%20In%3A%20CVPR%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="75.">
              <p class="c-article-references__text" id="ref-CR75">
               Zhu, M., Shi, D., Zheng, M., Sadiq, M.: Robust facial landmark detection via occlusion-adaptive deep networks. In: CVPR (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR75-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhu%2C%20M.%2C%20Shi%2C%20D.%2C%20Zheng%2C%20M.%2C%20Sadiq%2C%20M.%3A%20Robust%20facial%20landmark%20detection%20via%20occlusion-adaptive%20deep%20networks.%20In%3A%20CVPR%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="76.">
              <p class="c-article-references__text" id="ref-CR76">
               Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: a 3d solution. In: CVPR (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR76-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhu%2C%20X.%2C%20Lei%2C%20Z.%2C%20Liu%2C%20X.%2C%20Shi%2C%20H.%2C%20Li%2C%20S.Z.%3A%20Face%20alignment%20across%20large%20poses%3A%20a%203d%20solution.%20In%3A%20CVPR%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="77.">
              <p class="c-article-references__text" id="ref-CR77">
               Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: a 3d solution. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 146–155 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR77-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhu%2C%20X.%2C%20Lei%2C%20Z.%2C%20Liu%2C%20X.%2C%20Shi%2C%20H.%2C%20Li%2C%20S.Z.%3A%20Face%20alignment%20across%20large%20poses%3A%20a%203d%20solution.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%20146%E2%80%93155%20%282016%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="78.">
              <p class="c-article-references__text" id="ref-CR78">
               Zollhöfer, M., et al.: State of the art on monocular 3d face reconstruction, tracking, and applications. Comput. Graph. Forum
               <b>
                37
               </b>
               (2), 523–550 (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR78-links">
               <a aria-label="CrossRef reference 78" data-doi="10.1111/cgf.13382" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1111/cgf.13382" href="https://doi-org.proxy.lib.ohio-state.edu/10.1111%2Fcgf.13382" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 78" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=State%20of%20the%20art%20on%20monocular%203d%20face%20reconstruction%2C%20tracking%2C%20and%20applications&amp;journal=Comput.%20Graph.%20Forum&amp;volume=37&amp;issue=2&amp;pages=523-550&amp;publication_year=2018&amp;author=Zollh%C3%B6fer%2CM" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
            </ol>
            <p class="c-article-references__download u-hide-print">
             <a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-031-19778-9_10?format=refman&amp;flavour=references" rel="nofollow">
              Download references
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </p>
           </div>
          </div>
         </div>
        </section>
       </div>
       <section data-title="Acknowledgements" lang="en">
        <div class="c-article-section" id="Ack1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">
          <span class="c-article-section__title-number">
          </span>
          Acknowledgements
         </h2>
         <div class="c-article-section__content" id="Ack1-content">
          <p>
           Thanks to Chirag Raman and Jamie Shotton for their contributions, and Jiaolong Yang and Timo Bolkart for help with evaluation.
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="author-information" data-title="Author information">
        <div class="c-article-section" id="author-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">
          <span class="c-article-section__title-number">
          </span>
          Author information
         </h2>
         <div class="c-article-section__content" id="author-information-content">
          <h3 class="c-article__sub-heading" id="affiliations">
           Authors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff12">
            <p class="c-article-author-affiliation__address">
             Microsoft, Cambridge, UK
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Erroll Wood, Tadas Baltrušaitis, Charlie Hewitt, Matthew Johnson, Jingjing Shen, Daniel Wilde, Stephan Garbin, Toby Sharp &amp; Tom Cashman
            </p>
           </li>
           <li id="Aff13">
            <p class="c-article-author-affiliation__address">
             Microsoft, Belgrade, Serbia
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Nikola Milosavljević &amp; Ivan Stojiljković
            </p>
           </li>
           <li id="Aff14">
            <p class="c-article-author-affiliation__address">
             Microsoft, Zurich, Switzerland
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Julien Valentin
            </p>
           </li>
          </ol>
          <div class="u-js-hide u-hide-print" data-test="author-info">
           <span class="c-article__sub-heading">
            Authors
           </span>
           <ol class="c-article-authors-search u-list-reset">
            <li id="auth-Erroll-Wood">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Erroll Wood
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Erroll%20Wood" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Erroll%20Wood" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Erroll%20Wood%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Tadas-Baltru_aitis">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Tadas Baltrušaitis
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Tadas%20Baltru%C5%A1aitis" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Tadas%20Baltru%C5%A1aitis" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tadas%20Baltru%C5%A1aitis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Charlie-Hewitt">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Charlie Hewitt
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Charlie%20Hewitt" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Charlie%20Hewitt" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Charlie%20Hewitt%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Matthew-Johnson">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Matthew Johnson
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Matthew%20Johnson" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Matthew%20Johnson" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Matthew%20Johnson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Jingjing-Shen">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Jingjing Shen
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Jingjing%20Shen" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Jingjing%20Shen" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jingjing%20Shen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Nikola-Milosavljevi_">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Nikola Milosavljević
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Nikola%20Milosavljevi%C4%87" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Nikola%20Milosavljevi%C4%87" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nikola%20Milosavljevi%C4%87%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Daniel-Wilde">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Daniel Wilde
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Daniel%20Wilde" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Daniel%20Wilde" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daniel%20Wilde%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Stephan-Garbin">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Stephan Garbin
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Stephan%20Garbin" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Stephan%20Garbin" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stephan%20Garbin%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Toby-Sharp">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Toby Sharp
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Toby%20Sharp" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Toby%20Sharp" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Toby%20Sharp%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Ivan-Stojiljkovi_">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Ivan Stojiljković
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Ivan%20Stojiljkovi%C4%87" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Ivan%20Stojiljkovi%C4%87" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ivan%20Stojiljkovi%C4%87%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Tom-Cashman">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Tom Cashman
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Tom%20Cashman" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Tom%20Cashman" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tom%20Cashman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Julien-Valentin">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Julien Valentin
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Julien%20Valentin" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Julien%20Valentin" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Julien%20Valentin%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
           </ol>
          </div>
          <h3 class="c-article__sub-heading" id="corresponding-author">
           Corresponding author
          </h3>
          <p id="corresponding-author-list">
           Correspondence to
           <a href="mailto:errollw@gmail.com" id="corresp-c1">
            Erroll Wood
           </a>
           .
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="editor-information" data-title="Editor information">
        <div class="c-article-section" id="editor-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="editor-information">
          <span class="c-article-section__title-number">
          </span>
          Editor information
         </h2>
         <div class="c-article-section__content" id="editor-information-content">
          <h3 class="c-article__sub-heading" id="editor-affiliations">
           Editors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff7">
            <p class="c-article-author-affiliation__address">
             Tel Aviv University, Tel Aviv, Israel
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Shai Avidan
            </p>
           </li>
           <li id="Aff8">
            <p class="c-article-author-affiliation__address">
             University College London, London, UK
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Gabriel Brostow
            </p>
           </li>
           <li id="Aff9">
            <p class="c-article-author-affiliation__address">
             Google AI, Accra, Ghana
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Moustapha Cissé
            </p>
           </li>
           <li id="Aff10">
            <p class="c-article-author-affiliation__address">
             University of Catania, Catania, Italy
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Giovanni Maria Farinella
            </p>
           </li>
           <li id="Aff11">
            <p class="c-article-author-affiliation__address">
             Facebook (United States), Menlo Park, CA, USA
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Tal Hassner
            </p>
           </li>
          </ol>
         </div>
        </div>
       </section>
       <section data-title="Electronic supplementary material">
        <div class="c-article-section" id="Sec17-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">
          <span class="c-article-section__title-number">
           1
          </span>
          Electronic supplementary material
         </h2>
         <div class="c-article-section__content" id="Sec17-content">
          <div data-test="supplementary-info">
           <div class="c-article-figshare-container" data-test="figshare-container" id="figshareContainer">
           </div>
           <p>
            Below is the link to the electronic supplementary material.
           </p>
           <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1">
            <h3 class="c-article-supplementary__title u-h3">
             <a class="print-link" data-supp-info-image="" data-test="supp-info-link" data-track="click" data-track-action="view supplementary info" data-track-label="supplementary material 1 (pdf 18084 kb)" href="https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_MOESM1_ESM.pdf">
              Supplementary material 1 (pdf 18084 KB)
             </a>
            </h3>
           </div>
          </div>
         </div>
        </div>
       </section>
       <section data-title="Rights and permissions" lang="en">
        <div class="c-article-section" id="rightslink-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">
          <span class="c-article-section__title-number">
          </span>
          Rights and permissions
         </h2>
         <div class="c-article-section__content" id="rightslink-content">
          <p class="c-article-rights" data-test="rightslink-content">
           <a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?publisherName=SpringerNature&amp;orderBeanReset=true&amp;orderSource=SpringerLink&amp;title=3D%20Face%20Reconstruction%20with%C2%A0Dense%20Landmarks&amp;author=Erroll%20Wood%2C%20Tadas%20Baltru%C5%A1aitis%2C%20Charlie%20Hewitt%20et%20al&amp;contentID=10.1007%2F978-3-031-19778-9_10&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20license%20to%20Springer%20Nature%20Switzerland%20AG&amp;publication=eBook&amp;publicationDate=2022&amp;startPage=160&amp;endPage=177&amp;imprint=The%20Author%28s%29%2C%20under%20exclusive%20license%20to%20Springer%20Nature%20Switzerland%20AG">
            Reprints and Permissions
           </a>
          </p>
         </div>
        </div>
       </section>
       <section data-title="Copyright information">
        <div class="c-article-section" id="copyright-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="copyright-information">
          <span class="c-article-section__title-number">
          </span>
          Copyright information
         </h2>
         <div class="c-article-section__content" id="copyright-information-content">
          <p>
           © 2022 The Author(s), under exclusive license to Springer Nature Switzerland AG
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="chapter-info" data-title="About this paper" lang="en">
        <div class="c-article-section" id="chapter-info-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="chapter-info">
          <span class="c-article-section__title-number">
          </span>
          About this paper
         </h2>
         <div class="c-article-section__content" id="chapter-info-content">
          <div class="c-bibliographic-information">
           <div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border">
            <a data-crossmark="10.1007/978-3-031-19778-9_10" data-test="crossmark" data-track="click" data-track-action="Click Crossmark" data-track-label="link" href="https://crossmark-crossref-org.proxy.lib.ohio-state.edu/dialog/?doi=10.1007/978-3-031-19778-9_10" rel="noopener" target="_blank">
             <img alt="Check for updates. Verify currency and authenticity via CrossMark" height="81" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57"/>
            </a>
           </div>
           <div class="c-bibliographic-information__column">
            <h3 class="c-article__sub-heading" id="citeas">
             Cite this paper
            </h3>
            <p class="c-bibliographic-information__citation" data-test="bibliographic-information__cite_this_chapter">
             Wood, E.
             <i>
              et al.
             </i>
             (2022).  3D Face Reconstruction with Dense Landmarks.

                     In: Avidan, S., Brostow, G., Cissé, M., Farinella, G.M., Hassner, T. (eds) Computer Vision – ECCV 2022. ECCV 2022. Lecture Notes in Computer Science, vol 13673. Springer, Cham. https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-031-19778-9_10
            </p>
            <h3 class="c-bibliographic-information__download-citation u-mb-8 u-mt-16 u-hide-print">
             Download citation
            </h3>
            <ul class="c-bibliographic-information__download-citation-list">
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-031-19778-9_10?format=refman&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .RIS file">
               .RIS
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-031-19778-9_10?format=endnote&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .ENW file">
               .ENW
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-031-19778-9_10?format=bibtex&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .BIB file">
               .BIB
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
            </ul>
            <ul class="c-bibliographic-information__list u-mb-24" data-test="publication-history">
             <li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--chapter-doi">
              <p data-test="bibliographic-information__doi">
               <abbr title="Digital Object Identifier">
                DOI
               </abbr>
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                https://doi.org/10.1007/978-3-031-19778-9_10
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p>
               Published
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2022-11-03">
                 03 November 2022
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__publisher-name">
               Publisher Name
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                Springer, Cham
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__pisbn">
               Print ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-031-19777-2
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__eisbn">
               Online ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-031-19778-9
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__package">
               eBook Packages
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=11645&amp;facet-start-year=2022&amp;facet-end-year=2022">
                 Computer Science
                </a>
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=43710&amp;facet-start-year=2022&amp;facet-end-year=2022">
                 Computer Science (R0)
                </a>
               </span>
              </p>
             </li>
            </ul>
            <div data-component="share-box">
             <div class="c-article-share-box u-display-block">
              <h3 class="c-article__sub-heading">
               Share this paper
              </h3>
              <p class="c-article-share-box__description">
               Anyone you share the following link with will be able to read this content:
              </p>
              <button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url">
               Get shareable link
              </button>
              <div class="js-no-share-url-container u-display-none" hidden="">
               <p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">
                Sorry, a shareable link is not currently available for this article.
               </p>
              </div>
              <div class="js-share-url-container u-display-none" hidden="">
               <p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url">
               </p>
               <button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url">
                Copy to clipboard
               </button>
              </div>
              <p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
               Provided by the Springer Nature SharedIt content-sharing initiative
              </p>
             </div>
            </div>
            <div data-component="chapter-info-list">
            </div>
           </div>
          </div>
         </div>
        </div>
       </section>
      </div>
     </article>
    </main>
    <div class="c-article-extras u-text-sm u-hide-print" data-container-type="reading-companion" data-track-component="conference paper" id="sidebar">
     <aside>
      <div class="js-context-bar-sticky-point-desktop" data-test="download-article-link-wrapper">
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-031-19778-9.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-031-19778-9.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div data-test="editorial-summary">
      </div>
      <div class="c-reading-companion">
       <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky" style="top: 40px;">
        <ul class="c-reading-companion__tabs" role="tablist">
         <li role="presentation">
          <button aria-controls="tabpanel-sections" aria-selected="true" class="c-reading-companion__tab c-reading-companion__tab--active" data-tab-target="sections" data-track="click" data-track-action="sections tab" data-track-label="tab" id="tab-sections" role="tab">
           Sections
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-figures" aria-selected="false" class="c-reading-companion__tab" data-tab-target="figures" data-track="click" data-track-action="figures tab" data-track-label="tab" id="tab-figures" role="tab" tabindex="-1">
           Figures
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-references" aria-selected="false" class="c-reading-companion__tab" data-tab-target="references" data-track="click" data-track-action="references tab" data-track-label="tab" id="tab-references" role="tab" tabindex="-1">
           References
          </button>
         </li>
        </ul>
        <div aria-labelledby="tab-sections" class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections" role="tabpanel">
         <div class="c-reading-companion__scroll-pane" style="max-height: 4544px;">
          <ul class="c-reading-companion__sections-list">
           <li class="c-reading-companion__section-item c-reading-companion__section-item--active" id="rc-sec-Abs1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Abstract" href="#Abs1">
             <span class="c-article-section__title-number">
             </span>
             Abstract
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Introduction" href="#Sec1">
             <span class="c-article-section__title-number">
              1
             </span>
             Introduction
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec2">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Related Work" href="#Sec2">
             <span class="c-article-section__title-number">
              2
             </span>
             Related Work
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec3">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Method" href="#Sec3">
             <span class="c-article-section__title-number">
              3
             </span>
             Method
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec8">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Evaluation" href="#Sec8">
             <span class="c-article-section__title-number">
              4
             </span>
             Evaluation
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec16">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Limitations and Future Work" href="#Sec16">
             <span class="c-article-section__title-number">
              5
             </span>
             Limitations and Future Work
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Bib1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: References" href="#Bib1">
             <span class="c-article-section__title-number">
             </span>
             References
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Ack1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Acknowledgements" href="#Ack1">
             <span class="c-article-section__title-number">
             </span>
             Acknowledgements
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-author-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Author information" href="#author-information">
             <span class="c-article-section__title-number">
             </span>
             Author information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-editor-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Editor information" href="#editor-information">
             <span class="c-article-section__title-number">
             </span>
             Editor information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec17">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Electronic supplementary material" href="#Sec17">
             <span class="c-article-section__title-number">
              1
             </span>
             Electronic supplementary material
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-rightslink">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Rights and permissions" href="#rightslink">
             <span class="c-article-section__title-number">
             </span>
             Rights and permissions
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-copyright-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Copyright information" href="#copyright-information">
             <span class="c-article-section__title-number">
             </span>
             Copyright information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-chapter-info">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: About this paper" href="#chapter-info">
             <span class="c-article-section__title-number">
             </span>
             About this paper
            </a>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-figures" class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ul class="c-reading-companion__figures-list">
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig1_HTML.png?"/>
              <img alt="figure 1" aria-describedby="rc-Fig1" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig1_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/1" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig2_HTML.png?"/>
              <img alt="figure 2" aria-describedby="rc-Fig2" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig2_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/2" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig3_HTML.png?"/>
              <img alt="figure 3" aria-describedby="rc-Fig3" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig3_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/3" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig4_HTML.png?"/>
              <img alt="figure 4" aria-describedby="rc-Fig4" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig4_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/4" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig5_HTML.png?"/>
              <img alt="figure 5" aria-describedby="rc-Fig5" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig5_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/5" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig6_HTML.png?"/>
              <img alt="figure 6" aria-describedby="rc-Fig6" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig6_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/6" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig7_HTML.png?"/>
              <img alt="figure 7" aria-describedby="rc-Fig7" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig7_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/7" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig8">
               Fig. 8.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig8_HTML.png?"/>
              <img alt="figure 8" aria-describedby="rc-Fig8" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig8_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig8">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/8" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig9">
               Fig. 9.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig9_HTML.png?"/>
              <img alt="figure 9" aria-describedby="rc-Fig9" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig9_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig9">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/9" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig10">
               Fig. 10.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig10_HTML.png?"/>
              <img alt="figure 10" aria-describedby="rc-Fig10" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig10_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig10">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/10" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig11">
               Fig. 11.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig11_HTML.png?"/>
              <img alt="figure 11" aria-describedby="rc-Fig11" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig11_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig11">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/11" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig12">
               Fig. 12.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig12_HTML.png?"/>
              <img alt="figure 12" aria-describedby="rc-Fig12" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig12_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig12">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/12" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig13">
               Fig. 13.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig13_HTML.png?"/>
              <img alt="figure 13" aria-describedby="rc-Fig13" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig13_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig13">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/13" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig14">
               Fig. 14.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig14_HTML.png?"/>
              <img alt="figure 14" aria-describedby="rc-Fig14" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19778-9_10/MediaObjects/539940_1_En_10_Fig14_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig14">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19778-9_10/figures/14" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-references" class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ol class="c-reading-companion__references-list c-reading-companion__references-list--numeric">
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR1">
             Alp Güler, R., Trigeorgis, G., Antonakos, E., Snape, P., Zafeiriou, S., Kokkinos, I.: DenseReg: fully convolutional dense shape regression in-the-wild. In: CVPR (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Alp%20G%C3%BCler%2C%20R.%2C%20Trigeorgis%2C%20G.%2C%20Antonakos%2C%20E.%2C%20Snape%2C%20P.%2C%20Zafeiriou%2C%20S.%2C%20Kokkinos%2C%20I.%3A%20DenseReg%3A%20fully%20convolutional%20dense%20shape%20regression%20in-the-wild.%20In%3A%20CVPR%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR2">
             Bagdanov, A.D., Del Bimbo, A., Masi, I.: The Florence 2D/3D hybrid face dataset. In: Workshop on Human Gesture and Behavior Understanding. ACM (2011)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bagdanov%2C%20A.D.%2C%20Del%20Bimbo%2C%20A.%2C%20Masi%2C%20I.%3A%20The%20Florence%202D%2F3D%20hybrid%20face%20dataset.%20In%3A%20Workshop%20on%20Human%20Gesture%20and%20Behavior%20Understanding.%20ACM%20%282011%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR3">
             Bai, Z., Cui, Z., Liu, X., Tan, P.: Riggable 3D face reconstruction via in-network optimization. In: CVPR (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bai%2C%20Z.%2C%20Cui%2C%20Z.%2C%20Liu%2C%20X.%2C%20Tan%2C%20P.%3A%20Riggable%203D%20face%20reconstruction%20via%20in-network%20optimization.%20In%3A%20CVPR%20%282021%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR4">
             Beeler, T., Bickel, B., Beardsley, P., Sumner, B., Gross, M.: High-quality single-shot capture of facial geometry. In: ACM Transactions on Graphics (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Beeler%2C%20T.%2C%20Bickel%2C%20B.%2C%20Beardsley%2C%20P.%2C%20Sumner%2C%20B.%2C%20Gross%2C%20M.%3A%20High-quality%20single-shot%20capture%20of%20facial%20geometry.%20In%3A%20ACM%20Transactions%20on%20Graphics%20%282010%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR5">
             Beeler, T., et al.: High-quality passive facial performance capture using anchor frames. In: ACM Transactions on Graphics (2011)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Beeler%2C%20T.%2C%20et%20al.%3A%20High-quality%20passive%20facial%20performance%20capture%20using%20anchor%20frames.%20In%3A%20ACM%20Transactions%20on%20Graphics%20%282011%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR6">
             Blanz, V., Vetter, T.: A morphable model for the synthesis of 3D faces. In: Computer Graphics and Interactive Techniques (1999)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Blanz%2C%20V.%2C%20Vetter%2C%20T.%3A%20A%20morphable%20model%20for%20the%20synthesis%20of%203D%20faces.%20In%3A%20Computer%20Graphics%20and%20Interactive%20Techniques%20%281999%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR7">
             Blanz, V., Vetter, T.: Face recognition based on fitting a 3d morphable model. TPAMI
             <b>
              25
             </b>
             (9), 1063–1074 (2003)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1109/TPAMI.2003.1227983" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2003.1227983">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Face%20recognition%20based%20on%20fitting%20a%203d%20morphable%20model&amp;journal=TPAMI&amp;volume=25&amp;issue=9&amp;pages=1063-1074&amp;publication_year=2003&amp;author=Blanz%2CV&amp;author=Vetter%2CT">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR8">
             Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep It SMPL: automatic estimation of 3d human pose and shape from a single image. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9909, pp. 561–578. Springer, Cham (2016).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-319-46454-1_34" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46454-1_34">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-319-46454-1_34
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-319-46454-1_34" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-319-46454-1_34">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Keep%20It%20SMPL%3A%20automatic%20estimation%20of%203d%20human%20pose%20and%20shape%20from%20a%20single%20image&amp;pages=561-578&amp;publication_year=2016%202016%202016&amp;author=Bogo%2CF&amp;author=Kanazawa%2CA&amp;author=Lassner%2CC&amp;author=Gehler%2CP&amp;author=Romero%2CJ&amp;author=Black%2CMJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR9">
             Bradley, D., Heidrich, W., Popa, T., Sheffer, A.: High resolution passive facial performance capture. In: ACM Transactions on Graphics, vol. 29, no. 4 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bradley%2C%20D.%2C%20Heidrich%2C%20W.%2C%20Popa%2C%20T.%2C%20Sheffer%2C%20A.%3A%20High%20resolution%20passive%20facial%20performance%20capture.%20In%3A%20ACM%20Transactions%20on%20Graphics%2C%20vol.%2029%2C%20no.%204%20%282010%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR10">
             Browatzki, B., Wallraven, C.: 3FabRec: Fast Few-shot Face alignment by Reconstruction. In: CVPR (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Browatzki%2C%20B.%2C%20Wallraven%2C%20C.%3A%203FabRec%3A%20Fast%20Few-shot%20Face%20alignment%20by%20Reconstruction.%20In%3A%20CVPR%20%282020%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR11">
             Bulat, A., Sanchez, E., Tzimiropoulos, G.: Subpixel heatmap regression for facial landmark Localization. In: BMVC (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bulat%2C%20A.%2C%20Sanchez%2C%20E.%2C%20Tzimiropoulos%2C%20G.%3A%20Subpixel%20heatmap%20regression%20for%20facial%20landmark%20Localization.%20In%3A%20BMVC%20%282021%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR12">
             Bulat, A., Tzimiropoulos, G.: How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3D facial landmarks). In: ICCV (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bulat%2C%20A.%2C%20Tzimiropoulos%2C%20G.%3A%20How%20far%20are%20we%20from%20solving%20the%202d%20%26%203d%20face%20alignment%20problem%3F%20%28and%20a%20dataset%20of%20230%2C000%203D%20facial%20landmarks%29.%20In%3A%20ICCV%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR13">
             Cao, C., Chai, M., Woodford, O., Luo, L.: Stabilized real-time face tracking via a learned dynamic rigidity prior. ACM Trans. Graph.
             <b>
              37
             </b>
             (6), 1–11 (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Stabilized%20real-time%20face%20tracking%20via%20a%20learned%20dynamic%20rigidity%20prior&amp;journal=ACM%20Trans.%20Graph.&amp;volume=37&amp;issue=6&amp;pages=1-11&amp;publication_year=2018&amp;author=Cao%2CC&amp;author=Chai%2CM&amp;author=Woodford%2CO&amp;author=Luo%2CL">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR14">
             Chandran, P., Bradley, D., Gross, M., Beeler, T.: Semantic deep face models. In: International Conference on 3D Vision (3DV) (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Chandran%2C%20P.%2C%20Bradley%2C%20D.%2C%20Gross%2C%20M.%2C%20Beeler%2C%20T.%3A%20Semantic%20deep%20face%20models.%20In%3A%20International%20Conference%20on%203D%20Vision%20%283DV%29%20%282020%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR15">
             Cong, M., Lan, L., Fedkiw, R.: Local geometric indexing of high resolution data for facial reconstruction from sparse markers. CoRR abs/1903.00119 (2019).
             <a data-track="click" data-track-action="external reference" data-track-label="http://www.arxiv-org.proxy.lib.ohio-state.edu/abs/1903.00119" href="http://www.arxiv-org.proxy.lib.ohio-state.edu/abs/1903.00119">
              www.arxiv.org/abs/1903.00119
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR16">
             Deng, J., Guo, J., Ververas, E., Kotsia, I., Zafeiriou, S.: RetinaFace: single-shot multi-level face localisation in the wild. In: CVPR (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Deng%2C%20J.%2C%20Guo%2C%20J.%2C%20Ververas%2C%20E.%2C%20Kotsia%2C%20I.%2C%20Zafeiriou%2C%20S.%3A%20RetinaFace%3A%20single-shot%20multi-level%20face%20localisation%20in%20the%20wild.%20In%3A%20CVPR%20%282020%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR17">
             Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face reconstruction with weakly-supervised learning: from single image to image set. In: CVPR Workshops (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Deng%2C%20Y.%2C%20Yang%2C%20J.%2C%20Xu%2C%20S.%2C%20Chen%2C%20D.%2C%20Jia%2C%20Y.%2C%20Tong%2C%20X.%3A%20Accurate%203d%20face%20reconstruction%20with%20weakly-supervised%20learning%3A%20from%20single%20image%20to%20image%20set.%20In%3A%20CVPR%20Workshops%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR18">
             Dib, A., et al.: Practical face reconstruction via differentiable ray tracing. Comput. Graph. Forum
             <b>
              40
             </b>
             (2), 153–164 (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1111/cgf.142622" href="https://doi-org.proxy.lib.ohio-state.edu/10.1111%2Fcgf.142622">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Practical%20face%20reconstruction%20via%20differentiable%20ray%20tracing&amp;journal=Comput.%20Graph.%20Forum&amp;volume=40&amp;issue=2&amp;pages=153-164&amp;publication_year=2021&amp;author=Dib%2CA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR19">
             Dib, A., Thebault, C., Ahn, J., Gosselin, P.H., Theobalt, C., Chevallier, L.: Towards high fidelity monocular face reconstruction with rich reflectance using self-supervised learning and ray tracing. In: CVPR (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Dib%2C%20A.%2C%20Thebault%2C%20C.%2C%20Ahn%2C%20J.%2C%20Gosselin%2C%20P.H.%2C%20Theobalt%2C%20C.%2C%20Chevallier%2C%20L.%3A%20Towards%20high%20fidelity%20monocular%20face%20reconstruction%20with%20rich%20reflectance%20using%20self-supervised%20learning%20and%20ray%20tracing.%20In%3A%20CVPR%20%282021%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR20">
             Dou, P., Kakadiaris, I.A.: Multi-view 3D face reconstruction with deep recurrent neural networks. Image Vis. Comput.
             <b>
              80
             </b>
             , 80–91 (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1016/j.imavis.2018.09.004" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.imavis.2018.09.004">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Multi-view%203D%20face%20reconstruction%20with%20deep%20recurrent%20neural%20networks&amp;journal=Image%20Vis.%20Comput.&amp;volume=80&amp;pages=80-91&amp;publication_year=2018&amp;author=Dou%2CP&amp;author=Kakadiaris%2CIA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR21">
             Dou, P., Shah, S.K., Kakadiaris, I.A.: End-to-end 3D face reconstruction with deep neural networks. In: CVPR (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Dou%2C%20P.%2C%20Shah%2C%20S.K.%2C%20Kakadiaris%2C%20I.A.%3A%20End-to-end%203D%20face%20reconstruction%20with%20deep%20neural%20networks.%20In%3A%20CVPR%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR22">
             Falcon, W., et al.: Pytorch lightning
             <b>
              3
             </b>
             (6) (2019). GitHub. Note.
             <a data-track="click" data-track-action="external reference" data-track-label="https://github.com/PyTorchLightning/pytorch-lightning" href="https://github.com/PyTorchLightning/pytorch-lightning">
              https://github.com/PyTorchLightning/pytorch-lightning
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR23">
             Feng, Y., Feng, H., Black, M.J., Bolkart, T.: Learning an animatable detailed 3D face model from in-the-wild images. ACM Trans. Graph. (ToG)
             <b>
              40
             </b>
             (4), 1–13 (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/3450626.3459936" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3450626.3459936">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Learning%20an%20animatable%20detailed%203D%20face%20model%20from%20in-the-wild%20images&amp;journal=ACM%20Trans.%20Graph.%20%28ToG%29&amp;volume=40&amp;issue=4&amp;pages=1-13&amp;publication_year=2021&amp;author=Feng%2CY&amp;author=Feng%2CH&amp;author=Black%2CMJ&amp;author=Bolkart%2CT">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR24">
             Feng, Y., Wu, F., Shao, X., Wang, Y., Zhou, X.: Joint 3d face reconstruction and dense alignment with position map regression network. In: ECCV (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Feng%2C%20Y.%2C%20Wu%2C%20F.%2C%20Shao%2C%20X.%2C%20Wang%2C%20Y.%2C%20Zhou%2C%20X.%3A%20Joint%203d%20face%20reconstruction%20and%20dense%20alignment%20with%20position%20map%20regression%20network.%20In%3A%20ECCV%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR25">
             Garrido, P., et al.: Reconstruction of personalized 3d face rigs from monocular video. ACM Trans. Graph.
             <b>
              35
             </b>
             (3), 1–15 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/2890493" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2890493">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Reconstruction%20of%20personalized%203d%20face%20rigs%20from%20monocular%20video&amp;journal=ACM%20Trans.%20Graph.&amp;volume=35&amp;issue=3&amp;pages=1-15&amp;publication_year=2016&amp;author=Garrido%2CP">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR26">
             Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., Freeman, W.T.: Unsupervised training for 3d morphable model regression. In: CVPR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Genova%2C%20K.%2C%20Cole%2C%20F.%2C%20Maschinot%2C%20A.%2C%20Sarna%2C%20A.%2C%20Vlasic%2C%20D.%2C%20Freeman%2C%20W.T.%3A%20Unsupervised%20training%20for%203d%20morphable%20model%20regression.%20In%3A%20CVPR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR27">
             Gerig, T., et al.: Morphable face models-an open framework. In: Automatic Face &amp; Gesture Recognition (FG). IEEE (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Gerig%2C%20T.%2C%20et%20al.%3A%20Morphable%20face%20models-an%20open%20framework.%20In%3A%20Automatic%20Face%20%26%20Gesture%20Recognition%20%28FG%29.%20IEEE%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR28">
             Grishchenko, I., Ablavatski, A., Kartynnik, Y., Raveendran, K., Grundmann, M.: Attention mesh: high-fidelity face mesh prediction in real-time. In: CVPR Workshops (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Grishchenko%2C%20I.%2C%20Ablavatski%2C%20A.%2C%20Kartynnik%2C%20Y.%2C%20Raveendran%2C%20K.%2C%20Grundmann%2C%20M.%3A%20Attention%20mesh%3A%20high-fidelity%20face%20mesh%20prediction%20in%20real-time.%20In%3A%20CVPR%20Workshops%20%282020%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR29">
             Güler, R.A., Neverova, N., Kokkinos, I.: Densepose: dense human pose estimation in the wild. In: CVPR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=G%C3%BCler%2C%20R.A.%2C%20Neverova%2C%20N.%2C%20Kokkinos%2C%20I.%3A%20Densepose%3A%20dense%20human%20pose%20estimation%20in%20the%20wild.%20In%3A%20CVPR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR30">
             Guo, J., Zhu, X., Yang, Y., Yang, F., Lei, Z., Li, S.Z.: Towards fast, accurate and stable 3d dense face alignment. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12364, pp. 152–168. Springer, Cham (2020).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-030-58529-7_10" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58529-7_10">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58529-7_10
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-030-58529-7_10" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-030-58529-7_10">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Towards%20fast%2C%20accurate%20and%20stable%203d%20dense%20face%20alignment&amp;pages=152-168&amp;publication_year=2020%202020%202020&amp;author=Guo%2CJ&amp;author=Zhu%2CX&amp;author=Yang%2CY&amp;author=Yang%2CF&amp;author=Lei%2CZ&amp;author=Li%2CSZ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR31">
             Guo, Y., Cai, J., Jiang, B., Zheng, J., et al.: Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images. TPAMI
             <b>
              41
             </b>
             (6), 1294–1307 (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1109/TPAMI.2018.2837742" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTPAMI.2018.2837742">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Cnn-based%20real-time%20dense%20face%20reconstruction%20with%20inverse-rendered%20photo-realistic%20face%20images&amp;journal=TPAMI&amp;volume=41&amp;issue=6&amp;pages=1294-1307&amp;publication_year=2018&amp;author=Guo%2CY&amp;author=Cai%2CJ&amp;author=Jiang%2CB&amp;author=Zheng%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR32">
             Han, S., et al.: Megatrack: monochrome egocentric articulated hand-tracking for virtual reality. ACM Trans. Graph. (TOG)
             <b>
              39
             </b>
             (4), 1–87 (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/3386569.3392452" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3386569.3392452">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Megatrack%3A%20monochrome%20egocentric%20articulated%20hand-tracking%20for%20virtual%20reality&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=39&amp;issue=4&amp;pages=1-87&amp;publication_year=2020&amp;author=Han%2CS">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR33">
             Hassner, T., Harel, S., Paz, E., Enbar, R.: Effective face frontalization in unconstrained images. In: CVPR (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hassner%2C%20T.%2C%20Harel%2C%20S.%2C%20Paz%2C%20E.%2C%20Enbar%2C%20R.%3A%20Effective%20face%20frontalization%20in%20unconstrained%20images.%20In%3A%20CVPR%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR34">
             He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=He%2C%20K.%2C%20Zhang%2C%20X.%2C%20Ren%2C%20S.%2C%20Sun%2C%20J.%3A%20Deep%20residual%20learning%20for%20image%20recognition.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR35">
             Jeni, L.A., Cohn, J.F., Kanade, T.: Dense 3D face alignment from 2D videos in real-time. In: Automatic Face and Gesture Recognition (FG). IEEE (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Jeni%2C%20L.A.%2C%20Cohn%2C%20J.F.%2C%20Kanade%2C%20T.%3A%20Dense%203D%20face%20alignment%20from%202D%20videos%20in%20real-time.%20In%3A%20Automatic%20Face%20and%20Gesture%20Recognition%20%28FG%29.%20IEEE%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR36">
             Kartynnik, Y., Ablavatski, A., Grishchenko, I., Grundmann, M.: Real-time facial surface geometry from monocular video on mobile GPUs. In: CVPR Workshops (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kartynnik%2C%20Y.%2C%20Ablavatski%2C%20A.%2C%20Grishchenko%2C%20I.%2C%20Grundmann%2C%20M.%3A%20Real-time%20facial%20surface%20geometry%20from%20monocular%20video%20on%20mobile%20GPUs.%20In%3A%20CVPR%20Workshops%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR37">
             Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for computer vision? In: Advances in Neural Information Processing Systems, vol. 30 (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kendall%2C%20A.%2C%20Gal%2C%20Y.%3A%20What%20uncertainties%20do%20we%20need%20in%20bayesian%20deep%20learning%20for%20computer%20vision%3F%20In%3A%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%20vol.%2030%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR38">
             Kumar, A., et al.: Luvli face alignment: estimating landmarks’ location, uncertainty, and visibility likelihood. In: CVPR (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kumar%2C%20A.%2C%20et%20al.%3A%20Luvli%20face%20alignment%3A%20estimating%20landmarks%E2%80%99%20location%2C%20uncertainty%2C%20and%20visibility%20likelihood.%20In%3A%20CVPR%20%282020%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR39">
             Lewis, J.P., Cordner, M., Fong, N.: Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation. In: SIGGRAPH (2000)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Lewis%2C%20J.P.%2C%20Cordner%2C%20M.%2C%20Fong%2C%20N.%3A%20Pose%20space%20deformation%3A%20a%20unified%20approach%20to%20shape%20interpolation%20and%20skeleton-driven%20deformation.%20In%3A%20SIGGRAPH%20%282000%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR40">
             Li, T., Bolkart, T., Black, M.J., Li, H., Romero, J.: Learning a model of facial shape and expression from 4D scans. In: ACM Transactions on Graphics, (Proceedings SIGGRAPH Asia) (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Li%2C%20T.%2C%20Bolkart%2C%20T.%2C%20Black%2C%20M.J.%2C%20Li%2C%20H.%2C%20Romero%2C%20J.%3A%20Learning%20a%20model%20of%20facial%20shape%20and%20expression%20from%204D%20scans.%20In%3A%20ACM%20Transactions%20on%20Graphics%2C%20%28Proceedings%20SIGGRAPH%20Asia%29%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR41">
             Li, Y., Yang, S., Zhang, S., Wang, Z., Yang, W., Xia, S.T., Zhou, E.: Is 2d heatmap representation even necessary for human pose estimation? (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Li%2C%20Y.%2C%20Yang%2C%20S.%2C%20Zhang%2C%20S.%2C%20Wang%2C%20Z.%2C%20Yang%2C%20W.%2C%20Xia%2C%20S.T.%2C%20Zhou%2C%20E.%3A%20Is%202d%20heatmap%20representation%20even%20necessary%20for%20human%20pose%20estimation%3F%20%282021%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR42">
             Liu, D.C., Nocedal, J.: On the limited memory BFGS method for large scale optimization. Math. Program.
             <b>
              45
             </b>
             (1), 503–528 (1989).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/BF01589116" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/BF01589116">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/BF01589116
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/BF01589116" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2FBF01589116">
              CrossRef
             </a>
             <a data-track="click" data-track-action="mathscinet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=1038245">
              MathSciNet
             </a>
             <a data-track="click" data-track-action="math reference" data-track-label="link" href="http://www-emis-de.proxy.lib.ohio-state.edu/MATH-item?0696.90048">
              MATH
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=On%20the%20limited%20memory%20BFGS%20method%20for%20large%20scale%20optimization&amp;journal=Math.%20Program.&amp;doi=10.1007%2FBF01589116&amp;volume=45&amp;issue=1&amp;pages=503-528&amp;publication_year=1989&amp;author=Liu%2CDC&amp;author=Nocedal%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR43">
             Liu, F., Zhu, R., Zeng, D., Zhao, Q., Liu, X.: Disentangling features in 3D face shapes for joint face reconstruction and recognition. In: CVPR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liu%2C%20F.%2C%20Zhu%2C%20R.%2C%20Zeng%2C%20D.%2C%20Zhao%2C%20Q.%2C%20Liu%2C%20X.%3A%20Disentangling%20features%20in%203D%20face%20shapes%20for%20joint%20face%20reconstruction%20and%20recognition.%20In%3A%20CVPR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR44">
             Liu, Y., Jourabloo, A., Ren, W., Liu, X.: Dense face alignment. In: ICCV Workshops (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liu%2C%20Y.%2C%20Jourabloo%2C%20A.%2C%20Ren%2C%20W.%2C%20Liu%2C%20X.%3A%20Dense%20face%20alignment.%20In%3A%20ICCV%20Workshops%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR45">
             Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Loshchilov%2C%20I.%2C%20Hutter%2C%20F.%3A%20Decoupled%20weight%20decay%20regularization.%20In%3A%20ICLR%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR46">
             Morales, A., Piella, G., Sukno, F.M.: Survey on 3d face reconstruction from uncalibrated images. Comput. Sci. Rev.
             <b>
              40
             </b>
             , 100400 (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1016/j.cosrev.2021.100400" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.cosrev.2021.100400">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Survey%20on%203d%20face%20reconstruction%20from%20uncalibrated%20images&amp;journal=Comput.%20Sci.%20Rev.&amp;volume=40&amp;publication_year=2021&amp;author=Morales%2CA&amp;author=Piella%2CG&amp;author=Sukno%2CFM">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR47">
             Paszke, A., et al.: Pytorch: an imperative style, high-performance deep learning library. In: NeurIPS (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Paszke%2C%20A.%2C%20et%20al.%3A%20Pytorch%3A%20an%20imperative%20style%2C%20high-performance%20deep%20learning%20library.%20In%3A%20NeurIPS%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR48">
             Piotraschke, M., Blanz, V.: Automated 3D face reconstruction from multiple images using quality measures. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Piotraschke%2C%20M.%2C%20Blanz%2C%20V.%3A%20Automated%203D%20face%20reconstruction%20from%20multiple%20images%20using%20quality%20measures.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR49">
             Popa, T., South-Dickinson, I., Bradley, D., Sheffer, A., Heidrich, W.: Globally consistent space-time reconstruction. Comput. Graph. Forum
             <b>
              29
             </b>
             (5), 1633–1642 (2010)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1111/j.1467-8659.2010.01772.x" href="https://doi-org.proxy.lib.ohio-state.edu/10.1111%2Fj.1467-8659.2010.01772.x">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Globally%20consistent%20space-time%20reconstruction&amp;journal=Comput.%20Graph.%20Forum&amp;volume=29&amp;issue=5&amp;pages=1633-1642&amp;publication_year=2010&amp;author=Popa%2CT&amp;author=South-Dickinson%2CI&amp;author=Bradley%2CD&amp;author=Sheffer%2CA&amp;author=Heidrich%2CW">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR50">
             Richardson, E., Sela, M., Kimmel, R.: 3D face reconstruction by learning from synthetic data. In: 3DV. IEEE (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Richardson%2C%20E.%2C%20Sela%2C%20M.%2C%20Kimmel%2C%20R.%3A%203D%20face%20reconstruction%20by%20learning%20from%20synthetic%20data.%20In%3A%203DV.%20IEEE%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR51">
             Richardson, E., Sela, M., Or-El, R., Kimmel, R.: Learning detailed face reconstruction from a single image. In: CVPR (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Richardson%2C%20E.%2C%20Sela%2C%20M.%2C%20Or-El%2C%20R.%2C%20Kimmel%2C%20R.%3A%20Learning%20detailed%20face%20reconstruction%20from%20a%20single%20image.%20In%3A%20CVPR%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR52">
             Sagonas, C., Antonakos, E., Tzimiropoulos, G., Zafeiriou, S., Pantic, M.: 300 faces in-the-wild challenge: database and results. Image Vis. Computi. (IMAVIS)
             <b>
              47
             </b>
             , 3–18 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1016/j.imavis.2016.01.002" href="https://doi-org.proxy.lib.ohio-state.edu/10.1016%2Fj.imavis.2016.01.002">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=300%20faces%20in-the-wild%20challenge%3A%20database%20and%20results&amp;journal=Image%20Vis.%20Computi.%20%28IMAVIS%29&amp;volume=47&amp;pages=3-18&amp;publication_year=2016&amp;author=Sagonas%2CC&amp;author=Antonakos%2CE&amp;author=Tzimiropoulos%2CG&amp;author=Zafeiriou%2CS&amp;author=Pantic%2CM">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR53">
             Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenet V2: Inverted residuals and linear bottlenecks. In: CVPR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sandler%2C%20M.%2C%20Howard%2C%20A.%2C%20Zhu%2C%20M.%2C%20Zhmoginov%2C%20A.%2C%20Chen%2C%20L.C.%3A%20Mobilenet%20V2%3A%20Inverted%20residuals%20and%20linear%20bottlenecks.%20In%3A%20CVPR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR54">
             Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3d face shape and expression from an image without 3d supervision. In: CVPR (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sanyal%2C%20S.%2C%20Bolkart%2C%20T.%2C%20Feng%2C%20H.%2C%20Black%2C%20M.%3A%20Learning%20to%20regress%203d%20face%20shape%20and%20expression%20from%20an%20image%20without%203d%20supervision.%20In%3A%20CVPR%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR55">
             Seitz, S.M., Curless, B., Diebel, J., Scharstein, D., Szeliski, R.: A comparison and evaluation of multi-view stereo reconstruction algorithms. In: CVPR (2006)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Seitz%2C%20S.M.%2C%20Curless%2C%20B.%2C%20Diebel%2C%20J.%2C%20Scharstein%2C%20D.%2C%20Szeliski%2C%20R.%3A%20A%20comparison%20and%20evaluation%20of%20multi-view%20stereo%20reconstruction%20algorithms.%20In%3A%20CVPR%20%282006%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR56">
             Sela, M., Richardson, E., Kimmel, R.: Unrestricted facial geometry reconstruction using image-to-image translation. In: ICCV (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sela%2C%20M.%2C%20Richardson%2C%20E.%2C%20Kimmel%2C%20R.%3A%20Unrestricted%20facial%20geometry%20reconstruction%20using%20image-to-image%20translation.%20In%3A%20ICCV%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR57">
             Shang, J.: Self-supervised monocular 3d face reconstruction by occlusion-aware multi-view geometry consistency. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12360, pp. 53–70. Springer, Cham (2020).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-030-58555-6_4" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58555-6_4">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58555-6_4
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-030-58555-6_4" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-030-58555-6_4">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Self-supervised%20monocular%203d%20face%20reconstruction%20by%20occlusion-aware%20multi-view%20geometry%20consistency&amp;pages=53-70&amp;publication_year=2020%202020%202020&amp;author=Shang%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR58">
             Taylor, J., et al.: Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences. ACM Trans. Graph. (ToG)
             <b>
              35
             </b>
             (4), 1–12 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/2897824.2925965" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2897824.2925965">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Efficient%20and%20precise%20interactive%20hand%20tracking%20through%20joint%2C%20continuous%20optimization%20of%20pose%20and%20correspondences&amp;journal=ACM%20Trans.%20Graph.%20%28ToG%29&amp;volume=35&amp;issue=4&amp;pages=1-12&amp;publication_year=2016&amp;author=Taylor%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR59">
             Taylor, J., Shotton, J., Sharp, T., Fitzgibbon, A.: The vitruvian manifold: inferring dense correspondences for one-shot human pose estimation. In: CVPR (2012)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Taylor%2C%20J.%2C%20Shotton%2C%20J.%2C%20Sharp%2C%20T.%2C%20Fitzgibbon%2C%20A.%3A%20The%20vitruvian%20manifold%3A%20inferring%20dense%20correspondences%20for%20one-shot%20human%20pose%20estimation.%20In%3A%20CVPR%20%282012%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR60">
             Tewari, A., et al.: FML: face model learning from videos. In: CVPR (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tewari%2C%20A.%2C%20et%20al.%3A%20FML%3A%20face%20model%20learning%20from%20videos.%20In%3A%20CVPR%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR61">
             Tewari, A., et al: Self-supervised multi-level face model learning for monocular reconstruction at over 250 Hz. In: CVPR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tewari%2C%20A.%2C%20et%20al%3A%20Self-supervised%20multi-level%20face%20model%20learning%20for%20monocular%20reconstruction%20at%20over%20250%20Hz.%20In%3A%20CVPR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR62">
             Tewari, A., et al.: Mofa: model-based deep convolutional face autoencoder for unsupervised monocular reconstruction. In: ICCV Workshops (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tewari%2C%20A.%2C%20et%20al.%3A%20Mofa%3A%20model-based%20deep%20convolutional%20face%20autoencoder%20for%20unsupervised%20monocular%20reconstruction.%20In%3A%20ICCV%20Workshops%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR63">
             Thies, J., Zollhöfer, M., Nießner, M., Valgaerts, L., Stamminger, M., Theobalt, C.: Real-time expression transfer for facial reenactment. ACM Trans. Graph.
             <b>
              34
             </b>
             (6), 1–183 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/2816795.2818056" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2816795.2818056">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Real-time%20expression%20transfer%20for%20facial%20reenactment&amp;journal=ACM%20Trans.%20Graph.&amp;volume=34&amp;issue=6&amp;pages=1-183&amp;publication_year=2015&amp;author=Thies%2CJ&amp;author=Zollh%C3%B6fer%2CM&amp;author=Nie%C3%9Fner%2CM&amp;author=Valgaerts%2CL&amp;author=Stamminger%2CM&amp;author=Theobalt%2CC">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR64">
             Thies, J., Zollhöfer, M., Stamminger, M., Theobalt, C., Nießner, M.: Face2Face: real-time face capture and reenactment of RGB videos. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Thies%2C%20J.%2C%20Zollh%C3%B6fer%2C%20M.%2C%20Stamminger%2C%20M.%2C%20Theobalt%2C%20C.%2C%20Nie%C3%9Fner%2C%20M.%3A%20Face2Face%3A%20real-time%20face%20capture%20and%20reenactment%20of%20RGB%20videos.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR65">
             Tran, L., Liu, F., Liu, X.: Towards high-fidelity nonlinear 3D face morphable model. In: CVPR (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tran%2C%20L.%2C%20Liu%2C%20F.%2C%20Liu%2C%20X.%3A%20Towards%20high-fidelity%20nonlinear%203D%20face%20morphable%20model.%20In%3A%20CVPR%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR66">
             Tran, L., Liu, X.: Nonlinear 3d face morphable model. In: CVPR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tran%2C%20L.%2C%20Liu%2C%20X.%3A%20Nonlinear%203d%20face%20morphable%20model.%20In%3A%20CVPR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR67">
             Tuan Tran, A., Hassner, T., Masi, I., Medioni, G.: Regressing robust and discriminative 3D morphable models with a very deep neural network. In: CVPR (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tuan%20Tran%2C%20A.%2C%20Hassner%2C%20T.%2C%20Masi%2C%20I.%2C%20Medioni%2C%20G.%3A%20Regressing%20robust%20and%20discriminative%203D%20morphable%20models%20with%20a%20very%20deep%20neural%20network.%20In%3A%20CVPR%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR68">
             Wang, X., Bo, L., Fuxin, L.: Adaptive wing loss for robust face alignment via heatmap regression. In: ICCV (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wang%2C%20X.%2C%20Bo%2C%20L.%2C%20Fuxin%2C%20L.%3A%20Adaptive%20wing%20loss%20for%20robust%20face%20alignment%20via%20heatmap%20regression.%20In%3A%20ICCV%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR69">
             Wightman, R.: Pytorch image models (2019).
             <a data-track="click" data-track-action="external reference" data-track-label="https://www.github.com/rwightman/pytorch-image-models" href="https://www.github.com/rwightman/pytorch-image-models">
              https://www.github.com/rwightman/pytorch-image-models
             </a>
             ,
             <a data-track="click" data-track-action="external reference" data-track-label="10.5281/zenodo.4414861" href="https://doi-org.proxy.lib.ohio-state.edu/10.5281/zenodo.4414861">
              https://doi-org.proxy.lib.ohio-state.edu/10.5281/zenodo.4414861
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR70">
             Wood, E., et al.: Fake it till you make it: Face analysis in the wild using synthetic data alone (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wood%2C%20E.%2C%20et%20al.%3A%20Fake%20it%20till%20you%20make%20it%3A%20Face%20analysis%20in%20the%20wild%20using%20synthetic%20data%20alone%20%282021%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR71">
             Wu, W., Qian, C., Yang, S., Wang, Q., Cai, Y., Zhou, Q.: Look at boundary: a boundary-aware face alignment algorithm. In: CVPR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wu%2C%20W.%2C%20Qian%2C%20C.%2C%20Yang%2C%20S.%2C%20Wang%2C%20Q.%2C%20Cai%2C%20Y.%2C%20Zhou%2C%20Q.%3A%20Look%20at%20boundary%3A%20a%20boundary-aware%20face%20alignment%20algorithm.%20In%3A%20CVPR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR72">
             Yi, H., et al.: MMFace: a multi-metric regression network for unconstrained face reconstruction. In: CVPR (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yi%2C%20H.%2C%20et%20al.%3A%20MMFace%3A%20a%20multi-metric%20regression%20network%20for%20unconstrained%20face%20reconstruction.%20In%3A%20CVPR%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR73">
             Yoon, J.S., Shiratori, T., Yu, S.I., Park, H.S.: Self-supervised adaptation of high-fidelity face models for monocular performance tracking. In: CVPR (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yoon%2C%20J.S.%2C%20Shiratori%2C%20T.%2C%20Yu%2C%20S.I.%2C%20Park%2C%20H.S.%3A%20Self-supervised%20adaptation%20of%20high-fidelity%20face%20models%20for%20monocular%20performance%20tracking.%20In%3A%20CVPR%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR74">
             Zhou, Y., Deng, J., Kotsia, I., Zafeiriou, S.: Dense 3d face decoding over 2500fps: joint texture &amp; shape convolutional mesh decoders. In: CVPR (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhou%2C%20Y.%2C%20Deng%2C%20J.%2C%20Kotsia%2C%20I.%2C%20Zafeiriou%2C%20S.%3A%20Dense%203d%20face%20decoding%20over%202500fps%3A%20joint%20texture%20%26%20shape%20convolutional%20mesh%20decoders.%20In%3A%20CVPR%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR75">
             Zhu, M., Shi, D., Zheng, M., Sadiq, M.: Robust facial landmark detection via occlusion-adaptive deep networks. In: CVPR (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhu%2C%20M.%2C%20Shi%2C%20D.%2C%20Zheng%2C%20M.%2C%20Sadiq%2C%20M.%3A%20Robust%20facial%20landmark%20detection%20via%20occlusion-adaptive%20deep%20networks.%20In%3A%20CVPR%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR76">
             Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: a 3d solution. In: CVPR (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhu%2C%20X.%2C%20Lei%2C%20Z.%2C%20Liu%2C%20X.%2C%20Shi%2C%20H.%2C%20Li%2C%20S.Z.%3A%20Face%20alignment%20across%20large%20poses%3A%20a%203d%20solution.%20In%3A%20CVPR%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR77">
             Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: a 3d solution. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 146–155 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Zhu%2C%20X.%2C%20Lei%2C%20Z.%2C%20Liu%2C%20X.%2C%20Shi%2C%20H.%2C%20Li%2C%20S.Z.%3A%20Face%20alignment%20across%20large%20poses%3A%20a%203d%20solution.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%20146%E2%80%93155%20%282016%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR78">
             Zollhöfer, M., et al.: State of the art on monocular 3d face reconstruction, tracking, and applications. Comput. Graph. Forum
             <b>
              37
             </b>
             (2), 523–550 (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1111/cgf.13382" href="https://doi-org.proxy.lib.ohio-state.edu/10.1111%2Fcgf.13382">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=State%20of%20the%20art%20on%20monocular%203d%20face%20reconstruction%2C%20tracking%2C%20and%20applications&amp;journal=Comput.%20Graph.%20Forum&amp;volume=37&amp;issue=2&amp;pages=523-550&amp;publication_year=2018&amp;author=Zollh%C3%B6fer%2CM">
              Google Scholar
             </a>
            </p>
           </li>
          </ol>
         </div>
        </div>
       </div>
      </div>
     </aside>
    </div>
   </div>
   <div class="app-elements">
    <footer data-test="universal-footer">
     <div class="c-footer" data-track-component="unified-footer">
      <div class="c-footer__container">
       <div class="c-footer__grid c-footer__group--separator">
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Discover content
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="journals a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
            Journals A-Z
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="books a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/books/a/1">
            Books A-Z
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Publish with us
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="publish your research" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
            Publish your research
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="open access publishing" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/open-research/about/the-fundamentals-of-open-access-and-open-research">
            Open access publishing
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Products and services
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="our products" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/products">
            Our products
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="librarians" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/librarians">
            Librarians
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="societies" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/societies">
            Societies
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="partners and advertisers" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/partners">
            Partners and advertisers
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Our imprints
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Springer" data-track-label="link" href="https://www-springer-com.proxy.lib.ohio-state.edu/">
            Springer
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Nature Portfolio" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/">
            Nature Portfolio
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="BMC" data-track-label="link" href="https://www-biomedcentral-com.proxy.lib.ohio-state.edu/">
            BMC
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Palgrave Macmillan" data-track-label="link" href="https://www.palgrave.com/">
            Palgrave Macmillan
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Apress" data-track-label="link" href="https://www.apress.com/">
            Apress
           </a>
          </li>
         </ul>
        </div>
       </div>
      </div>
      <div class="c-footer__container">
       <nav aria-label="footer navigation">
        <ul class="c-footer__links">
         <li class="c-footer__item">
          <button class="c-footer__link" data-cc-action="preferences" data-track="click" data-track-action="Manage cookies" data-track-label="link">
           <span class="c-footer__button-text">
            Your privacy choices/Manage cookies
           </span>
          </button>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="california privacy statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/legal/ccpa">
           Your US state privacy rights
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="accessibility statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/info/accessibility">
           Accessibility statement
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="terms and conditions" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/termsandconditions">
           Terms and conditions
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="privacy policy" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/privacystatement">
           Privacy policy
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="help and support" data-track-label="link" href="https://support-springernature-com.proxy.lib.ohio-state.edu/en/support/home">
           Help and support
          </a>
         </li>
        </ul>
       </nav>
       <div class="c-footer__user">
        <p class="c-footer__user-info">
         <span data-test="footer-user-ip">
          3.128.143.42
         </span>
        </p>
        <p class="c-footer__user-info" data-test="footer-business-partners">
         OhioLINK Consortium (3000266689)  - Ohio State University Libraries (8200724141)
        </p>
       </div>
       <a class="c-footer__link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/">
        <img alt="Springer Nature" height="20" loading="lazy" src="/oscar-static/images/darwin/footer/img/logo-springernature_white-64dbfad7d8.svg" width="200"/>
       </a>
       <p class="c-footer__legal" data-test="copyright">
        © 2023 Springer Nature
       </p>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <div aria-hidden="true" class="u-visually-hidden">
   <!--?xml version="1.0" encoding="UTF-8"?-->
   <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <defs>
     <path d="M0 .74h56.72v55.24H0z" id="a">
     </path>
    </defs>
    <symbol id="icon-access" viewbox="0 0 18 18">
     <path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-account" viewbox="0 0 18 18">
     <path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-alert" viewbox="0 0 18 18">
     <path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-broad" viewbox="0 0 16 16">
     <path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)">
     </path>
    </symbol>
    <symbol id="icon-arrow-down" viewbox="0 0 16 16">
     <path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-left" viewbox="0 0 16 16">
     <path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-right" viewbox="0 0 16 16">
     <path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-sub" viewbox="0 0 16 16">
     <path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-up" viewbox="0 0 16 16">
     <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-article" viewbox="0 0 18 18">
     <path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-audio" viewbox="0 0 18 18">
     <path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-block" viewbox="0 0 24 24">
     <path d="m0 0h24v24h-24z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-book" viewbox="0 0 18 18">
     <path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-broad" viewbox="0 0 24 24">
     <path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)">
     </path>
    </symbol>
    <symbol id="icon-calendar" viewbox="0 0 18 18">
     <path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-cart" viewbox="0 0 18 18">
     <path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z">
     </path>
    </symbol>
    <symbol id="icon-chevron-less" viewbox="0 0 10 10">
     <path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)">
     </path>
    </symbol>
    <symbol id="icon-chevron-more" viewbox="0 0 10 10">
     <path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-chevron-right" viewbox="0 0 10 10">
     <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
     </path>
    </symbol>
    <symbol id="icon-circle-fill" viewbox="0 0 16 16">
     <path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-circle" viewbox="0 0 16 16">
     <path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-citation" viewbox="0 0 18 18">
     <path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-close" viewbox="0 0 16 16">
     <path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-collections" viewbox="0 0 18 18">
     <path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-compare" viewbox="0 0 18 18">
     <path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download-file" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download" viewbox="0 0 16 16">
     <path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-editors" viewbox="0 0 18 18">
     <path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-email" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-error" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-ethics" viewbox="0 0 18 18">
     <path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-expand">
     <path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-explore" viewbox="0 0 18 18">
     <path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-filter" viewbox="0 0 16 16">
     <path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z">
     </path>
    </symbol>
    <symbol id="icon-home" viewbox="0 0 18 18">
     <path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-image" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-info" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-institution" viewbox="0 0 18 18">
     <path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-location" viewbox="0 0 18 18">
     <path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-minus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-newsletter" viewbox="0 0 18 18">
     <path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-orcid" viewbox="0 0 18 18">
     <path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-plus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-print" viewbox="0 0 18 18">
     <path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-search" viewbox="0 0 22 22">
     <path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-facebook" viewbox="0 0 24 24">
     <path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-twitter" viewbox="0 0 24 24">
     <path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-youtube" viewbox="0 0 24 24">
     <path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-subject-medicine" viewbox="0 0 18 18">
     <path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-success" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-table" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick-circle" viewbox="0 0 24 24">
     <path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick" viewbox="0 0 16 16">
     <path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-update" viewbox="0 0 18 18">
     <path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-upload" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-video" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-warning" viewbox="0 0 18 18">
     <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-altmetric">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm-1.886 9.684-1.101 1.845a1 1 0 0 1-.728.479l-.13.008H3.056a9.001 9.001 0 0 0 17.886 0l-4.564-.001-2.779 4.156c-.454.68-1.467.55-1.758-.179l-.038-.113-1.69-6.195ZM12 3a9.001 9.001 0 0 0-8.947 8.016h4.533l2.017-3.375c.452-.757 1.592-.6 1.824.25l1.73 6.345 1.858-2.777a1 1 0 0 1 .707-.436l.124-.008h5.1A9.001 9.001 0 0 0 12 3Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-checklist-banner" viewbox="0 0 56.69 56.69">
     <path d="M0 0h56.69v56.69H0z" style="fill:none">
     </path>
     <clippath id="b">
      <use style="overflow:visible" xlink:href="#a">
      </use>
     </clippath>
     <path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round">
     </path>
     <path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round">
     </path>
    </symbol>
    <symbol id="icon-chevron-down" viewbox="0 0 16 16">
     <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-citations">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-eds-checklist" viewbox="0 0 32 32">
     <path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-external-link-medium" viewbox="0 0 24 24">
     <path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-info-filled-medium" viewbox="0 0 24 24">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z">
     </path>
    </symbol>
    <symbol id="icon-eds-menu" viewbox="0 0 24 24">
     <path d="M21.09 5c.503 0 .91.448.91 1s-.407 1-.91 1H2.91C2.406 7 2 6.552 2 6s.407-1 .91-1h18.18Zm-3.817 6c.401 0 .727.448.727 1s-.326 1-.727 1H2.727C2.326 13 2 12.552 2 12s.326-1 .727-1h14.546Zm3.818 6c.502 0 .909.448.909 1s-.407 1-.91 1H2.91c-.503 0-.91-.448-.91-1s.407-1 .91-1h18.18Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-search" viewbox="0 0 24 24">
     <path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-small-arrow-right" viewbox="0 0 16 16">
     <g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035">
      </path>
     </g>
    </symbol>
    <symbol id="icon-eds-user-single" viewbox="0 0 24 24">
     <path d="M12 12c5.498 0 10 4.001 10 9a1 1 0 0 1-2 0c0-3.838-3.557-7-8-7s-8 3.162-8 7a1 1 0 0 1-2 0c0-4.999 4.502-9 10-9Zm0-11a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm0 2a3 3 0 1 1 0 6 3 3 0 0 1 0-6Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-email-new" viewbox="0 0 24 24">
     <path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z">
     </path>
    </symbol>
    <symbol id="icon-expand-image" viewbox="0 0 18 18">
     <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-github" viewbox="0 0 100 100">
     <path clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-mentions">
     <g fill-rule="evenodd" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M22 15.255A9.373 9.373 0 0 1 8.745 2L22 15.255ZM15.477 8.523l4.215-4.215">
      </path>
      <path d="m7 13-5 9h10l-1-5">
      </path>
     </g>
    </symbol>
    <symbol id="icon-metrics-accesses">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM7.708 13.308c2.004 0 3.969 1.198 5.802 2.995l.23.23a2.285 2.285 0 0 1 .009 3.233C11.853 21.693 9.799 23 7.707 23c-2.091 0-4.14-1.305-6.033-3.226a2.285 2.285 0 0 1-.007-3.233c1.9-1.93 3.949-3.233 6.04-3.233Zm0 2c-1.396 0-3.064 1.062-4.623 2.644a.285.285 0 0 0 .007.41C4.642 19.938 6.311 21 7.707 21c1.397 0 3.069-1.065 4.623-2.644a.285.285 0 0 0 0-.404l-.23-.229c-1.487-1.451-3.064-2.415-4.393-2.415Zm-.036 1.077a1.77 1.77 0 1 1 .126 3.537 1.77 1.77 0 0 1-.126-3.537Zm.072 1.538a.23.23 0 1 0-.017.461.23.23 0 0 0 .017-.46Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-metrics">
     <path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-left">
     <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-right">
     <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z">
     </path>
    </symbol>
    <symbol id="icon-submit-open" viewbox="0 0 16 17">
     <path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero">
     </path>
    </symbol>
   </svg>
  </div>
  <script nomodule="true" src="/oscar-static/js/app-es5-bundle-774ca0a0f5.js">
  </script>
  <script src="/oscar-static/js/app-es6-bundle-047cc3c848.js" type="module">
  </script>
  <script nomodule="true" src="/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js">
  </script>
  <script src="/oscar-static/js/global-article-es6-bundle-c14b406246.js" type="module">
  </script>
  <div class="c-cookie-banner">
   <div class="c-cookie-banner__container">
    <p>
     This website sets only cookies which are necessary for it to function. They are used to enable core functionality such as security, network management and accessibility. These cookies cannot be switched off in our systems. You may disable these by changing your browser settings, but this may affect how the website functions. Please view our privacy policy for further details on how we process your information.
     <button class="c-cookie-banner__dismiss">
      Dismiss
     </button>
    </p>
   </div>
  </div>
 </body>
</html>
