1 Introduction:
Landmarks are points in correspondence across all faces, like the tip of the nose or the corner of the eye. They often play a role in face-related computer vision, e.g., being used to extract facial regions of interest [33], or helping to constrain 3D model fitting [25, 78]. Unfortunately, many aspects of facial identity or expression cannot be encoded by a typical sparse set of 68 landmarks alone. For example, without landmarks on the cheeks, we cannot tell whether or not someone has high cheek-bones. Likewise, without landmarks around the outer eye region, we cannot tell if someone is softly closing their eyes, or scrunching up their face. Fig. 1.Given a single image (top), we first robustly and accurately predict 703 landmarks (middle). To aid visualization, we draw lines between landmarks. We then fit our 3D morphable face model to these landmarks to reconstruct faces in 3D (bottom). 
In order to reconstruct faces more accurately, previous work has therefore used additional signals beyond color images, such as depth images [63] or optical flow [13]. However, these signals may not be available or reliable to compute. Instead, given color images alone, others have approached the problem using analysis-by-synthesis: minimizing a photometric error [25] between a generative 3D face model and an observed image using differentiable rendering [18, 26]. Unfortunately, these approaches are limited by the approximations that must be made in order for differentiable rendering to be computationally feasible. In reality, faces are not purely Lambertian [23], and many important illumination effects are not explained using spherical harmonics alone [18], e.g., ambient occlusion or shadows cast by the nose. Faced with this complexity, wouldn’t it be great if we could just use more landmarks? We present the first method that predicts over 700 landmarks both accurately and robustly. Instead of only the frontal “hockey-mask” portion of the face, our landmarks cover the entire head, including the ears, eyeballs, and teeth. As shown in Fig. 1, these landmarks provide a rich signal for both facial identity and expression. Even with as few as 68, it is hard for humans to precisely annotate landmarks that are not aligned with a salient image feature. That is why we use synthetic training data which guarantees consistent annotations. Furthermore, instead of representing each landmark as just a 2D coordinate, we predict each one as a random variable: a 2D circular Gaussian with position and uncertainty [37]. This allows our predictor to express uncertainty about certain landmarks, e.g., occluded landmarks on the back of the head. Since our dense landmarks represent points of correspondence across all faces, we can perform 3D face reconstruction by fitting a morphable face model [6] to them. Although previous approaches have fit models to landmarks in a similar way [76], we are the first to show that landmarks are the only signal required to achieve state-of-the-art results for monocular face reconstruction in the wild. The probabilistic nature of our predictions also makes them ideal for fitting a 3D model over a temporal sequence, or across multiple views. An optimizer can discount uncertain landmarks and rely on more certain ones. We demonstrate this with accurate and expressive results for both multi-view and monocular facial performance capture. Finally, we show that predicting dense landmarks and then fitting a model can be highly efficient by demonstrating real-time facial performance capture at over 150FPS on a single CPU thread. In summary, our main contribution is to show that you can achieve more with less. You don’t need parametric appearance models, illumination models, or differentiable rendering for accurate 3D face reconstruction. All you need is a sufficiently large quantity of accurate 2D landmarks and a 3D model to fit to them. In addition, we show that combining probabilistic landmarks and model fitting lets us intelligently aggregate face shape information across multiple images by demonstrating robust and expressive results for both multi-view and monocular facial performance capture. Fig. 2.Compared to a typical sparse set of 68 facial landmarks (a), our dense landmarks (b) cover the entire head in great detail, including ears, eyes, and teeth. These dense landmarks are better at encoding facial identity and subtle expressions. 


2 Related Work:
Reconstructing faces in 3D from images is a mature field at the intersection of vision and graphics. We focus our literature review on methods that are closer to our own, and refer the reader to Morales et al. [46] for an extensive survey. Regression-Based 3D Face Reconstruction. DNN-based regression has been extensively used as a tool for 3D face reconstruction. Techniques fall into two broad categories: supervised, and self-supervised. Approaches either use 3D Morphable Models (3DMMs) [7, 27, 40], or eschew linear models and instead learn a non-linear one as part of the training process [65]. Fully supervised techniques either use parameter values from a 3DMM that is fit to the data via optimization as labels [14, 67, 72], or known face geometry is posed by sampling from a 3DMM and rendered to create synthetic datasets [21, 26, 50, 56]. Self-supervised approaches commonly use landmark reprojection error and/or perceptual loss via differentiable rendering [17, 23, 26, 30, 31, 44, 51, 54, 61, 62, 65, 66]. Other techniques augment this with 3D or multiview constraints [20, 43, 57, 60, 73, 74]. While this is similar to our technique, we only use a DNN to regress landmark positions which are then used to optimize 3DMM parameters, as in the large body of hybrid model-fitting methods [8, 32]. Optimization-Based 3D Face Reconstruction. Traditionally, markerless reconstruction of face geometry is achieved with multi-view stereo [4, 55], followed by optical flow based alignment, and then optimisation using geometric and temporal priors [5, 9, 49]. While such methods produce detailed results, each step takes hours to complete. They also suffer from drift and other issues due to their reliance on optical flow and multi-view stereo [15]. While our method cannot reconstruct faces in such fine detail, it accurately recovers the low-frequency shape of the face, and aligns it with a common topology. This enriches the raw data with semantics, making it useful for other tasks. If only a single image is available, dense photometric [18, 64], depth [63], or optical flow [13] constraints are commonly used to recover face shape and motion. However, these methods still rely on sparse landmarks for initializing the optimization close to the dense constraint’s basin of convergence, and coping with fast head motion [78]. In contrast, we argue that dense landmarks alone are sufficient for accurately recovering the overall shape of the face. 
Dense Landmark Prediction.
While sparse landmark prediction is a mainstay of the field [12], few methods directly predict dense landmarks or correspondences. This is because annotating a face with dense landmarks is a highly ambiguous task, so either synthetic data [70], pseudo-labels made with model-fitting [16, 24, 77], or semi-automatic refinement of training data [35, 36] are used. Another issue with predicting dense landmarks is that heatmaps, the de facto technique for predicting landmarks [11, 12], rise in computational complexity with the number of landmarks. While a few previous methods have predicted dense frontal-face landmarks via cascade regression [35] or direct regression [16, 28, 36], we are the first to accurately and robustly predict over 700 landmarks covering the whole head, including eyes and teeth. Some methods choose to predict dense correspondences as an image instead, where each pixel corresponds to a fixed point in a UV-unwrapping of the face [1, 24] or body [29, 59]. Such parameterization suffers from several drawbacks. How does one handle self-occluded portions of the face, e.g., the back of the head? Furthermore, what occurs at UV-island boundaries? If a pixel is half-nose and half-cheek, to which does it correspond? Instead, we choose to discretize the face into dense landmarks. This lets us predict parts of the face that are self-occluded, or lie outside image bounds. Having a fixed set of correspondences also benefits the model-fitter, making it more amenable to running in real-time. Fig. 3.Given an image, we first predict probabilistic dense landmarks L, each with position \mu  and certainty \sigma . Then, we fit our 3D face model to L, minimizing an energy E by optimizing model parameters \boldsymbol{\mathbf {\Phi }}. 


3 Method:
In recent years, methods for 3D face reconstruction have become more and more complicated, involving differentiable rendering and complex neural network training strategies. We show instead that success can be found by keeping things simple. Our approach consists of two stages: First we predict probabilistic dense 2D landmarks L using a traditional convolutional neural network (CNN). Then, we fit a 3D face model, parameterized by \boldsymbol{\mathbf {\Phi }}, to the 2D landmarks by minimizing an energy function E(\boldsymbol{\mathbf {\Phi }}; L). Images themselves are not part of this optimization; the only data used are 2D landmarks. The main difference between our work and previous approaches is the number and quality of landmarks. No one before has predicted so many 2D landmarks, so accurately. This lets us achieve accurate 3D face reconstruction results by fitting a 3D model to these landmarks alone. Fig. 4.Examples of our synthetic training data. Without the perfectly consistent annotations provided by synthetic data, dense landmark prediction would not be possible. 


3.1 Landmark Prediction:
Synthetic Training Data. Our results are only possible because we use synthetic training data. While a human can consistently label face images with e.g., 68 landmarks, it would be almost impossible for them to annotate an image with dense landmarks. How would it be possible to consistently annotate occluded landmarks on the back of the head, or multiple landmarks over a largely featureless patch of skin e.g., the forehead? In previous work, pseudo-labelled real images with dense correspondences are obtained by fitting a 3DMM to images [1], but the resulting label consistency heavily depends on the quality of the 3D fitting. Using synthetic data has the advantage of guaranteeing perfectly consistent labels. We rendered a training dataset of 100 k images using the method of Wood et al. [70] with some minor modifications: we include expression-dependent wrinkle texture maps for more realistic skin appearance, and additional clothing, accessory, and hair assets. See Fig. 4 for some examples. Fig. 5.When parts of the face are occluded by e.g. hair or clothing, the corresponding landmarks are predicted with high uncertainty (red), compared to those visible (green). (Color figure online) 
Probabilistic Landmark Regression. We predict each landmark as a random variable with the probability density function of a circular 2D Gaussian. So L_i = \{\boldsymbol{\mathbf {\mu }}_i, \sigma _i\}, where \boldsymbol{\mathbf {\mu }}_i = [x_i, y_i] is the expected position of that landmark, and \sigma _i (the standard deviation) is a measure of uncertainty. Our training data includes labels for landmark positions \boldsymbol{\mathbf {\mu }}_i^\prime = [x_i^\prime , y_i^\prime ], but not for \sigma . The network learns to output \sigma  in an unsupervised fashion to show that it is certain about some landmarks, e.g., visible landmarks on the front of the face, and uncertain about others, e.g., landmarks hidden behind hair (see Fig. 5). This is achieved by training the network with a Gaussian negative log likelihood (GNLL) loss [37]: \begin{aligned} \text {Loss}(L) = \sum _{i=1}^{|L|} \lambda _i \Bigg (\underbrace{\log \left( \sigma _i^2\right) }_{\text {Loss}_{\sigma }} + \underbrace{\frac{\Vert \boldsymbol{\mathbf {\mu }}_{i} - \boldsymbol{\mathbf {\mu }}_{i}^\prime \Vert ^2}{2 \sigma _{i}^2}}_{\text {Loss}_{\mu }}\Bigg ) \end{aligned}
(1)
\text {Loss}_{\sigma } penalizes the network for being too uncertain, and \text {Loss}_{\mu } penalizes the network for being inaccurate. \lambda _i is a per-landmark weight that focuses the loss on certain parts of the face. This is the only loss used during training. The probabilistic nature of our landmark predictions is important for accuracy. A network trained with the GNLL loss is more accurate than a network trained with L2 loss on positions only. Perhaps this is the result of the CNN being able to discount challenging landmarks (e.g., fully occluded ones), and spend more capacity on making precise predictions about visible landmarks. Landmarks are commonly predicted via heatmaps [11]. However, generating heatmaps is computationally expensive [41]; it would not be feasible to output over 700 heatmaps in real-time. Heatmaps also prevent us predicting landmarks outside image bounds. Instead, we keep things simple, and directly regress position and uncertainty using a traditional CNN. We are able to take any off-the-shelf architecture, and alter the final fully-connected layer to output three values per-landmark: two for position and one for uncertainty. Since this final layer represents a small percentage of total CNN compute, our method scales well with landmark quantity. Training Details. Landmark coordinates are normalized from [0, S] to [-1, 1], for a square image of size S \! \times \! S. Rather than directly outputting \sigma , we predict \log \sigma , and take its exponential to ensure \sigma  is positive. Using PyTorch [47], we train ResNet [34] and MobileNet V2 [53] models from the timm [69] library using AdamW [45] with automatically determined learning rate [22]. We use data augmentation to help our synthetic data cross the domain gap [70]. 

3.2 3D Model Fitting:
Given probabilistic dense 2D landmarks L, our goal is to find optimal model parameters \boldsymbol{\mathbf {\Phi }}^* that minimize the following energy:  E(\boldsymbol{\mathbf {\Phi }}; L) = \underbrace{ E_{\text {landmarks} } }_{ \text {Data term} } + \underbrace{ E_{\text {identity}} + E_{\text {expression}} + E_{\text {joints}} + E_{\text {temporal}} + E_{\text {intersect}} }_{ \text {Regularizers} } E_{\text {landmarks}} is the only term that encourages the 3D model to explain the observed 2D landmarks. The other terms use prior knowledge to regularize the fit. Part of the beauty of our approach is how naturally it scales to multiple images and cameras. In this section we present the general form of our method, suitable for F frames over C cameras, i.e., multi-view performance capture. Fig. 6.We implemented two versions of our approach: one for processing multi-view recordings offline (a), and one for real-time facial performance capture (b). 
3D Face Model. We use the face model described in [70], comprising N\!=\!7,\!667 vertices and K\!=\!4 skeletal joints (the head, neck, and two eyes). Vertex positions are determined by the mesh-generating function \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}, \boldsymbol{\mathbf {\theta }}) \!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|+|\boldsymbol{\mathbf {\psi }}|+|\boldsymbol{\mathbf {\theta }}|}\!\rightarrow \!\mathbb {R}^{3N} which takes parameters \boldsymbol{\mathbf {\beta }}\in \mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|} for identity, \boldsymbol{\mathbf {\psi }}\in \mathbb {R}^{|\boldsymbol{\mathbf {\psi }}|} for expression, and \boldsymbol{\mathbf {\theta }}\in \mathbb {R}^{3K + 3} for skeletal pose (including root joint translation).  \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}, \boldsymbol{\mathbf {\theta }}) = \mathcal {L}(\mathcal {T}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}), \boldsymbol{\mathbf {\theta }}, \mathcal {J}(\boldsymbol{\mathbf {\beta }}); \textbf{W}) where \mathcal {L}(\textbf{V}, \boldsymbol{\mathbf {\theta }}, \textbf{J}; \textbf{W}) is a standard linear blend skinning (LBS) function [39] that rotates vertex positions \textbf{V}\in \mathbb {R}^{3N} about joint locations \textbf{J}\in \mathbb {R}^{3K} by local joint rotations in \boldsymbol{\mathbf {\theta }}, with per-vertex weights \textbf{W}\in \mathbb {R}^{K\times N}. The face mesh and joint locations in the bind pose are determined by \mathcal {T}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }})\!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}| + |\boldsymbol{\mathbf {\psi }}|}\rightarrow \mathbb {R}^{3N} and \mathcal {J}(\boldsymbol{\mathbf {\beta }})\!:\!\mathbb {R}^{|\boldsymbol{\mathbf {\beta }}|}\rightarrow \mathbb {R}^{3K} respectively. See Wood et al. [70] for more details. Cameras are described by a world-to-camera rigid transform \boldsymbol{\textbf{X}} \in \mathbb {R}^{3 \times 4} = \left[ \boldsymbol{\textbf{R}} | \boldsymbol{\textbf{T}} \right]  comprising rotation and translation, and a pinhole camera projection matrix \boldsymbol{\mathbf {\Pi }} \in \mathbb {R}^{3 \times 3}. Thus, the image-space projection of the jth landmark in the ith camera is  \boldsymbol{\textbf{x}}_{i,j} = \boldsymbol{\mathbf {\Pi }}_i \boldsymbol{\textbf{X}}_i \mathcal {M}_j . In the monocular case, \boldsymbol{\textbf{X}} can be ignored. Parameters \boldsymbol{\mathbf {\Phi }} are optimized to minimize E. The main parameters of interest control the face, but we also optimize camera parameters if they are unknown. \begin{aligned} \boldsymbol{\mathbf {\Phi }} = \{ \underbrace{ \boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\Psi }}_{F \times |\boldsymbol{\mathbf {\psi }}|}, \boldsymbol{\mathbf {\Theta }}_{F \times |\boldsymbol{\mathbf {\theta }}|} }_{\text {Face}} ;\, \underbrace{ \boldsymbol{\textbf{R}}_{C \times 3}, \boldsymbol{\textbf{T}}_{C \times 3}, \boldsymbol{\textbf{f}}_{C} }_{\text {Camera(s)}} \} \end{aligned}Facial identity \boldsymbol{\mathbf {\beta }} is shared over a sequence of F frames, but expression \boldsymbol{\mathbf {\Psi }} and pose \boldsymbol{\mathbf {\Theta }} vary per frame. For each of our C cameras we have six degrees of freedom for rotation \boldsymbol{\textbf{R}} and translation \boldsymbol{\textbf{T}}, and a single focal length parameter f. In the monocular case, we only optimize focal length. E_{\text {landmarks}} encourages the 3D model to explain the predicted 2D landmarks: \begin{aligned} E_{\text {landmarks}} = \sum _{i, j, k}^{F, C, |L|} \frac{\Vert \boldsymbol{\textbf{x}}_{ijk} - \boldsymbol{\mathbf {\mu }}_{ijk} \Vert ^2}{2 \sigma _{ijk}^2} \end{aligned}
(2)
where, for the kth landmark seen by the jth camera in the ith frame, [\boldsymbol{\mathbf {\mu }}_{ijk}, \sigma _{ijk}] is the 2D location and uncertainty predicted by our dense landmark CNN, and \boldsymbol{\textbf{x}}_{ijk} = \boldsymbol{\mathbf {\Pi }}_j \boldsymbol{\textbf{X}}_j \mathcal {M}(\boldsymbol{\mathbf {\beta }}, \boldsymbol{\mathbf {\psi }}_i, \boldsymbol{\mathbf {\theta }}_i)_k is the 2D projection of that landmark on our 3D model. The similarity of Eq. 2 to \text {Loss}_{\mu } in Eq. 1 is no accident: treating landmarks as 2D random variables during both prediction and model-fitting allows our approach to elegantly handle uncertainty, taking advantage of landmarks the CNN is confident in, and discounting those it is uncertain about. Fig. 7.We encourage the optimizer to avoid face mesh self-intersections by penalizing skin vertices that enter the convex hulls of the eyeballs or teeth parts. 
E_{\text {identity}} penalizes unlikely face shape by maximizing the relative log-likelihood of shape parameters \boldsymbol{\mathbf {\beta }} under a multivariate Gaussian Mixture Model (GMM) of G components fit to a library of 3D head scans [70]. E_{\text {identity}} = -\log \left( p(\boldsymbol{\mathbf {\beta }})\right)  where  p(\boldsymbol{\mathbf {\beta }}) = \sum _{i=1}^{G} \gamma _i\; \mathcal {N}\! \left( \boldsymbol{\mathbf {\beta }} | \boldsymbol{\mathbf {\nu }}_i,\, \boldsymbol{\mathbf {\Sigma }}_i\right) . \boldsymbol{\mathbf {\nu }}_i and \boldsymbol{\mathbf {\Sigma }}_i are the mean and covariance matrix of the ith component, and \gamma _i is the weight of that component. E_{\text {expression}} = \Vert \boldsymbol{\mathbf {\psi }} \Vert ^2 and E_{\text {joints}} = \Vert \boldsymbol{\mathbf {\theta }}_{i:i\in [2,K]} \Vert ^2 encourage the optimizer to explain the data with as little expression and joint rotation as possible. We do not penalize global translation or rotation by ignoring the root joint \boldsymbol{\mathbf {\theta }}_1. E_{\text {temporal}} = \sum _{i=2, j, k}^{F, C, |L|} \Vert \boldsymbol{\textbf{x}}_{i,j,k} - \boldsymbol{\textbf{x}}_{i-1,j,k} \Vert ^2 reduces jitter by encouraging face mesh vertices \boldsymbol{\textbf{x}} to remain still between neighboring frames i-1 and i. E_{\text {intersect}} encourages the optimizer to find solutions without intersections between the skin and eyeballs or teeth (Fig. 7). Please refer to the supplementary material for further details. 

3.3 Implementation:
We implemented two versions of our system: one for processing multi-camera recordings offline, and one for real-time facial performance capture. Our offline system produces the best quality results without constraints on compute. We predict 703 landmarks with a ResNet 101 [34]. To extract a facial Region-of-Interest (ROI) from an image we run a full-head probabilistic landmark CNN on multi-scale sliding windows, and select the window with the lowest uncertainty. When fitting our 3DMM, we use PyTorch [47] to minimize E(\boldsymbol{\mathbf {\Phi }}) with L-BFGS [42], optimizing all parameters across all frames simultaneously. For our real-time system, we trained a lightweight dense landmark model with MobileNet V2 architecture [53]. To compensate for a reduction in network capacity, we predict 320 landmarks rather than 703, and modify the ROI strategy: aligning the face so it appears upright with the eyes a fixed distance apart. This makes the CNN’s job easier for frontal faces at the expense of profile ones. 3.3.1 Real-Time Model Fitting.We use the Levenberg-Marquardt algorithm to optimize our model-fitting energy. Camera and identity parameters are only fit occasionally. For the majority of frames we fit pose and expression parameters only. We rewrite the energy E in terms of the vector of residuals, \boldsymbol{\textbf{r}}, as E(\boldsymbol{\mathbf {\Phi }}) = \Vert \boldsymbol{\mathbf {r(\boldsymbol{\mathbf {\Phi }})}}\Vert ^2 = \sum _i r_i(\boldsymbol{\mathbf {\Phi }})^2. Then at each iteration k of our optimization, we can compute \boldsymbol{\textbf{r}}(\boldsymbol{\mathbf {\Phi }}_k) and the Jacobian, J(\boldsymbol{\mathbf {\Phi }}_k) = \frac{\partial \boldsymbol{\textbf{r}}(\boldsymbol{\mathbf {\Phi }})}{\partial \boldsymbol{\mathbf {\Phi }}} |^{\boldsymbol{\mathbf {\Phi }}=\boldsymbol{\mathbf {\Phi }}_k}, and use these to solve the symmetric, positive-semi-definite linear system, (J^T J + \lambda \text {diag}(J^T J))\boldsymbol{\mathbf {\delta }}_k = -J^T\boldsymbol{\textbf{r}} via Cholesky decomposition. We then apply the update rule, \boldsymbol{\mathbf {\Phi }}_{k+1} = \boldsymbol{\mathbf {\Phi }}_k + \boldsymbol{\mathbf {\delta }}_k. In practice we do not actually form the residual vector \boldsymbol{\textbf{r}} nor the Jacobian matrix J. Instead, for performance reasons, we directly compute the quantities J^T J and J^T \boldsymbol{\textbf{r}} as we visit each term r_i(\boldsymbol{\mathbf {\Phi }}_k) of the energy. Most of the computational cost is incurred in evaluating these products for the landmark data term, as expected. However, the Jacobian of landmark term residuals is not fully dense. Each individual landmark depends on its own subset of expression parameters, and is invariant to other expression parameters. We performed a static analysis of the sparsity of each landmark term with respect to parameters, \partial r_i / \partial \Phi _j, and we use this set of i, j indices to reduce the cost of our outer products from O(|\boldsymbol{\mathbf {\Phi }}|^2) to O(m_i^2), where m_i is the sparsified dimensionality of \partial r_i / \partial \boldsymbol{\mathbf {\Phi }}. We further enhance the sparsity by ignoring any components of the Jacobian with an absolute value below a certain empirically-determined threshold. By exploiting sparsity in this way, the landmark term residuals and their derivatives become very cheap to evaluate. This formulation avoids the correspondence problem usually seen with depth images [58], which requires a more expensive optimization. In addition, adding more landmarks does not significantly increase the cost of optimization. It therefore becomes possible to implement a very detailed and well-regularized fitter with a relatively small compute burden, simply by adding a sufficient number of landmarks. The cost of the Cholesky solve for the update \boldsymbol{\mathbf {\delta }}_k is independent of the number of landmarks. 

4 Evaluation:


4.1 Landmark Accuracy:
We measure the accuracy of a ResNet 101 dense landmark model on the 300W [52] dataset. For benchmark purposes only, we employ label translation [70] to deal with systematic inconsistencies between our 703 predicted dense landmarks and the 68 sparse landmarks labelled as ground truth (see Table 8). While previous work [70] used label translation to evaluate a synthetically-trained sparse landmark predictor, we use it to evaluate a dense landmark predictor. We use the standard normalized mean error (NME) and failure rate (FR_{10\%}) error metrics [52]. Our model’s results in Table 8 are competitive with the state of the art, despite being trained with synthetic data alone. Note: these results provide a conservative estimate of our method’s accuracy as the translation network may introduce error, especially for rarely seen expressions. Fig. 8.Left: results on 300W dataset, lower is better. Note competitive performance of our model (despite being evaluated across-dataset) and importance of GNLL loss. Right: sample predictions (top row) with label-translated results (bottom row). 
Ablation Study. We measured the importance of predicting each landmark as a random variable rather than as a 2D coordinate. We trained two landmark prediction models, one with our proposed GNLL loss (Eq. 1), and one with a simpler L2 loss on landmark coordinate only. Results in Table 8 confirm that including uncertainty in landmark regression results in better accuracy. Fig. 9.We compare our real-time landmark CNN (MobileNet V2) with MediaPipe Attention Mesh [28], a publicly available method for dense landmark prediction. Our approach is more robust to challenging expressions and illumination. 
Qualitative Comparisons are shown in Fig. 9 between our real-time dense landmark model (MobileNet V2) and MediaPipe Attention Mesh [28], a publicly available dense landmark method designed for mobile devices. Our method is more robust, perhaps due to the consistency and diversity of our synthetic training data. See the supplementary material for additional qualitative results, including landmark predictions on the Challenging subset of 300W. 

4.2 3D Face Reconstruction:
Quantitatively, we compare our offline approach with recent methods on two benchmarks: the NoW Challenge [54] and the MICC dataset [2]. The Now Challenge [54] provides a standard evaluation protocol for measuring the accuracy and robustness of 3D face reconstruction in the wild. It consists of 2054 face images of 100 subjects along with a 3D head scan for each subject which serves as ground truth. We undertake the challenge in two ways: single view, where we fit our face model to each image separately, and multi-view, where we fit a per-subject face model to all image of a particular subject. As shown in Fig. 10, we achieve state of the art results. Fig. 10.Results for the now challenge [54]. We outperform the state of the art on both single- and multi-view 3D face reconstruction. 
The MICC Dataset [2] consists of 3D face scans and videos of 53 subjects. The videos were recorded in three environments: a “cooperative” laboratory environment, an indoor environment, and an outdoor environment. We follow Deng et al. [17], and evaluate our method in two ways: single view, where we estimate one face shape per frame in a video, and average the resulting face meshes, and multi-view, where we fit a single face model to all frames in a video jointly. As shown in Table 1, we achieve state of the art results. Note that many previous methods are incapable of aggregating face shape information across multiple views. The fact ours can benefit from multiple views highlights the flexibility of our hybrid model-fitting approach. Table 1. Results on the MICC dataset [2], following the single and multi-frame evaluation protocol of Deng et al. [17]. We achieve state-of-the-art results.
4.2.1 Ablation Studies.We conducted an experiment to measure the importance of landmark quantity for 3D face reconstruction. We trained three landmark CNNs, predicting 703, 320, and 68 landmarks respectively, and used these on the NoW Challenge (validation set). As shown in Fig. 11, fitting with more landmarks results in more accurate 3D face reconstruction. In addition, we investigated the importance of using landmark uncertainty \sigma  in model fitting. We fit our model to 703 landmark predictions on the NoW validation set, but using fixed rather than predicted \sigma . Figure 11 (bottom row of table) shows that fitting without \sigma  leads to worse results. 4.2.2 Qualitative Comparisonsbetween our work and several publicly available methods [17, 23, 30, 54, 57] can be found in Fig. 13. Fig. 11.Ablation studies on the NoW [54] validation set confirm that denser is better: model fitting with more landmarks leads to more accurate results. In addition, we see that fitting without using \sigma  leads to worse results. 


4.3 Facial Performance Capture:
4.3.1 Multi-view.Good synthetic training data requires a database of facial expression parameters from which to sample. We acquired such a database by conducting markerless facial performance capture for 108 subjects. We recorded each subject in our 17-camera studio, and processed each recording with our offline multi-view model fitter. For a 520 frame sequence it takes 3 min to predict dense landmarks for all images, and a further 9 min to optimize face model parameters. See Fig. 12 for some of the 125,000 frames of expression data captured with our system. As the system which is used to create the database is then subsequently re-trained with it, we produced several databases in this manner until no further improvement was seen. We do not reconstruct faces in fine detail like previous multi-view stereo approaches [5, 9, 49]. However, while previous work can track a detailed 3D mesh over a performance, our approach reconstructs the performance with richer semantics: identity and expression parameters for our generative model. In many cases it is sufficient to reconstruct the low-frequency shape of the face accurately, without fine details. Fig. 12.We demonstrate the robustness and reliability of our method by using it to collect a massive database of 125,000 facial expressions, fully automatically. 
4.3.2 Real-Time Monocular.See the last two columns of Fig. 13 for a comparison between our offline and real-time systems for monocular 3D model-fitting. While our offline system produces the best possible results by using a large CNN and optimizing over all frames simultaneously, our real-time system can still produce accurate and expressive results fitting frame-to-frame. Please refer to the supplementary material for more results. Running on a single CPU thread (i5-11600K), our real-time system spends 6.5 ms processing a frame (150FPS), of which 4.1 ms is spent predicting dense landmarks and 2.3 ms is spent fitting our face model. Fig. 13.Compared to previous recent monocular 3D face reconstruction methods, ours better captures gaze, expressions like winks and sneers, and subtleties of facial identity. In addition, our method can run in real time with only a minor loss of fidelity. 
Fig. 14.Bad landmarks result in bad fits, and we are incapable of tracking the tongue. 


5 Limitations and Future Work:
Our method depends entirely on accurate landmarks. As shown in Fig. 14, if landmarks are poorly predicted, the resulting model fit suffers. We plan to address this by improving our synthetic training data. Additionally, since our model does not include tongue articulation we cannot recover tongue movement. Heatmaps have dominated landmark prediction for some time [11, 12]. We were pleasantly surprised to find that directly regressing 2D landmark coordinates with unspecialized architectures works well and eliminates the need for computationally-costly heatmap generation. In addition, we were surprised that predicting \sigma  helps accuracy. We look forward to further investigating direct probabilistic landmark regression as an alternative to heatmaps in future work. In conclusion, we have demonstrated that dense landmarks are an ideal signal for 3D face reconstruction. Quantitative and qualitative evaluations have shown that our approach outperforms those previous by a significant margin, and excels at multi-view and monocular facial performance capture. Finally, our approach is highly efficient, and runs at over 150FPS on a single CPU thread.