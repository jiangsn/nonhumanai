1 Introduction:

Fig. 1.Given conditional inputs in multiple modalities (the left column), our approach can synthesize images that satisfy all input conditions (the right column (g)) or an arbitrary subset of input conditions (the middle column (a)–(f)) with a single model. 
Conditional image synthesis allows users to use their creative inputs to control the output of image synthesis methods. It has found applications in many content creation tools. Over the years, a variety of input modalities have been studied, mostly based on conditional GANs [11, 19, 27, 34]. To this end, we have various single modality-to-image models. When the input modality is text, we have the text-to-image model [40, 41, 51, 58, 60, 62, 67]. When the input modality is a segmentation mask, we have the segmentation-to-image model [8, 19, 29, 36, 45, 54]. When the input modality is a sketch, we have the sketch-to-image model [4, 6, 10, 44]. However, different input modalities are best suited for conveying different types of conditioning information. For example, as seen in the first column of Fig. 1, segmentation makes it easy to define the coarse layout of semantic classes in an image—the relative locations of sky, cloud, mountain, and water regions. Sketch allows us to specify the structure and details within the same semantic region, such as individual mountain ridges. On the other hand, text is well-suited for modifying and describing objects or regions in the image, which cannot be achieved via segmentation or sketch, e.g., ‘frozen lake’ and ‘pink clouds’ in Fig. 1. Despite this synergy among modalities, prior work has considered image generation conditioned on each modality as a distinct task and studied it in isolation. Existing models thus fail to utilize complementary information available in different modalities. Clearly, a conditional generative model that can combine input information from all available modalities would be of immense value. Even though the benefits are enormous, the task of conditional image synthesis with multiple input modalities poses several challenges. First, it is unclear how to combine multiple modalities with different dimensions and structures in a single framework. Second, from a practical standpoint, the generator needs to handle missing modalities since it is cumbersome to ask users to provide every single modality all the time. This means that the generator should work well even when only a subset of modalities are provided. Lastly, conditional GANs are known to be susceptible to mode collapse [19, 34], wherein the generator produces identical images when conditioned on the same inputs. This makes it difficult for the generator to produce diverse output images that capture the full conditional distribution when conditioned on an arbitrary set of modalities. We present Product-of-Experts Generative Adversarial Networks (PoE-GAN), a framework that can generate images conditioned on any subset of the input modalities presented during training, as illustrated in Fig. 1 (a)-(g). This framework provides users unprecedented control, allowing them to specify exactly what they want using multiple complementary input modalities. When users provide no inputs, it falls back to an unconditional GAN model [2, 11, 20,21,22,23, 32, 39]. One key ingredient of our framework is a novel product-of-experts generator that can effectively fuse multimodal user inputs and handle missing modalities (Sect. 3.1). A novel hierarchical and multiscale latent representation leads to better usage of the structure in spatial modalities, such as segmentation and sketch (Sect. 3.2). Our model is trained with a multimodal projection discriminator (Sect. 3.4) together with contrastive losses for better input-output alignment. In addition, we adopt modality dropout for additional robustness to missing inputs (Sect. 3.5). Extensive experiment results show that PoE-GAN outperforms prior work in both multimodal and unimodal settings (Sect. 4), including state-of-the-art approaches specifically designed for a single modality. We also show that PoE-GAN can generate diverse images when conditioned on the same inputs. 

2 Related Work:
Image Synthesis. Our network architecture design is inspired by previous work in unconditional image synthesis. Our decoder employs some techniques proposed in StyleGAN [22] such as global modulation. Our latent space is constructed in a way similar to hierarchical variational auto-encoders (VAEs) [7, 30, 48, 52]. While hierarchical VAEs encode the image itself to the latent space, our network encodes conditional information from different modalities into a unified latent space. Our discriminator design is inspired by the projection discriminator [33] and multiscale discriminators [29, 54], which we extend to our multimodal setting. Multimodal Image Synthesis. Prior work [25, 46, 49, 50, 53, 56] has explored learning the joint distribution of multiple modalities using VAEs [24, 42]. Some of them [25, 53, 56] use a product-of-experts inference network to approximate the posterior distribution. This is conceptually similar to how our generator combines information from multiple modalities. While their goal is to estimate the complete joint distribution, we focus on learning the image distribution conditioned on other modalities. Besides, our framework is based on GANs rather than VAEs and we perform experiments on high-resolution and large-scale datasets, unlike the above work. Recently, Xia et al.. [57] propose a GAN-based multimodal image synthesis method named TediGAN. Their method relies on a pretrained unconditional generator. However, such a generator is difficult to train on a complex dataset such as MS-COCO [26]. Concurrently, Zhang et al.. [65] propose a multimodal image synthesis method based on VQGAN. The way they combine different modalities is similar to our baseline using concatenation and modality dropout (Sect. 4.2). We will show that our product-of-experts generator design significantly improves upon this baseline. Another parallel work by Gafniet al. [9] propose a VQGAN model that is conditioned on both text and segmentation. They assume both modalities are always present at inference time and cannot deal with missing modalities. 

3 Product-of-Experts GANs:
Given a dataset of images x paired with M different input modalities (y_{1}, y_{2}, ..., y_{M}), our goal is to train a single generative model that learns to capture the image distribution conditioned on an arbitrary subset of possible modalities p(x|\mathcal {Y}), \forall \mathcal {Y}\subseteq \{y_{1}, y_{2}, ..., y_{M}\}. In this paper, we consider four different modalities including text, semantic segmentation, sketch, and style reference. Note that our framework is general and can easily incorporate additional modalities. Learning image distributions conditioned on any subset of M modalities is challenging because it requires a single generator to simultaneously model 2^M distributions. Of particular note, the generator needs to capture the unconditional image distribution p(x) when \mathcal {Y} is an empty set, and the unimodal conditional distributions p(x|y_{i}), \forall i\in \{1, 2, ..., M\}, such as the image distribution conditioned on text alone. These settings have been popular and widely studied in isolation, and we aim to bring them all under a unified framework. 

3.1 Product-of-Experts Modeling:
Our generator consists of a decoder G that deterministically maps a latent code z to an output image x, and a set of encoders that estimate the latent distribution p(z|\mathcal {Y}) conditioned on a set of modalities \mathcal {Y}. The conditional image distribution p(x|\mathcal {Y}) is implicitly defined as x=G(z), z\sim p(z|\mathcal {Y}). A naive approach would require us to train 2^M different encoder networks, one for each possible combination of modalities. This is highly parameter-inefficient and does not scale to a large number of modalities. Fortunately, if we assume all modalities (y_{1}, ..., y_{M}) are conditionally independent given the image (x or equivalently z), i.e., p(y_{1}, ..., y_{M}|z)= \prod _{i=1}^{M}p(y_{i}|z)Footnote 1, we can prove that the distribution p(z|\mathcal {Y}) is proportional to a product of distributions: \begin{aligned}&p(z|\mathcal {Y}) = \frac{p(\mathcal {Y}|z)p(z)}{p(\mathcal {Y})} = \frac{p(z)}{p(\mathcal {Y})} \prod _{y_{i}\in \mathcal {Y}}p(y_{i}|z) = \frac{p(z)}{p(\mathcal {Y})} \prod _{y_{i}\in \mathcal {Y}}\frac{p(z|y_{i})p(y_{i})}{p(z)} \nonumber \\&= \frac{\prod _{y_{i}\in \mathcal {Y}}p(z|y_{i})}{(p(z))^{|\mathcal {Y}|-1}} \cdot \frac{\prod _{y_{i}\in \mathcal {Y}}p(y_{i})}{p(\mathcal {Y})} \propto \frac{\prod _{y_{i}\in \mathcal {Y}}p(z|y_{i})}{(p(z))^{|\mathcal {Y}|-1}} = p(z)\prod _{y_{i}\in \mathcal {Y}}\tilde{q}(z|y_{i})\,, \end{aligned}
(1)
where \tilde{q}(z|y_{i})\equiv \frac{p(z|y_{i})}{p(z)}. Dividing it by the normalization constant, we have \begin{aligned} p(z|\mathcal {Y})\propto p(z)\prod _{y_{i}\in \mathcal {Y}}q(z|y_{i}), q(z|y_{i})=\frac{\tilde{q}(z|y_{i})}{\int \tilde{q}(z|y_{i})dz}\,, \end{aligned}
(2)
where q(z|y_{i}) is a latent distribution only dependent on a single modality y_{i} and p(z) is the unconditional prior distribution. As a result, we can reduce the number of encoders from 2^M to M, with each encoder estimating the distribution q(z|y_{i}) from a single modalityFootnote 2. This idea of combining several distributions (“experts”) by multiplying them has been previously referred to as product-of-experts [16]. Fig. 2.The product of distributions (a) is analogous to the intersection of sets (b). 
Figures. 2a and 2b show that the product of distributions is intuitively analogous to the intersection of sets. The product distribution only has a high density in regions where all distributions have a relatively high density. Also, the product distribution is always narrower (of lower entropy) than the individual distributions, just like the intersection of sets is always smaller than the individual set. While each set poses a hard constraint, each individual distribution in a product represents a soft constraint, which is more amenable to neural network learning. In the multimodal conditional image synthesis setting, the model samples images from the prior p(z) when no modalities are given. Each additional modality y_i specifies a set of images that satisfy a certain constraint and we model that by multiplying the prior with an additional distribution q(z|y_i). 

3.2 Multiscale and Hierarchical Latent Space:
Some of the modalities we consider (e.g., sketch, segmentation) are two-dimensional and naturally contain information at multiple scales. Therefore, we devise a hierarchical latent space with latent variables at different resolutions. This allows us to directly pass information from each resolution of the encoder to the corresponding resolution of the latent space, so that the high-resolution control signals can be better preserved. Mathematically, our latent code is partitioned into groups z=(z^0, z^1, ..., z^N) where z^0\in \mathbb {R}^{c_0} is a feature vector and z^k\in \mathbb {R}^{c_k\times r_k\times r_k}, 1\le k\le N are feature maps of increasing resolutions (r_{k+1}=2r_{k}, r_{1}=4, r_{N} is the image resolution). We can therefore decompose the prior p(z) into \prod _{k=0}^{N}p(z^k|z^{<k}) and the experts q(z|y_{i}) into \prod _{k=0}^{N}q(z^k|z^{<k}, y_i), where z^{<k} denotes (z^{0}, z^{1}, ..., z^{k-1}). Following Eq. (2), we assume the conditional latent distribution at each resolution is a product-of-experts given by \begin{aligned} p(z^k|z^{<k},\mathcal {Y})\propto p(z^k|z^{<k})\prod _{y_{i}\in \mathcal {Y}}q(z^{k}|z^{<k},y_{i})\,, \end{aligned}
(3)

Fig. 3.An overview of our generator. The architecture of Global PoE-Net and decoder are detailed in Fig. 4a and Fig. 4b respectively. The architecture of modality encoders are described in Supplementary Material B. 
where p(z^k|z^{<k})=\mathcal {N}\left( \mu ^{k}_{0}, \sigma ^{k}_{0}\right)  and q(z^{k}|z^{<k},y_{i})=\mathcal {N}\left( \mu ^{k}_{i}, \sigma ^{k}_{i}\right)  are independent Gaussian distributions with mean and standard deviation parameterized by a neural network.Footnote 3 It can be shown [55] that the product of Gaussian experts is also a Gaussian p(z^k|z^{<k},\mathcal {Y})=\mathcal {N}(\mu ^{k}, \sigma ^{k}), with \begin{aligned}&\mu ^{k}=\frac{\frac{\mu ^{k}_{0}}{(\sigma ^{k}_{0})^{2}}+\sum _{i}\frac{\mu ^{k}_{i}}{(\sigma ^{k}_{i})^{2}}}{\frac{1}{(\sigma ^{k}_{0})^{2}}+\sum _{i}\frac{1}{(\sigma ^{k}_{i})^{2}}},\ \sigma ^{k}=\frac{1}{\frac{1}{(\sigma ^{k}_{0})^{2}}+\sum _{i}\frac{1}{(\sigma ^{k}_{i})^{2}}}\,. \end{aligned}
(4)

Fig. 4.(a) Global PoE-Net. We sample a latent feature vector z^{0} using product-of-experts (Eq. (4) in Sect. 3.2), which is then processed by an MLP to output a feature vector w. (b) A residual block in our decoder. Local PoE-Net samples a latent feature map z^k using product-of-experts. Here \oplus  denotes concatenation. LG-AdaIN uses w and z^{k} to modulate the feature activations in the residual branch. 


3.3 Generator Architecture:
Fig. 3 shows an overview of our generator architecture. We encode each modality into a feature vector which is then aggregated in Global PoE-Net. We use convolutional networks with input skip connections to encode segmentation and sketch maps, a residual network to encode style images, and CLIP [38] to encode text. Details of all modality encoders are given in Supplementary Material B. The decoder generates the image using the output of Global PoE-Net and skip connections from the segmentation and sketch encoders. In Global PoE-Net (Fig. 4a), we predict a Gaussian q(z^{0}|y_{i})=\mathcal {N}\left( \mu ^{0}_{i}, \sigma ^{0}_{i}\right)  from the feature vector of each modality using an MLP. We then compute the product of Gaussians including the prior p(z^{0})=\mathcal {N}(\mu ^0_0, \sigma ^0_0)=\mathcal {N}(0, I) and sample z^{0} from the product distribution. An MLP further convert z^0 to the vector w. The decoder mainly consists of a stack of residual blocksFootnote 4 [14], each of which is shown in Fig. 4b. Local PoE-Net samples the latent feature map z^{k} at the current resolution from the product of p(z^{k}|z^{<k})=\mathcal {N}\left( \mu ^{k}_{0}, \sigma ^{k}_{0}\right)  and q(z^{k}|z^{<k}, y_{i})=\mathcal {N}\left( \mu ^{k}_{i}, \sigma ^{k}_{i}\right) , \forall y_{i}\in \mathcal {Y}, where \left( \mu ^{k}_{0}, \sigma ^{k}_{0}\right)  is computed from the output of the last layer and \left( \mu ^{k}_{i}, \sigma ^{k}_{i}\right)  is computed by concatenating the output of the last layer and the skip connection from the corresponding modality. Note that only modalities that have skip connections (segmentation and sketch, i.e. i=1,4) contribute to the computation. Other modalities (text and style reference) only provide global information but not local details. The latent feature map z^k produced by Local PoE-Net and the feature vector w produced by Global PoE-Net are fed to our local-global adaptive instance normalization (LG-AdaIN) layer, \begin{aligned} \text {LG-AdaIN}(h^{k}, z^{k}, w) = \gamma _{w}\left( \gamma _{z^{k}}\frac{h^{k}-\mu (h^{k})}{\sigma (h^{k})}+\beta _{z^{k}}\right) +\beta _{w}\,, \end{aligned}
(5)
where h^k is a feature map in the residual branch after convolution, \mu (h^{k}) and \sigma (h^{k}) are channel-wise mean and standard deviation. \beta _{w}, \gamma _{w} are feature vectors computed from w, while \beta _{z^{k}}, \gamma _{z^{k}} are feature maps computed from z^{k}. The LG-AdaIN layer can be viewed as a combination of AdaIN [17] and SPADE [36] that takes both a global feature vector and a spatially-varying feature map to modulate the activations. Fig. 5.Comparison between the standard projection discriminator and our proposed multiscale multimodal projection discriminator. 


3.4 Multiscale Multimodal Projection Discriminator:
Our discriminator receives the image x and a set of conditions \mathcal {Y} as inputs and produces a score D(x, \mathcal {Y})=\text {sigmoid}(f(x, \mathcal {Y})) indicating the realness of x given \mathcal {Y} . Under the GAN objective [11], the optimal solution of f is \begin{aligned} f^*(x, \mathcal {Y}) = \underbrace{\log \frac{q(x)}{p(x)}}_{\text {unconditional term}}+\sum _{y_{i}\in \mathcal {Y}}\underbrace{\log \frac{q(y_{i}|x)}{p(y_{i}|x)}}_{\text {conditional term}}\,, \end{aligned}
(6)
if we assume conditional independence of different modalities given x. The projection discriminator (PD) [33] proposes to use the inner product to estimate the conditional term. This implementation restricts the conditional term to be relatively simple, which imposes a good inductive bias that leads to strong empirical results. We propose a multimodal projection discriminator (MPD) that generalizes PD to our multimodal setting. As shown in Fig. 5a, the original PD first encodes both the image and the conditional input into a shared latent space. It then uses a linear layer to estimate the unconditional term from the image embedding and uses the inner product between the image embedding and the conditional embedding to estimate the conditional term. The unconditional term and the conditional term are summed to obtain the final discriminator logits. In our multimodal scenario, we simply encode each observed modality and add its inner product with the image embedding to the final loss (Fig. 5b) \begin{aligned} f(x, \mathcal {Y}) = \text {Linear}(D_{x}(x)) + \sum _{y_{i}\in \mathcal {Y}}D^{T}_{y_{i}}(y_i)D_{x}(x)\,. \end{aligned}
(7)

Fig. 6.Visual comparison of segmentation-to-image synthesis on MS-COCO 2017. 
Fig. 7.Visual comparison of text-to-image synthesis on MS-COCO 2017. 
For spatial modalities such as segmentation and sketch, it is more effective to enforce their alignment with the image in multiple scales [29]. As shown in Fig. 5c, we encode the image and spatial modalities into feature maps of different resolutions and compute the MPD loss at each resolution. We compute a loss value at each location and resolution, and obtain the final loss by averaging first across locations then across resolutions. The resulting discriminator is named as the multiscale multimodal projection discriminator (MMPD) and detailed in Supplementary Material B. 

3.5 Losses and Training Procedure:
Latent regularization. Under the PoE assumption (Eq. (2)), the marginalized conditional latent distribution should match the unconditional prior: \begin{aligned} \int p(z|y_{i})p(y_{i})dy_{i}=p(z|\varnothing )=p(z)\,. \end{aligned}
(8)
To this end, we minimize the Kullback-Leibler (KL) divergence from the prior distribution p(z) to the conditional latent distribution p(z|y_{i}) at every resolution \begin{aligned} \mathcal {L}_{\text {KL}} = \sum _{y_{i}\in \mathcal {Y}}\omega _{i}\sum _{k}\omega ^{k}\mathbb {E}_{p(z^{<k}|y_{i})}\big [D_{\text {KL}}(p(z^{k}|z^{<k}, y_{i})||p(z^{k}|z^{<k}))\big ]\,, \end{aligned}
(9)
where \omega ^{k} is a resolution-dependent rebalancing weight and \omega _{i} is a modality-specific loss weight. We describe both weights in detail in Supplementary Material B. The KL loss also reduces conditional mode collapse since it encourages the conditional latent distribution to be close to the prior and therefore have high entropy. From the perspective of information bottleneck [1], the KL loss encourages each modality to only provide the minimum information necessary to specify the conditional image distribution. Contrastive Losses. The contrastive loss has been widely adopted in representation learning [5, 13] and more recently in image synthesis [12, 28, 35, 61]. Given a batch of paired vectors (\textbf{u}, \textbf{v})=\{(u_{i}, v_{i}), i=1, 2, ..., N\}, the symmetric cross-entropy loss [38, 64] maximizes the similarity of the vectors in a pair while keeping non-paired vectors apart \begin{aligned} \mathcal {L}^{\text {ce}}(&\textbf{u}, \textbf{v}) = -\frac{1}{2N}\sum _{i=1}^{N}\log \frac{\exp (\cos (u_{i}, v_{i})/\tau )}{\sum _{j=1}^{N}\exp (\cos (u_{i}, v_{j})/\tau )} \nonumber \\ {}&-\frac{1}{2N}\sum _{i=1}^{N}\log \frac{\exp (\cos (u_{i}, v_{i})/\tau )}{\sum _{j=1}^{N}\exp (\cos (u_{j}, v_{i})/\tau )} \,, \end{aligned}
(10)
where \tau  is a temperature hyper-parameter. We use two kinds of pairs to construct two loss terms: the image contrastive loss and the conditional contrastive loss. The image contrastive loss maximizes the similarity between a real image x and a fake image \tilde{x} synthesized given the corresponding conditional inputs: \begin{aligned} \mathcal {L}_{cx} = \mathcal {L}^{\text {ce}}(E_{\text {vgg}}(\textbf{x}), E_{\text {vgg}}(\tilde{\textbf{x}}))\,, \end{aligned}
(11)
where E_{\text {vgg}} is a pretrained VGG [47] encoder. This loss serves a similar purpose to the widely used perceptual loss but has been found to perform better [35, 61]. The conditional contrastive loss aims to better align images with the corresponding conditions. Specifically, the discriminator is trained to maximize the similarity between its embedding of a real image \textbf{x} and the conditional input \textbf{y}_{i}. \begin{aligned} \mathcal {L}_{cy}^{D} = \sum _{i=1}^{M}\mathcal {L}^{\text {ce}}(D_{x}(\textbf{x}), D_{y_{i}}(\textbf{y}_{i}))\,, \end{aligned}
(12)
where D_x and D_{y_{i}} are two modules in the discriminator that extract features from x and y_i, respectively, as shown in Eq. (7) and Fig. 5b. The generator is trained with the same loss, but using the generated image \tilde{\textbf{x}} instead of the real image to compute the discriminator embedding, \begin{aligned} \mathcal {L}_{cy}^{G} = \sum _{i=1}^{M}\mathcal {L}^{\text {ce}}(D_{x}(\tilde{\textbf{x}}), D_{y_{i}}(\textbf{y}_{i}))\,. \end{aligned}
(13)
In practice, we only use the conditional contrastive loss for text since it consumes too much GPU memory to use the conditional contrastive loss for the other modalities, especially when the image resolution and batch size are large. A similar image-text contrastive loss is used in XMC-GAN [61], where they use a non-symmetric cross-entropy loss that only includes the first term in Eq. (10). Full Training Objective. In summary, the generator loss \mathcal {L}^{G} and the discriminator loss \mathcal {L}^{D} can be written as \begin{aligned}&\mathcal {L}^{G}=\mathcal {L}^{G}_{\text {GAN}} + \mathcal {L}_{\text {KL}} + \lambda _{1}\mathcal {L}_{cx} + \lambda _{2}\mathcal {L}_{cy}^{G},\ \mathcal {L}^{D}=\mathcal {L}^{D}_{\text {GAN}} + \lambda _{2}\mathcal {L}_{cy}^{D} + \lambda _{3}\mathcal {L}_{\text {GP}}\,, \end{aligned}
(14)
where \mathcal {L}^{G}_{\text {GAN}} and \mathcal {L}^{D}_{\text {GAN}} are non-saturated GAN losses [11], \mathcal {L}_{\text {GP}} is the R_1 gradient penalty loss [31], and \lambda _{1}, \lambda _{2}, \lambda _{3} are weights associated with the loss terms. Table 1. FID Comparison on MM-CelebA-HQ (1024\times 1024). We evaluate models conditioned on different modalities (from left to right: no conditions, text, segmentation, sketch, and all three modalities). The best scores are highlighted in bold
Table 2. Comparison on MS-COCO 2017 (256\times 256) using FID. We evaluate models conditioned on different modalities (from left to right: no conditions, text, segmentation, sketch, and all three modalities). The best scores are highlighted in bold
Modality Dropout. By design, our generator, discriminator, and loss terms are able to handle missing modalities. We also find that randomly dropping out some input modalities before each training iteration further improves the robustness of the generator towards missing modalities at test time. 

4 Experiments:
We evaluate the proposed approach on several datasets, including MM-CelebA-HQ [57], MS-COCO 2017 [26] with COCO-Stuff annotations [3], and a proprietary dataset of landscape images. Images are labeled with all input modalities obtained from either manual annotation or pseudo-labeling methods. More details about datasets and the pseudo-labeling procedure are in Supplementary Material A. 

4.1 Main Results:
We compare PoE-GAN with a recent multimodal image synthesis method named TediGAN [57] and also with state-of-the-art approaches specifically designed for each modality. For text-to-image, we compare with DF-GAN [51] and DM-GAN + CL [60] on MS-COCO. Since the original models are trained on the 2014 split, we retrain their models on the 2017 split using the official code. For segmentation-to-image synthesis, we compare with SPADE [36], VQGAN [8], OASIS [45], and pSp [43]. For sketch-to-image synthesis, we compare with SPADE [36] and pSp [43]. We additionally compare with StyleGAN2 [23] in the unconditional setting. We use Clean-FID [37] for benchmarking due to its reported benefits over previous implementations of FID [15].Footnote 5
Table 3. Ablation study on MM-CelebA-HQ (256\times 256). The best scores are highlighted in bold and the second best ones are underlined
Results on MM-CelebA-HQ and MS-COCO are summarized in Table 1 and Table 2, respectively. PoE-GAN obtains a much lower FID than TediGAN in all settings on MM-CelebA-HQ. In Supplementary Material C.3, we compare PoE-GAN with TediGAN in more detail and show that PoE-GAN is faster and more general than TediGAN. When conditioned on a single modality, PoE-GAN surprisingly outperforms the state-of-the-art method designed specifically for that modality on both datasets, although PoE-GAN is trained for a more general purpose. We note that PoE-GAN and TediGAN are trained on multiple modalities while other baselines are trained on an individual modality or unconditionally (StyleGAN2). In Supplementary Material C.4, we further show that PoE-GAN trained on a single modality always outperforms the multimodal-trained PoE-GAN when evaluated on that modality. This shows that the improvement of PoE-GAN over state-of-the-art unimodal image synthesis methods comes from our architecture and training scheme rather than additional annotations. In Figs. 6 and 7, we qualitatively compare PoE-GAN with previous segmentation-to-image and text-to-image methods on MS-COCO. We find that PoE-GAN produces images of much better quality and can synthesize realistic objects with complex structures, such as cats and stop signs. More qualitative comparisons are included in Supplementary Material C.5. Multimodal Generation Examples. In Fig. 8, we show example images generated by our PoE-GAN using multiple input modalities on the landscape dataset. Our model is able to synthesize a wide range of landscapes in high resolution with photo-realistic details. More results are included in Supplementary Material C.5, where we additionally show that PoE-GAN can generate diverse images when given the same conditional inputs. 

4.2 Ablation Studies:
In Tables. 3 and 4, we analyze the importance of different components of PoE-GAN. We use LPIPS [63] as an additional metric to evaluate the diversity of images conditioned on the same input. Specifically, we randomly sample two output images conditioned on the same input and report the average LPIPS distance between the two outputs. A higher LPIPS score indicates more diverse outputs. Fig. 8.Examples of multimodal conditional image synthesis results produced by PoE-GAN trained on the 1024\times 1024 landscape dataset. We show the segmentation/sketch/style inputs on the bottom right of the generated images for the results in the first row. The results in the second row additionally leverage text inputs, which are shown below the corresponding generated images. Please zoom in for details. 
Table 4. Ablation study on MS-COCO 2017 (64\times 64). The best scores are highlighted in bold and the second best ones are underlined
First, we compare our product-of-experts generator (row (g)) with a baseline that simply concatenates the embedding of all modalities, while performing modality dropout (missing modality embeddings set to zero). As seen in row (a), this baseline only works well when all modalities are available and its FID significantly drops when some modalities are missing. Further, the output images have low diversity as indicated by the LPIPS score. This is not surprising as previous work has shown that conditional GANs are prone to mode collapse [18, 59, 66]. Row (b) of Tables. 3 and 4 shows that the KL loss is important for training our model. Without it, our model suffers from low sample diversity and lack of robustness towards missing modalities, similar to the concatenation baseline described above. The variances of individual experts become near zero without the KL loss. The latent code z^k then becomes a deterministic weighted average of the mean of each expert, which is equivalent to concatenating all modality embeddings and projecting it with a linear layer. This explains why our model without the KL loss behaves similarly to the concatenation baseline. Row (c) shows that our modality dropout scheme is important for handling missing modalities. Without it, the model tends to overly rely on the most informative modality, such as segmentation in MS-COCO. Table 5. User study on text-to-image synthesis. Each column shows the percentage of users that prefer the image generated by our model over the baseline
Table 6. User study on segmentation-to-image synthesis. Each column shows the percentage of users that prefer the image generated from our model over the baseline
To evaluate the proposed multiscale multimodal discriminator architecture, we replace MMPD with a discriminator that receives concatenated images and all conditional inputs. Row (d) shows that MMPD is much more effective than such a concatenation-based discriminator in all settings. Finally in rows (e) and (f), we show that contrastive losses are useful but not essential. The image contrastive loss slightly improves FID in most settings, while the text contrastive loss improves FID for text-to-image synthesis. 

4.3 User Study:
We conduct a user study to compare PoE-GAN with state-of-the-art text-to-image and segmentation-to-image synthesis methods on MS-COCO. We show users two images generated by different algorithms from the same conditional input and ask them which one is more realistic. As shown in Table. 5 and Table. 6, the majority of users prefer PoE-GAN over the baseline methods. 

5 Conclusion:
We introduce a multimodal conditional image synthesis model based on product-of-experts and show its effectiveness for converting an arbitrary subset of input modalities to an image satisfying all conditions. While empirically superior than the prior multimodal synthesis work, it also outperforms state-of-the-art unimodal conditional image synthesis approaches when conditioned on a single modality.