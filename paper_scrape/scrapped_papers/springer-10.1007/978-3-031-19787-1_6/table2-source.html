<table class="data last-table">
 <thead class="c-article-table-head">
  <tr>
   <th class="u-text-left">
   </th>
   <th class="u-text-left">
    <p>
     Uncond
    </p>
   </th>
   <th class="u-text-left">
    <p>
     Text
    </p>
   </th>
   <th class="u-text-left">
    <p>
     Seg
    </p>
   </th>
   <th class="u-text-left">
    <p>
     Sketch
    </p>
   </th>
   <th class="u-text-left">
    <p>
     All
    </p>
   </th>
  </tr>
 </thead>
 <tbody>
  <tr>
   <td class="u-text-left">
    <p>
     StyleGAN2 [
     <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d67102611e8368" title="Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of StyleGAN. In: CVPR (2020)">
      23
     </a>
     ]
    </p>
   </td>
   <td class="u-text-left">
    <p>
     43.6
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     DF-GAN [
     <a aria-label="Reference 51" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR51" id="ref-link-section-d67102611e8406" title="Tao, M., et al.: DF-GAN: deep fusion generative adversarial networks for text-to-image synthesis. arXiv preprint 
                arXiv:2008.05865
                
               (2020)">
      51
     </a>
     ]
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     45.2
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     DM-GAN + CL [
     <a aria-label="Reference 60" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR60" id="ref-link-section-d67102611e8444" title="Ye, H., Yang, X., Takac, M., Sunderraman, R., Ji, S.: Improving text-to-image synthesis using contrastive learning. arXiv preprint 
                arXiv:2107.02423
                
               (2021)">
      60
     </a>
     ]
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     29.9
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     SPADE-Seg [
     <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d67102611e8482" title="Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with spatially-adaptive normalization. In: CVPR (2019)">
      36
     </a>
     ]
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     22.1
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     VQGAN [
     <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d67102611e8520" title="Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image synthesis. In: CVPR (2021)">
      8
     </a>
     ]
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     21.6
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     OASIS [
     <a aria-label="Reference 45" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR45" id="ref-link-section-d67102611e8559" title="Schönfeld, E., Sushko, V., Zhang, D., Gall, J., Schiele, B., Khoreva, A.: You only need adversarial supervision for semantic image synthesis. In: ICLR (2020)">
      45
     </a>
     ]
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     19.2
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     SPADE-Sketch [
     <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d67102611e8597" title="Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with spatially-adaptive normalization. In: CVPR (2019)">
      36
     </a>
     ]
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
   <td class="u-text-left">
    <p>
     63.7
    </p>
   </td>
   <td class="u-text-left">
    <p>
     —
    </p>
   </td>
  </tr>
  <tr>
   <td class="u-text-left">
    <p>
     PoE-GAN (Ours)
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      43.4
     </b>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      20.5
     </b>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      15.8
     </b>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      25.5
     </b>
    </p>
   </td>
   <td class="u-text-left">
    <p>
     <b>
      13.6
     </b>
    </p>
   </td>
  </tr>
 </tbody>
</table>
