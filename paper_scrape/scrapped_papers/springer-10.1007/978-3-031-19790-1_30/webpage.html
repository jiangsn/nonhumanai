<html class="js" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta content="pc,mobile" name="applicable-device"/>
  <meta content="Yes" name="access"/>
  <meta content="SpringerLink" name="twitter:site"/>
  <meta content="summary" name="twitter:card"/>
  <meta content="Content cover image" name="twitter:image:alt"/>
  <meta content="Diverse Generation from a Single Video Made Possible" name="twitter:title"/>
  <meta content="GANs are able to perform generation and manipulation tasks, trained on a single video. However, these single video GANs require unreasonable amount of time to train on a single video, rendering them almost impractical. In this paper we question the necessity of a GAN..." name="twitter:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-031-19790-1.jpg" name="twitter:image"/>
  <meta content="10.1007/978-3-031-19790-1_30" name="dc.identifier"/>
  <meta content="10.1007/978-3-031-19790-1_30" name="DOI"/>
  <meta content="GANs are able to perform generation and manipulation tasks, trained on a single video. However, these single video GANs require unreasonable amount of time to train on a single video, rendering them almost impractical. In this paper we question the necessity of a GAN..." name="dc.description"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/content/pdf/10.1007/978-3-031-19790-1_30.pdf" name="citation_pdf_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30" name="citation_fulltext_html_url"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30" name="citation_abstract_html_url"/>
  <meta content="Computer Vision – ECCV 2022" name="citation_inbook_title"/>
  <meta content="Diverse Generation from a Single Video Made Possible" name="citation_title"/>
  <meta content="2022" name="citation_publication_date"/>
  <meta content="491" name="citation_firstpage"/>
  <meta content="509" name="citation_lastpage"/>
  <meta content="en" name="citation_language"/>
  <meta content="10.1007/978-3-031-19790-1_30" name="citation_doi"/>
  <meta content="springer/eccv, dblp/eccv" name="citation_conference_series_id"/>
  <meta content="European Conference on Computer Vision" name="citation_conference_title"/>
  <meta content="ECCV" name="citation_conference_abbrev"/>
  <meta content="184873" name="size"/>
  <meta content="GANs are able to perform generation and manipulation tasks, trained on a single video. However, these single video GANs require unreasonable amount of time to train on a single video, rendering them almost impractical. In this paper we question the necessity of a GAN..." name="description"/>
  <meta content="Haim, Niv" name="citation_author"/>
  <meta content="niv.haim@weizmann.ac.il" name="citation_author_email"/>
  <meta content="Weizmann Institute of Science" name="citation_author_institution"/>
  <meta content="Feinstein, Ben" name="citation_author"/>
  <meta content="Weizmann Institute of Science" name="citation_author_institution"/>
  <meta content="Granot, Niv" name="citation_author"/>
  <meta content="Weizmann Institute of Science" name="citation_author_institution"/>
  <meta content="Shocher, Assaf" name="citation_author"/>
  <meta content="Weizmann Institute of Science" name="citation_author_institution"/>
  <meta content="Bagon, Shai" name="citation_author"/>
  <meta content="Weizmann Institute of Science" name="citation_author_institution"/>
  <meta content="Dekel, Tali" name="citation_author"/>
  <meta content="Weizmann Institute of Science" name="citation_author_institution"/>
  <meta content="Irani, Michal" name="citation_author"/>
  <meta content="Weizmann Institute of Science" name="citation_author_institution"/>
  <meta content="Springer, Cham" name="citation_publisher"/>
  <meta content="http://api.springer-com.proxy.lib.ohio-state.edu/xmldata/jats?q=doi:10.1007/978-3-031-19790-1_30&amp;api_key=" name="citation_springer_api_url"/>
  <meta content="telephone=no" name="format-detection"/>
  <meta content="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30" property="og:url"/>
  <meta content="Paper" property="og:type"/>
  <meta content="SpringerLink" property="og:site_name"/>
  <meta content="Diverse Generation from a Single Video Made Possible" property="og:title"/>
  <meta content="GANs are able to perform generation and manipulation tasks, trained on a single video. However, these single video GANs require unreasonable amount of time to train on a single video, rendering them almost impractical. In this paper we question the necessity of a GAN..." property="og:description"/>
  <meta content="https://static-content.springer.com/cover/book/978-3-031-19790-1.jpg" property="og:image"/>
  <title>
   Diverse Generation from a Single Video Made Possible | SpringerLink
  </title>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="shortcut icon"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico" rel="icon" sizes="16x16 32x32 48x48"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png" rel="icon" sizes="16x16" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png" rel="icon" sizes="32x32" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png" rel="icon" sizes="48x48" type="image/png"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png" rel="apple-touch-icon" sizes="72x72"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png" rel="apple-touch-icon" sizes="76x76"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png" rel="apple-touch-icon" sizes="114x114"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png" rel="apple-touch-icon" sizes="120x120"/>
  <link href="/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png" rel="apple-touch-icon" sizes="144x144"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png" rel="apple-touch-icon" sizes="152x152"/>
  <link href="/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png" rel="apple-touch-icon" sizes="180x180"/>
  <script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_SVG.js">
  </script>
  <script>
   (function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)
  </script>
  <script data-consent="link-springer-com.proxy.lib.ohio-state.edu" src="/static/js/lib/cookie-consent.min.js">
  </script>
  <style>
   @media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}body,img{max-width:100%}body{background:#fcfcfc;font-size:1.125rem;line-height:1.5;min-height:100%}main{display:block}h1{font-family:Georgia,Palatino,serif;font-size:2.25rem;font-style:normal;font-weight:400;line-height:1.4;margin:.67em 0}a{background-color:transparent;color:#004b83;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}sup{font-size:75%;line-height:0;position:relative;top:-.5em;vertical-align:baseline}img{border:0;height:auto;vertical-align:middle}button,input{font-family:inherit;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}*{margin:0}h2{font-family:Georgia,Palatino,serif;font-size:1.75rem;font-style:normal;font-weight:400;line-height:1.4}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input,p{margin:0;padding:0}a>img{vertical-align:middle}p{overflow-wrap:break-word;word-break:break-word}.c-app-header__theme{border-top-left-radius:2px;border-top-right-radius:2px;height:50px;margin:-16px -16px 0;overflow:hidden;position:relative}@media only screen and (min-width:1024px){.c-app-header__theme:after{background-color:hsla(0,0%,100%,.15);bottom:0;content:"";position:absolute;right:0;top:0;width:456px}}.c-app-header__content{padding-top:16px}@media only screen and (min-width:1024px){.c-app-header__content{display:flex}}.c-app-header__main{display:flex;flex:1 1 auto}.c-app-header__cover{margin-right:16px;margin-top:-50px;position:relative;z-index:5}.c-app-header__cover img{border:2px solid #fff;border-radius:4px;box-shadow:0 0 5px 2px hsla(0,0%,50%,.2);max-height:125px;max-width:96px}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-breadcrumbs>li{display:inline}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;list-style:none;margin:0;padding:16px}@media only screen and (min-width:540px){.c-pagination{justify-content:center}}.c-pagination__item{margin-bottom:8px;margin-right:16px}.c-pagination__item:last-child{margin-right:0}.c-pagination__link{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;min-width:30px;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link svg,.c-pagination__link--disabled svg{fill:currentcolor}.c-pagination__link:visited{color:#004b83}.c-pagination__link:focus,.c-pagination__link:hover{border:1px solid #666;text-decoration:none}.c-pagination__link:focus,.c-pagination__link:hover{background-color:#666;background-image:none;color:#fff}.c-pagination__link:focus svg path,.c-pagination__link:hover svg path{fill:#fff}.c-pagination__link--disabled{align-items:center;background-color:transparent;background-image:none;border-radius:2px;color:#333;cursor:default;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;opacity:.67;padding:8px;position:relative;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link--disabled:visited{color:#333}.c-pagination__link--disabled,.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{border:1px solid #ccc;text-decoration:none}.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{background-color:transparent;background-image:none;color:#333}.c-pagination__link--disabled:focus svg path,.c-pagination__link--disabled:hover svg path{fill:#333}.c-pagination__link--active{background-color:#666;background-image:none;border-color:#666;color:#fff;cursor:default}.c-pagination__ellipsis{background:0 0;border:0;min-width:auto;padding-left:0;padding-right:0}.c-pagination__icon{fill:#999;height:12px;width:16px}.c-pagination__icon--active{fill:#004b83}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#666;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-box{background-color:#fff;border:1px solid #ccc;border-radius:2px;line-height:1.3;padding:16px}.c-box--shadowed{box-shadow:0 0 5px 0 hsla(0,0%,50%,.1)}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #f2f2f2;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__section-item .c-article-section__title-number{display:none}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff}.c-pdf-download__link .u-icon{padding-top:2px}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}.u-button svg,.u-button--primary svg{fill:currentcolor}.app-elements .c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;line-height:1.4;padding:8px 0 0}.app-elements .c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav{border-top:2px solid #cedbe0;padding-top:4px;position:relative}.app-elements .c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav-container>:not(:last-child){margin-right:32px}.app-elements .c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.app-elements .c-header__list{list-style:none;margin:0;padding:0}.app-elements .c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.app-elements .c-header__list-item:not(:last-child){border-bottom:2px solid #cedbe0}.app-elements .c-header__item{color:inherit}@media only screen and (min-width:540px){.app-elements .c-header__item--menu{display:none;visibility:hidden}.app-elements .c-header__item--menu:first-child+*{margin-block-start:0}}.app-elements .c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:540px){.app-elements .c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.app-elements .c-header__item--divider:before{border-left:2px solid #cedbe0;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.app-elements .c-header__brand a{display:block;line-height:1;padding:16px 8px;text-decoration:none}.app-elements .c-header__brand img{height:24px;width:auto}.app-elements .c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-elements .c-header__link--static{flex:0 0 auto}.app-elements .c-header__icon{fill:currentcolor;display:inline-block;font-size:24px;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.app-elements .c-header__icon+*{margin-left:8px}.app-elements .c-header__expander{background-color:#ebf1f5}.app-elements .c-header__search{padding:24px 0}@media only screen and (min-width:540px){.app-elements .c-header__search{max-width:70%}}.app-elements .c-header__search-container{position:relative}.app-elements .c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.app-elements .c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.app-elements .c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.app-elements .has-tethered.c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:540px){.app-elements .has-tethered.c-header__expander--menu{display:none;visibility:hidden}}.app-elements .has-tethered .c-header__heading{display:none;visibility:hidden}.app-elements .has-tethered .c-header__heading:first-child+*{margin-block-start:0}.app-elements .has-tethered .c-header__search{margin:auto}.app-elements .c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-justify-content-space-between{justify-content:space-between}.u-flex-shrink{flex:0 1 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-mt-0{margin-top:0}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-float-left{float:left}.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-text-sm{font-size:1rem}.u-text-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-h3{font-family:Georgia,Palatino,serif;font-size:1.5rem;font-style:normal;font-weight:400;line-height:1.4}.c-article-section__content p{line-height:1.8}.c-pagination__input{border:1px solid #bfbfbf;border-radius:2px;box-shadow:inset 0 2px 6px 0 rgba(51,51,51,.2);box-sizing:initial;display:inline-block;height:28px;margin:0;max-width:64px;min-width:16px;padding:0 8px;text-align:center;transition:width .15s ease 0s}.c-pagination__input::-webkit-inner-spin-button,.c-pagination__input::-webkit-outer-spin-button{-webkit-appearance:none;margin:0}.c-article-associated-content__container .c-article-associated-content__collection-label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.063rem}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.063rem;font-weight:400}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-section__title,.c-article-title{font-weight:400}.c-chapter-book-series{font-size:1rem}.c-chapter-identifiers{margin:16px 0 8px}.c-chapter-book-details{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative}.c-chapter-book-details__title{font-weight:700}.c-chapter-book-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-chapter-book-details a{color:inherit}@media only screen and (max-width:539px){.c-chapter-book-details__meta{display:block}}.c-cover-image-lightbox{align-items:center;bottom:0;display:flex;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;transition:all .15s ease-in 0s;visibility:hidden;z-index:-1}.js-cover-image-lightbox--close{background:0 0;border:0;color:#fff;cursor:pointer;font-size:1.875rem;padding:13px;position:absolute;right:10px;top:0}.c-cover-image-lightbox__image{max-height:90vh;width:auto}.c-expand-overlay{background:#fff;color:#333;opacity:.5;padding:2px;position:absolute;right:3px;top:3px}.c-pdf-download__link{padding:13px 24px} }
  </style>
  <link data-inline-css-source="critical-css" data-test="critical-css-handler" href="/oscar-static/app-springerlink/css/enhanced-article-927ffe4eaf.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null" rel="stylesheet"/>
  <script>
   window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/978-3-031-19790-1_30","Page":"chapter","page":{"attributes":{"environment":"live"}},"Country":"US","japan":false,"doi":"10.1007-978-3-031-19790-1_30","Keywords":"","kwrd":[],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate","cobranding","doNotAutoAssociate","cobranding"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["3000266689","8200724141"],"businessPartnerIDString":"3000266689|8200724141"}},"Access Type":"subscription","Bpids":"3000266689, 8200724141","Bpnames":"OhioLINK Consortium, Ohio State University Libraries","BPID":["3000266689","8200724141"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-978-3-031-19790-1","Full HTML":"Y","session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1611-3349","pissn":"0302-9743"},"book":{"doi":"10.1007/978-3-031-19790-1","title":"Computer Vision – ECCV 2022","pisbn":"978-3-031-19789-5","eisbn":"978-3-031-19790-1","bookProductType":"Proceedings","seriesTitle":"Lecture Notes in Computer Science","seriesId":"558"},"chapter":{"doi":"10.1007/978-3-031-19790-1_30"},"type":"ConferencePaper","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"SCI","secondarySubjects":{"1":"Image Processing and Computer Vision","2":"Information Systems and Communication Service","3":"Computer Appl. in Social and Behavioral Sciences","4":"Pattern Recognition","5":"Machine Learning"},"secondarySubjectCodes":{"1":"SCI22021","2":"SCI18008","3":"SCI23028","4":"SCI2203X","5":"SCI21010"}},"sucode":"SUCO11645"},"attributes":{"deliveryPlatform":"oscar"},"country":"US","Has Preview":"N","subjectCodes":"SCI,SCI22021,SCI18008,SCI23028,SCI2203X,SCI21010","PMC":["SCI","SCI22021","SCI18008","SCI23028","SCI2203X","SCI21010"]},"Event Category":"Conference Paper","ConferenceSeriesId":"eccv, eccv","productId":"9783031197901"}];
  </script>
  <script>
   window.dataLayer.push({
        ga4MeasurementId: 'G-B3E4QL2TPR',
        ga360TrackingId: 'UA-26408784-1',
        twitterId: 'o47a7',
        ga4ServerUrl: 'https://collect-springer-com.proxy.lib.ohio-state.edu',
        imprint: 'springerlink'
    });
  </script>
  <script data-test="gtm-head">
   window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
  </script>
  <script>
   (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp-static-springer-com.proxy.lib.ohio-state.edu/production_live/en/consent-bundle-17-36.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/lib/cookie-consent.min.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
  </script>
  <script>
   (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
  </script>
  <script>
   (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
  </script>
  <script class="js-entry">
   if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-f934ac6316.js', 'async': false},
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/app-es5-bundle-774ca0a0f5.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/app-es6-bundle-047cc3c848.js', 'async': false, 'module': true}
                
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-c14b406246.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
  </script>
  <script src="/oscar-static/js/airbrake-es5-bundle-f934ac6316.js">
  </script>
  <script src="/oscar-static/js/polyfill-es5-bundle-17b14d8af4.js">
  </script>
  <link href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30" rel="canonical"/>
  <script type="application/ld+json">
   {"headline":"Diverse Generation from a Single Video Made Possible","pageEnd":"509","pageStart":"491","image":"https://media-springernature-com.proxy.lib.ohio-state.edu/w153/springer-static/cover/book/978-3-031-19790-1.jpg","genre":["Computer Science","Computer Science (R0)"],"isPartOf":{"name":"Computer Vision – ECCV 2022","isbn":["978-3-031-19790-1","978-3-031-19789-5"],"@type":"Book"},"publisher":{"name":"Springer Nature Switzerland","logo":{"url":"https://www-springernature-com.proxy.lib.ohio-state.edu/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Niv Haim","affiliation":[{"name":"Weizmann Institute of Science","address":{"name":"Weizmann Institute of Science, Rehovot, Israel","@type":"PostalAddress"},"@type":"Organization"}],"email":"niv.haim@weizmann.ac.il","@type":"Person"},{"name":"Ben Feinstein","affiliation":[{"name":"Weizmann Institute of Science","address":{"name":"Weizmann Institute of Science, Rehovot, Israel","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Niv Granot","affiliation":[{"name":"Weizmann Institute of Science","address":{"name":"Weizmann Institute of Science, Rehovot, Israel","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Assaf Shocher","affiliation":[{"name":"Weizmann Institute of Science","address":{"name":"Weizmann Institute of Science, Rehovot, Israel","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Shai Bagon","affiliation":[{"name":"Weizmann Institute of Science","address":{"name":"Weizmann Institute of Science, Rehovot, Israel","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Tali Dekel","affiliation":[{"name":"Weizmann Institute of Science","address":{"name":"Weizmann Institute of Science, Rehovot, Israel","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Michal Irani","affiliation":[{"name":"Weizmann Institute of Science","address":{"name":"Weizmann Institute of Science, Rehovot, Israel","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"keywords":"","description":"GANs are able to perform generation and manipulation tasks, trained on a single video. However, these single video GANs require unreasonable amount of time to train on a single video, rendering them almost impractical. In this paper we question the necessity of a GAN for generation from a single video, and introduce a non-parametric baseline for a variety of generation and manipulation tasks. We revive classical space-time patches-nearest-neighbors approaches and adapt them to a scalable unconditional generative model, without any learning. This simple baseline surprisingly outperforms single-video GANs in visual quality and realism (confirmed by quantitative and qualitative evaluations), and is disproportionately faster (runtime reduced from several days to seconds). Other than diverse video generation, we demonstrate other applications using the same framework, including video analogies and spatio-temporal retargeting. Our proposed approach is easily scaled to Full-HD videos. These observations show that the classical approaches, if adapted correctly, significantly outperform heavy deep learning machinery for these tasks. This sets a new baseline for single-video generation and manipulation tasks, and no less important – makes diverse generation from a single video practically possible for the first time.","datePublished":"2022","isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle","@context":"https://schema.org"}
  </script>
  <style type="text/css">
   .c-cookie-banner {
			background-color: #01324b;
			color: white;
			font-size: 1rem;
			position: fixed;
			bottom: 0;
			left: 0;
			right: 0;
			padding: 16px 0;
			font-family: sans-serif;
			z-index: 100002;
			text-align: center;
		}
		.c-cookie-banner__container {
			margin: 0 auto;
			max-width: 1280px;
			padding: 0 16px;
		}
		.c-cookie-banner p {
			margin-bottom: 8px;
		}
		.c-cookie-banner p:last-child {
			margin-bottom: 0;
		}	
		.c-cookie-banner__dismiss {
			background-color: transparent;
			border: 0;
			padding: 0;
			margin-left: 4px;
			color: inherit;
			text-decoration: underline;
			font-size: inherit;
		}
		.c-cookie-banner__dismiss:hover {
			text-decoration: none;
		}
  </style>
  <style type="text/css">
   .MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
  </style>
  <style type="text/css">
   #MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
  </style>
  <style type="text/css">
   .MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
  </style>
  <style type="text/css">
   #MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
  </style>
  <style type="text/css">
   .MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
  </style>
  <style type="text/css">
   .MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <style type="text/css">
   .MathJax_SVG_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax_SVG .MJX-monospace {font-family: monospace}
.MathJax_SVG .MJX-sans-serif {font-family: sans-serif}
#MathJax_SVG_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax_SVG {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax_SVG * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_SVG > div {display: inline-block}
.mjx-svg-href {fill: blue; stroke: blue}
.MathJax_SVG_Processing {visibility: hidden; position: absolute; top: 0; left: 0; width: 0; height: 0; overflow: hidden; display: block!important}
.MathJax_SVG_Processed {display: none!important}
.MathJax_SVG_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_SVG_test.mjx-test-display {display: table!important}
.MathJax_SVG_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_SVG_test.mjx-test-default {display: block!important; clear: both}
.MathJax_SVG_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .MathJax_SVG_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_SVG_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_SVG_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax_SVG .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
  </style>
  <script>
   window.dataLayer = window.dataLayer || [];
            window.dataLayer.push({
                recommendations: {
                    recommender: 'semantic',
                    model: 'specter',
                    policy_id: 'NA',
                    timestamp: 1698023123,
                    embedded_user: 'null'
                }
            });
  </script>
 </head>
 <body class="shared-article-renderer">
  <div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;">
   <div id="MathJax_SVG_Hidden">
   </div>
   <svg>
    <defs id="MathJax_SVG_glyphs">
     <path d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z" id="MJMATHI-4F" stroke-width="1">
     </path>
     <path d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" id="MJMAIN-28" stroke-width="1">
     </path>
     <path d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" id="MJMATHI-4E" stroke-width="1">
     </path>
     <path d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" id="MJMAIN-32" stroke-width="1">
     </path>
     <path d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" id="MJMAIN-29" stroke-width="1">
     </path>
     <path d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" id="MJMAIN-6C" stroke-width="1">
     </path>
     <path d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" id="MJMAIN-6F" stroke-width="1">
     </path>
     <path d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" id="MJMAIN-67" stroke-width="1">
     </path>
     <path d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" id="MJMATHI-78" stroke-width="1">
     </path>
     <path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" id="MJMAIN-30" stroke-width="1">
     </path>
     <path d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" id="MJMATHI-79" stroke-width="1">
     </path>
     <path d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z" id="MJMATHI-51" stroke-width="1">
     </path>
     <path d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" id="MJMATHI-6E" stroke-width="1">
     </path>
     <path d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z" id="MJMAIN-7B" stroke-width="1">
     </path>
     <path d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z" id="MJMAIN-2026" stroke-width="1">
     </path>
     <path d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" id="MJMAIN-2C" stroke-width="1">
     </path>
     <path d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z" id="MJMAIN-7D" stroke-width="1">
     </path>
     <path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" id="MJMAIN-3D" stroke-width="1">
     </path>
     <path d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" id="MJMAIN-2212" stroke-width="1">
     </path>
     <path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" id="MJMAIN-31" stroke-width="1">
     </path>
     <path d="M473 86Q483 86 483 67Q483 63 483 61T483 56T481 53T480 50T478 48T474 47T470 46T464 44Q428 35 391 14T316 -55T264 -168Q264 -170 263 -173T262 -180T261 -184Q259 -194 251 -194Q242 -194 238 -176T221 -121T180 -49Q169 -34 155 -21T125 2T95 20T67 33T44 42T27 47L21 49Q17 53 17 67Q17 87 28 87Q33 87 42 84Q158 52 223 -45L230 -55V312Q230 391 230 482T229 591Q229 662 231 676T243 693Q244 694 251 694Q264 692 270 679V-55L277 -45Q307 1 353 33T430 76T473 86Z" id="MJMAIN-2193" stroke-width="1">
     </path>
     <path d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" id="MJMATHI-72" stroke-width="1">
     </path>
     <path d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" id="MJMATHI-48" stroke-width="1">
     </path>
     <path d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" id="MJMATHI-57" stroke-width="1">
     </path>
     <path d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" id="MJMATHI-54" stroke-width="1">
     </path>
     <path d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" id="MJMATHI-7A" stroke-width="1">
     </path>
     <path d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" id="MJMAIN-2B" stroke-width="1">
     </path>
     <path d="M27 414Q17 414 17 433Q17 437 17 439T17 444T19 447T20 450T22 452T26 453T30 454T36 456Q80 467 120 494T180 549Q227 607 238 678Q240 694 251 694Q259 694 261 684Q261 677 265 659T284 608T320 549Q340 525 363 507T405 479T440 463T467 455T479 451Q483 447 483 433Q483 413 472 413Q467 413 458 416Q342 448 277 545L270 555V-179Q262 -193 252 -193H250H248Q236 -193 230 -179V555L223 545Q192 499 146 467T70 424T27 414Z" id="MJMAIN-2191" stroke-width="1">
     </path>
     <path d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z" id="MJMATHI-56" stroke-width="1">
     </path>
     <path d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z" id="MJMATHI-4B" stroke-width="1">
     </path>
     <path d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" id="MJMATHI-6A" stroke-width="1">
     </path>
     <path d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" id="MJMATHI-69" stroke-width="1">
     </path>
     <path d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" id="MJMATHI-53" stroke-width="1">
     </path>
     <path d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" id="MJMAIN-3A" stroke-width="1">
     </path>
     <path d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" id="MJMATHI-3B1" stroke-width="1">
     </path>
     <path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="MJMAIN-6D" stroke-width="1">
     </path>
     <path d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" id="MJMAIN-69" stroke-width="1">
     </path>
     <path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="MJMAIN-6E" stroke-width="1">
     </path>
     <path d="M345 104T349 104T361 95T369 80T352 59Q268 -20 206 -20Q170 -20 146 3T113 53T99 104L94 129Q94 130 79 116T48 86T28 70Q22 70 15 79T7 94Q7 98 12 103T58 147L91 179V185Q91 186 91 191T92 200Q92 282 128 400T223 612T336 705Q397 705 397 636V627Q397 453 194 233Q185 223 180 218T174 211T171 208T165 201L163 186Q159 142 159 123Q159 17 208 17Q228 17 253 30T293 56T335 94Q345 104 349 104ZM360 634Q360 655 354 661T336 668Q328 668 322 666T302 645T272 592Q252 547 229 467T192 330L179 273Q179 272 186 280T204 300T221 322Q327 453 355 590Q360 612 360 634Z" id="MJMAIN-2113" stroke-width="1">
     </path>
     <path d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" id="MJMATHI-44" stroke-width="1">
     </path>
     <path d="M42 46Q74 48 94 56T118 69T128 86V634H124Q114 637 52 637H25V683H232L235 680Q237 679 322 554T493 303L578 178V598Q572 608 568 613T544 627T492 637H475V683H483Q498 680 600 680Q706 680 715 683H724V637H707Q634 633 622 598L621 302V6L614 0H600Q585 0 582 3T481 150T282 443T171 605V345L172 86Q183 50 257 46H274V0H265Q250 3 150 3Q48 3 33 0H25V46H42Z" id="MJMAIN-4E" stroke-width="1">
     </path>
     <path d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" id="MJMAIN-46" stroke-width="1">
     </path>
     <path d="M32 442L123 446Q214 450 215 450H221V409Q222 409 229 413T251 423T284 436T328 446T382 450Q480 450 540 388T600 223Q600 128 539 61T361 -6H354Q292 -6 236 28L227 34V-132H296V-194H287Q269 -191 163 -191Q56 -191 38 -194H29V-132H98V113V284Q98 330 97 348T93 370T83 376Q69 380 42 380H29V442H32ZM457 224Q457 303 427 349T350 395Q282 395 235 352L227 345V104L233 97Q274 45 337 45Q383 45 420 86T457 224Z" id="MJMAINB-70" stroke-width="1">
     </path>
     <path d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" id="MJMAIN-61" stroke-width="1">
     </path>
     <path d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" id="MJMAIN-72" stroke-width="1">
     </path>
     <path d="M401 444Q413 441 495 441Q568 441 574 444H580V382H510L409 156Q348 18 339 6Q331 -4 320 -4Q318 -4 313 -4T303 -3H288Q273 -3 264 12T221 102Q206 135 197 156L96 382H26V444H34Q49 441 145 441Q252 441 270 444H279V382H231L284 264Q335 149 338 149Q338 150 389 264T442 381Q442 382 418 382H394V444H401Z" id="MJMAINB-76" stroke-width="1">
     </path>
     <path d="M64 232T64 250T87 281H416V444Q416 608 418 612Q426 633 446 633T475 613Q477 608 477 444V281H807Q808 280 811 278T817 274T823 269T827 262T829 251Q829 230 807 221L642 220H477V57Q477 -107 475 -112Q468 -131 446 -131Q425 -131 418 -112Q416 -107 416 57V220H251L87 221Q64 232 64 250Z" id="MJMAINB-2B" stroke-width="1">
     </path>
     <path d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" id="MJMAIN-22C5" stroke-width="1">
     </path>
     <path d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" id="MJMATHI-74" stroke-width="1">
     </path>
     <path d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" id="MJMAIN-2032" stroke-width="1">
     </path>
     <path d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" id="MJMAIN-D7" stroke-width="1">
     </path>
     <path d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" id="MJMATHI-64" stroke-width="1">
     </path>
     <path d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" id="MJMAIN-33" stroke-width="1">
     </path>
     <path d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" id="MJMAIN-34" stroke-width="1">
     </path>
     <path d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" id="MJMAIN-35" stroke-width="1">
     </path>
     <path d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" id="MJMAIN-36" stroke-width="1">
     </path>
     <path d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z" id="MJMAIN-38" stroke-width="1">
     </path>
     <path d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" id="MJMAIN-39" stroke-width="1">
     </path>
     <path d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" id="MJMAIN-64" stroke-width="1">
     </path>
     <path d="M69 -66Q91 -66 104 -80T118 -116Q118 -134 109 -145T91 -160Q84 -163 97 -166Q104 -168 111 -168Q131 -168 148 -159T175 -138T197 -106T213 -75T225 -43L242 0L170 183Q150 233 125 297Q101 358 96 368T80 381Q79 382 78 382Q66 385 34 385H19V431H26L46 430Q65 430 88 429T122 428Q129 428 142 428T171 429T200 430T224 430L233 431H241V385H232Q183 385 185 366L286 112Q286 113 332 227L376 341V350Q376 365 366 373T348 383T334 385H331V431H337H344Q351 431 361 431T382 430T405 429T422 429Q477 429 503 431H508V385H497Q441 380 422 345Q420 343 378 235T289 9T227 -131Q180 -204 113 -204Q69 -204 44 -177T19 -116Q19 -89 35 -78T69 -66Z" id="MJMAIN-79" stroke-width="1">
     </path>
     <path d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" id="MJMATHI-43" stroke-width="1">
     </path>
     <path d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" id="MJMAIN-4C" stroke-width="1">
     </path>
     <path d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" id="MJMAIN-65" stroke-width="1">
     </path>
     <path d="M338 431Q344 429 422 429Q479 429 503 431H508V385H497Q439 381 423 345Q421 341 356 172T288 -2Q283 -11 263 -11Q244 -11 239 -2Q99 359 98 364Q93 378 82 381T43 385H19V431H25L33 430Q41 430 53 430T79 430T104 429T122 428Q217 428 232 431H240V385H226Q187 384 184 370Q184 366 235 234L286 102L377 341V349Q377 363 367 372T349 383T335 385H331V431H338Z" id="MJMAIN-76" stroke-width="1">
     </path>
     <path d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z" id="MJMAIN-63" stroke-width="1">
     </path>
     <path d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" id="MJMAIN-73" stroke-width="1">
     </path>
     <path d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" id="MJMAIN-74" stroke-width="1">
     </path>
     <path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="MJMAIN-68" stroke-width="1">
     </path>
     <path d="M133 736Q138 750 153 750Q164 750 170 739Q172 735 172 250T170 -239Q164 -250 152 -250Q144 -250 138 -244L137 -243Q133 -241 133 -179T132 250Q132 731 133 736ZM329 739Q334 750 346 750Q353 750 361 744L362 743Q366 741 366 679T367 250T367 -178T362 -243L361 -244Q355 -250 347 -250Q335 -250 329 -239Q327 -235 327 250T329 739Z" id="MJMAIN-2225" stroke-width="1">
     </path>
     <path d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z" id="MJMAIN-2192" stroke-width="1">
     </path>
     <path d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z" id="MJMAIN-221E" stroke-width="1">
     </path>
     <path d="M118 -250V750H255V710H158V-210H255V-250H118Z" id="MJMAIN-5B" stroke-width="1">
     </path>
     <path d="M22 710V750H159V-250H22V-210H119V710H22Z" id="MJMAIN-5D" stroke-width="1">
     </path>
    </defs>
   </svg>
  </div>
  <div id="MathJax_Message" style="">
   Typesetting math: 100%
  </div>
  <!-- Google Tag Manager (noscript) -->
  <noscript data-test="gtm-body">
   <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ" style="display:none;visibility:hidden" width="0">
   </iframe>
  </noscript>
  <!-- End Google Tag Manager (noscript) -->
  <div class="u-vh-full">
   <a class="c-skip-link" href="#main-content">
    Skip to main content
   </a>
   <div class="u-hide u-show-following-ad">
   </div>
   <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
    <div class="c-ad__inner">
     <p class="c-ad__label">
      Advertisement
     </p>
     <div data-gpt="" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;" data-gpt-unitpath="/270604982/springerlink/book/chapter" data-pa11y-ignore="" data-test="LB1-ad" id="div-gpt-ad-LB1" style="min-width:728px;min-height:90px">
     </div>
    </div>
   </aside>
   <div class="app-elements u-mb-24">
    <header class="c-header" data-header="">
     <div class="c-header__container" data-header-expander-anchor="">
      <div class="c-header__brand">
       <a data-test="logo" data-track="click" data-track-action="click logo link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu">
        <img alt="SpringerLink" src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg"/>
       </a>
      </div>
      <a class="c-header__link c-header__link--static" data-test="login-link" data-track="click" data-track-action="click log in link" data-track-category="unified header" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-031-19790-1_30">
       <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
        <use xlink:href="#icon-eds-user-single">
        </use>
       </svg>
       <span>
        Log in
       </span>
      </a>
     </div>
     <nav aria-label="header navigation" class="c-header__nav">
      <div class="c-header__nav-container">
       <div class="c-header__item c-header__item--menu">
        <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
         <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
          <use xlink:href="#icon-eds-menu">
          </use>
         </svg>
         <span>
          Menu
         </span>
        </a>
       </div>
       <div class="c-header__item c-header__item--inline-links">
        <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
         Find a journal
        </a>
        <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
         Publish with us
        </a>
       </div>
       <div class="c-header__link-container">
        <div class="c-header__item c-header__item--divider">
         <a aria-expanded="false" aria-haspopup="true" class="c-header__link" data-header-expander="" href="javascript:;" role="button">
          <svg aria-hidden="true" class="c-header__icon" focusable="false" height="24" width="24">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span>
           Search
          </span>
         </a>
        </div>
        <div class="c-header__item">
         <div class="c-header__item ecommerce-cart" id="ecommerce-header-cart-icon-link" style="display:inline-block">
          <a class="c-header__link" href="https://order-springer-com.proxy.lib.ohio-state.edu/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
           <svg aria-hidden="true" focusable="false" height="24" id="eds-i-cart" style="vertical-align:bottom" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <path d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z" fill="currentColor" fill-rule="nonzero">
            </path>
           </svg>
           <span style="padding-left:10px">
            Cart
           </span>
           <span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center">
           </span>
          </a>
          <script>
           (function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    console.log("listen to updatedCart event");
    document.body.addEventListener("updatedCart", function () {
        console.log("updatedCart happened");
        updateCartIcon().then(function () { return console.log("Cart state update upon event"); });
    }, false);
    return updateCartIcon().then(function () { return console.log("Initial cart state update"); });
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function () { return console.log("Could not fetch cart info"); });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart-springer-com.proxy.lib.ohio-state.edu/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()
          </script>
         </div>
        </div>
       </div>
      </div>
     </nav>
    </header>
    <div class="c-header__expander has-tethered u-js-hide" hidden="" id="popup-search">
     <h2 class="c-header__heading">
      Search
     </h2>
     <div class="u-container">
      <div class="c-header__search">
       <form action="//link-springer-com.proxy.lib.ohio-state.edu/search" data-track="submit" data-track-action="submit search form" data-track-category="unified header" data-track-label="form" method="GET" role="search">
        <label class="c-header__search-label" for="header-search">
         Search by keyword or author
        </label>
        <div class="c-header__search-container">
         <input autocomplete="off" class="c-header__search-input" id="header-search" name="query" required="" type="text" value=""/>
         <button class="c-header__search-button" type="submit">
          <svg aria-hidden="true" class="c-header__icon" focusable="false">
           <use xlink:href="#icon-eds-search">
           </use>
          </svg>
          <span class="u-visually-hidden">
           Search
          </span>
         </button>
        </div>
       </form>
      </div>
     </div>
    </div>
    <div class="c-header__expander c-header__expander--menu has-tethered u-js-hide" hidden="" id="header-nav">
     <h2 class="c-header__heading">
      Navigation
     </h2>
     <ul class="c-header__list">
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click find a journal" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
        Find a journal
       </a>
      </li>
      <li class="c-header__list-item">
       <a class="c-header__link" data-track="click" data-track-action="click publish with us link" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
        Publish with us
       </a>
      </li>
     </ul>
    </div>
   </div>
   <div class="u-container u-mb-32 u-clearfix" data-component="article-container" id="main-content">
    <div class="u-hide-at-lg js-context-bar-sticky-point-mobile">
     <div class="c-pdf-container">
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-031-19790-1.pdf?pdf=button" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book PDF
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
      <div class="c-pdf-download u-clear-both">
       <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-031-19790-1.epub" rel="noopener">
        <span class="c-pdf-download__text">
         <span class="u-sticky-visually-hidden">
          Download
         </span>
         book EPUB
        </span>
        <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
         <use xlink:href="#icon-download">
         </use>
        </svg>
       </a>
      </div>
     </div>
    </div>
    <header class="u-mb-24" data-test="chapter-information-header">
     <div class="c-box c-box--shadowed">
      <div class="c-app-header">
       <div class="c-app-header__theme" style="background-image: url('https://media-springernature-com.proxy.lib.ohio-state.edu/dominant-colour/springer-static/cover/book/978-3-031-19790-1.jpg')">
       </div>
       <div class="c-app-header__content">
        <div class="c-app-header__main">
         <div class="c-app-header__cover">
          <div class="c-app-expand-overlay-wrapper">
           <a data-component="cover-zoom" data-img-src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-031-19790-1" href="/chapter/10.1007/978-3-031-19790-1_30/cover" rel="nofollow">
            <picture>
             <source srcset="                                                         //media.springernature.com/w92/springer-static/cover/book/978-3-031-19790-1.jpg?as=webp 1x,                                                         //media.springernature.com/w184/springer-static/cover/book/978-3-031-19790-1.jpg?as=webp 2x" type="image/webp"/>
             <img alt="Book cover" height="130" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92/springer-static/cover/book/978-3-031-19790-1.jpg" width="90"/>
            </picture>
            <svg aria-hidden="true" class="c-expand-overlay u-icon" data-component="expand-icon" focusable="false" height="18" width="18">
             <use xlink:href="#icon-expand-image" xmlns:xlink="http://www.w3.org/1999/xlink">
             </use>
            </svg>
           </a>
          </div>
         </div>
         <div class="c-cover-image-lightbox u-hide" data-component="cover-lightbox">
          <button aria-label="Close expanded book cover" class="js-cover-image-lightbox--close" data-component="close-cover-lightbox" type="button">
           <span aria-hidden="true">
            ×
           </span>
          </button>
          <picture>
           <source srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-031-19790-1?as=webp" type="image/webp"/>
           <img alt="Book cover" class="c-cover-image-lightbox__image" height="1200" src="https://media-springernature-com.proxy.lib.ohio-state.edu/full/springer-static/cover-hires/book/978-3-031-19790-1" width="800"/>
          </picture>
         </div>
         <div class="u-flex-shrink">
          <p class="c-chapter-info-details u-mb-8">
           <a data-track="click" data-track-action="open conference" data-track-label="link" href="/conference/eccv eccv">
            European Conference on Computer Vision
           </a>
          </p>
          <p class="c-chapter-book-details right-arrow">
           ECCV 2022:
           <a class="c-chapter-book-details__title" data-test="book-link" data-track="click" data-track-action="open book series" data-track-label="link" href="/book/10.1007/978-3-031-19790-1">
            Computer Vision – ECCV 2022
           </a>
           pp
                                         491–509
           <a class="c-chapter-book-details__cite-as u-hide-print" data-track="click" data-track-action="cite this chapter" data-track-label="link" href="#citeas">
            Cite as
           </a>
          </p>
         </div>
        </div>
       </div>
      </div>
     </div>
    </header>
    <nav aria-label="breadcrumbs" class="u-mb-16" data-test="article-breadcrumbs">
     <ol class="c-breadcrumbs c-breadcrumbs--truncated" itemscope="" itemtype="https://schema.org/BreadcrumbList">
      <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb1" href="/" itemprop="item">
        <span itemprop="name">
         Home
        </span>
       </a>
       <meta content="1" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <a class="c-breadcrumbs__link" data-track="click" data-track-action="breadcrumbs" data-track-category="Conference paper" data-track-label="breadcrumb2" href="/book/10.1007/978-3-031-19790-1" itemprop="item">
        <span itemprop="name">
         Computer Vision – ECCV 2022
        </span>
       </a>
       <meta content="2" itemprop="position"/>
       <svg aria-hidden="true" class="c-breadcrumbs__chevron" focusable="false" height="10" role="img" viewbox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
        </path>
       </svg>
      </li>
      <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
       <span itemprop="name">
        Conference paper
       </span>
       <meta content="3" itemprop="position"/>
      </li>
     </ol>
    </nav>
    <main class="c-article-main-column u-float-left js-main-column u-text-sans-serif" data-track-component="conference paper">
     <div aria-hidden="true" class="c-context-bar u-hide" data-context-bar="" data-context-bar-with-recommendations="" data-test="context-bar">
      <div class="c-context-bar__container u-container">
       <div class="c-context-bar__title">
        Diverse Generation from a Single Video Made Possible
       </div>
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-031-19790-1.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-031-19790-1.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div id="recommendations">
       <div class="c-recommendations__container u-container u-display-none" data-component-recommendations="">
        <aside class="c-status-message c-status-message--success u-display-none" data-component-status-msg="">
         <svg aria-label="success:" class="c-status-message__icon" focusable="false" height="24" role="img" width="24">
          <use xlink:href="#icon-success">
          </use>
         </svg>
         <div class="c-status-message__message" id="success-message" tabindex="-1">
          Your content has downloaded
         </div>
        </aside>
        <div class="c-recommendations-header u-display-flex u-justify-content-space-between">
         <h2 class="c-recommendations-title" id="recommendation-heading">
          Similar content being viewed by others
         </h2>
         <button aria-label="Close" class="c-recommendations-close u-flex-static" data-track="click" data-track-action="close recommendations" type="button">
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-close">
           </use>
          </svg>
         </button>
        </div>
        <section aria-labelledby="recommendation-heading" aria-roledescription="carousel">
         <p class="u-visually-hidden">
          Slider with three content items shown per slide. Use the Previous and Next buttons to navigate the slides or the slide controller buttons at the end to navigate through each slide.
         </p>
         <div class="c-recommendations-list-container">
          <div class="c-recommendations-list">
           <div aria-label="Recommendation 1 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-11012-3?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 1" data-track-label="10.1007/978-3-030-11012-3_37" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-11012-3_37" itemprop="url">
                  Generating Synthetic Video Sequences by Explicitly Modeling Object Motion
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2019
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 2 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-19769-7?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 2" data-track-label="10.1007/978-3-031-19769-7_30" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-19769-7_30" itemprop="url">
                  InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2022
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 3 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-25063-7?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 3" data-track-label="10.1007/978-3-031-25063-7_13" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-25063-7_13" itemprop="url">
                  Third Time’s the Charm? Image and Video Editing with StyleGAN3
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2023
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 4 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w136h75/springer-static/image/art%3A10.1007%2Fs11263-020-01334-x/MediaObjects/11263_2020_1334_Fig1_HTML.png"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 4" data-track-label="10.1007/s11263-020-01334-x" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/s11263-020-01334-x" itemprop="url">
                  High-Quality Video Generation from Static Structural Annotations
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Article
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  28 May 2020
                 </span>
                </div>
               </div>
               <p class="c-recommendations-authors u-hide-at-sm u-sans-serif">
                Lu Sheng, Junting Pan, … Chen Change Loy
               </p>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 5 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58558-7?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 5" data-track-label="10.1007/978-3-030-58558-7_18" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58558-7_18" itemprop="url">
                  DTVNet: Dynamic Time-Lapse Video Generation via Single Still Image
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 6 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-19784-0?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 6" data-track-label="10.1007/978-3-031-19784-0_21" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-19784-0_21" itemprop="url">
                  Temporally Consistent Semantic Video Editing
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2022
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 7 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-030-58548-8?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 7" data-track-label="10.1007/978-3-030-58548-8_23" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-030-58548-8_23" itemprop="url">
                  Rotationally-Temporally Consistent Novel View Synthesis of Human Performance Video
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2020
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 8 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-20071-7?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 8" data-track-label="10.1007/978-3-031-20071-7_15" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-20071-7_15" itemprop="url">
                  FILM: Frame Interpolation for Large Motion
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2022
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
           <div aria-label="Recommendation 9 of 9" aria-roledescription="slide" class="c-recommendations-list__item" role="group">
            <article class="u-full-height c-card c-card--flush" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
             <div class="c-card__layout u-full-height">
              <div class="c-card__image">
               <img alt="" src="https://media-springernature-com.proxy.lib.ohio-state.edu/w92h120/springer-static/cover-hires/book/978-3-031-19787-1?as=webp"/>
              </div>
              <div class="c-card__body u-display-flex u-flex-direction-column">
               <div class="c-recommendations-column-switch">
                <h3 class="c-card__title-recommendation u-sans-serif" itemprop="name headline">
                 <a class="c-card__link" data-track="click" data-track-action="click recommendations - 9" data-track-label="10.1007/978-3-031-19787-1_31" href="https://link-springer-com.proxy.lib.ohio-state.edu/10.1007/978-3-031-19787-1_31" itemprop="url">
                  Layered Controllable Video Generation
                 </a>
                </h3>
                <div class="c-card__section c-meta">
                 <span class="c-meta__item u-sans-serif">
                  Chapter
                 </span>
                 <span class="c-meta__item u-sans-serif">
                  © 2022
                 </span>
                </div>
               </div>
              </div>
             </div>
            </article>
           </div>
          </div>
         </div>
        </section>
       </div>
       <div class="js-greyout-page-background" data-component-grey-background="" style="display:none">
       </div>
      </div>
     </div>
     <article lang="en">
      <header data-test="chapter-detail-header">
       <div class="c-article-header">
        <h1 class="c-article-title" data-chapter-title="" data-test="chapter-title">
         Diverse Generation from a Single Video Made Possible
        </h1>
        <ul class="c-article-author-list c-article-author-list--short js-no-scroll" data-component-authors-activator="authors-list" data-test="authors-list">
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Niv-Haim" data-corresp-id="c1" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Niv-Haim">
           Niv Haim
           <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
            <use xlink:href="#icon-email" xmlns:xlink="http://www.w3.org/1999/xlink">
            </use>
           </svg>
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Ben-Feinstein" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ben-Feinstein">
           Ben Feinstein
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Niv-Granot" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Niv-Granot">
           Niv Granot
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Assaf-Shocher" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Assaf-Shocher">
           Assaf Shocher
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Shai-Bagon" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shai-Bagon">
           Shai Bagon
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          ,
         </li>
         <li class="c-article-author-list__item c-article-author-list__item--hide-small-screen">
          <a data-author-popup="auth-Tali-Dekel" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tali-Dekel">
           Tali Dekel
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
          &amp;
         </li>
         <li aria-label="Show all 7 authors for this article" class="c-article-author-list__show-more" title="Show all 7 authors for this article">
          …
         </li>
         <li class="c-article-author-list__item">
          <a data-author-popup="auth-Michal-Irani" data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Michal-Irani">
           Michal Irani
          </a>
          <sup class="u-js-hide">
           <a href="#Aff12" tabindex="-1">
            12
           </a>
          </sup>
         </li>
        </ul>
        <button aria-expanded="false" class="c-article-author-list__button">
         <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
          <use xlink:href="#icon-plus" xmlns:xlink="http://www.w3.org/1999/xlink">
          </use>
         </svg>
         <span>
          Show authors
         </span>
        </button>
        <ul class="c-article-identifiers c-chapter-identifiers">
         <li class="c-article-identifiers__item" data-test="article-category">
          Conference paper
         </li>
         <li class="c-article-identifiers__item">
          <a data-track="click" data-track-action="publication date" data-track-label="link" href="#chapter-info">
           First Online:
           <time datetime="2022-10-24">
            24 October 2022
           </time>
          </a>
         </li>
        </ul>
        <div data-test="article-metrics">
         <div id="altmetric-container">
          <div class="c-article-metrics-bar__wrapper u-clear-both">
           <ul class="c-article-metrics-bar u-list-reset">
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              1736
              <span class="c-article-metrics-bar__label">
               Accesses
              </span>
             </p>
            </li>
            <li class="c-article-metrics-bar__item">
             <p class="c-article-metrics-bar__count">
              3
              <a class="c-article-metrics-bar__label" data-track="click" data-track-action="Citation count" data-track-label="link" href="http://citations.springer-com.proxy.lib.ohio-state.edu/item?doi=10.1007/978-3-031-19790-1_30" rel="noopener" target="_blank" title="Visit Springer Citations for full citation details">
               Citations
              </a>
             </p>
            </li>
           </ul>
          </div>
         </div>
        </div>
        <p class="c-chapter-book-series">
         Part of the
         <a data-track="click" data-track-action="open book series" data-track-label="link" href="/bookseries/558">
          Lecture Notes in Computer Science
         </a>
         book series (LNCS,volume 13677)
        </p>
       </div>
      </header>
      <div class="c-article-body" data-article-body="true">
       <section aria-labelledby="Abs1" data-title="Abstract" lang="en">
        <div class="c-article-section" id="Abs1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">
          <span class="c-article-section__title-number">
          </span>
          Abstract
         </h2>
         <div class="c-article-section__content" id="Abs1-content">
          <p>
           GANs are able to perform generation and manipulation tasks, trained on a single video. However, these single video GANs require unreasonable amount of time to train on a single video, rendering them almost impractical. In this paper we question the necessity of a GAN for generation from a single video, and introduce a non-parametric baseline for a variety of generation and manipulation tasks. We revive classical space-time patches-nearest-neighbors approaches and adapt them to a scalable unconditional generative model, without any learning. This simple baseline surprisingly outperforms single-video GANs in visual quality and realism (confirmed by quantitative and qualitative evaluations), and is disproportionately faster (runtime reduced from several days to seconds). Other than diverse video generation, we demonstrate other applications using the same framework, including video analogies and spatio-temporal retargeting. Our proposed approach is easily scaled to Full-HD videos. These observations show that the classical approaches, if adapted correctly, significantly outperform heavy deep learning machinery for these tasks. This sets a new baseline for single-video generation and manipulation tasks, and no less important – makes diverse generation from a single video practically possible for the first time.
          </p>
         </div>
        </div>
       </section>
       <div class="c-article-section__content c-article-section__content--separator">
        <p>
         N. Haim and B. Feinstein—Equal contribution.
        </p>
       </div>
       <div data-test="chapter-cobranding-and-download">
        <div class="note test-pdf-link" id="cobranding-and-download-availability-text">
         <div class="c-article-access-provider" data-component="provided-by-box">
          <p class="c-article-access-provider__text">
           Access provided by
           <span class="js-institution-name">
            Ohio State University Libraries
           </span>
          </p>
          <p class="c-article-access-provider__text">
           <a class="c-pdf-download__link" data-track="click" data-track-action="Pdf download" data-track-label="inline link" download="" href="/content/pdf/10.1007/978-3-031-19790-1_30.pdf?pdf=inline%20link" id="js-body-chapter-download" rel="noopener" style="display: inline; padding:0px!important;" target="_blank">
            Download
           </a>
           conference paper PDF
          </p>
         </div>
        </div>
       </div>
       <div class="main-content">
        <section data-title="Introduction">
         <div class="c-article-section" id="Sec1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">
           <span class="c-article-section__title-number">
            1
           </span>
           Introduction
          </h2>
          <div class="c-article-section__content" id="Sec1-content">
           <p>
            Generation and editing of natural videos remain challenging, mainly due to their large dimensionality and the enormous space of motion they span. Most modern frameworks train generative models on a large collection of videos, producing high quality results for only a limited class of videos. These include extensions of GANs [
            <a aria-label="Reference 23" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR23" id="ref-link-section-d66064129e718" title="Goodfellow, I.J., et al.: Generative adversarial networks. arXiv preprint 
                arXiv:1406.2661
                
               (2014)">
             23
            </a>
            ] to video data [
            <a aria-label="Reference 2" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR2" id="ref-link-section-d66064129e721" title="Aigner, S., Körner, M.: Futuregan: anticipating the future frames of video sequences using spatio-temporal 3d convolutions in progressively growing gans. arXiv preprint 
                arXiv:1810.01325
                
               (2018)">
             2
            </a>
            ,
            <a aria-label="Reference 36" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR36" id="ref-link-section-d66064129e724" title="Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., Levine, S.: Stochastic adversarial video prediction. arXiv preprint 
                arXiv:1804.01523
                
               (2018)">
             36
            </a>
            ,
            <a aria-label="Reference 48" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR48" id="ref-link-section-d66064129e727" title="Saito, M., Matsumoto, E., Saito, S.: Temporal generative adversarial nets with singular value clipping. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2830–2839 (2017)">
             48
            </a>
            ,
            <a aria-label="Reference 58" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR58" id="ref-link-section-d66064129e730" title="Tulyakov, S., Liu, M. Y., Yang, X., Kautz, J.: Mocogan: decomposing motion and content for video generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1526–1535 (2018)">
             58
            </a>
            ,
            <a aria-label="Reference 62" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR62" id="ref-link-section-d66064129e734" title="Vondrick, C., Pirsiavash, H., Torralba, A.: Generating videos with scene dynamics. arXiv preprint 
                arXiv:1609.02612
                
               (2016)">
             62
            </a>
            ,
            <a aria-label="Reference 66" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR66" id="ref-link-section-d66064129e737" title="Wang, Y., Bremond, F., Dantcheva, A.: Inmodegan: interpretable motion decomposition generative adversarial network for video generation. arXiv preprint 
                arXiv:2101.03049
                
               (2021)">
             66
            </a>
            ], video to video translation [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d66064129e740" title="Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: unsupervised video retargeting. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 119–135 (2018)">
             8
            </a>
            ,
            <a aria-label="Reference 16" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR16" id="ref-link-section-d66064129e743" title="Chan, C., Ginosar, S., Zhou, T., Efros, A.A.: Everybody dance now. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5933–5942 (2019)">
             16
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d66064129e746" title="Mallya, A., Wang, T.-C., Sapra, K., Liu, M.-Y.: World-consistent video-to-video synthesis. arXiv preprint 
                arXiv:2007.08509
                
               (2020)">
             40
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR63" id="ref-link-section-d66064129e749" title="Wang, T.-C., et al.: Video-to-video synthesis. arXiv preprint 
                arXiv:1808.06601
                
               (2018)">
             63
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR64" id="ref-link-section-d66064129e749_1" title="Wang, T.-C., Liu, M.-Y., Tao, A., Liu, G., Kautz, J., Catanzaro, B.: Few-shot video-to-video synthesis. arXiv preprint 
                arXiv:1910.12713
                
               (2019)">
             64
            </a>
            ,
            <a aria-label="Reference 65" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR65" id="ref-link-section-d66064129e753" title="Wang, Y., Bilinski, P., Bremond, F., Dantcheva, A: Imaginator: conditional spatio-temporal gan for video generation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1160–1169 (2020)">
             65
            </a>
            ,
            <a aria-label="Reference 71" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR71" id="ref-link-section-d66064129e756" title="Yang, Z., et al.: Transmomo: invariance-driven unsupervised video motion retargeting. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5306–5315 (2020)">
             71
            </a>
            ] and autoregressive sequence prediction [
            <a aria-label="Reference 3" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR3" id="ref-link-section-d66064129e759" title="Aksan, E., Hilliges, O.: Stcn: stochastic temporal convolutional networks. arXiv preprint 
                arXiv:1902.06568
                
               (2019)">
             3
            </a>
            ,
            <a aria-label="Reference 6" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR6" id="ref-link-section-d66064129e762" title="Babaeizadeh, M., Finn, C., Erhan, D., Campbell, R.H., Levine, S.: Stochastic variational video prediction. arXiv preprint 
                arXiv:1710.11252
                
               (2017)">
             6
            </a>
            ,
            <a aria-label="Reference 7" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR7" id="ref-link-section-d66064129e765" title="Ballas, N., Yao, L., Pal, C., Courville, A.: Delving deeper into convolutional networks for learning video representations. arXiv preprint 
                arXiv:1511.06432
                
               (2015)">
             7
            </a>
            ,
            <a aria-label="Reference 17" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR17" id="ref-link-section-d66064129e768" title="Denton, E., Fergus, R.: Stochastic video generation with a learned prior. In: International Conference on Machine Learning, pp. 1174–1183. PMLR (2018)">
             17
            </a>
            ,
            <a aria-label="Reference 22" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR22" id="ref-link-section-d66064129e772" title="Franceschi, J.-Y., Delasalles, E., Chen, M., Lamprier, S., Gallinari, P.: Stochastic latent residual video prediction. In: International Conference on Machine Learning, pp. 3233–3246. PMLR (2020)">
             22
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR59" id="ref-link-section-d66064129e775" title="Villegas, R., Yang, J., Hong, S., Lin, X., Lee, H.: Decomposing motion and content for natural video sequence prediction. arXiv preprint 
                arXiv:1706.08033
                
               (2017)">
             59
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR60" id="ref-link-section-d66064129e775_1" title="Villegas, R., Erhan, D., Lee, H., et al.: Hierarchical long-term video prediction without supervision. In: International Conference on Machine Learning, pp. 6038–6046. PMLR (2018)">
             60
            </a>
            ,
            <a aria-label="Reference 61" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR61" id="ref-link-section-d66064129e778" title="Villegas, R., Pathak, A., Kannan, H., Erhan, D., Le, Q.V., Lee, H.: High fidelity video prediction with large stochastic recurrent neural networks. arXiv preprint 
                arXiv:1911.01655
                
               (2019)">
             61
            </a>
            ].
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 1." id="figure-1">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19790-1_30/figures/1" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig1_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 1" aria-describedby="Fig1" height="335" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig1_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc">
               <p>
                We adapt classical patch-based approaches as a better, much faster non-parametric alternative to single video GANs, for a variety of video generation and manipulation tasks.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 1" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure1 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19790-1_30/figures/1" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            While externally-trained generative models produce impressive results, they are restricted to the types of video dynamics in their training set. On the other side of the spectrum are
            <i>
             single-video GANs
            </i>
            . These video generative models train on a
            <i>
             single
            </i>
            input video, learn its distribution of space-time patches, and are then able to generate a diversity of new videos with the same patch distribution [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66064129e808" title="Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)">
             5
            </a>
            ,
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e811" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
             25
            </a>
            ]. However, these take very long time to train for each input video, making them applicable to only small spatial resolutions and to very short videos (typically, very few small frames). Furthermore, their output oftentimes shows poor visual quality and noticeable visual artifacts. These shortcomings render existing single-video GANs impractical and unscalable.
           </p>
           <p>
            Video synthesis and manipulation of a single video sequence based on its distribution of space-time patches dates back to classical pre-deep learning methods. These classical methods demonstrated impressive results for various applications, such as video retargeting [
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d66064129e817" title="Krähenbühl, P., Lang, M., Hornung, A., Gross, M.: A system for retargeting of streaming video. In: ACM SIGGRAPH Asia 2009 papers (2009)">
             30
            </a>
            ,
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d66064129e820" title="Rubinstein, M., Shamir, A., Avidan, S.: Improved seam carving for video retargeting. ACM Trans. Graph. (TOG) 27(3), 1–9 (2008)">
             47
            </a>
            ,
            <a aria-label="Reference 55" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR55" id="ref-link-section-d66064129e823" title="Simakov, D., Caspi, Y., Shechtman, E., Irani, M.: Summarizing visual data using bidirectional similarity. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8. IEEE (2008)">
             55
            </a>
            ,
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d66064129e826" title="Wolf, L., Guttmann, M., Cohen-Or, D.: Non-homogeneous content-driven video-retargeting. In: Proceedings of the Eleventh IEEE International Conference on Computer Vision (ICCV) (2007)">
             70
            </a>
            ], video completion [
            <a aria-label="Reference 27" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR27" id="ref-link-section-d66064129e829" title="Huang, J.-B., Kang, S.B., Ahuja, N., Kopf, J.: Temporally coherent completion of dynamic video. ACM Trans. Graph. (TOG) 35(6), 1–11 (2016)">
             27
            </a>
            ,
            <a aria-label="Reference 34" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR34" id="ref-link-section-d66064129e833" title="Le, T.T., Almansa, A., Gousseau, Y., Masnou, S.: Motion-consistent video inpainting. In: 2017 IEEE International Conference on Image Processing (ICIP), pp. 2094–2098. IEEE (2017)">
             34
            </a>
            ,
            <a aria-label="Reference 39" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR39" id="ref-link-section-d66064129e836" title="Liu, M., Chen, S., Liu, J., Tang, X.: Video completion via motion guided spatial-temporal global optimization. In: Proceedings of the 17th ACM International Conference on Multimedia, pp. 537–540 (2009)">
             39
            </a>
            ,
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d66064129e839" title="Newson, A., Almansa, A., Fradet, M., Gousseau, Y., Pérez, P.: Towards fast, generic video inpainting. In: Proceedings of the 10th European Conference on Visual Media Production, pp. 1–8 (2013)">
             41
            </a>
            ,
            <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR42" id="ref-link-section-d66064129e842" title="Newson, A., Almansa, A., Fradet, M., Gousseau, Y., Pérez, P.: Video inpainting of complex scenes. SIAM J. Imag. Sci. 7(4), 1993–2019 (2014)">
             42
            </a>
            ,
            <a aria-label="Reference 69" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR69" id="ref-link-section-d66064129e845" title="Wexler, Y., Shechtman, E., Irani, M.: Space-time video completion. In: Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004, vol. 1, pp. I-I. IEEE (2004)">
             69
            </a>
            ], video texture synthesis [
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR14" id="ref-link-section-d66064129e848" title="Bhat, K.S., Seitz, S.M., Hodgins, J.K., Khosla, P.K.: Flow-based video synthesis and editing. In: ACM SIGGRAPH 2004 Papers, pp. 360–363 (2004)">
             14
            </a>
            ,
            <a aria-label="Reference 21" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR21" id="ref-link-section-d66064129e852" title="Fišer, J., et al.: Stylit: illumination-guided example-based stylization of 3d renderings. ACM Trans. Graph. (TOG) 35(4), 1–11 (2016)">
             21
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d66064129e855" title="Jamriška, O., Fišer, J., Asente, P., Jingwan, L., Shechtman, E., Sỳkora, D.: Lazyfluids: appearance transfer for fluid animations. ACM Trans. Graph. (TOG) 34(4), 1–10 (2015)">
             28
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR31" id="ref-link-section-d66064129e858" title="Kwatra, V., Schödl, A., Essa, I., Turk, G., Bobick, A.: Graphcut textures: Image and video synthesis using graph cuts. Acm Trans. Graph. (ToG) 22(3), 277–286 (2003)">
             31
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR32" id="ref-link-section-d66064129e858_1" title="Kwatra, V., Essa, I., Bobick, A., Kwatra, N.: Texture optimization for example-based synthesis. In: ACM SIGGRAPH 2005 Papers, pp. 795–802 (2005)">
             32
            </a>
            ,
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR33" id="ref-link-section-d66064129e861" title="Kwatra, V., Adalsteinsson, D., Kim, T., Kwatra, N., Carlson, M., Lin, M.: Texturing fluids. IEEE Trans. Visual Comput. Graph. 13(5), 939–952 (2007)">
             33
            </a>
            ] and more. With the rise of deep-learning, these methods gradually, perhaps unjustifiably, became less popular. Recently, Granot et al. [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66064129e864" title="Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)">
             24
            </a>
            ] revived classical patch-based approaches for image synthesis, and was shown to significantly outperform
            <i>
             single-image
            </i>
            GANs in both run-time and visual quality.
           </p>
           <p>
            In light of the above-mentioned deficiencies of single-video GANs, and inspired by [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66064129e873" title="Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)">
             24
            </a>
            ], we propose a fast and practical method for video generation from a single video that we term VGPNN (
            <i>
             Video Generative Patch Nearest Neighbors
            </i>
            ). In order to handle the huge amounts of space-time patches in a single video sequence, we use the classical fast approximate nearest neighbor search method PatchMatch by Barnes et al. [
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d66064129e879" title="Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. Graph. 28(3), 24 (2009)">
             10
            </a>
            ]. By adding stochastic noise to the process, our approach can generate a large diversity of random different video outputs from a single input video in an unconditional manner.
           </p>
           <p>
            Like single-video GANs, our approach enables the diverse and random generation of videos. However, in contrast to existing single-video GANs, we can generate
            <i>
             high resolution
            </i>
            videos, while reducing runtime by many orders of magnitude, thus making diverse unconditional video generation from a single video realistically possible for the first time.
           </p>
           <p>
            In addition to diverse generation from a single video, by employing robust optical-flow based descriptors we use our framework to transfer the dynamics and motions between two videos with different appearance (which we call “video analogies”). We also show the applicability of our framework to spatio-temporal video retargeting and to conditional video inpainting.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 2." id="figure-2">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19790-1_30/figures/2" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig2_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 2" aria-describedby="Fig2" height="618" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig2_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc">
               <p>
                Diverse Single Video Generation: Given an input video (red), we generate similarly looking videos (black) capturing both appearance of objects as well as their dynamics. Note the high quality of our generated videos.
                <i>
                 Please watch the full resolution videos in the supplementary material
                </i>
                . (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 2" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure2 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19790-1_30/figures/2" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            To summarize, our contributions are as follows:
           </p>
           <ul class="u-list-style-dash">
            <li>
             <p>
              We show that our space-time patch nearest-neighbors approach, despite its simplicity, outperforms single-video GANs by a large margin, both in runtime and in quality.
             </p>
            </li>
            <li>
             <p>
              Our approach is the first to generate diverse high resolution videos (spatial or temporal) from a single video.
             </p>
            </li>
            <li>
             <p>
              We demonstrate the applicability of our framework to other applications: video analogies, sketch-to-video, spatio-temporal video retargeting and conditional video inpainting.
             </p>
            </li>
           </ul>
          </div>
         </div>
        </section>
        <section data-title="Related Work">
         <div class="c-article-section" id="Sec2-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">
           <span class="c-article-section__title-number">
            2
           </span>
           Related Work
          </h2>
          <div class="c-article-section__content" id="Sec2-content">
           <p>
            Classical video generation methods, many of whom inspired by similar
            <i>
             image
            </i>
            methods [
            <a aria-label="Reference 19" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR19" id="ref-link-section-d66064129e949" title="Efros, A.A., Freeman, W.T.: Image quilting for texture synthesis and transfer. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 341–346 (2001)">
             19
            </a>
            ,
            <a aria-label="Reference 20" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR20" id="ref-link-section-d66064129e952" title="Efros, A.A., Leung, T.K.: Texture synthesis by non-parametric sampling. In: Proceedings of the Seventh IEEE International Conference on Computer Vision, vol. 2, pp. 1033–1038. IEEE (1999)">
             20
            </a>
            ,
            <a aria-label="Reference 67" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR67" id="ref-link-section-d66064129e955" title="Wei, L.-Y., Levoy, M.: Fast texture synthesis using tree-structured vector quantization. In: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, pp. 479–488 (2000)">
             67
            </a>
            ], include video texture synthesis [
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR31" id="ref-link-section-d66064129e958" title="Kwatra, V., Schödl, A., Essa, I., Turk, G., Bobick, A.: Graphcut textures: Image and video synthesis using graph cuts. Acm Trans. Graph. (ToG) 22(3), 277–286 (2003)">
             31
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR32" id="ref-link-section-d66064129e958_1" title="Kwatra, V., Essa, I., Bobick, A., Kwatra, N.: Texture optimization for example-based synthesis. In: ACM SIGGRAPH 2005 Papers, pp. 795–802 (2005)">
             32
            </a>
            ,
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR33" id="ref-link-section-d66064129e962" title="Kwatra, V., Adalsteinsson, D., Kim, T., Kwatra, N., Carlson, M., Lin, M.: Texturing fluids. IEEE Trans. Visual Comput. Graph. 13(5), 939–952 (2007)">
             33
            </a>
            ], MRF-based controllable synthesis [
            <a aria-label="Reference 51" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR51" id="ref-link-section-d66064129e965" title="Schödl, A., Szeliski, R., Salesin, D.H., Essa, I.: Video textures. In: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, pp. 489–498 (2000)">
             51
            </a>
            ], flow-guided synthesis [
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR14" id="ref-link-section-d66064129e968" title="Bhat, K.S., Seitz, S.M., Hodgins, J.K., Khosla, P.K.: Flow-based video synthesis and editing. In: ACM SIGGRAPH 2004 Papers, pp. 360–363 (2004)">
             14
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d66064129e971" title="Jamriška, O., Fišer, J., Asente, P., Jingwan, L., Shechtman, E., Sỳkora, D.: Lazyfluids: appearance transfer for fluid animations. ACM Trans. Graph. (TOG) 34(4), 1–10 (2015)">
             28
            </a>
            ,
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR33" id="ref-link-section-d66064129e974" title="Kwatra, V., Adalsteinsson, D., Kim, T., Kwatra, N., Carlson, M., Lin, M.: Texturing fluids. IEEE Trans. Visual Comput. Graph. 13(5), 939–952 (2007)">
             33
            </a>
            ,
            <a aria-label="Reference 43" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR43" id="ref-link-section-d66064129e977" title="Okabe, M., Anjyo, K., Igarashi, T., Seidel, H.P.: Animating pictures of fluid using video examples. In: Computer Graphics Forum, vol. 28, pp. 677–686. Wiley Online Library (2009)">
             43
            </a>
            ,
            <a aria-label="Reference 49" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR49" id="ref-link-section-d66064129e981" title="Sato, S., Dobashi, Y., Kim, T., Nishita, T.: Example-based turbulence style transfer. ACM Trans. Graph. (TOG) 37(4), 1–9 (2018)">
             49
            </a>
            ,
            <a aria-label="Reference 50" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR50" id="ref-link-section-d66064129e984" title="Sato, S., Dobashi, Y., Nishita, T.: Editing fluid animation using flow interpolation. ACM Trans. Graph. (TOG) 37(5), 1–12 (2018)">
             50
            </a>
            ] and more (see surveys by [
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR9" id="ref-link-section-d66064129e987" title="Barnes, C., Zhang, F.-L.: A survey of the state-of-the-art in patch-based synthesis. Comput. Vis. Media 3(1), 3–20 (2017). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/s41095-016-0064-2
                
              ">
             9
            </a>
            ,
            <a aria-label="Reference 68" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR68" id="ref-link-section-d66064129e990" title="Wei, L. Y., Lefebvre, S., Kwatra, V., Turk, G.: State of the art in example-based texture synthesis. In: Eurographics 2009, State of the Art Report, EG-STAR, pp. 93–117. Eurographics Association (2009)">
             68
            </a>
            ]). While some used a generative model to model patch distribution, none of them considered unconditional generation of natural videos, beyond dynamic textures.
           </p>
           <p>
            Classical methods typically involve comparing and matching of image/video patches. Efficient computation for such matching is therefore critical.
            <i>
             PatchMatch
            </i>
            [
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d66064129e999" title="Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. Graph. 28(3), 24 (2009)">
             10
            </a>
            ] proposed a fast method for finding an approximate nearest-neighbor field (NNF) between patches of two images
            <i>
             A
            </i>
            ,
            <i>
             B
            </i>
            . Namely, for each patch in image
            <i>
             A
            </i>
            , find its nearest neighbor in
            <i>
             B
            </i>
            . While solving NNF exhaustively takes
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-1-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.888ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -894.5 2910.5 1243.6" width="6.76ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-4F" y="0">
                </use>
                <use x="763" xlink:href="#MJMAIN-28" y="0">
                </use>
                <g transform="translate(1153,0)">
                 <use x="0" xlink:href="#MJMATHI-4E" y="0">
                 </use>
                 <use transform="scale(0.707)" x="1292" xlink:href="#MJMAIN-32" y="513">
                 </use>
                </g>
                <use x="2520" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-1" type="math/tex">
              O(N^2)
             </script>
            </span>
            time (
            <i>
             N
            </i>
            being the number of patches to match), PatchMatch provides an approximation in
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-2-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 4932.3 1194" width="11.456ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-4F" y="0">
                </use>
                <use x="763" xlink:href="#MJMAIN-28" y="0">
                </use>
                <use x="1153" xlink:href="#MJMATHI-4E" y="0">
                </use>
                <g transform="translate(2208,0)">
                 <use xlink:href="#MJMAIN-6C">
                 </use>
                 <use x="278" xlink:href="#MJMAIN-6F" y="0">
                 </use>
                 <use x="779" xlink:href="#MJMAIN-67" y="0">
                 </use>
                </g>
                <use x="3654" xlink:href="#MJMATHI-4E" y="0">
                </use>
                <use x="4542" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-2" type="math/tex">
              O(N\log {N})
             </script>
            </span>
            time.
           </p>
           <p>
            PatchMatch starts by a random guess for the NNF, then iteratively refines it for each patch. The main observation is that since natural images are smooth (as opposed to e.g., noise), w.h.p, the nearest neighbour patches of two (spatially) adjacent patches are also adjacent. Therefore, each patch can refine its own guess by either “peeking” at its neighbor’s guess, keeping the current guess or sampling a new guess. At random guess, w.h.p at least one patch has a correct solution, and this is propagated to adjacent patches in further iterations.
           </p>
           <p>
            Being very efficient, PatchMatch allowed for many applications of image/video manipulation [
            <a aria-label="Reference 9" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR9" id="ref-link-section-d66064129e1090" title="Barnes, C., Zhang, F.-L.: A survey of the state-of-the-art in patch-based synthesis. Comput. Vis. Media 3(1), 3–20 (2017). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/s41095-016-0064-2
                
              ">
             9
            </a>
            ]. It was also extended for k-nearest neighbors search [
            <a aria-label="Reference 11" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR11" id="ref-link-section-d66064129e1093" title="Barnes, C., Shechtman, E., Goldman, D.B., Finkelstein, A.: The generalized patchmatch correspondence algorithm. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6313, pp. 29–43. Springer, Heidelberg (2010). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-15558-1_3
                
              ">
             11
            </a>
            ], faster search [
            <a aria-label="Reference 12" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR12" id="ref-link-section-d66064129e1096" title="Barnes, C., Zhang, F.-L., Lou, L., Xian, W., Shi-Min, H.: Patchtable: efficient patch queries for large datasets and applications. ACM Trans. Graph. (ToG) 34(4), 1–10 (2015)">
             12
            </a>
            ] and being differentiably learnable [
            <a aria-label="Reference 18" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR18" id="ref-link-section-d66064129e1099" title="Duggal, S., Wang, S., Ma, W. C., Hu, R., Urtasun, R.: Deeppruner: learning efficient stereo matching via differentiable patchmatch. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4384–4393 (2019)">
             18
            </a>
            ]. We use PatchMatch in order to dramatically reduce the running times of video generation from a single video.
           </p>
           <p>
            The tasks of generation and inpainting are closely related. Both are required to “invent” new content. Wexler et al. [
            <a aria-label="Reference 69" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR69" id="ref-link-section-d66064129e1106" title="Wexler, Y., Shechtman, E., Irani, M.: Space-time video completion. In: Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004, vol. 1, pp. I-I. IEEE (2004)">
             69
            </a>
            ] (and later extensions [
            <a aria-label="Reference 34" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR34" id="ref-link-section-d66064129e1109" title="Le, T.T., Almansa, A., Gousseau, Y., Masnou, S.: Motion-consistent video inpainting. In: 2017 IEEE International Conference on Image Processing (ICIP), pp. 2094–2098. IEEE (2017)">
             34
            </a>
            ,
            <a aria-label="Reference 41" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR41" id="ref-link-section-d66064129e1112" title="Newson, A., Almansa, A., Fradet, M., Gousseau, Y., Pérez, P.: Towards fast, generic video inpainting. In: Proceedings of the 10th European Conference on Visual Media Production, pp. 1–8 (2013)">
             41
            </a>
            ,
            <a aria-label="Reference 42" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR42" id="ref-link-section-d66064129e1115" title="Newson, A., Almansa, A., Fradet, M., Gousseau, Y., Pérez, P.: Video inpainting of complex scenes. SIAM J. Imag. Sci. 7(4), 1993–2019 (2014)">
             42
            </a>
            ]) proposed a patch-based method for video completion. Missing space-time patches are replaced by their nearest neighbours from the rest of the video. This is also done in a multi-scale manner by computing a spatio-temporal pyramid. The task is first solved in the coarsest level, and the upsampled result is the initial guess for the next level in the pyramid. Our approach uses a different metric for patch similarity and much deeper spatio-temporal pyramids (higher down-scale ratio). More importantly, we focus on video generation and video analogies and their relation to recent deep learning methods.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Method">
         <div class="c-article-section" id="Sec3-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">
           <span class="c-article-section__title-number">
            3
           </span>
           Method
          </h2>
          <div class="c-article-section__content" id="Sec3-content">
           <p>
            Our main task is to generate diverse video samples based on a single input video, such that the generated outputs have similar appearance and motions as the original input video, but are also visually different from one another. We want our model to operate on natural input videos that can vary in their appearance and dynamics. In order to capture both spatial and temporal information of a single video, we start by building a spatio-temporal pyramid and operate coarse-to-fine to capture the internal statistics of the input video at multiple scales (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            ). This multi-scale approach is extensively used in classical image synthesis methods as well as in modern GANs [eg., [
            <a aria-label="Reference 29" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR29" id="ref-link-section-d66064129e1129" title="Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for improved quality, stability, and variation. arXiv preprint 
                arXiv:1710.10196
                
              , 2017">
             29
            </a>
            ], [
            <a aria-label="Reference 52" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR52" id="ref-link-section-d66064129e1132" title="Shaham, T.R., Dekel, T., Michaeli, T.: Singan: learning a generative model from a single natural image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4570–4580 (2019)">
             52
            </a>
            ], [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e1135" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
             25
            </a>
            ]). At each scale we employ a Video-Patch-Nearest-Neighbor module (
            <i>
             VPNN
            </i>
            ); VGPNN is in fact a sequence of VPNN layers. The inputs to each layer depend on the application, where we first focus on our main application of diverse video generation (see Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec5">
             5
            </a>
            for the specific details of the other applications).
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 3." id="figure-3">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19790-1_30/figures/3" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig3_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 3" aria-describedby="Fig3" height="271" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig3_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc">
               <p>
                VGPNN Architecture
                <i>
                 Left
                </i>
                : given a single input video
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                 </span>
                 <span class="MathJax_SVG" id="MathJax-Element-3-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.778ex" role="img" style="vertical-align: -0.549ex;" viewbox="0 -529.2 1026.4 765.5" width="2.384ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMATHI-78" y="0">
                    </use>
                    <use transform="scale(0.707)" x="809" xlink:href="#MJMAIN-30" y="-213">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-3" type="math/tex">
                  x_0
                 </script>
                </span>
                , a spatio-temporal pyramid is constructed and an output video
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                 </span>
                 <span class="MathJax_SVG" id="MathJax-Element-4-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.914ex" role="img" style="vertical-align: -0.685ex;" viewbox="0 -529.2 944.4 824.1" width="2.193ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMATHI-79" y="0">
                    </use>
                    <use transform="scale(0.707)" x="693" xlink:href="#MJMAIN-30" y="-213">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-4" type="math/tex">
                  y_0
                 </script>
                </span>
                is generated coarse-to-fine. At each scale, VPNN module (
                <i>
                 right
                </i>
                ) is applied to transfer an initial guess
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                 </span>
                 <span class="MathJax_SVG" id="MathJax-Element-5-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="2.594ex" role="img" style="vertical-align: -0.685ex;" viewbox="0 -822.1 1316.1 1117" width="3.057ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMATHI-51" y="0">
                    </use>
                    <use transform="scale(0.707)" x="1119" xlink:href="#MJMATHI-6E" y="-213">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-5" type="math/tex">
                  Q_n
                 </script>
                </span>
                to the output
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                 </span>
                 <span class="MathJax_SVG" id="MathJax-Element-6-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.914ex" role="img" style="vertical-align: -0.685ex;" viewbox="0 -529.2 1015.1 824.1" width="2.358ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMATHI-79" y="0">
                    </use>
                    <use transform="scale(0.707)" x="693" xlink:href="#MJMATHI-6E" y="-213">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-6" type="math/tex">
                  y_n
                 </script>
                </span>
                which shares the same space-time patch distribution as the input
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                 </span>
                 <span class="MathJax_SVG" id="MathJax-Element-7-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.778ex" role="img" style="vertical-align: -0.549ex;" viewbox="0 -529.2 1097.1 765.5" width="2.548ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMATHI-78" y="0">
                    </use>
                    <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-6E" y="-213">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-7" type="math/tex">
                  x_n
                 </script>
                </span>
                . At the coarsest scale, noise is injected to induce spatial and temporal randomness.
                <i>
                 Right
                </i>
                : VPNN module gets as input query, key and value RGB videos (QKV respectively) and outputs an RGB video. Q and K can be concatenated to additional auxiliary channels. (a) Inputs are unfolded to patches (each position holds a concatenation of neighboring positions); (b) Each patch in Q finds its nearest neighbor patch in K. This is achieved by solving the NNF using PatchMatch [
                <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d66064129e1275" title="Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. Graph. 28(3), 24 (2009)">
                 10
                </a>
                ]; (c) Each patch in Q is replaced with a patch from V, according to the correspondences found in stage (b); (d) Resulting patches are “folded” back to an RGB video output (using the
                <i>
                 median
                </i>
                of all suggested votes).
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 3" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure3 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19790-1_30/figures/3" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <i>
             Multi-scale Approach:
            </i>
            Given an input video
            <i>
             x
            </i>
            , we construct a spatio-temporal pyramid
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-8-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 5279.2 1194" width="12.261ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAIN-7B" y="0">
                </use>
                <g transform="translate(500,0)">
                 <use x="0" xlink:href="#MJMATHI-78" y="0">
                 </use>
                 <use transform="scale(0.707)" x="809" xlink:href="#MJMAIN-30" y="-213">
                 </use>
                </g>
                <use x="1693" xlink:href="#MJMAIN-2026" y="0">
                </use>
                <use x="3032" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(3477,0)">
                 <use x="0" xlink:href="#MJMATHI-78" y="0">
                 </use>
                 <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-4E" y="-213">
                 </use>
                </g>
                <use x="4778" xlink:href="#MJMAIN-7D" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-8" type="math/tex">
              \left\{ x_0 \dots , x_N\right\}
             </script>
            </span>
            , where
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-9-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 2933 747.8" width="6.812ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMAIN-30" y="-213">
                </use>
                <use x="1304" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <use x="2360" xlink:href="#MJMATHI-78" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-9" type="math/tex">
              x_0=x
             </script>
            </span>
            , and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-10-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.543ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -795.3 5352.4 1094.8" width="12.432ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-6E" y="-213">
                </use>
                <use x="1374" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <g transform="translate(2431,0)">
                 <use x="0" xlink:href="#MJMATHI-78" y="0">
                 </use>
                 <g transform="translate(572,-150)">
                  <use transform="scale(0.707)" x="0" xlink:href="#MJMATHI-6E" y="0">
                  </use>
                  <use transform="scale(0.707)" x="600" xlink:href="#MJMAIN-2212" y="0">
                  </use>
                  <use transform="scale(0.707)" x="1379" xlink:href="#MJMAIN-31" y="0">
                  </use>
                 </g>
                </g>
                <g transform="translate(4432,0)">
                 <use x="0" xlink:href="#MJMAIN-2193" y="0">
                 </use>
                 <use transform="scale(0.707)" x="707" xlink:href="#MJMATHI-72" y="-213">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-10" type="math/tex">
              x_n = x_{n-1} {\downarrow _{r}}
             </script>
            </span>
            is a bicubically downscaled version of
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-11-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.852ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -497.8 2001.5 797.3" width="4.649ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <g transform="translate(572,-150)">
                 <use transform="scale(0.707)" x="0" xlink:href="#MJMATHI-6E" y="0">
                 </use>
                 <use transform="scale(0.707)" x="600" xlink:href="#MJMAIN-2212" y="0">
                 </use>
                 <use transform="scale(0.707)" x="1379" xlink:href="#MJMAIN-31" y="0">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-11" type="math/tex">
              x_{n-1}
             </script>
            </span>
            by factor
            <i>
             r
            </i>
            (
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-12-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 6977.2 1194" width="16.205ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-72" y="0">
                </use>
                <use x="729" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <use x="1785" xlink:href="#MJMAIN-28" y="0">
                </use>
                <g transform="translate(2175,0)">
                 <use x="0" xlink:href="#MJMATHI-72" y="0">
                 </use>
                 <use transform="scale(0.707)" x="638" xlink:href="#MJMATHI-48" y="-213">
                 </use>
                </g>
                <use x="3354" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(3799,0)">
                 <use x="0" xlink:href="#MJMATHI-72" y="0">
                 </use>
                 <use transform="scale(0.707)" x="638" xlink:href="#MJMATHI-57" y="-213">
                 </use>
                </g>
                <use x="5092" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(5538,0)">
                 <use x="0" xlink:href="#MJMATHI-72" y="0">
                 </use>
                 <use transform="scale(0.707)" x="638" xlink:href="#MJMATHI-54" y="-213">
                 </use>
                </g>
                <use x="6587" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-12" type="math/tex">
              {r=(r_H,r_W,r_T)}
             </script>
            </span>
            , where
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-13-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 3806.7 747.8" width="8.841ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-72" y="0">
                </use>
                <use transform="scale(0.707)" x="638" xlink:href="#MJMATHI-48" y="-213">
                </use>
                <use x="1457" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <g transform="translate(2513,0)">
                 <use x="0" xlink:href="#MJMATHI-72" y="0">
                 </use>
                 <use transform="scale(0.707)" x="638" xlink:href="#MJMATHI-57" y="-213">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-13" type="math/tex">
              r_H=r_W
             </script>
            </span>
            are the spatial factors and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-14-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1049.7 747.8" width="2.438ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-72" y="0">
                </use>
                <use transform="scale(0.707)" x="638" xlink:href="#MJMATHI-54" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-14" type="math/tex">
              r_T
             </script>
            </span>
            is the temporal factor, which can be different).
           </p>
           <p>
            At the coarsest level, the input to the first VPNN layer is an initial coarse guess of the output video. This is created by adding random Gaussian noise
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-15-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1193.8 747.8" width="2.773ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-7A" y="0">
                </use>
                <use transform="scale(0.707)" x="658" xlink:href="#MJMATHI-4E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-15" type="math/tex">
              z_N
             </script>
            </span>
            to
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-16-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1300.8 747.8" width="3.021ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-4E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-16" type="math/tex">
              x_N
             </script>
            </span>
            . The noise
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-17-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1193.8 747.8" width="2.773ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-7A" y="0">
                </use>
                <use transform="scale(0.707)" x="658" xlink:href="#MJMATHI-4E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-17" type="math/tex">
              z_N
             </script>
            </span>
            promotes high diversity in the generated output samples from the single input. The global structure (e.g., a head is above the body) and global motion (e.g., humans walk forward), is prompted by
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-18-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1300.8 747.8" width="3.021ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-4E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-18" type="math/tex">
              x_N
             </script>
            </span>
            , where such structure and motion can be captured by
            <i>
             small space-time
            </i>
            patches. The std of the noise
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-19-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 990.1 747.8" width="2.3ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-7A" y="0">
                </use>
                <use transform="scale(0.707)" x="658" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-19" type="math/tex">
              z_n
             </script>
            </span>
            is much larger than that of
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-20-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1097.1 747.8" width="2.548ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-20" type="math/tex">
              x_n
             </script>
            </span>
            and is related to the typical distance between neighbouring patches in the video (for full details see our supplementary).
           </p>
           <p>
            Each space-time patch of the initial coarse guess (
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-21-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.082ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -646.6 3717.5 896.5" width="8.634ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-4E" y="-213">
                </use>
                <use x="1522" xlink:href="#MJMAIN-2B" y="0">
                </use>
                <g transform="translate(2523,0)">
                 <use x="0" xlink:href="#MJMATHI-7A" y="0">
                 </use>
                 <use transform="scale(0.707)" x="658" xlink:href="#MJMATHI-4E" y="-213">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-21" type="math/tex">
              x_N + z_N
             </script>
            </span>
            ) is then replaced with its nearest neighbor patch from the corresponding coarse input
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-22-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1300.8 747.8" width="3.021ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-4E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-22" type="math/tex">
              x_N
             </script>
            </span>
            . The coarsest-level output
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-23-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.852ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -497.8 1218.8 797.3" width="2.831ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-79" y="0">
                </use>
                <use transform="scale(0.707)" x="693" xlink:href="#MJMATHI-4E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-23" type="math/tex">
              y_N
             </script>
            </span>
            is generated by choosing at each space-time position the median of all suggestions from neighboring patches (known as “voting” or “folding”).
           </p>
           <p>
            At each subsequent level, the input to the VPNN layer is the bicubically-upscaled output of the previous layer (
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-24-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.543ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -795.3 3117 1094.8" width="7.24ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-79" y="0">
                </use>
                <g transform="translate(490,-150)">
                 <use transform="scale(0.707)" x="0" xlink:href="#MJMATHI-6E" y="0">
                 </use>
                 <use transform="scale(0.707)" x="600" xlink:href="#MJMAIN-2B" y="0">
                 </use>
                 <use transform="scale(0.707)" x="1379" xlink:href="#MJMAIN-31" y="0">
                 </use>
                </g>
                <use x="2197" xlink:href="#MJMAIN-2191" y="0">
                </use>
                <use transform="scale(0.707)" x="3815" xlink:href="#MJMATHI-72" y="596">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-24" type="math/tex">
              {y_{n+1}\uparrow }^r
             </script>
            </span>
            ). Each space-time patch is replaced with its nearest neighbor patch from the corresponding input
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-25-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1097.1 747.8" width="2.548ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-25" type="math/tex">
              x_n
             </script>
            </span>
            (using the same patch-size as before, now capturing finer details). This way, the output
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-26-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.852ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -497.8 1015.1 797.3" width="2.358ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-79" y="0">
                </use>
                <use transform="scale(0.707)" x="693" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-26" type="math/tex">
              y_n
             </script>
            </span>
            in each level is similar in structure and in motion to the initial guess, but contains the same space-time patch statistics of the corresponding input
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-27-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1097.1 747.8" width="2.548ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-27" type="math/tex">
              x_n
             </script>
            </span>
            . The output
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-28-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.852ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -497.8 1015.1 797.3" width="2.358ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-79" y="0">
                </use>
                <use transform="scale(0.707)" x="693" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-28" type="math/tex">
              y_n
             </script>
            </span>
            is generated by median voting as described above.
           </p>
           <p>
            To further improve the quality and sharpness of the generated output at each pyramid level (
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-29-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.852ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -497.8 1015.1 797.3" width="2.358ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-79" y="0">
                </use>
                <use transform="scale(0.707)" x="693" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-29" type="math/tex">
              y_n
             </script>
            </span>
            ), we iterate several times through the current level, each time using the current output
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-30-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.852ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -497.8 1015.1 797.3" width="2.358ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-79" y="0">
                </use>
                <use transform="scale(0.707)" x="693" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-30" type="math/tex">
              y_n
             </script>
            </span>
            as input to the current VPNN layer (similar to the EM-like approach employed in many patch-based works e.g., [
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d66064129e1960" title="Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. Graph. 28(3), 24 (2009)">
             10
            </a>
            ,
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66064129e1963" title="Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)">
             24
            </a>
            ,
            <a aria-label="Reference 55" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR55" id="ref-link-section-d66064129e1966" title="Simakov, D., Caspi, Y., Shechtman, E., Irani, M.: Summarizing visual data using bidirectional similarity. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8. IEEE (2008)">
             55
            </a>
            ,
            <a aria-label="Reference 69" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR69" id="ref-link-section-d66064129e1970" title="Wexler, Y., Shechtman, E., Irani, M.: Space-time video completion. In: Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004, vol. 1, pp. I-I. IEEE (2004)">
             69
            </a>
            ]). Full implementation details (e.g., parameters of noise, pyramid, EM-iterations, etc.) are found in the supplementary material.
           </p>
           <p>
            <i>
             QKV Scheme:
            </i>
            In several cases it is necessary to compare patches in another search space than the original RGB input space. To this end we adopt a QKV scheme (query, key and value, respectively) as used by [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66064129e1978" title="Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)">
             24
            </a>
            ]. We denote
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-31-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.313ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -745.8 3200.7 995.7" width="7.434ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-56" y="0">
                </use>
                <use x="1047" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <g transform="translate(2103,0)">
                 <use x="0" xlink:href="#MJMATHI-78" y="0">
                 </use>
                 <use transform="scale(0.707)" x="809" xlink:href="#MJMATHI-6E" y="-213">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-31" type="math/tex">
              V=x_n
             </script>
            </span>
            (the corresponding level from the pyramid of the original video) and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-32-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.543ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -795.3 4823.3 1094.8" width="11.203ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-51" y="0">
                </use>
                <use x="1069" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <g transform="translate(2125,0)">
                 <use x="0" xlink:href="#MJMATHI-79" y="0">
                 </use>
                 <g transform="translate(490,-150)">
                  <use transform="scale(0.707)" x="0" xlink:href="#MJMATHI-6E" y="0">
                  </use>
                  <use transform="scale(0.707)" x="600" xlink:href="#MJMAIN-2B" y="0">
                  </use>
                  <use transform="scale(0.707)" x="1379" xlink:href="#MJMAIN-31" y="0">
                  </use>
                 </g>
                 <use x="2197" xlink:href="#MJMAIN-2191" y="0">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-32" type="math/tex">
              Q={y_{n+1}\uparrow }
             </script>
            </span>
            (the upscaled output of previous layer). Note that since
            <i>
             Q
            </i>
            is an
            <i>
             upscaled
            </i>
            version of previous output, its patches are blurry. Seeking their nearest neighbors in
            <i>
             V
            </i>
            (whose patches are sharp) often results in improper matches. This is mitigated by setting
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-33-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.543ex" role="img" style="vertical-align: -0.696ex;" viewbox="0 -795.3 5144.8 1094.8" width="11.949ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-4B" y="0">
                </use>
                <use x="1167" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <g transform="translate(2223,0)">
                 <use x="0" xlink:href="#MJMATHI-78" y="0">
                 </use>
                 <g transform="translate(572,-150)">
                  <use transform="scale(0.707)" x="0" xlink:href="#MJMATHI-6E" y="0">
                  </use>
                  <use transform="scale(0.707)" x="600" xlink:href="#MJMAIN-2B" y="0">
                  </use>
                  <use transform="scale(0.707)" x="1379" xlink:href="#MJMAIN-31" y="0">
                  </use>
                 </g>
                </g>
                <g transform="translate(4225,0)">
                 <use x="0" xlink:href="#MJMAIN-2191" y="0">
                 </use>
                 <use transform="scale(0.707)" x="707" xlink:href="#MJMATHI-72" y="513">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-33" type="math/tex">
              K=x_{n+1}{\uparrow ^r}
             </script>
            </span>
            (in the first iteration of each level), which has a similar degree of blur/degradation as
            <i>
             Q
            </i>
            . After finding its match in
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-34-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.543ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -745.8 1241.2 1094.8" width="2.883ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-4B" y="0">
                </use>
                <use transform="scale(0.707)" x="1201" xlink:href="#MJMATHI-6A" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-34" type="math/tex">
              K_j
             </script>
            </span>
            , each patch
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-35-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.428ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -795.3 1135.8 1045.3" width="2.638ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-51" y="0">
                </use>
                <use transform="scale(0.707)" x="1119" xlink:href="#MJMATHI-69" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-35" type="math/tex">
              Q_i
             </script>
            </span>
            is then replaced with a patch
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-36-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.543ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -745.8 975.2 1094.8" width="2.265ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-56" y="0">
                </use>
                <use transform="scale(0.707)" x="825" xlink:href="#MJMATHI-6A" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-36" type="math/tex">
              V_j
             </script>
            </span>
            (where
            <i>
             i
            </i>
            ,
            <i>
             j
            </i>
            are spatio-temporal positions. Also note that
            <i>
             K
            </i>
            and
            <i>
             V
            </i>
            are of the same shape). The QKV scheme is especially important in our video analogies application where it is used to include additional temporal information in the queries and the keys. We discuss it in detail in Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec6">
             5.1
            </a>
            .
           </p>
           <p>
            <i>
             Completeness Score:
            </i>
            In the applications of video analogies, spatio-temporal video retargeting and conditional video inpainting we use the normalized similarity score [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66064129e2202" title="Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)">
             24
            </a>
            ] that encourages visual completeness. The score between a query patch
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-37-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.428ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -795.3 1135.8 1045.3" width="2.638ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-51" y="0">
                </use>
                <use transform="scale(0.707)" x="1119" xlink:href="#MJMATHI-69" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-37" type="math/tex">
              Q_i
             </script>
            </span>
            and a key patch
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-38-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.543ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -745.8 1241.2 1094.8" width="2.883ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-4B" y="0">
                </use>
                <use transform="scale(0.707)" x="1201" xlink:href="#MJMATHI-6A" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-38" type="math/tex">
              K_j
             </script>
            </span>
            is defined as:
           </p>
           <div class="c-article-equation" id="Equ1">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              <span class="MathJax_Preview" style="">
              </span>
              <div class="MathJax_SVG_Display" style="text-align: left;">
               <span class="MathJax_SVG" id="MathJax-Element-39-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                <svg focusable="false" height="5.767ex" role="img" style="vertical-align: -2.152ex; margin-bottom: -0.156ex;" viewbox="0 -1489.5 19319.9 2483.2" width="44.872ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                 <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                  <g transform="translate(167,0)">
                   <g transform="translate(-16,0)">
                    <g transform="translate(0,83)">
                     <use x="0" xlink:href="#MJMATHI-53" y="0">
                     </use>
                     <g transform="translate(812,0)">
                      <use xlink:href="#MJMAIN-28">
                      </use>
                      <g transform="translate(389,0)">
                       <use x="0" xlink:href="#MJMATHI-51" y="0">
                       </use>
                       <use transform="scale(0.707)" x="1119" xlink:href="#MJMATHI-69" y="-213">
                       </use>
                      </g>
                      <use x="1525" xlink:href="#MJMAIN-2C" y="0">
                      </use>
                      <g transform="translate(1970,0)">
                       <use x="0" xlink:href="#MJMATHI-4B" y="0">
                       </use>
                       <use transform="scale(0.707)" x="1201" xlink:href="#MJMATHI-6A" y="-213">
                       </use>
                      </g>
                      <use x="3211" xlink:href="#MJMAIN-29" y="0">
                      </use>
                     </g>
                     <use x="4579" xlink:href="#MJMAIN-3A" y="0">
                     </use>
                     <use x="4858" xlink:href="#MJMAIN-3D" y="0">
                     </use>
                     <g transform="translate(5636,0)">
                      <g transform="translate(120,0)">
                       <rect height="60" stroke="none" width="8694" x="0" y="220">
                       </rect>
                       <use x="4096" xlink:href="#MJMAIN-31" y="676">
                       </use>
                       <g transform="translate(60,-715)">
                        <use x="0" xlink:href="#MJMATHI-3B1" y="0">
                        </use>
                        <use x="862" xlink:href="#MJMAIN-2B" y="0">
                        </use>
                        <g transform="translate(1863,0)">
                         <use xlink:href="#MJMAIN-6D">
                         </use>
                         <use x="833" xlink:href="#MJMAIN-69" y="0">
                         </use>
                         <use x="1112" xlink:href="#MJMAIN-6E" y="0">
                         </use>
                         <use transform="scale(0.707)" x="2359" xlink:href="#MJMAIN-2113" y="-220">
                         </use>
                        </g>
                        <g transform="translate(4093,0)">
                         <use x="0" xlink:href="#MJMATHI-44" y="0">
                         </use>
                         <use x="828" xlink:href="#MJMAIN-28" y="0">
                         </use>
                         <g transform="translate(1218,0)">
                          <use x="0" xlink:href="#MJMATHI-51" y="0">
                          </use>
                          <use transform="scale(0.707)" x="1119" xlink:href="#MJMAIN-2113" y="-220">
                          </use>
                         </g>
                         <use x="2404" xlink:href="#MJMAIN-2C" y="0">
                         </use>
                         <g transform="translate(2849,0)">
                          <use x="0" xlink:href="#MJMATHI-4B" y="0">
                          </use>
                          <use transform="scale(0.707)" x="1201" xlink:href="#MJMATHI-6A" y="-213">
                          </use>
                         </g>
                         <use x="4091" xlink:href="#MJMAIN-29" y="0">
                         </use>
                        </g>
                       </g>
                      </g>
                     </g>
                     <use x="14571" xlink:href="#MJMATHI-44" y="0">
                     </use>
                     <use x="15399" xlink:href="#MJMAIN-28" y="0">
                     </use>
                     <g transform="translate(15789,0)">
                      <use x="0" xlink:href="#MJMATHI-51" y="0">
                      </use>
                      <use transform="scale(0.707)" x="1119" xlink:href="#MJMATHI-69" y="-213">
                      </use>
                     </g>
                     <use x="16925" xlink:href="#MJMAIN-2C" y="0">
                     </use>
                     <g transform="translate(17370,0)">
                      <use x="0" xlink:href="#MJMATHI-4B" y="0">
                      </use>
                      <use transform="scale(0.707)" x="1201" xlink:href="#MJMATHI-6A" y="-213">
                      </use>
                     </g>
                     <use x="18611" xlink:href="#MJMAIN-29" y="0">
                     </use>
                    </g>
                   </g>
                  </g>
                 </g>
                </svg>
               </span>
              </div>
              <script id="MathJax-Element-39" type="math/tex; mode=display">
               \begin{aligned} S\left( Q_i, K_j\right) {:}{=} \frac{1}{\alpha + \min _\ell {D(Q_\ell , K_j)}} D(Q_i, K_j) \end{aligned}
              </script>
             </span>
            </div>
            <div class="c-article-equation__number">
             (1)
            </div>
           </div>
           <p>
            where
            <i>
             D
            </i>
            is mean square error, and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-40-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.391ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -497.8 640.5 599" width="1.488ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-3B1" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-40" type="math/tex">
              \alpha
             </script>
            </span>
            controls the degree of completeness (smaller
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-41-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.391ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -497.8 640.5 599" width="1.488ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-3B1" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-41" type="math/tex">
              \alpha
             </script>
            </span>
            encourages more completeness).
            <i>
             S
            </i>
            is essentially a weighted version of
            <i>
             D
            </i>
            , whose weights depend
            <i>
             globally
            </i>
            on
            <i>
             K
            </i>
            and
            <i>
             Q
            </i>
            .
           </p>
           <p>
            <i>
             Finding Correspondences:
            </i>
            We find the nearest neighbors between
            <i>
             Q
            </i>
            and
            <i>
             K
            </i>
            (Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
             3
            </a>
            right-b) using PatchMatch (Barnes et al. [
            <a aria-label="Reference 10" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR10" id="ref-link-section-d66064129e2450" title="Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. Graph. 28(3), 24 (2009)">
             10
            </a>
            ]). To cope with the completeness score, we apply PatchMatch twice. First we find a “rareness” score for the keys - for each
            <i>
             key
            </i>
            we find its closest
            <i>
             query
            </i>
            . Then, for each
            <i>
             query
            </i>
            we find its closest
            <i>
             key
            </i>
            while factoring in the rareness of the keys as weights in the PatchMatch search. Namely, we solve for:
           </p>
           <div class="c-article-equation" id="Equ2">
            <div class="c-article-equation__content">
             <span class="mathjax-tex">
              <span class="MathJax_Preview" style="">
              </span>
              <div class="MathJax_SVG_Display" style="text-align: left;">
               <span class="MathJax_SVG" id="MathJax-Element-42-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                <svg focusable="false" height="3.925ex" role="img" style="vertical-align: -1.387ex;" viewbox="0 -1092.8 22274.8 1689.9" width="51.735ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                 <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                  <g transform="translate(167,0)">
                   <g transform="translate(-16,0)">
                    <g transform="translate(0,202)">
                     <use xlink:href="#MJMAIN-4E">
                     </use>
                     <use x="750" xlink:href="#MJMAIN-4E" y="0">
                     </use>
                     <use x="1501" xlink:href="#MJMAIN-46" y="0">
                     </use>
                     <use x="2154" xlink:href="#MJMAIN-28" y="0">
                     </use>
                     <use x="2544" xlink:href="#MJMAINB-70" y="0">
                     </use>
                     <use x="3183" xlink:href="#MJMAIN-29" y="0">
                     </use>
                     <use x="3850" xlink:href="#MJMAIN-3D" y="0">
                     </use>
                     <g transform="translate(4907,0)">
                      <use xlink:href="#MJMAIN-61">
                      </use>
                      <use x="500" xlink:href="#MJMAIN-72" y="0">
                      </use>
                      <use x="893" xlink:href="#MJMAIN-67" y="0">
                      </use>
                     </g>
                     <g transform="translate(6467,0)">
                      <use xlink:href="#MJMAIN-6D">
                      </use>
                      <use x="833" xlink:href="#MJMAIN-69" y="0">
                      </use>
                      <use x="1112" xlink:href="#MJMAIN-6E" y="0">
                      </use>
                      <use transform="scale(0.707)" x="876" xlink:href="#MJMAINB-76" y="-850">
                      </use>
                     </g>
                     <use x="8302" xlink:href="#MJMATHI-57" y="0">
                     </use>
                     <use x="9350" xlink:href="#MJMAIN-28" y="0">
                     </use>
                     <g transform="translate(9740,0)">
                      <use x="0" xlink:href="#MJMAINB-70" y="0">
                      </use>
                      <use x="861" xlink:href="#MJMAINB-2B" y="0">
                      </use>
                      <use x="1978" xlink:href="#MJMAINB-76" y="0">
                      </use>
                     </g>
                     <use x="12326" xlink:href="#MJMAIN-29" y="0">
                     </use>
                     <use x="12938" xlink:href="#MJMAIN-22C5" y="0">
                     </use>
                     <use x="13438" xlink:href="#MJMATHI-44" y="0">
                     </use>
                     <use x="14267" xlink:href="#MJMAIN-28" y="0">
                     </use>
                     <use x="14656" xlink:href="#MJMATHI-51" y="0">
                     </use>
                     <use x="15448" xlink:href="#MJMAIN-28" y="0">
                     </use>
                     <use x="15837" xlink:href="#MJMAINB-70" y="0">
                     </use>
                     <use x="16477" xlink:href="#MJMAIN-29" y="0">
                     </use>
                     <use x="16866" xlink:href="#MJMAIN-2C" y="0">
                     </use>
                     <use x="17311" xlink:href="#MJMATHI-4B" y="0">
                     </use>
                     <use x="18201" xlink:href="#MJMAIN-28" y="0">
                     </use>
                     <g transform="translate(18590,0)">
                      <use x="0" xlink:href="#MJMAINB-70" y="0">
                      </use>
                      <use x="861" xlink:href="#MJMAINB-2B" y="0">
                      </use>
                      <use x="1978" xlink:href="#MJMAINB-76" y="0">
                      </use>
                     </g>
                     <use x="21176" xlink:href="#MJMAIN-29" y="0">
                     </use>
                     <use x="21566" xlink:href="#MJMAIN-29" y="0">
                     </use>
                    </g>
                   </g>
                  </g>
                 </g>
                </svg>
               </span>
              </div>
              <script id="MathJax-Element-42" type="math/tex; mode=display">
               \begin{aligned} \text {NNF}(\textbf{p}) = \text {arg} \min _{\textbf{v}} W(\mathbf {p+v}) \cdot D(Q(\textbf{p}), K(\mathbf {p+v})) \end{aligned}
              </script>
             </span>
            </div>
            <div class="c-article-equation__number">
             (2)
            </div>
           </div>
           <p>
            where
            <i>
             D
            </i>
            is a distance function,
            <i>
             W
            </i>
            are per-patch weights,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-43-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 5074.4 1194" width="11.786ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAINB-70" y="0">
                </use>
                <use x="917" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <use x="1973" xlink:href="#MJMAIN-28" y="0">
                </use>
                <use x="2363" xlink:href="#MJMATHI-74" y="0">
                </use>
                <use x="2724" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <use x="3169" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use x="3742" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <use x="4187" xlink:href="#MJMATHI-79" y="0">
                </use>
                <use x="4684" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-43" type="math/tex">
              \textbf{p}=(t, x, y)
             </script>
            </span>
            a position in
            <i>
             Q
            </i>
            and
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-44-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 5929 1194" width="13.771ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAINB-76" y="0">
                </use>
                <use x="885" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <use x="1941" xlink:href="#MJMAIN-28" y="0">
                </use>
                <g transform="translate(2331,0)">
                 <use x="0" xlink:href="#MJMATHI-74" y="0">
                 </use>
                 <use transform="scale(0.707)" x="511" xlink:href="#MJMAIN-2032" y="513">
                 </use>
                </g>
                <use x="2987" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(3432,0)">
                 <use x="0" xlink:href="#MJMATHI-78" y="0">
                 </use>
                 <use transform="scale(0.707)" x="809" xlink:href="#MJMAIN-2032" y="513">
                 </use>
                </g>
                <use x="4299" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <g transform="translate(4745,0)">
                 <use x="0" xlink:href="#MJMATHI-79" y="0">
                 </use>
                 <use transform="scale(0.707)" x="706" xlink:href="#MJMAIN-2032" y="513">
                 </use>
                </g>
                <use x="5539" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-44" type="math/tex">
              \textbf{v}=(t',x',y')
             </script>
            </span>
            are possible NNF candidates (such as the NNF at the current position
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-45-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 5255.3 1194" width="12.206ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-4E">
                </use>
                <use x="750" xlink:href="#MJMAIN-4E" y="0">
                </use>
                <use x="1501" xlink:href="#MJMAIN-46" y="0">
                </use>
                <use x="2154" xlink:href="#MJMAIN-28" y="0">
                </use>
                <use x="2544" xlink:href="#MJMATHI-74" y="0">
                </use>
                <use x="2905" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <use x="3350" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use x="3923" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <use x="4368" xlink:href="#MJMATHI-79" y="0">
                </use>
                <use x="4865" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-45" type="math/tex">
              \text {NNF}(t,x,y)
             </script>
            </span>
            or at a neighbor position
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-46-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 6978.8 1194" width="16.209ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-4E">
                </use>
                <use x="750" xlink:href="#MJMAIN-4E" y="0">
                </use>
                <use x="1501" xlink:href="#MJMAIN-46" y="0">
                </use>
                <use x="2154" xlink:href="#MJMAIN-28" y="0">
                </use>
                <use x="2544" xlink:href="#MJMATHI-74" y="0">
                </use>
                <use x="2905" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <use x="3350" xlink:href="#MJMATHI-78" y="0">
                </use>
                <use x="4145" xlink:href="#MJMAIN-2212" y="0">
                </use>
                <use x="5146" xlink:href="#MJMAIN-31" y="0">
                </use>
                <use x="5646" xlink:href="#MJMAIN-2C" y="0">
                </use>
                <use x="6091" xlink:href="#MJMATHI-79" y="0">
                </use>
                <use x="6589" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-46" type="math/tex">
              \text {NNF}(t,x-1,y)
             </script>
            </span>
            in the propagation step).
           </p>
           <p>
            This requires a slight modification of PatchMatch to support per-key weights. This additional support makes it possible to approximately solve Eq.
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            with two passes of PatchMatch. Even though this gives an approximation of Eq.
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            , we do not suffer loss in quality or lack of completeness, as apparent from our results.
           </p>
           <p>
            The algorithm is implemented on GPU using PyTorch [
            <a aria-label="Reference 44" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR44" id="ref-link-section-d66064129e2796" title="Paszke, A., et al.: Pytorch: an imperative style, high-performance deep learning library. In: Wallach, H., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates Inc (2019). 
                http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
                
              ">
             44
            </a>
            ], with time complexity
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-47-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 3889.4 1194" width="9.034ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-4F" y="0">
                </use>
                <use x="763" xlink:href="#MJMAIN-28" y="0">
                </use>
                <use x="1153" xlink:href="#MJMATHI-6E" y="0">
                </use>
                <use x="1975" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <use x="2976" xlink:href="#MJMATHI-64" y="0">
                </use>
                <use x="3499" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-47" type="math/tex">
              O(n \times d)
             </script>
            </span>
            and
            <i>
             O
            </i>
            (
            <i>
             n
            </i>
            ) additional memory (where
            <i>
             n
            </i>
            is the video size and
            <i>
             d
            </i>
            is the patch size; also see Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            ).
           </p>
           <p>
            <i>
             Temporal Diversity and Consistency:
            </i>
            A simple but effective trick to enhance the temporal diversity of our samples is to generate outputs with less frames than in the input video. Generating samples with similar number of frames as in the input video result in outputs that are “in sync”. Intuitively, the motion of the input video is the only motion that is coherent for this amount of frames. By generating shorter videos allow for shorter motions from different times in the input video, to occur simultaneously in the generated outputs (see for example how the generated dancers in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            are not “synced”). We also found that the temporal consistency is best preserved in the generated output when the initial noise
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-48-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.737ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -497.8 1193.8 747.8" width="2.773ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-7A" y="0">
                </use>
                <use transform="scale(0.707)" x="658" xlink:href="#MJMATHI-4E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-48" type="math/tex">
              z_N
             </script>
            </span>
            is randomized for each spatial position, but is the same (replicated) in the temporal dimension.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Experimental Results">
         <div class="c-article-section" id="Sec4-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">
           <span class="c-article-section__title-number">
            4
           </span>
           Experimental Results
          </h2>
          <div class="c-article-section__content" id="Sec4-content">
           <p>
            In this section we evaluate and compare the performance of our main application – diverse video generation from a single input video. Figures
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            and
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
             2
            </a>
            illustrate diverse videos generated from a single input video, all sharing the same space-time patch distribution. The diversity is both spatially (e.g., number of dancers and their positions are different from the input video) and temporally (generated dancers are not synced).
            <b>
             Please refer to the supplementary material
            </b>
            to view the full resolution videos and many more examples.
           </p>
           <p>
            <i>
             Evaluation of Video Generation from a Single Video:
            </i>
            We compare our results to recently published methods for diverse video generation from single video: HP-VAE-GAN  [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e2900" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
             25
            </a>
            ] and SinGAN-GIF [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66064129e2903" title="Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)">
             5
            </a>
            ]. We show that our results are both qualitatively and quantitatively superior while reducing the runtime by a factor of
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-49-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.428ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -944.1 3178.4 1045.3" width="7.382ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAIN-33" y="0">
                </use>
                <use x="722" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <g transform="translate(1723,0)">
                 <use xlink:href="#MJMAIN-31">
                 </use>
                 <use x="500" xlink:href="#MJMAIN-30" y="0">
                 </use>
                 <use transform="scale(0.707)" x="1415" xlink:href="#MJMAIN-34" y="557">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-49" type="math/tex">
              3 \times 10^4
             </script>
            </span>
            (from 8 d training on one video to 18 s for new generated video). Since SinGAN-GIF did not make their code available, and the training time of HP-VAE-GAN for a single video is roughly 8 days, we are only able to compare to the videos published by these methods (we use all published videos).
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 4." id="figure-4">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19790-1_30/figures/4" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig4_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 4" aria-describedby="Fig4" height="452" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig4_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc">
               <p>
                Comparing Visual Quality between our generated frames and those of HP-VAE-GAN  [
                <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e2945" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
                 25
                </a>
                ] and SinGAN-GIF [
                <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66064129e2948" title="Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)">
                 5
                </a>
                ] (please
                <b>
                 zoom in
                </b>
                on the frames). Note that our generated frames are sharper and also exhibit more coherent and plausible arrangements of the scene. For details see Sect.
                <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec4">
                 4
                </a>
                . See supplementary for full videos and more comparisons.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 4" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure4 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19790-1_30/figures/4" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <i>
             Evaluation Set:
            </i>
            “HP-VAE-GAN dataset” comprises of 10 input videos with 13 frames each, and of spatial resolution of
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="display: none;">
             </span>
             <span class="MathJax_SVG" id="MathJax-Element-50-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 3781.5 846.9" width="8.783ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-31">
                </use>
                <use x="500" xlink:href="#MJMAIN-34" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-34" y="0">
                </use>
                <use x="1501" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <g transform="translate(2280,0)">
                 <use xlink:href="#MJMAIN-32">
                 </use>
                 <use x="500" xlink:href="#MJMAIN-35" y="0">
                 </use>
                 <use x="1001" xlink:href="#MJMAIN-36" y="0">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-50" type="math/tex">
              144{\times }256
             </script>
            </span>
            pixels. “SinGAN-GIF dataset” has 5 input videos with maximal resolution of
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              168{\times }298
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-51-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 3781.5 846.9" width="8.783ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-31">
                </use>
                <use x="500" xlink:href="#MJMAIN-36" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-38" y="0">
                </use>
                <use x="1501" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <g transform="translate(2280,0)">
                 <use xlink:href="#MJMAIN-32">
                 </use>
                 <use x="500" xlink:href="#MJMAIN-39" y="0">
                 </use>
                 <use x="1001" xlink:href="#MJMAIN-38" y="0">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-51" type="math/tex">
              168{\times }298
             </script>
            </span>
            pixels and 8–16 frames.
           </p>
           <p>
            <i>
             Qualitative Comparison:
            </i>
            In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
             4
            </a>
            we show a side-by-side comparison of representative generated frames of our method to frames generated by HP-VAE-GAN  [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e3027" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
             25
            </a>
            ] and SinGAN-GIF [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66064129e3030" title="Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)">
             5
            </a>
            ]. Note that while [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66064129e3033" title="Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)">
             5
            </a>
            ,
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e3036" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
             25
            </a>
            ] are limited to generated outputs of small resolution (
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              144{\times }256
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-52-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 3781.5 846.9" width="8.783ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-31">
                </use>
                <use x="500" xlink:href="#MJMAIN-34" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-34" y="0">
                </use>
                <use x="1501" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <g transform="translate(2280,0)">
                 <use xlink:href="#MJMAIN-32">
                 </use>
                 <use x="500" xlink:href="#MJMAIN-35" y="0">
                 </use>
                 <use x="1001" xlink:href="#MJMAIN-36" y="0">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-52" type="math/tex">
              144{\times }256
             </script>
            </span>
            ), we can generate outputs in the same resolution of the input video (full-HD
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              1280{\times }1920
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-53-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 4782.5 846.9" width="11.108ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-31">
                </use>
                <use x="500" xlink:href="#MJMAIN-32" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-38" y="0">
                </use>
                <use x="1501" xlink:href="#MJMAIN-30" y="0">
                </use>
                <use x="2002" xlink:href="#MJMAIN-D7" y="0">
                </use>
                <g transform="translate(2780,0)">
                 <use xlink:href="#MJMAIN-31">
                 </use>
                 <use x="500" xlink:href="#MJMAIN-39" y="0">
                 </use>
                 <use x="1001" xlink:href="#MJMAIN-32" y="0">
                 </use>
                 <use x="1501" xlink:href="#MJMAIN-30" y="0">
                 </use>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-53" type="math/tex">
              1280{\times }1920
             </script>
            </span>
            , shown in the figure). The full videos (as well as a comparison to our generated outputs of similar low resolution) can be viewed in the supplementary material. As can be seen, our generated samples (in low and high resolution) are more spatially and temporally coherent, as well as having higher visual quality. It is evident that generating videos using the space-time patches of the original input video, rather than regressing output RGB values, gives rise to high quality outputs.
           </p>
           <p>
            <i>
             Quantitative Comparison:
            </i>
            In Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab1">
             1
            </a>
            we report the Single-Video-FID (SVFID) [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66064129e3097" title="Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)">
             5
            </a>
            ] of our generated samples, compared to those generated by HP-VAE-GAN  [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e3100" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
             25
            </a>
            ] and SinGAN-GIF [
            <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66064129e3103" title="Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)">
             5
            </a>
            ]
            <sup>
             <a href="#Fn1">
              <span class="u-visually-hidden">
               Footnote
              </span>
              1
             </a>
            </sup>
            . SVFID was proposed by [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e3113" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
             25
            </a>
            ] to measure the patch statistics similarity between the input video and a generated video. It computes the Fréchet distance between the statistics of the input video and the generated video using pre-computed C3D [
            <a aria-label="Reference 57" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR57" id="ref-link-section-d66064129e3116" title="Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotemporal features with 3d convolutional networks. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 4489–4497 (2015)">
             57
            </a>
            ] features (Lower SVFID is better). As can be seen in Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab1">
             1
            </a>
            , our generated samples bear more substantial similarity to the input videos (indicated by lower SVFID). [
            <a aria-label="Reference 52" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR52" id="ref-link-section-d66064129e3122" title="Shaham, T.R., Dekel, T., Michaeli, T.: Singan: learning a generative model from a single natural image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4570–4580 (2019)">
             52
            </a>
            ] proposed a diversity index to make sure that generated outputs are indeed different (and not simply “copying” the input). We adapt the index for videos. The index is zero if all generated outputs are the same, and higher otherwise. While our and HP-VAE-GAN generated samples have similar index (0.45/0.41 respectively), those of SinGAN-GIF have higher index (0.86 vs. our 0.6). Such high diversity is not an advantage, when paired with SVFID about twice worse than ours. It stems from low quality appearance with out-of-distribution patches. All inputs and generated videos can be found in the supplementary material.
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 5." id="figure-5">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19790-1_30/figures/5" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig5_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 5" aria-describedby="Fig5" height="228" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig5_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc">
               <p>
                Generation Runtime.
                <b>
                 Left:
                </b>
                Comparing between our approach (VGPNN), a naïve extension of GPNN [
                <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66064129e3138" title="Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)">
                 24
                </a>
                ] from 2D to 3D and HP-VAE-GAN  [
                <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e3141" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
                 25
                </a>
                ]. We compared the generation time of 13-frames videos with different spatial resolutions (X-axis). All videos have 16:9 aspect ratio (e.g., 144p is 144
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                  \,\times \,
                 </span>
                 <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-54-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.506ex" role="img" style="vertical-align: -0.141ex;" viewbox="0 -587.8 1111.8 648.4" width="2.582ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="166" xlink:href="#MJMAIN-D7" y="0">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-54" type="math/tex">
                  \,\times \,
                 </script>
                </span>
                256 and 1080p is 1080
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                  \,\times \,
                 </span>
                 <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-55-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.506ex" role="img" style="vertical-align: -0.141ex;" viewbox="0 -587.8 1111.8 648.4" width="2.582ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="166" xlink:href="#MJMAIN-D7" y="0">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-55" type="math/tex">
                  \,\times \,
                 </script>
                </span>
                1920 – full-HD).
                <b>
                 Right:
                </b>
                Close-up of our generation run-time (black line in left). Our approach takes seconds/minutes to generate low-res/high-res video outputs. The drop in 480p is the result of decreasing the patch-size in finer-levels of high-res videos. See Sect.
                <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec4">
                 4
                </a>
                for details, and supplementary for implementation details.
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 5" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure5 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19790-1_30/figures/5" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            <i>
             User Study:
            </i>
            We conducted a user study evaluation using Amazon Mechanical Turk (AMT). For each dataset, 100 subjects were shown multiple pairs of videos, each consisting of a video generated by our method, and a video generated by the other method (both were generated from the same input video). The subjects were asked to judge which sample is better in terms of sharpness, natural look and coherence. In Table
            <a data-track="click" data-track-action="table anchor" data-track-label="link" href="#Tab1">
             1
            </a>
            we report the percentage of users who favored our method over the other. Compared to videos generated from HP-VAE-GAN dataset, there is a clear preference in favor of our patch-based method. The results on the SinGAN-GIF dataset are not that clear-cut, this might be due to the somewhat restricted nature of the videos in that particular dataset (as mentioned above, it was not possible to check SinGAN-GIF on other samples, since the authors did not publish their code, nor stated the amount of time it took to generate their samples).
           </p>
           <p>
            <i>
             Reducing Running Times:
            </i>
            In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
             5
            </a>
            we show a comparison of the runtime taken to generate random video samples using our method, compared to a naïve extension of GPNN [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66064129e3224" title="Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)">
             24
            </a>
            ] (from 2D to 3D patches) and compared to the training time of HP-VAE-GAN  [
            <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e3227" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
             25
            </a>
            ]. As discussed in Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec3">
             3
            </a>
            , the use of efficient PatchMatch algorithm for nearest neighbors search, as opposed to the exhaustive search done in GPNN, dramatically reduces both run time and memory footprint used for video generation, making it possible to generate high-resolution videos (including Full-HD 1080p). All experiments were conducted on Quadro RTX 8000 GPU.
           </p>
           <div class="c-article-table" data-container-section="table" data-test="inline-table" id="table-1">
            <figure>
             <figcaption class="c-article-table__figcaption">
              <b data-test="table-caption" id="Tab1">
               Table 1. Quantitative Evaluation: A comparison of our generated video samples to that of HP-VAE-GAN  [
               <a aria-label="Reference 25" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR25" id="ref-link-section-d66064129e3243" title="Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint 
                arXiv:2006.12226
                
               (2020)">
                25
               </a>
               ] and SinGAN-GIF [
               <a aria-label="Reference 5" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR5" id="ref-link-section-d66064129e3246" title="Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)">
                5
               </a>
               ], conducted on input videos provided in their papers. Our diverse samples have more resemblance to the input videos (indicated by lower SVFID). In a user study, users scored in favor of our method (see Sect.
               <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec4">
                4
               </a>
               for details).
              </b>
             </figcaption>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size table 1" class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" href="/chapter/10.1007/978-3-031-19790-1_30/tables/1" rel="nofollow">
               <span>
                Full size table
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
          </div>
         </div>
        </section>
        <section data-title="Applications">
         <div class="c-article-section" id="Sec5-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">
           <span class="c-article-section__title-number">
            5
           </span>
           Applications
          </h2>
          <div class="c-article-section__content" id="Sec5-content">
           <p>
            Other than unconditional diverse generation, we demonstrate the utility of our framework on several other video manipulation applications.
           </p>
           <h3 class="c-article__sub-heading" id="Sec6">
            <span class="c-article-section__title-number">
             5.1
            </span>
            Video Analogies
           </h3>
           <p>
            Video to video translation methods typically train on large datasets and are either conditioned on human poses or keypoint detection e.g. [
            <a aria-label="Reference 15" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR15" id="ref-link-section-d66064129e3500" title="Blattmann, A., Milbich, T., Dorkenwald, M., Ommer, B.: ipoke: poking a still image for controlled stochastic video synthesis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14707–14717 (2021)">
             15
            </a>
            ,
            <a aria-label="Reference 40" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR40" id="ref-link-section-d66064129e3503" title="Mallya, A., Wang, T.-C., Sapra, K., Liu, M.-Y.: World-consistent video-to-video synthesis. arXiv preprint 
                arXiv:2007.08509
                
               (2020)">
             40
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR63" id="ref-link-section-d66064129e3506" title="Wang, T.-C., et al.: Video-to-video synthesis. arXiv preprint 
                arXiv:1808.06601
                
               (2018)">
             63
            </a>
            ,
            <a data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR64" id="ref-link-section-d66064129e3506_1" title="Wang, T.-C., Liu, M.-Y., Tao, A., Liu, G., Kautz, J., Catanzaro, B.: Few-shot video-to-video synthesis. arXiv preprint 
                arXiv:1910.12713
                
               (2019)">
             64
            </a>
            ,
            <a aria-label="Reference 65" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR65" id="ref-link-section-d66064129e3509" title="Wang, Y., Bilinski, P., Bremond, F., Dantcheva, A: Imaginator: conditional spatio-temporal gan for video generation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1160–1169 (2020)">
             65
            </a>
            ], or require knowledge of a human/animal model e.g. [
            <a aria-label="Reference 1" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR1" id="ref-link-section-d66064129e3512" title="Aberman, K., Weng, Y., Lischinski, D., Cohen-Or, D., Chen, B.: Unpaired motion style transfer from video to animation. ACM Trans. Graph. (TOG) 39(4), 1–64 (2020)">
             1
            </a>
            ,
            <a aria-label="Reference 16" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR16" id="ref-link-section-d66064129e3516" title="Chan, C., Ginosar, S., Zhou, T., Efros, A.A.: Everybody dance now. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5933–5942 (2019)">
             16
            </a>
            ,
            <a aria-label="Reference 37" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR37" id="ref-link-section-d66064129e3519" title="Lee, J., Ramanan, D., Girdhar, R.: Metapix: few-shot video retargeting. arXiv preprint 
                arXiv:1910.04742
                
               (2019)">
             37
            </a>
            ,
            <a aria-label="Reference 46" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR46" id="ref-link-section-d66064129e3522" title="Ren, J., Chai, M., Tulyakov, S., Fang, C., Shen, X., Yang, J.: Human motion transfer from poses in the wild. In: Bartoli, A., Fusiello, A. (eds.) ECCV 2020. LNCS, vol. 12537, pp. 262–279. Springer, Cham (2020). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-67070-2_16
                
              ">
             46
            </a>
            ,
            <a aria-label="Reference 53" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR53" id="ref-link-section-d66064129e3525" title="Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., Sebe, N.: Animating arbitrary objects via deep motion transfer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2377–2386 (2019)">
             53
            </a>
            ,
            <a aria-label="Reference 54" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR54" id="ref-link-section-d66064129e3528" title="Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion model for image animation. Adv. Neural. Inf. Process. Syst. 32, 7137–7147 (2019)">
             54
            </a>
            ,
            <a aria-label="Reference 71" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR71" id="ref-link-section-d66064129e3531" title="Yang, Z., et al.: Transmomo: invariance-driven unsupervised video motion retargeting. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5306–5315 (2020)">
             71
            </a>
            ]. We show that when videos’ dynamics are similar in both their motion and semantic context within their video, one can use our framework to transfer the motion and appearance between the two (see Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
             6
            </a>
            ). We term this task “video analogies” (inspired by
            <i>
             image analogies
            </i>
            [
            <a aria-label="Reference 13" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR13" id="ref-link-section-d66064129e3541" title="Benaim, S., Mokady, R., Bermano, A., Wolf, L.: Structural analogy from a single image pair. In: Computer Graphics Forum, vol. 40, pp. 249–265. Wiley Online Library (2021)">
             13
            </a>
            ,
            <a aria-label="Reference 26" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR26" id="ref-link-section-d66064129e3544" title="Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analogies. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 327–340 (2001)">
             26
            </a>
            ,
            <a aria-label="Reference 38" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR38" id="ref-link-section-d66064129e3547" title="Liao, J., Yao, Y., Yuan, L., Hua, G., Kang, S.B.: Visual attribute transfer through deep image analogy. arXiv preprint 
                arXiv:1705.01088
                
               (2017)">
             38
            </a>
            ]). More formally, we generate a new video whose spatio-temporal layout is taken from a content video
            <i>
             C
            </i>
            , and overall appearance and dynamics from a style video
            <i>
             S
            </i>
            .
           </p>
           <p>
            Our goal is to find a mapping of dynamic elements (3D patches) between the two videos, which can be very different in their appearance (RGB space). This is achieved by using the magnitude of the optical flow (extracted via RAFT [
            <a aria-label="Reference 56" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR56" id="ref-link-section-d66064129e3560" title="Teed, Z., Deng, J.: RAFT: recurrent all-pairs field transforms for optical flow. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12347, pp. 402–419. Springer, Cham (2020). 
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58536-5_24
                
              ">
             56
            </a>
            ]), quantized into few bins (using k-means)
            <sup>
             <a href="#Fn2">
              <span class="u-visually-hidden">
               Footnote
              </span>
              2
             </a>
            </sup>
            . We term this the
            <i>
             dynamic structure
            </i>
            of the video. By concatenating the dynamic structure to the RGB values of the video (along the channels axis), each patch can now be compared using its RGB values and its dynamic values. This provides a good mapping between the dynamic elements of the two input videos.
           </p>
           <p>
            We compute spatio-temporal pyramids from the style video
            <i>
             S
            </i>
            , as well as from the dynamic structure of the content video
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \text {dyn}(C)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-56-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 3181 1194" width="7.388ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-64">
                </use>
                <use x="556" xlink:href="#MJMAIN-79" y="0">
                </use>
                <use x="1085" xlink:href="#MJMAIN-6E" y="0">
                </use>
                <use x="1641" xlink:href="#MJMAIN-28" y="0">
                </use>
                <use x="2031" xlink:href="#MJMATHI-43" y="0">
                </use>
                <use x="2791" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-56" type="math/tex">
              \text {dyn}(C)
             </script>
            </span>
            , and the dynamic structure the style video
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \text {dyn}(S)
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-57-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 3066 1194" width="7.121ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use xlink:href="#MJMAIN-64">
                </use>
                <use x="556" xlink:href="#MJMAIN-79" y="0">
                </use>
                <use x="1085" xlink:href="#MJMAIN-6E" y="0">
                </use>
                <use x="1641" xlink:href="#MJMAIN-28" y="0">
                </use>
                <use x="2031" xlink:href="#MJMATHI-53" y="0">
                </use>
                <use x="2676" xlink:href="#MJMAIN-29" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-57" type="math/tex">
              \text {dyn}(S)
             </script>
            </span>
            . The output video is generated by setting
            <i>
             Q
            </i>
            ,
            <i>
             K
            </i>
            ,
            <i>
             V
            </i>
            at each level as follows:
           </p>
           <p>
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \displaystyle \begin{array}{@{}l|lll} \text {Level} &amp;{} Q &amp;{} K &amp;{} V \\ \hline \text {N (coarsest)}&amp;{} \text {dyn}(C)_N &amp;{} \text {dyn}(S)_N &amp;{} S_N \\ \text {n (any other)} &amp;{} \text {dyn}(C)_n \Vert Q_{n+1}\uparrow &amp;{} \text {dyn}(S)_n \Vert S_n &amp;{} S_n \end{array}
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-58-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="10.374ex" role="img" style="vertical-align: -4.611ex;" viewbox="0 -2481.2 23599.3 4466.6" width="54.811ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <g transform="translate(167,0)">
                 <g transform="translate(384,0)">
                  <g transform="translate(0,1401)">
                   <use xlink:href="#MJMAIN-4C">
                   </use>
                   <use x="625" xlink:href="#MJMAIN-65" y="0">
                   </use>
                   <use x="1070" xlink:href="#MJMAIN-76" y="0">
                   </use>
                   <use x="1598" xlink:href="#MJMAIN-65" y="0">
                   </use>
                   <use x="2043" xlink:href="#MJMAIN-6C" y="0">
                   </use>
                  </g>
                  <g transform="translate(0,1)">
                   <use xlink:href="#MJMAIN-4E">
                   </use>
                   <use x="1000" xlink:href="#MJMAIN-28" y="0">
                   </use>
                   <use x="1390" xlink:href="#MJMAIN-63" y="0">
                   </use>
                   <use x="1834" xlink:href="#MJMAIN-6F" y="0">
                   </use>
                   <use x="2335" xlink:href="#MJMAIN-61" y="0">
                   </use>
                   <use x="2835" xlink:href="#MJMAIN-72" y="0">
                   </use>
                   <use x="3228" xlink:href="#MJMAIN-73" y="0">
                   </use>
                   <use x="3622" xlink:href="#MJMAIN-65" y="0">
                   </use>
                   <use x="4067" xlink:href="#MJMAIN-73" y="0">
                   </use>
                   <use x="4461" xlink:href="#MJMAIN-74" y="0">
                   </use>
                   <use x="4851" xlink:href="#MJMAIN-29" y="0">
                   </use>
                  </g>
                  <g transform="translate(0,-1450)">
                   <use xlink:href="#MJMAIN-6E">
                   </use>
                   <use x="806" xlink:href="#MJMAIN-28" y="0">
                   </use>
                   <use x="1196" xlink:href="#MJMAIN-61" y="0">
                   </use>
                   <use x="1696" xlink:href="#MJMAIN-6E" y="0">
                   </use>
                   <use x="2253" xlink:href="#MJMAIN-79" y="0">
                   </use>
                   <use x="3031" xlink:href="#MJMAIN-6F" y="0">
                   </use>
                   <use x="3532" xlink:href="#MJMAIN-74" y="0">
                   </use>
                   <use x="3921" xlink:href="#MJMAIN-68" y="0">
                   </use>
                   <use x="4478" xlink:href="#MJMAIN-65" y="0">
                   </use>
                   <use x="4922" xlink:href="#MJMAIN-72" y="0">
                   </use>
                   <use x="5315" xlink:href="#MJMAIN-29" y="0">
                   </use>
                  </g>
                 </g>
                 <line stroke-linecap="square" stroke-width="30.25" transform="translate(6589,-1917)" x1="15" x2="15" y1="15" y2="4317">
                 </line>
                 <g transform="translate(7089,0)">
                  <g transform="translate(0,1401)">
                   <use x="0" xlink:href="#MJMATHI-51" y="0">
                   </use>
                  </g>
                  <g transform="translate(0,1)">
                   <use xlink:href="#MJMAIN-64">
                   </use>
                   <use x="556" xlink:href="#MJMAIN-79" y="0">
                   </use>
                   <use x="1085" xlink:href="#MJMAIN-6E" y="0">
                   </use>
                   <use x="1641" xlink:href="#MJMAIN-28" y="0">
                   </use>
                   <use x="2031" xlink:href="#MJMATHI-43" y="0">
                   </use>
                   <g transform="translate(2791,0)">
                    <use x="0" xlink:href="#MJMAIN-29" y="0">
                    </use>
                    <use transform="scale(0.707)" x="550" xlink:href="#MJMATHI-4E" y="-213">
                    </use>
                   </g>
                  </g>
                  <g transform="translate(0,-1450)">
                   <use xlink:href="#MJMAIN-64">
                   </use>
                   <use x="556" xlink:href="#MJMAIN-79" y="0">
                   </use>
                   <use x="1085" xlink:href="#MJMAIN-6E" y="0">
                   </use>
                   <use x="1641" xlink:href="#MJMAIN-28" y="0">
                   </use>
                   <use x="2031" xlink:href="#MJMATHI-43" y="0">
                   </use>
                   <g transform="translate(2791,0)">
                    <use x="0" xlink:href="#MJMAIN-29" y="0">
                    </use>
                    <use transform="scale(0.707)" x="550" xlink:href="#MJMATHI-6E" y="-213">
                    </use>
                   </g>
                   <use x="3705" xlink:href="#MJMAIN-2225" y="0">
                   </use>
                   <g transform="translate(4206,0)">
                    <use x="0" xlink:href="#MJMATHI-51" y="0">
                    </use>
                    <g transform="translate(791,-150)">
                     <use transform="scale(0.707)" x="0" xlink:href="#MJMATHI-6E" y="0">
                     </use>
                     <use transform="scale(0.707)" x="600" xlink:href="#MJMAIN-2B" y="0">
                     </use>
                     <use transform="scale(0.707)" x="1379" xlink:href="#MJMAIN-31" y="0">
                     </use>
                    </g>
                   </g>
                   <use x="6704" xlink:href="#MJMAIN-2191" y="0">
                   </use>
                  </g>
                 </g>
                 <g transform="translate(15294,0)">
                  <g transform="translate(0,1401)">
                   <use x="0" xlink:href="#MJMATHI-4B" y="0">
                   </use>
                  </g>
                  <g transform="translate(0,1)">
                   <use xlink:href="#MJMAIN-64">
                   </use>
                   <use x="556" xlink:href="#MJMAIN-79" y="0">
                   </use>
                   <use x="1085" xlink:href="#MJMAIN-6E" y="0">
                   </use>
                   <use x="1641" xlink:href="#MJMAIN-28" y="0">
                   </use>
                   <use x="2031" xlink:href="#MJMATHI-53" y="0">
                   </use>
                   <g transform="translate(2676,0)">
                    <use x="0" xlink:href="#MJMAIN-29" y="0">
                    </use>
                    <use transform="scale(0.707)" x="550" xlink:href="#MJMATHI-4E" y="-213">
                    </use>
                   </g>
                  </g>
                  <g transform="translate(0,-1450)">
                   <use xlink:href="#MJMAIN-64">
                   </use>
                   <use x="556" xlink:href="#MJMAIN-79" y="0">
                   </use>
                   <use x="1085" xlink:href="#MJMAIN-6E" y="0">
                   </use>
                   <use x="1641" xlink:href="#MJMAIN-28" y="0">
                   </use>
                   <use x="2031" xlink:href="#MJMATHI-53" y="0">
                   </use>
                   <g transform="translate(2676,0)">
                    <use x="0" xlink:href="#MJMAIN-29" y="0">
                    </use>
                    <use transform="scale(0.707)" x="550" xlink:href="#MJMATHI-6E" y="-213">
                    </use>
                   </g>
                   <use x="3590" xlink:href="#MJMAIN-2225" y="0">
                   </use>
                   <g transform="translate(4091,0)">
                    <use x="0" xlink:href="#MJMATHI-53" y="0">
                    </use>
                    <use transform="scale(0.707)" x="867" xlink:href="#MJMATHI-6E" y="-213">
                    </use>
                   </g>
                  </g>
                 </g>
                 <g transform="translate(21523,0)">
                  <g transform="translate(0,1401)">
                   <use x="0" xlink:href="#MJMATHI-56" y="0">
                   </use>
                  </g>
                  <g transform="translate(0,1)">
                   <use x="0" xlink:href="#MJMATHI-53" y="0">
                   </use>
                   <use transform="scale(0.707)" x="867" xlink:href="#MJMATHI-4E" y="-213">
                   </use>
                  </g>
                  <g transform="translate(0,-1450)">
                   <use x="0" xlink:href="#MJMATHI-53" y="0">
                   </use>
                   <use transform="scale(0.707)" x="867" xlink:href="#MJMATHI-6E" y="-213">
                   </use>
                  </g>
                 </g>
                 <line stroke-linecap="square" stroke-width="30.25" transform="translate(0,985)" x1="15" x2="23250" y1="15" y2="15">
                 </line>
                </g>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-58" type="math/tex">
              \displaystyle \begin{array}{@{}l|lll} \text {Level} &{} Q &{} K &{} V \\ \hline \text {N (coarsest)}&{} \text {dyn}(C)_N &{} \text {dyn}(S)_N &{} S_N \\ \text {n (any other)} &{} \text {dyn}(C)_n \Vert Q_{n+1}\uparrow &{} \text {dyn}(S)_n \Vert S_n &{} S_n \end{array}
             </script>
            </span>
           </p>
           <p>
            where
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \Vert
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-59-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 500.5 1194" width="1.162ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAIN-2225" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-59" type="math/tex">
              \Vert
             </script>
            </span>
            denotes concatenation along the channels axis, and
            <i>
             n
            </i>
            denote the current level in the pyramid. Note that in the coarsest level, the two videos are only compared by their dynamic structure. In finer levels, the dynamic structure of
            <i>
             C
            </i>
            (the content video) is used to “guide” the output to the desired spatio-temporal layout.
           </p>
           <p>
            In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            we show a snapshot of the analogies between a waterfall and a lava stream, and in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
             6
            </a>
            we show snapshots of the analogies of all possible pairs between three videos (the lava stream, a waterfall and a meat grinder).
            <i>
             The full videos are in the supplementary material
            </i>
            .
           </p>
           <p>
            We can use the above mentioned mechanism for “sketch-to-video” transfer, where the dynamic structure is given by a sketch video instead of an actual video. See Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
             6
            </a>
            for a few snapshots of transfering the motion of morphed MNIST [
            <a aria-label="Reference 35" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR35" id="ref-link-section-d66064129e3956" title="LeCun, Y.: The mnist database of handwritten digits. 
                http://yann.lecun.com/exdb/mnist/
                
               (1998)">
             35
            </a>
            ] digits to a video of marching soldiers, and
            <i>
             please see the full videos and many more results in the supplementary material
            </i>
            .
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 6." id="figure-6">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19790-1_30/figures/6" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig6_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 6" aria-describedby="Fig6" height="283" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig6_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc">
               <p>
                Video Analogies:
                <i>
                 Left:
                </i>
                an example of video analogies between all pairs of three input videos (red). Each generated video (black) takes the spatio-temporal layout from the input video in its row, and the appearance and dynamics of the input video from its column.
                <i>
                 Right:
                </i>
                an example of sketch-to-video – the generated video (bottom) takes its spatio-temporal layout from the sketch video of morphed MNIST digits (middle) and its appearance and dynamics from the input video of parading soldiers (top).
                <i>
                 Please find full videos and additional examples in the supplementary material
                </i>
                . (Color figure online)
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 6" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure6 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19790-1_30/figures/6" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            Flow-based appearance transfer of fluids has been studied by [
            <a aria-label="Reference 14" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR14" id="ref-link-section-d66064129e3993" title="Bhat, K.S., Seitz, S.M., Hodgins, J.K., Khosla, P.K.: Flow-based video synthesis and editing. In: ACM SIGGRAPH 2004 Papers, pp. 360–363 (2004)">
             14
            </a>
            ,
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d66064129e3996" title="Jamriška, O., Fišer, J., Asente, P., Jingwan, L., Shechtman, E., Sỳkora, D.: Lazyfluids: appearance transfer for fluid animations. ACM Trans. Graph. (TOG) 34(4), 1–10 (2015)">
             28
            </a>
            ,
            <a aria-label="Reference 33" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR33" id="ref-link-section-d66064129e3999" title="Kwatra, V., Adalsteinsson, D., Kim, T., Kwatra, N., Carlson, M., Lin, M.: Texturing fluids. IEEE Trans. Visual Comput. Graph. 13(5), 939–952 (2007)">
             33
            </a>
            ,
            <a aria-label="Reference 43" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR43" id="ref-link-section-d66064129e4002" title="Okabe, M., Anjyo, K., Igarashi, T., Seidel, H.P.: Animating pictures of fluid using video examples. In: Computer Graphics Forum, vol. 28, pp. 677–686. Wiley Online Library (2009)">
             43
            </a>
            ,
            <a aria-label="Reference 49" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR49" id="ref-link-section-d66064129e4005" title="Sato, S., Dobashi, Y., Kim, T., Nishita, T.: Example-based turbulence style transfer. ACM Trans. Graph. (TOG) 37(4), 1–9 (2018)">
             49
            </a>
            ,
            <a aria-label="Reference 50" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR50" id="ref-link-section-d66064129e4009" title="Sato, S., Dobashi, Y., Nishita, T.: Editing fluid animation using flow interpolation. ACM Trans. Graph. (TOG) 37(5), 1–12 (2018)">
             50
            </a>
            ]. Most similar to us is [
            <a aria-label="Reference 28" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR28" id="ref-link-section-d66064129e4012" title="Jamriška, O., Fišer, J., Asente, P., Jingwan, L., Shechtman, E., Sỳkora, D.: Lazyfluids: appearance transfer for fluid animations. ACM Trans. Graph. (TOG) 34(4), 1–10 (2015)">
             28
            </a>
            ] that uses a patch nearest neighbor approach to transfer the appearance of a fluid exemplar (a still image) into a video given a human annotated flow+alpha mask. Our method differs in how we model the flow guidance and in the mapping we have between two flows of two videos (instead of a still image exemplar). Also similar to us is Recycle-GAN by Bansal et al. [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d66064129e4015" title="Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: unsupervised video retargeting. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 119–135 (2018)">
             8
            </a>
            ] that pose unsupervised video-to-video translation as a domain transfer problem (each video is a domain). They train convolutional encoders to map between the two videos using adversarial loss with cyclic constraints.
           </p>
           <p>
            In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            we compare our results to [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d66064129e4024" title="Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: unsupervised video retargeting. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 119–135 (2018)">
             8
            </a>
            ]. As can be seen, [
            <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d66064129e4027" title="Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: unsupervised video retargeting. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 119–135 (2018)">
             8
            </a>
            ] results generally fail to converge to the visual quality of the original inputs (partially due to the difficulty of training a parameteric model on small amounts of data), and in many cases converge to the input style video (probably due to the instability of training a GAN).
           </p>
           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-container-section="figure" data-test="figure" data-title="Fig. 7." id="figure-7">
            <figure>
             <figcaption>
              <b class="c-article-section__figure-caption" data-test="figure-caption-text" id="Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <div class="c-article-section__figure-content">
              <div class="c-article-section__figure-item">
               <a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-action="view figure" data-track-label="image" href="/chapter/10.1007/978-3-031-19790-1_30/figures/7" rel="nofollow">
                <picture>
                 <source srcset="//media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig7_HTML.png?as=webp" type="image/webp"/>
                 <img alt="figure 7" aria-describedby="Fig7" height="515" loading="lazy" src="//media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig7_HTML.png" width="685"/>
                </picture>
               </a>
              </div>
              <div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc">
               <p>
                Video Analogies: (i) Ablations for the choice of
                <span class="mathjax-tex">
                 <span class="MathJax_Preview" style="">
                  \alpha
                 </span>
                 <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-60-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
                  <svg focusable="false" height="1.506ex" role="img" style="vertical-align: -0.277ex;" viewbox="0 -529.2 640.5 648.4" width="1.488ex" xmlns:xlink="http://www.w3.org/1999/xlink">
                   <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                    <use x="0" xlink:href="#MJMATHI-3B1" y="0">
                    </use>
                   </g>
                  </svg>
                 </span>
                 <script id="MathJax-Element-60" type="math/tex">
                  \alpha
                 </script>
                </span>
                (completeness score); (ii) Ablations of choice of auxiliary channel; (iii) Comparisons our results to that of ReCycle-GAN [
                <a aria-label="Reference 8" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR8" id="ref-link-section-d66064129e4057" title="Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: unsupervised video retargeting. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 119–135 (2018)">
                 8
                </a>
                ].
               </p>
              </div>
             </div>
             <div class="u-text-right u-hide-print">
              <a aria-label="Full size image figure 7" class="c-article__pill-button" data-test="chapter-link" data-track="click" data-track-action="view figure" data-track-dest="link:Figure7 Full size image" data-track-label="button" href="/chapter/10.1007/978-3-031-19790-1_30/figures/7" rel="nofollow">
               <span>
                Full size image
               </span>
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-chevron-right" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </div>
            </figure>
           </div>
           <p>
            In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            we show ablations for the main parameters used for video analogies. Figure
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            (i) we show the role of using the completeness term
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \alpha
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-61-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.391ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -497.8 640.5 599" width="1.488ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-3B1" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-61" type="math/tex">
              \alpha
             </script>
            </span>
            (see Eq.
            <a data-track="click" data-track-action="equation anchor" data-track-label="link" href="#Equ1">
             1
            </a>
            ). No completeness term (equivalent to
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \alpha \rightarrow \infty
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-62-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.622ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -597 3197.1 698.2" width="7.425ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-3B1" y="0">
                </use>
                <use x="918" xlink:href="#MJMAIN-2192" y="0">
                </use>
                <use x="2196" xlink:href="#MJMAIN-221E" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-62" type="math/tex">
              \alpha \rightarrow \infty
             </script>
            </span>
            ) results in over smoothed outputs due to many patches in
            <i>
             Q
            </i>
            being mapped to a similar patch in
            <i>
             K
            </i>
            . On the other hand, too “strong” completeness term results in many undesired visual details. We found
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              \alpha =1
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-63-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="1.967ex" role="img" style="vertical-align: -0.235ex;" viewbox="0 -745.8 2475.1 846.9" width="5.749ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-3B1" y="0">
                </use>
                <use x="918" xlink:href="#MJMAIN-3D" y="0">
                </use>
                <use x="1974" xlink:href="#MJMAIN-31" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-63" type="math/tex">
              \alpha =1
             </script>
            </span>
            to be a good balance for our results. In Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
             7
            </a>
            (ii) we ablate the use of quantized optical flow in the auxiliary channel. Without employing temporal features (RGB only) or using optical flow without quantizing to
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              [0-1]
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-64-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.773ex" role="img" style="vertical-align: -0.811ex;" viewbox="0 -844.9 2780.9 1194" width="6.459ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMAIN-5B" y="0">
                </use>
                <use x="278" xlink:href="#MJMAIN-30" y="0">
                </use>
                <use x="1001" xlink:href="#MJMAIN-2212" y="0">
                </use>
                <use x="2001" xlink:href="#MJMAIN-31" y="0">
                </use>
                <use x="2502" xlink:href="#MJMAIN-5D" y="0">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-64" type="math/tex">
              [0-1]
             </script>
            </span>
            , the resulting mapping fails to match similar dynamical elements to those with similar semantics in the context of their video.
            <i>
             Please find full videos in the supplementary material
            </i>
            .
           </p>
           <h3 class="c-article__sub-heading" id="Sec7">
            <span class="c-article-section__title-number">
             5.2
            </span>
            Spatial Retargeting
           </h3>
           <p>
            The goal of video retargeting is to change the dimensions of a video without distorting its visual contents (e.g., fit a portrait video to a wide screen display). It can be performed in a very similar manner to our video generation described in Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec3">
             3
            </a>
            . Given a target shape, we first resize (bicubically) the input video to the target shape, then compute two pyramids (for the input and resized videos) with the same depth and downscale factor. The initial guess at the coarsest level
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              Q_N
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-65-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.428ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -795.3 1519.8 1045.3" width="3.53ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-51" y="0">
                </use>
                <use transform="scale(0.707)" x="1119" xlink:href="#MJMATHI-4E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-65" type="math/tex">
              Q_N
             </script>
            </span>
            would be the coarsest level of the resized pyramid (without any additional noise). We then compute the rest of the output video in the same manner as in Sect.
            <a data-track="click" data-track-action="section anchor" data-track-label="link" href="#Sec3">
             3
            </a>
            . Note that at each level,
            <span class="mathjax-tex">
             <span class="MathJax_Preview" style="">
              V_n
             </span>
             <span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-66-Frame" style="font-size: 100%; display: inline-block;" tabindex="0">
              <svg focusable="false" height="2.313ex" role="img" style="vertical-align: -0.58ex;" viewbox="0 -745.8 1108.1 995.7" width="2.574ex" xmlns:xlink="http://www.w3.org/1999/xlink">
               <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)">
                <use x="0" xlink:href="#MJMATHI-56" y="0">
                </use>
                <use transform="scale(0.707)" x="825" xlink:href="#MJMATHI-6E" y="-213">
                </use>
               </g>
              </svg>
             </span>
             <script id="MathJax-Element-66" type="math/tex">
              V_n
             </script>
            </span>
            are unchanged, hence no distortion is introduced to the patches reconstructing the retargeted video.
           </p>
           <p>
            As can be seen in Fig.
            <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
             1
            </a>
            and in the supplementary material, the results preserve the original size and aspect ratio of objects from the input videos while keeping the overall appearance coherent even though the aspect ratio is significantly altered. The dynamics and motions in the videos are also preserved. For instance, the balloons are not “squashed” but rather packed more compactly in the sky and more members were added to the choir instead of stretching them. Nevertheless, the motion of the balloons or the sway of the choir members are preserved. Other classical works for video retargeting, such as [
            <a aria-label="Reference 30" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR30" id="ref-link-section-d66064129e4254" title="Krähenbühl, P., Lang, M., Hornung, A., Gross, M.: A system for retargeting of streaming video. In: ACM SIGGRAPH Asia 2009 papers (2009)">
             30
            </a>
            ,
            <a aria-label="Reference 47" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR47" id="ref-link-section-d66064129e4257" title="Rubinstein, M., Shamir, A., Avidan, S.: Improved seam carving for video retargeting. ACM Trans. Graph. (TOG) 27(3), 1–9 (2008)">
             47
            </a>
            ,
            <a aria-label="Reference 55" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR55" id="ref-link-section-d66064129e4260" title="Simakov, D., Caspi, Y., Shechtman, E., Irani, M.: Summarizing visual data using bidirectional similarity. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8. IEEE (2008)">
             55
            </a>
            ,
            <a aria-label="Reference 70" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR70" id="ref-link-section-d66064129e4263" title="Wolf, L., Guttmann, M., Cohen-Or, D.: Non-homogeneous content-driven video-retargeting. In: Proceedings of the Eleventh IEEE International Conference on Computer Vision (ICCV) (2007)">
             70
            </a>
            ] did not make their implementation available, therefore we were unable to provide a comparison.
           </p>
           <h3 class="c-article__sub-heading" id="Sec8">
            <span class="c-article-section__title-number">
             5.3
            </span>
            Temporal Retargeting
           </h3>
           <p>
            Similar to spatial retargeting, one can generate a realistic video with a different
            <i>
             temporal
            </i>
            length. One possible use is generating a shorter summary of the video. While most deep video summarization techniques are achieved by selecting a subset of frames (see survey [
            <a aria-label="Reference 4" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR4" id="ref-link-section-d66064129e4277" title="Apostolidis, E., Adamantidou, E., Metsai, A.I., Mezaris, V., Patras, I.: Video summarization using deep neural networks: a survey. arXiv preprint 
                arXiv:2101.06072
                
               (2021)">
             4
            </a>
            ]), classical methods have demonstrated summaries that consist of
            <i>
             novel frames
            </i>
            in which dynamics that are originally sequential can be parallelized or vice versa [
            <a aria-label="Reference 45" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR45" id="ref-link-section-d66064129e4283" title="Rav-Acha, A., Pritch, Y., Peleg, S.: Making a long video short: dynamic video synopsis. In: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), vol. 1, pp. 435–441. IEEE (2006)">
             45
            </a>
            ,
            <a aria-label="Reference 55" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR55" id="ref-link-section-d66064129e4286" title="Simakov, D., Caspi, Y., Shechtman, E., Irani, M.: Summarizing visual data using bidirectional similarity. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8. IEEE (2008)">
             55
            </a>
            ]. By applying the retargeting approach to the temporal dimenstion, we are able to generate summaries with novel frames. The temporal retargeting section in the supplementary material shows several examples. For example, in the dog training summarized video, the trainer and dog turn around simultaneously as opposed to sequentially in the original video. Moreover, we can, in a similar manner, extend the temporal duration of a video creating longer dynamics while preserving the speed of the individual actions. In the ballet dancer video for example, the choreography is longer, but the pace of the dance motions remains the same.
           </p>
           <h3 class="c-article__sub-heading" id="Sec9">
            <span class="c-article-section__title-number">
             5.4
            </span>
            Video Conditional Inpainting
           </h3>
           <p>
            In this task we are given an input video with some occluded space-time volume, where the missing parts should be completed based on crude color cues placed by the user in the occluded space (similar to conditional image inpainting [
            <a aria-label="Reference 24" data-test="citation-ref" data-track="click" data-track-action="reference anchor" data-track-label="link" href="#ref-CR24" id="ref-link-section-d66064129e4298" title="Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)">
             24
            </a>
            ]). Here we set the number of levels in the pyramid such that the occluded part in the coarsest level is roughly in the size of a single patch. The masked part is then coherently reconstructed using other space-time patches of similar colors to that of the cue. In finer levels, details and dynamic elements are added correctly. The conditional inpainting section in the supplementary material shows how different cues are completed with different elements from the non-occluded parts. See for instance, how a blue cue will be replaced by a player from Barcelona while a white cue by a player from Real Madrid. See more examples in the supplementary material.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Limitations">
         <div class="c-article-section" id="Sec10-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">
           <span class="c-article-section__title-number">
            6
           </span>
           Limitations
          </h2>
          <div class="c-article-section__content" id="Sec10-content">
           <p>
            Generation of local patches lacks high-level semantic or global geometric understanding of the scene. For example, This is apparent when scenes with significant depth variations are introduced with large camera motion. While each frame is plausible, different patches are not being transformed consistently, resulting in non-rigid deformations to entities that are realistically rigid. See the generated videos of mountains in the supplementary.
           </p>
          </div>
         </div>
        </section>
        <section data-title="Conclusion">
         <div class="c-article-section" id="Sec11-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">
           <span class="c-article-section__title-number">
            7
           </span>
           Conclusion
          </h2>
          <div class="c-article-section__content" id="Sec11-content">
           <p>
            We demonstrated that random diverse video generation from a single video can be efficiently done by simple patch-based methods. We also demonstrated how small modifications to our framework give rise to other tasks such as video analogies and spatio-temporal retargeting. We showed that our non-parametric approach outperforms existing single-video GANs in the visual quality of the generated outputs, while being orders of magnitude faster. The low run time required for generating videos using our approach makes it a good baseline for future works in the field.
           </p>
          </div>
         </div>
        </section>
       </div>
       <section data-title="Notes" lang="en">
        <div class="c-article-section" id="notes-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">
          <span class="c-article-section__title-number">
          </span>
          Notes
         </h2>
         <div class="c-article-section__content" id="notes-content">
          <ol class="c-article-footnote c-article-footnote--listed">
           <li class="c-article-footnote--listed__item" id="Fn1">
            <span class="c-article-footnote--listed__index">
             1.
            </span>
            <div class="c-article-footnote--listed__content">
             <p>
              All quantitative comparisons were done on generated samples of the same resolution and video length as that of the other method.
             </p>
            </div>
           </li>
           <li class="c-article-footnote--listed__item" id="Fn2">
            <span class="c-article-footnote--listed__index">
             2.
            </span>
            <div class="c-article-footnote--listed__content">
             <p>
              Each cluster has an integer cluster index. We divide each index by the total number of clusters/bins to be in [0, 1].
             </p>
            </div>
           </li>
          </ol>
         </div>
        </div>
       </section>
       <div id="MagazineFulltextChapterBodySuffix">
        <section aria-labelledby="Bib1" data-title="References">
         <div class="c-article-section" id="Bib1-section">
          <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">
           <span class="c-article-section__title-number">
           </span>
           References
          </h2>
          <div class="c-article-section__content" id="Bib1-content">
           <div data-container-section="references">
            <ol class="c-article-references" data-track-component="outbound reference">
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1.">
              <p class="c-article-references__text" id="ref-CR1">
               Aberman, K., Weng, Y., Lischinski, D., Cohen-Or, D., Chen, B.: Unpaired motion style transfer from video to animation. ACM Trans. Graph. (TOG)
               <b>
                39
               </b>
               (4), 1–64 (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR1-links">
               <a aria-label="CrossRef reference 1" data-doi="10.1145/3386569.3392469" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/3386569.3392469" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3386569.3392469" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 1" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Unpaired%20motion%20style%20transfer%20from%20video%20to%20animation&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=39&amp;issue=4&amp;pages=1-64&amp;publication_year=2020&amp;author=Aberman%2CK&amp;author=Weng%2CY&amp;author=Lischinski%2CD&amp;author=Cohen-Or%2CD&amp;author=Chen%2CB" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2.">
              <p class="c-article-references__text" id="ref-CR2">
               Aigner, S., Körner, M.: Futuregan: anticipating the future frames of video sequences using spatio-temporal 3d convolutions in progressively growing gans. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1810.01325" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1810.01325">
                arXiv:1810.01325
               </a>
               (2018)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3.">
              <p class="c-article-references__text" id="ref-CR3">
               Aksan, E., Hilliges, O.: Stcn: stochastic temporal convolutional networks. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1902.06568" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1902.06568">
                arXiv:1902.06568
               </a>
               (2019)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4.">
              <p class="c-article-references__text" id="ref-CR4">
               Apostolidis, E., Adamantidou, E., Metsai, A.I., Mezaris, V., Patras, I.: Video summarization using deep neural networks: a survey. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2101.06072" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2101.06072">
                arXiv:2101.06072
               </a>
               (2021)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5.">
              <p class="c-article-references__text" id="ref-CR5">
               Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR5-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Arora%2C%20R.%2C%20Lee%2C%20Y.J.%3A%20Singan-gif%3A%20learning%20a%20generative%20video%20model%20from%20a%20single%20gif.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Winter%20Conference%20on%20Applications%20of%20Computer%20Vision%2C%20pp.%201310%E2%80%931319%20%282021%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6.">
              <p class="c-article-references__text" id="ref-CR6">
               Babaeizadeh, M., Finn, C., Erhan, D., Campbell, R.H., Levine, S.: Stochastic variational video prediction. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.11252" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.11252">
                arXiv:1710.11252
               </a>
               (2017)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7.">
              <p class="c-article-references__text" id="ref-CR7">
               Ballas, N., Yao, L., Pal, C., Courville, A.: Delving deeper into convolutional networks for learning video representations. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1511.06432" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1511.06432">
                arXiv:1511.06432
               </a>
               (2015)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8.">
              <p class="c-article-references__text" id="ref-CR8">
               Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: unsupervised video retargeting. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 119–135 (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR8-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bansal%2C%20A.%2C%20Ma%2C%20S.%2C%20Ramanan%2C%20D.%2C%20Sheikh%2C%20Y.%3A%20Recycle-gan%3A%20unsupervised%20video%20retargeting.%20In%3A%20Proceedings%20of%20the%20European%20Conference%20on%20Computer%20Vision%20%28ECCV%29%2C%20pp.%20119%E2%80%93135%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9.">
              <p class="c-article-references__text" id="ref-CR9">
               Barnes, C., Zhang, F.-L.: A survey of the state-of-the-art in patch-based synthesis. Comput. Vis. Media
               <b>
                3
               </b>
               (1), 3–20 (2017).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/s41095-016-0064-2" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/s41095-016-0064-2">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/s41095-016-0064-2
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR9-links">
               <a aria-label="CrossRef reference 9" data-doi="10.1007/s41095-016-0064-2" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/s41095-016-0064-2" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs41095-016-0064-2" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 9" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20the%20state-of-the-art%20in%20patch-based%20synthesis&amp;journal=Comput.%20Vis.%20Media&amp;doi=10.1007%2Fs41095-016-0064-2&amp;volume=3&amp;issue=1&amp;pages=3-20&amp;publication_year=2017&amp;author=Barnes%2CC&amp;author=Zhang%2CF-L" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10.">
              <p class="c-article-references__text" id="ref-CR10">
               Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. Graph.
               <b>
                28
               </b>
               (3), 24 (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR10-links">
               <a aria-label="CrossRef reference 10" data-doi="10.1145/1531326.1531330" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/1531326.1531330" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F1531326.1531330" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 10" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Patchmatch%3A%20a%20randomized%20correspondence%20algorithm%20for%20structural%20image%20editing&amp;journal=ACM%20Trans.%20Graph.&amp;volume=28&amp;issue=3&amp;publication_year=2009&amp;author=Barnes%2CC&amp;author=Shechtman%2CE&amp;author=Finkelstein%2CA&amp;author=Goldman%2CDB" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11.">
              <p class="c-article-references__text" id="ref-CR11">
               Barnes, C., Shechtman, E., Goldman, D.B., Finkelstein, A.: The generalized patchmatch correspondence algorithm. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6313, pp. 29–43. Springer, Heidelberg (2010).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-15558-1_3" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-15558-1_3">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-15558-1_3
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR11-links">
               <a aria-label="CrossRef reference 11" data-doi="10.1007/978-3-642-15558-1_3" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-642-15558-1_3" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-642-15558-1_3" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 11" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=The%20generalized%20patchmatch%20correspondence%20algorithm&amp;pages=29-43&amp;publication_year=2010 2010 2010&amp;author=Barnes%2CC&amp;author=Shechtman%2CE&amp;author=Goldman%2CDB&amp;author=Finkelstein%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12.">
              <p class="c-article-references__text" id="ref-CR12">
               Barnes, C., Zhang, F.-L., Lou, L., Xian, W., Shi-Min, H.: Patchtable: efficient patch queries for large datasets and applications. ACM Trans. Graph. (ToG)
               <b>
                34
               </b>
               (4), 1–10 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR12-links">
               <a aria-label="CrossRef reference 12" data-doi="10.1145/2766934" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/2766934" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2766934" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 12" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Patchtable%3A%20efficient%20patch%20queries%20for%20large%20datasets%20and%20applications&amp;journal=ACM%20Trans.%20Graph.%20%28ToG%29&amp;volume=34&amp;issue=4&amp;pages=1-10&amp;publication_year=2015&amp;author=Barnes%2CC&amp;author=Zhang%2CF-L&amp;author=Lou%2CL&amp;author=Xian%2CW&amp;author=Shi-Min%2CH" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13.">
              <p class="c-article-references__text" id="ref-CR13">
               Benaim, S., Mokady, R., Bermano, A., Wolf, L.: Structural analogy from a single image pair. In: Computer Graphics Forum, vol. 40, pp. 249–265. Wiley Online Library (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR13-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Benaim%2C%20S.%2C%20Mokady%2C%20R.%2C%20Bermano%2C%20A.%2C%20Wolf%2C%20L.%3A%20Structural%20analogy%20from%20a%20single%20image%20pair.%20In%3A%20Computer%20Graphics%20Forum%2C%20vol.%2040%2C%20pp.%20249%E2%80%93265.%20Wiley%20Online%20Library%20%282021%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14.">
              <p class="c-article-references__text" id="ref-CR14">
               Bhat, K.S., Seitz, S.M., Hodgins, J.K., Khosla, P.K.: Flow-based video synthesis and editing. In: ACM SIGGRAPH 2004 Papers, pp. 360–363 (2004)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR14-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bhat%2C%20K.S.%2C%20Seitz%2C%20S.M.%2C%20Hodgins%2C%20J.K.%2C%20Khosla%2C%20P.K.%3A%20Flow-based%20video%20synthesis%20and%20editing.%20In%3A%20ACM%20SIGGRAPH%202004%20Papers%2C%20pp.%20360%E2%80%93363%20%282004%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15.">
              <p class="c-article-references__text" id="ref-CR15">
               Blattmann, A., Milbich, T., Dorkenwald, M., Ommer, B.: ipoke: poking a still image for controlled stochastic video synthesis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14707–14717 (2021)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR15-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Blattmann%2C%20A.%2C%20Milbich%2C%20T.%2C%20Dorkenwald%2C%20M.%2C%20Ommer%2C%20B.%3A%20ipoke%3A%20poking%20a%20still%20image%20for%20controlled%20stochastic%20video%20synthesis.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%2014707%E2%80%9314717%20%282021%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16.">
              <p class="c-article-references__text" id="ref-CR16">
               Chan, C., Ginosar, S., Zhou, T., Efros, A.A.: Everybody dance now. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5933–5942 (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR16-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Chan%2C%20C.%2C%20Ginosar%2C%20S.%2C%20Zhou%2C%20T.%2C%20Efros%2C%20A.A.%3A%20Everybody%20dance%20now.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%205933%E2%80%935942%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17.">
              <p class="c-article-references__text" id="ref-CR17">
               Denton, E., Fergus, R.: Stochastic video generation with a learned prior. In: International Conference on Machine Learning, pp. 1174–1183. PMLR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR17-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Denton%2C%20E.%2C%20Fergus%2C%20R.%3A%20Stochastic%20video%20generation%20with%20a%20learned%20prior.%20In%3A%20International%20Conference%20on%20Machine%20Learning%2C%20pp.%201174%E2%80%931183.%20PMLR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18.">
              <p class="c-article-references__text" id="ref-CR18">
               Duggal, S., Wang, S., Ma, W. C., Hu, R., Urtasun, R.: Deeppruner: learning efficient stereo matching via differentiable patchmatch. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4384–4393 (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR18-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Duggal%2C%20S.%2C%20Wang%2C%20S.%2C%20Ma%2C%20W.%20C.%2C%20Hu%2C%20R.%2C%20Urtasun%2C%20R.%3A%20Deeppruner%3A%20learning%20efficient%20stereo%20matching%20via%20differentiable%20patchmatch.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%204384%E2%80%934393%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19.">
              <p class="c-article-references__text" id="ref-CR19">
               Efros, A.A., Freeman, W.T.: Image quilting for texture synthesis and transfer. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 341–346 (2001)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR19-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Efros%2C%20A.A.%2C%20Freeman%2C%20W.T.%3A%20Image%20quilting%20for%20texture%20synthesis%20and%20transfer.%20In%3A%20Proceedings%20of%20the%2028th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20341%E2%80%93346%20%282001%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20.">
              <p class="c-article-references__text" id="ref-CR20">
               Efros, A.A., Leung, T.K.: Texture synthesis by non-parametric sampling. In: Proceedings of the Seventh IEEE International Conference on Computer Vision, vol. 2, pp. 1033–1038. IEEE (1999)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR20-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Efros%2C%20A.A.%2C%20Leung%2C%20T.K.%3A%20Texture%20synthesis%20by%20non-parametric%20sampling.%20In%3A%20Proceedings%20of%20the%20Seventh%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20vol.%202%2C%20pp.%201033%E2%80%931038.%20IEEE%20%281999%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21.">
              <p class="c-article-references__text" id="ref-CR21">
               Fišer, J., et al.: Stylit: illumination-guided example-based stylization of 3d renderings. ACM Trans. Graph. (TOG)
               <b>
                35
               </b>
               (4), 1–11 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR21-links">
               <a aria-label="CrossRef reference 21" data-doi="10.1145/2897824.2925948" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/2897824.2925948" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2897824.2925948" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 21" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Stylit%3A%20illumination-guided%20example-based%20stylization%20of%203d%20renderings&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=35&amp;issue=4&amp;pages=1-11&amp;publication_year=2016&amp;author=Fi%C5%A1er%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22.">
              <p class="c-article-references__text" id="ref-CR22">
               Franceschi, J.-Y., Delasalles, E., Chen, M., Lamprier, S., Gallinari, P.: Stochastic latent residual video prediction. In: International Conference on Machine Learning, pp. 3233–3246. PMLR (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR22-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Franceschi%2C%20J.-Y.%2C%20Delasalles%2C%20E.%2C%20Chen%2C%20M.%2C%20Lamprier%2C%20S.%2C%20Gallinari%2C%20P.%3A%20Stochastic%20latent%20residual%20video%20prediction.%20In%3A%20International%20Conference%20on%20Machine%20Learning%2C%20pp.%203233%E2%80%933246.%20PMLR%20%282020%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23.">
              <p class="c-article-references__text" id="ref-CR23">
               Goodfellow, I.J., et al.: Generative adversarial networks. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1406.2661" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1406.2661">
                arXiv:1406.2661
               </a>
               (2014)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24.">
              <p class="c-article-references__text" id="ref-CR24">
               Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR24-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Granot%2C%20N.%2C%20Feinstein%2C%20B.%2C%20Shocher%2C%20A.%2C%20Bagon%2C%20S.%2C%20Irani%2C%20M.%3A%20Drop%20the%20gan%3A%20in%20defense%20of%20patches%20nearest%20neighbors%20as%20single%20image%20generative%20models.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%2013460%E2%80%9313469%20%282022%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25.">
              <p class="c-article-references__text" id="ref-CR25">
               Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2006.12226" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2006.12226">
                arXiv:2006.12226
               </a>
               (2020)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26.">
              <p class="c-article-references__text" id="ref-CR26">
               Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analogies. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 327–340 (2001)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR26-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hertzmann%2C%20A.%2C%20Jacobs%2C%20C.E.%2C%20Oliver%2C%20N.%2C%20Curless%2C%20B.%2C%20Salesin%2C%20D.H.%3A%20Image%20analogies.%20In%3A%20Proceedings%20of%20the%2028th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20327%E2%80%93340%20%282001%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27.">
              <p class="c-article-references__text" id="ref-CR27">
               Huang, J.-B., Kang, S.B., Ahuja, N., Kopf, J.: Temporally coherent completion of dynamic video. ACM Trans. Graph. (TOG)
               <b>
                35
               </b>
               (6), 1–11 (2016)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR27-links">
               <a aria-label="Google Scholar reference 27" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Temporally%20coherent%20completion%20of%20dynamic%20video&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=35&amp;issue=6&amp;pages=1-11&amp;publication_year=2016&amp;author=Huang%2CJ-B&amp;author=Kang%2CSB&amp;author=Ahuja%2CN&amp;author=Kopf%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28.">
              <p class="c-article-references__text" id="ref-CR28">
               Jamriška, O., Fišer, J., Asente, P., Jingwan, L., Shechtman, E., Sỳkora, D.: Lazyfluids: appearance transfer for fluid animations. ACM Trans. Graph. (TOG)
               <b>
                34
               </b>
               (4), 1–10 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR28-links">
               <a aria-label="CrossRef reference 28" data-doi="10.1145/2766983" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/2766983" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2766983" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 28" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Lazyfluids%3A%20appearance%20transfer%20for%20fluid%20animations&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=34&amp;issue=4&amp;pages=1-10&amp;publication_year=2015&amp;author=Jamri%C5%A1ka%2CO&amp;author=Fi%C5%A1er%2CJ&amp;author=Asente%2CP&amp;author=Jingwan%2CL&amp;author=Shechtman%2CE&amp;author=S%E1%BB%B3kora%2CD" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29.">
              <p class="c-article-references__text" id="ref-CR29">
               Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for improved quality, stability, and variation. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.10196" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.10196">
                arXiv:1710.10196
               </a>
               , 2017
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30.">
              <p class="c-article-references__text" id="ref-CR30">
               Krähenbühl, P., Lang, M., Hornung, A., Gross, M.: A system for retargeting of streaming video. In: ACM SIGGRAPH Asia 2009 papers (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR30-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kr%C3%A4henb%C3%BChl%2C%20P.%2C%20Lang%2C%20M.%2C%20Hornung%2C%20A.%2C%20Gross%2C%20M.%3A%20A%20system%20for%20retargeting%20of%20streaming%20video.%20In%3A%20ACM%20SIGGRAPH%20Asia%202009%20papers%20%282009%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31.">
              <p class="c-article-references__text" id="ref-CR31">
               Kwatra, V., Schödl, A., Essa, I., Turk, G., Bobick, A.: Graphcut textures: Image and video synthesis using graph cuts. Acm Trans. Graph. (ToG)
               <b>
                22
               </b>
               (3), 277–286 (2003)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR31-links">
               <a aria-label="CrossRef reference 31" data-doi="10.1145/882262.882264" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/882262.882264" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F882262.882264" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 31" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Graphcut%20textures%3A%20Image%20and%20video%20synthesis%20using%20graph%20cuts&amp;journal=Acm%20Trans.%20Graph.%20%28ToG%29&amp;volume=22&amp;issue=3&amp;pages=277-286&amp;publication_year=2003&amp;author=Kwatra%2CV&amp;author=Sch%C3%B6dl%2CA&amp;author=Essa%2CI&amp;author=Turk%2CG&amp;author=Bobick%2CA" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32.">
              <p class="c-article-references__text" id="ref-CR32">
               Kwatra, V., Essa, I., Bobick, A., Kwatra, N.: Texture optimization for example-based synthesis. In: ACM SIGGRAPH 2005 Papers, pp. 795–802 (2005)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR32-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kwatra%2C%20V.%2C%20Essa%2C%20I.%2C%20Bobick%2C%20A.%2C%20Kwatra%2C%20N.%3A%20Texture%20optimization%20for%20example-based%20synthesis.%20In%3A%20ACM%20SIGGRAPH%202005%20Papers%2C%20pp.%20795%E2%80%93802%20%282005%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33.">
              <p class="c-article-references__text" id="ref-CR33">
               Kwatra, V., Adalsteinsson, D., Kim, T., Kwatra, N., Carlson, M., Lin, M.: Texturing fluids. IEEE Trans. Visual Comput. Graph.
               <b>
                13
               </b>
               (5), 939–952 (2007)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR33-links">
               <a aria-label="CrossRef reference 33" data-doi="10.1109/TVCG.2007.1044" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1109/TVCG.2007.1044" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTVCG.2007.1044" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 33" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Texturing%20fluids&amp;journal=IEEE%20Trans.%20Visual%20Comput.%20Graph.&amp;volume=13&amp;issue=5&amp;pages=939-952&amp;publication_year=2007&amp;author=Kwatra%2CV&amp;author=Adalsteinsson%2CD&amp;author=Kim%2CT&amp;author=Kwatra%2CN&amp;author=Carlson%2CM&amp;author=Lin%2CM" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34.">
              <p class="c-article-references__text" id="ref-CR34">
               Le, T.T., Almansa, A., Gousseau, Y., Masnou, S.: Motion-consistent video inpainting. In: 2017 IEEE International Conference on Image Processing (ICIP), pp. 2094–2098. IEEE (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR34-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Le%2C%20T.T.%2C%20Almansa%2C%20A.%2C%20Gousseau%2C%20Y.%2C%20Masnou%2C%20S.%3A%20Motion-consistent%20video%20inpainting.%20In%3A%202017%20IEEE%20International%20Conference%20on%20Image%20Processing%20%28ICIP%29%2C%20pp.%202094%E2%80%932098.%20IEEE%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35.">
              <p class="c-article-references__text" id="ref-CR35">
               LeCun, Y.: The mnist database of handwritten digits.
               <a data-track="click" data-track-action="external reference" data-track-label="http://yann.lecun.com/exdb/mnist/" href="http://yann.lecun.com/exdb/mnist/">
                http://yann.lecun.com/exdb/mnist/
               </a>
               (1998)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36.">
              <p class="c-article-references__text" id="ref-CR36">
               Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., Levine, S.: Stochastic adversarial video prediction. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1804.01523" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1804.01523">
                arXiv:1804.01523
               </a>
               (2018)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37.">
              <p class="c-article-references__text" id="ref-CR37">
               Lee, J., Ramanan, D., Girdhar, R.: Metapix: few-shot video retargeting. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1910.04742" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1910.04742">
                arXiv:1910.04742
               </a>
               (2019)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38.">
              <p class="c-article-references__text" id="ref-CR38">
               Liao, J., Yao, Y., Yuan, L., Hua, G., Kang, S.B.: Visual attribute transfer through deep image analogy. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.01088" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.01088">
                arXiv:1705.01088
               </a>
               (2017)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39.">
              <p class="c-article-references__text" id="ref-CR39">
               Liu, M., Chen, S., Liu, J., Tang, X.: Video completion via motion guided spatial-temporal global optimization. In: Proceedings of the 17th ACM International Conference on Multimedia, pp. 537–540 (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR39-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liu%2C%20M.%2C%20Chen%2C%20S.%2C%20Liu%2C%20J.%2C%20Tang%2C%20X.%3A%20Video%20completion%20via%20motion%20guided%20spatial-temporal%20global%20optimization.%20In%3A%20Proceedings%20of%20the%2017th%20ACM%20International%20Conference%20on%20Multimedia%2C%20pp.%20537%E2%80%93540%20%282009%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40.">
              <p class="c-article-references__text" id="ref-CR40">
               Mallya, A., Wang, T.-C., Sapra, K., Liu, M.-Y.: World-consistent video-to-video synthesis. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2007.08509" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2007.08509">
                arXiv:2007.08509
               </a>
               (2020)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41.">
              <p class="c-article-references__text" id="ref-CR41">
               Newson, A., Almansa, A., Fradet, M., Gousseau, Y., Pérez, P.: Towards fast, generic video inpainting. In: Proceedings of the 10th European Conference on Visual Media Production, pp. 1–8 (2013)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR41-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Newson%2C%20A.%2C%20Almansa%2C%20A.%2C%20Fradet%2C%20M.%2C%20Gousseau%2C%20Y.%2C%20P%C3%A9rez%2C%20P.%3A%20Towards%20fast%2C%20generic%20video%20inpainting.%20In%3A%20Proceedings%20of%20the%2010th%20European%20Conference%20on%20Visual%20Media%20Production%2C%20pp.%201%E2%80%938%20%282013%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42.">
              <p class="c-article-references__text" id="ref-CR42">
               Newson, A., Almansa, A., Fradet, M., Gousseau, Y., Pérez, P.: Video inpainting of complex scenes. SIAM J. Imag. Sci.
               <b>
                7
               </b>
               (4), 1993–2019 (2014)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR42-links">
               <a aria-label="CrossRef reference 42" data-doi="10.1137/140954933" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1137/140954933" href="https://doi-org.proxy.lib.ohio-state.edu/10.1137%2F140954933" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="MathSciNet reference 42" data-track="click" data-track-action="MathSciNet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=3268608" rel="nofollow noopener">
                MathSciNet
               </a>
               <a aria-label="Google Scholar reference 42" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Video%20inpainting%20of%20complex%20scenes&amp;journal=SIAM%20J.%20Imag.%20Sci.&amp;volume=7&amp;issue=4&amp;pages=1993-2019&amp;publication_year=2014&amp;author=Newson%2CA&amp;author=Almansa%2CA&amp;author=Fradet%2CM&amp;author=Gousseau%2CY&amp;author=P%C3%A9rez%2CP" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43.">
              <p class="c-article-references__text" id="ref-CR43">
               Okabe, M., Anjyo, K., Igarashi, T., Seidel, H.P.: Animating pictures of fluid using video examples. In: Computer Graphics Forum, vol. 28, pp. 677–686. Wiley Online Library (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR43-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Okabe%2C%20M.%2C%20Anjyo%2C%20K.%2C%20Igarashi%2C%20T.%2C%20Seidel%2C%20H.P.%3A%20Animating%20pictures%20of%20fluid%20using%20video%20examples.%20In%3A%20Computer%20Graphics%20Forum%2C%20vol.%2028%2C%20pp.%20677%E2%80%93686.%20Wiley%20Online%20Library%20%282009%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44.">
              <p class="c-article-references__text" id="ref-CR44">
               Paszke, A., et al.: Pytorch: an imperative style, high-performance deep learning library. In: Wallach, H., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates Inc (2019).
               <a data-track="click" data-track-action="external reference" data-track-label="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">
                http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45.">
              <p class="c-article-references__text" id="ref-CR45">
               Rav-Acha, A., Pritch, Y., Peleg, S.: Making a long video short: dynamic video synopsis. In: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), vol. 1, pp. 435–441. IEEE (2006)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR45-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Rav-Acha%2C%20A.%2C%20Pritch%2C%20Y.%2C%20Peleg%2C%20S.%3A%20Making%20a%20long%20video%20short%3A%20dynamic%20video%20synopsis.%20In%3A%202006%20IEEE%20Computer%20Society%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%E2%80%9906%29%2C%20vol.%201%2C%20pp.%20435%E2%80%93441.%20IEEE%20%282006%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46.">
              <p class="c-article-references__text" id="ref-CR46">
               Ren, J., Chai, M., Tulyakov, S., Fang, C., Shen, X., Yang, J.: Human motion transfer from poses in the wild. In: Bartoli, A., Fusiello, A. (eds.) ECCV 2020. LNCS, vol. 12537, pp. 262–279. Springer, Cham (2020).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-030-67070-2_16" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-67070-2_16">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-67070-2_16
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR46-links">
               <a aria-label="CrossRef reference 46" data-doi="10.1007/978-3-030-67070-2_16" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-030-67070-2_16" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-030-67070-2_16" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 46" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Human%20motion%20transfer%20from%20poses%20in%20the%20wild&amp;pages=262-279&amp;publication_year=2020 2020 2020&amp;author=Ren%2CJ&amp;author=Chai%2CM&amp;author=Tulyakov%2CS&amp;author=Fang%2CC&amp;author=Shen%2CX&amp;author=Yang%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47.">
              <p class="c-article-references__text" id="ref-CR47">
               Rubinstein, M., Shamir, A., Avidan, S.: Improved seam carving for video retargeting. ACM Trans. Graph. (TOG)
               <b>
                27
               </b>
               (3), 1–9 (2008)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR47-links">
               <a aria-label="CrossRef reference 47" data-doi="10.1145/1360612.1360615" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/1360612.1360615" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F1360612.1360615" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 47" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Improved%20seam%20carving%20for%20video%20retargeting&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=27&amp;issue=3&amp;pages=1-9&amp;publication_year=2008&amp;author=Rubinstein%2CM&amp;author=Shamir%2CA&amp;author=Avidan%2CS" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48.">
              <p class="c-article-references__text" id="ref-CR48">
               Saito, M., Matsumoto, E., Saito, S.: Temporal generative adversarial nets with singular value clipping. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2830–2839 (2017)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR48-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Saito%2C%20M.%2C%20Matsumoto%2C%20E.%2C%20Saito%2C%20S.%3A%20Temporal%20generative%20adversarial%20nets%20with%20singular%20value%20clipping.%20In%3A%20Proceedings%20of%20the%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%202830%E2%80%932839%20%282017%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49.">
              <p class="c-article-references__text" id="ref-CR49">
               Sato, S., Dobashi, Y., Kim, T., Nishita, T.: Example-based turbulence style transfer. ACM Trans. Graph. (TOG)
               <b>
                37
               </b>
               (4), 1–9 (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR49-links">
               <a aria-label="CrossRef reference 49" data-doi="10.1145/3197517.3201398" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/3197517.3201398" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3197517.3201398" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 49" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Example-based%20turbulence%20style%20transfer&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=37&amp;issue=4&amp;pages=1-9&amp;publication_year=2018&amp;author=Sato%2CS&amp;author=Dobashi%2CY&amp;author=Kim%2CT&amp;author=Nishita%2CT" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50.">
              <p class="c-article-references__text" id="ref-CR50">
               Sato, S., Dobashi, Y., Nishita, T.: Editing fluid animation using flow interpolation. ACM Trans. Graph. (TOG)
               <b>
                37
               </b>
               (5), 1–12 (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR50-links">
               <a aria-label="CrossRef reference 50" data-doi="10.1145/3213771" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1145/3213771" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3213771" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 50" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Editing%20fluid%20animation%20using%20flow%20interpolation&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=37&amp;issue=5&amp;pages=1-12&amp;publication_year=2018&amp;author=Sato%2CS&amp;author=Dobashi%2CY&amp;author=Nishita%2CT" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51.">
              <p class="c-article-references__text" id="ref-CR51">
               Schödl, A., Szeliski, R., Salesin, D.H., Essa, I.: Video textures. In: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, pp. 489–498 (2000)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR51-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sch%C3%B6dl%2C%20A.%2C%20Szeliski%2C%20R.%2C%20Salesin%2C%20D.H.%2C%20Essa%2C%20I.%3A%20Video%20textures.%20In%3A%20Proceedings%20of%20the%2027th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20489%E2%80%93498%20%282000%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52.">
              <p class="c-article-references__text" id="ref-CR52">
               Shaham, T.R., Dekel, T., Michaeli, T.: Singan: learning a generative model from a single natural image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4570–4580 (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR52-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Shaham%2C%20T.R.%2C%20Dekel%2C%20T.%2C%20Michaeli%2C%20T.%3A%20Singan%3A%20learning%20a%20generative%20model%20from%20a%20single%20natural%20image.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%204570%E2%80%934580%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53.">
              <p class="c-article-references__text" id="ref-CR53">
               Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., Sebe, N.: Animating arbitrary objects via deep motion transfer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2377–2386 (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR53-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Siarohin%2C%20A.%2C%20Lathuili%C3%A8re%2C%20S.%2C%20Tulyakov%2C%20S.%2C%20Ricci%2C%20E.%2C%20Sebe%2C%20N.%3A%20Animating%20arbitrary%20objects%20via%20deep%20motion%20transfer.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%202377%E2%80%932386%20%282019%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54.">
              <p class="c-article-references__text" id="ref-CR54">
               Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion model for image animation. Adv. Neural. Inf. Process. Syst.
               <b>
                32
               </b>
               , 7137–7147 (2019)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR54-links">
               <a aria-label="Google Scholar reference 54" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=First%20order%20motion%20model%20for%20image%20animation&amp;journal=Adv.%20Neural.%20Inf.%20Process.%20Syst.&amp;volume=32&amp;pages=7137-7147&amp;publication_year=2019&amp;author=Siarohin%2CA&amp;author=Lathuili%C3%A8re%2CS&amp;author=Tulyakov%2CS&amp;author=Ricci%2CE&amp;author=Sebe%2CN" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55.">
              <p class="c-article-references__text" id="ref-CR55">
               Simakov, D., Caspi, Y., Shechtman, E., Irani, M.: Summarizing visual data using bidirectional similarity. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8. IEEE (2008)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR55-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Simakov%2C%20D.%2C%20Caspi%2C%20Y.%2C%20Shechtman%2C%20E.%2C%20Irani%2C%20M.%3A%20Summarizing%20visual%20data%20using%20bidirectional%20similarity.%20In%3A%202008%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%201%E2%80%938.%20IEEE%20%282008%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56.">
              <p class="c-article-references__text" id="ref-CR56">
               Teed, Z., Deng, J.: RAFT: recurrent all-pairs field transforms for optical flow. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12347, pp. 402–419. Springer, Cham (2020).
               <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-030-58536-5_24" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58536-5_24">
                https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58536-5_24
               </a>
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR56-links">
               <a aria-label="CrossRef reference 56" data-doi="10.1007/978-3-030-58536-5_24" data-track="click" data-track-action="CrossRef reference" data-track-label="10.1007/978-3-030-58536-5_24" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-030-58536-5_24" rel="nofollow noopener">
                CrossRef
               </a>
               <a aria-label="Google Scholar reference 56" data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=RAFT%3A%20recurrent%20all-pairs%20field%20transforms%20for%20optical%20flow&amp;pages=402-419&amp;publication_year=2020 2020 2020&amp;author=Teed%2CZ&amp;author=Deng%2CJ" rel="nofollow noopener">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57.">
              <p class="c-article-references__text" id="ref-CR57">
               Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotemporal features with 3d convolutional networks. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 4489–4497 (2015)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR57-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tran%2C%20D.%2C%20Bourdev%2C%20L.%2C%20Fergus%2C%20R.%2C%20Torresani%2C%20L.%2C%20Paluri%2C%20M.%3A%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks.%20In%3A%20Proceedings%20of%20the%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%204489%E2%80%934497%20%282015%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58.">
              <p class="c-article-references__text" id="ref-CR58">
               Tulyakov, S., Liu, M. Y., Yang, X., Kautz, J.: Mocogan: decomposing motion and content for video generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1526–1535 (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR58-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tulyakov%2C%20S.%2C%20Liu%2C%20M.%20Y.%2C%20Yang%2C%20X.%2C%20Kautz%2C%20J.%3A%20Mocogan%3A%20decomposing%20motion%20and%20content%20for%20video%20generation.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%201526%E2%80%931535%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59.">
              <p class="c-article-references__text" id="ref-CR59">
               Villegas, R., Yang, J., Hong, S., Lin, X., Lee, H.: Decomposing motion and content for natural video sequence prediction. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1706.08033" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1706.08033">
                arXiv:1706.08033
               </a>
               (2017)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60.">
              <p class="c-article-references__text" id="ref-CR60">
               Villegas, R., Erhan, D., Lee, H., et al.: Hierarchical long-term video prediction without supervision. In: International Conference on Machine Learning, pp. 6038–6046. PMLR (2018)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR60-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Villegas%2C%20R.%2C%20Erhan%2C%20D.%2C%20Lee%2C%20H.%2C%20et%20al.%3A%20Hierarchical%20long-term%20video%20prediction%20without%20supervision.%20In%3A%20International%20Conference%20on%20Machine%20Learning%2C%20pp.%206038%E2%80%936046.%20PMLR%20%282018%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61.">
              <p class="c-article-references__text" id="ref-CR61">
               Villegas, R., Pathak, A., Kannan, H., Erhan, D., Le, Q.V., Lee, H.: High fidelity video prediction with large stochastic recurrent neural networks. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1911.01655" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1911.01655">
                arXiv:1911.01655
               </a>
               (2019)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62.">
              <p class="c-article-references__text" id="ref-CR62">
               Vondrick, C., Pirsiavash, H., Torralba, A.: Generating videos with scene dynamics. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1609.02612" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1609.02612">
                arXiv:1609.02612
               </a>
               (2016)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63.">
              <p class="c-article-references__text" id="ref-CR63">
               Wang, T.-C., et al.: Video-to-video synthesis. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1808.06601" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1808.06601">
                arXiv:1808.06601
               </a>
               (2018)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64.">
              <p class="c-article-references__text" id="ref-CR64">
               Wang, T.-C., Liu, M.-Y., Tao, A., Liu, G., Kautz, J., Catanzaro, B.: Few-shot video-to-video synthesis. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1910.12713" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1910.12713">
                arXiv:1910.12713
               </a>
               (2019)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65.">
              <p class="c-article-references__text" id="ref-CR65">
               Wang, Y., Bilinski, P., Bremond, F., Dantcheva, A: Imaginator: conditional spatio-temporal gan for video generation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1160–1169 (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR65-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wang%2C%20Y.%2C%20Bilinski%2C%20P.%2C%20Bremond%2C%20F.%2C%20Dantcheva%2C%20A%3A%20Imaginator%3A%20conditional%20spatio-temporal%20gan%20for%20video%20generation.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Winter%20Conference%20on%20Applications%20of%20Computer%20Vision%2C%20pp.%201160%E2%80%931169%20%282020%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66.">
              <p class="c-article-references__text" id="ref-CR66">
               Wang, Y., Bremond, F., Dantcheva, A.: Inmodegan: interpretable motion decomposition generative adversarial network for video generation. arXiv preprint
               <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2101.03049" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2101.03049">
                arXiv:2101.03049
               </a>
               (2021)
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67.">
              <p class="c-article-references__text" id="ref-CR67">
               Wei, L.-Y., Levoy, M.: Fast texture synthesis using tree-structured vector quantization. In: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, pp. 479–488 (2000)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR67-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wei%2C%20L.-Y.%2C%20Levoy%2C%20M.%3A%20Fast%20texture%20synthesis%20using%20tree-structured%20vector%20quantization.%20In%3A%20Proceedings%20of%20the%2027th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20479%E2%80%93488%20%282000%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68.">
              <p class="c-article-references__text" id="ref-CR68">
               Wei, L. Y., Lefebvre, S., Kwatra, V., Turk, G.: State of the art in example-based texture synthesis. In: Eurographics 2009, State of the Art Report, EG-STAR, pp. 93–117. Eurographics Association (2009)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR68-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wei%2C%20L.%20Y.%2C%20Lefebvre%2C%20S.%2C%20Kwatra%2C%20V.%2C%20Turk%2C%20G.%3A%20State%20of%20the%20art%20in%20example-based%20texture%20synthesis.%20In%3A%20Eurographics%202009%2C%20State%20of%20the%20Art%20Report%2C%20EG-STAR%2C%20pp.%2093%E2%80%93117.%20Eurographics%20Association%20%282009%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69.">
              <p class="c-article-references__text" id="ref-CR69">
               Wexler, Y., Shechtman, E., Irani, M.: Space-time video completion. In: Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004, vol. 1, pp. I-I. IEEE (2004)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR69-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wexler%2C%20Y.%2C%20Shechtman%2C%20E.%2C%20Irani%2C%20M.%3A%20Space-time%20video%20completion.%20In%3A%20Proceedings%20of%20the%202004%20IEEE%20Computer%20Society%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%202004.%20CVPR%202004%2C%20vol.%201%2C%20pp.%20I-I.%20IEEE%20%282004%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70.">
              <p class="c-article-references__text" id="ref-CR70">
               Wolf, L., Guttmann, M., Cohen-Or, D.: Non-homogeneous content-driven video-retargeting. In: Proceedings of the Eleventh IEEE International Conference on Computer Vision (ICCV) (2007)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR70-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wolf%2C%20L.%2C%20Guttmann%2C%20M.%2C%20Cohen-Or%2C%20D.%3A%20Non-homogeneous%20content-driven%20video-retargeting.%20In%3A%20Proceedings%20of%20the%20Eleventh%20IEEE%20International%20Conference%20on%20Computer%20Vision%20%28ICCV%29%20%282007%29">
                Google Scholar
               </a>
              </p>
             </li>
             <li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71.">
              <p class="c-article-references__text" id="ref-CR71">
               Yang, Z., et al.: Transmomo: invariance-driven unsupervised video motion retargeting. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5306–5315 (2020)
              </p>
              <p class="c-article-references__links u-hide-print" id="ref-CR71-links">
               <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yang%2C%20Z.%2C%20et%20al.%3A%20Transmomo%3A%20invariance-driven%20unsupervised%20video%20motion%20retargeting.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%205306%E2%80%935315%20%282020%29">
                Google Scholar
               </a>
              </p>
             </li>
            </ol>
            <p class="c-article-references__download u-hide-print">
             <a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-031-19790-1_30?format=refman&amp;flavour=references" rel="nofollow">
              Download references
              <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
               <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
               </use>
              </svg>
             </a>
            </p>
           </div>
          </div>
         </div>
        </section>
       </div>
       <section data-title="Acknowledgements" lang="en">
        <div class="c-article-section" id="Ack1-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">
          <span class="c-article-section__title-number">
          </span>
          Acknowledgements
         </h2>
         <div class="c-article-section__content" id="Ack1-content">
          <p>
           This project received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 788535), from the D. Dan and Betty Kahn Foundation, and from the Israel Science Foundation (grant 2303/20). Dr. Bagon is a Robin Chemers Neustein AI Fellow.
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="author-information" data-title="Author information">
        <div class="c-article-section" id="author-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">
          <span class="c-article-section__title-number">
          </span>
          Author information
         </h2>
         <div class="c-article-section__content" id="author-information-content">
          <h3 class="c-article__sub-heading" id="affiliations">
           Authors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff12">
            <p class="c-article-author-affiliation__address">
             Weizmann Institute of Science, Rehovot, Israel
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Niv Haim, Ben Feinstein, Niv Granot, Assaf Shocher, Shai Bagon, Tali Dekel &amp; Michal Irani
            </p>
           </li>
          </ol>
          <div class="u-js-hide u-hide-print" data-test="author-info">
           <span class="c-article__sub-heading">
            Authors
           </span>
           <ol class="c-article-authors-search u-list-reset">
            <li id="auth-Niv-Haim">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Niv Haim
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Niv%20Haim" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Niv%20Haim" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Niv%20Haim%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Ben-Feinstein">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Ben Feinstein
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Ben%20Feinstein" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Ben%20Feinstein" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ben%20Feinstein%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Niv-Granot">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Niv Granot
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Niv%20Granot" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Niv%20Granot" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Niv%20Granot%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Assaf-Shocher">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Assaf Shocher
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Assaf%20Shocher" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Assaf%20Shocher" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Assaf%20Shocher%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Shai-Bagon">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Shai Bagon
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Shai%20Bagon" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Shai%20Bagon" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shai%20Bagon%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Tali-Dekel">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Tali Dekel
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Tali%20Dekel" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Tali%20Dekel" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tali%20Dekel%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
            <li id="auth-Michal-Irani">
             <span class="c-article-authors-search__title u-h3 js-search-name">
              Michal Irani
             </span>
             <div class="c-article-authors-search__list">
              <div class="c-article-authors-search__item c-article-authors-search__list-item--left">
               <a class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" href="/search?dc.creator=Michal%20Irani" rel="nofollow">
                View author publications
               </a>
              </div>
              <div class="c-article-authors-search__item c-article-authors-search__list-item--right">
               <p class="search-in-title-js c-article-authors-search__text">
                You can also search for this author in
                <span class="c-article-identifiers">
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - pubmed" data-track-label="link" href="http://www-ncbi-nlm-nih-gov.proxy.lib.ohio-state.edu/entrez/query.fcgi?cmd=search&amp;term=Michal%20Irani" rel="nofollow">
                  PubMed
                 </a>
                 <span class="u-hide">
                 </span>
                 <a class="c-article-identifiers__item" data-track="click" data-track-action="author link - scholar" data-track-label="link" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michal%20Irani%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" rel="nofollow">
                  Google Scholar
                 </a>
                </span>
               </p>
              </div>
             </div>
            </li>
           </ol>
          </div>
          <h3 class="c-article__sub-heading" id="corresponding-author">
           Corresponding author
          </h3>
          <p id="corresponding-author-list">
           Correspondence to
           <a href="mailto:niv.haim@weizmann.ac.il" id="corresp-c1">
            Niv Haim
           </a>
           .
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="editor-information" data-title="Editor information">
        <div class="c-article-section" id="editor-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="editor-information">
          <span class="c-article-section__title-number">
          </span>
          Editor information
         </h2>
         <div class="c-article-section__content" id="editor-information-content">
          <h3 class="c-article__sub-heading" id="editor-affiliations">
           Editors and Affiliations
          </h3>
          <ol class="c-article-author-affiliation__list">
           <li id="Aff7">
            <p class="c-article-author-affiliation__address">
             Tel Aviv University, Tel Aviv, Israel
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Shai Avidan
            </p>
           </li>
           <li id="Aff8">
            <p class="c-article-author-affiliation__address">
             University College London, London, UK
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Gabriel Brostow
            </p>
           </li>
           <li id="Aff9">
            <p class="c-article-author-affiliation__address">
             Google AI, Accra, Ghana
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Moustapha Cissé
            </p>
           </li>
           <li id="Aff10">
            <p class="c-article-author-affiliation__address">
             University of Catania, Catania, Italy
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Giovanni Maria Farinella
            </p>
           </li>
           <li id="Aff11">
            <p class="c-article-author-affiliation__address">
             Facebook (United States), Menlo Park, CA, USA
            </p>
            <p class="c-article-author-affiliation__authors-list">
             Tal Hassner
            </p>
           </li>
          </ol>
         </div>
        </div>
       </section>
       <section data-title="Rights and permissions" lang="en">
        <div class="c-article-section" id="rightslink-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">
          <span class="c-article-section__title-number">
          </span>
          Rights and permissions
         </h2>
         <div class="c-article-section__content" id="rightslink-content">
          <p class="c-article-rights" data-test="rightslink-content">
           <a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?publisherName=SpringerNature&amp;orderBeanReset=true&amp;orderSource=SpringerLink&amp;title=Diverse%20Generation%20from%C2%A0a%C2%A0Single%20Video%20Made%20Possible&amp;author=Niv%20Haim%2C%20Ben%20Feinstein%2C%20Niv%20Granot%20et%20al&amp;contentID=10.1007%2F978-3-031-19790-1_30&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20license%20to%20Springer%20Nature%20Switzerland%20AG&amp;publication=eBook&amp;publicationDate=2022&amp;startPage=491&amp;endPage=509&amp;imprint=The%20Author%28s%29%2C%20under%20exclusive%20license%20to%20Springer%20Nature%20Switzerland%20AG">
            Reprints and Permissions
           </a>
          </p>
         </div>
        </div>
       </section>
       <section data-title="Copyright information">
        <div class="c-article-section" id="copyright-information-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="copyright-information">
          <span class="c-article-section__title-number">
          </span>
          Copyright information
         </h2>
         <div class="c-article-section__content" id="copyright-information-content">
          <p>
           © 2022 The Author(s), under exclusive license to Springer Nature Switzerland AG
          </p>
         </div>
        </div>
       </section>
       <section aria-labelledby="chapter-info" data-title="About this paper" lang="en">
        <div class="c-article-section" id="chapter-info-section">
         <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="chapter-info">
          <span class="c-article-section__title-number">
          </span>
          About this paper
         </h2>
         <div class="c-article-section__content" id="chapter-info-content">
          <div class="c-bibliographic-information">
           <div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border">
            <a data-crossmark="10.1007/978-3-031-19790-1_30" data-test="crossmark" data-track="click" data-track-action="Click Crossmark" data-track-label="link" href="https://crossmark-crossref-org.proxy.lib.ohio-state.edu/dialog/?doi=10.1007/978-3-031-19790-1_30" rel="noopener" target="_blank">
             <img alt="Check for updates. Verify currency and authenticity via CrossMark" height="81" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" width="57"/>
            </a>
           </div>
           <div class="c-bibliographic-information__column">
            <h3 class="c-article__sub-heading" id="citeas">
             Cite this paper
            </h3>
            <p class="c-bibliographic-information__citation" data-test="bibliographic-information__cite_this_chapter">
             Haim, N.
             <i>
              et al.
             </i>
             (2022).  Diverse Generation from a Single Video Made Possible.

                     In: Avidan, S., Brostow, G., Cissé, M., Farinella, G.M., Hassner, T. (eds) Computer Vision – ECCV 2022. ECCV 2022. Lecture Notes in Computer Science, vol 13677. Springer, Cham. https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-031-19790-1_30
            </p>
            <h3 class="c-bibliographic-information__download-citation u-mb-8 u-mt-16 u-hide-print">
             Download citation
            </h3>
            <ul class="c-bibliographic-information__download-citation-list">
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-031-19790-1_30?format=refman&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .RIS file">
               .RIS
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-031-19790-1_30?format=endnote&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .ENW file">
               .ENW
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
             <li class="c-bibliographic-information__download-citation-item">
              <a data-test="citation-link" data-track="click" data-track-action="download chapter citation" data-track-external="" data-track-label="link" href="https://citation-needed-springer-com.proxy.lib.ohio-state.edu/v2/references/10.1007/978-3-031-19790-1_30?format=bibtex&amp;flavour=citation" rel="nofollow" title="Download this article's citation as a .BIB file">
               .BIB
               <svg aria-hidden="true" class="u-icon" focusable="false" height="16" role="img" width="16">
                <use xlink:href="#icon-download" xmlns:xlink="http://www.w3.org/1999/xlink">
                </use>
               </svg>
              </a>
             </li>
            </ul>
            <ul class="c-bibliographic-information__list u-mb-24" data-test="publication-history">
             <li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--chapter-doi">
              <p data-test="bibliographic-information__doi">
               <abbr title="Digital Object Identifier">
                DOI
               </abbr>
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                https://doi.org/10.1007/978-3-031-19790-1_30
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p>
               Published
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                <time datetime="2022-10-24">
                 24 October 2022
                </time>
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__publisher-name">
               Publisher Name
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                Springer, Cham
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__pisbn">
               Print ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-031-19789-5
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__eisbn">
               Online ISBN
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__value">
                978-3-031-19790-1
               </span>
              </p>
             </li>
             <li class="c-bibliographic-information__list-item">
              <p data-test="bibliographic-information__package">
               eBook Packages
               <span class="u-hide">
                :
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=11645&amp;facet-start-year=2022&amp;facet-end-year=2022">
                 Computer Science
                </a>
               </span>
               <span class="c-bibliographic-information__multi-value">
                <a href="/search?facet-content-type=%22Book%22&amp;package=43710&amp;facet-start-year=2022&amp;facet-end-year=2022">
                 Computer Science (R0)
                </a>
               </span>
              </p>
             </li>
            </ul>
            <div data-component="share-box">
             <div class="c-article-share-box u-display-block">
              <h3 class="c-article__sub-heading">
               Share this paper
              </h3>
              <p class="c-article-share-box__description">
               Anyone you share the following link with will be able to read this content:
              </p>
              <button class="js-get-share-url c-article-share-box__button" data-track="click" data-track-action="get shareable link" data-track-external="" data-track-label="button" id="get-share-url">
               Get shareable link
              </button>
              <div class="js-no-share-url-container u-display-none" hidden="">
               <p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">
                Sorry, a shareable link is not currently available for this article.
               </p>
              </div>
              <div class="js-share-url-container u-display-none" hidden="">
               <p class="js-share-url c-article-share-box__only-read-input" data-track="click" data-track-action="select share url" data-track-label="button" id="share-url">
               </p>
               <button class="js-copy-share-url c-article-share-box__button--link-like" data-track="click" data-track-action="copy share url" data-track-external="" data-track-label="button" id="copy-share-url">
                Copy to clipboard
               </button>
              </div>
              <p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
               Provided by the Springer Nature SharedIt content-sharing initiative
              </p>
             </div>
            </div>
            <div data-component="chapter-info-list">
            </div>
           </div>
          </div>
         </div>
        </div>
       </section>
      </div>
     </article>
    </main>
    <div class="c-article-extras u-text-sm u-hide-print" data-container-type="reading-companion" data-track-component="conference paper" id="sidebar">
     <aside>
      <div class="js-context-bar-sticky-point-desktop" data-test="download-article-link-wrapper">
       <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-book-pdf="true" data-test="pdf-link" data-track="click" data-track-action="Book download - pdf" data-track-label="link" download="" href="/content/pdf/10.1007/978-3-031-19790-1.pdf?pdf=button" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book PDF
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
        <div class="c-pdf-download u-clear-both">
         <a class="u-button u-button--full-width u-justify-content-space-between c-pdf-download__link" data-book-epub="true" data-test="epub-link" data-track="click" data-track-action="Book download - ePub" data-track-label="link" download="" href="/download/epub/10.1007/978-3-031-19790-1.epub" rel="noopener">
          <span class="c-pdf-download__text">
           <span class="u-sticky-visually-hidden">
            Download
           </span>
           book EPUB
          </span>
          <svg aria-hidden="true" class="u-icon" focusable="false" height="16" width="16">
           <use xlink:href="#icon-download">
           </use>
          </svg>
         </a>
        </div>
       </div>
      </div>
      <div data-test="editorial-summary">
      </div>
      <div class="c-reading-companion">
       <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky" style="top: 40px;">
        <ul class="c-reading-companion__tabs" role="tablist">
         <li role="presentation">
          <button aria-controls="tabpanel-sections" aria-selected="true" class="c-reading-companion__tab c-reading-companion__tab--active" data-tab-target="sections" data-track="click" data-track-action="sections tab" data-track-label="tab" id="tab-sections" role="tab">
           Sections
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-figures" aria-selected="false" class="c-reading-companion__tab" data-tab-target="figures" data-track="click" data-track-action="figures tab" data-track-label="tab" id="tab-figures" role="tab" tabindex="-1">
           Figures
          </button>
         </li>
         <li role="presentation">
          <button aria-controls="tabpanel-references" aria-selected="false" class="c-reading-companion__tab" data-tab-target="references" data-track="click" data-track-action="references tab" data-track-label="tab" id="tab-references" role="tab" tabindex="-1">
           References
          </button>
         </li>
        </ul>
        <div aria-labelledby="tab-sections" class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections" role="tabpanel">
         <div class="c-reading-companion__scroll-pane" style="max-height: 4544px;">
          <ul class="c-reading-companion__sections-list">
           <li class="c-reading-companion__section-item c-reading-companion__section-item--active" id="rc-sec-Abs1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Abstract" href="#Abs1">
             <span class="c-article-section__title-number">
             </span>
             Abstract
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Introduction" href="#Sec1">
             <span class="c-article-section__title-number">
              1
             </span>
             Introduction
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec2">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Related Work" href="#Sec2">
             <span class="c-article-section__title-number">
              2
             </span>
             Related Work
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec3">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Method" href="#Sec3">
             <span class="c-article-section__title-number">
              3
             </span>
             Method
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec4">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Experimental Results" href="#Sec4">
             <span class="c-article-section__title-number">
              4
             </span>
             Experimental Results
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec5">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Applications" href="#Sec5">
             <span class="c-article-section__title-number">
              5
             </span>
             Applications
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec10">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Limitations" href="#Sec10">
             <span class="c-article-section__title-number">
              6
             </span>
             Limitations
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Sec11">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Conclusion" href="#Sec11">
             <span class="c-article-section__title-number">
              7
             </span>
             Conclusion
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-notes">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Notes" href="#notes">
             <span class="c-article-section__title-number">
             </span>
             Notes
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Bib1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: References" href="#Bib1">
             <span class="c-article-section__title-number">
             </span>
             References
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-Ack1">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Acknowledgements" href="#Ack1">
             <span class="c-article-section__title-number">
             </span>
             Acknowledgements
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-author-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Author information" href="#author-information">
             <span class="c-article-section__title-number">
             </span>
             Author information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-editor-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Editor information" href="#editor-information">
             <span class="c-article-section__title-number">
             </span>
             Editor information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-rightslink">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Rights and permissions" href="#rightslink">
             <span class="c-article-section__title-number">
             </span>
             Rights and permissions
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-copyright-information">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: Copyright information" href="#copyright-information">
             <span class="c-article-section__title-number">
             </span>
             Copyright information
            </a>
           </li>
           <li class="c-reading-companion__section-item" id="rc-sec-chapter-info">
            <a data-track="click" data-track-action="section anchor" data-track-label="link: About this paper" href="#chapter-info">
             <span class="c-article-section__title-number">
             </span>
             About this paper
            </a>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-figures" class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ul class="c-reading-companion__figures-list">
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig1">
               Fig. 1.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig1_HTML.png?"/>
              <img alt="figure 1" aria-describedby="rc-Fig1" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig1_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig1">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30/figures/1" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig2">
               Fig. 2.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig2_HTML.png?"/>
              <img alt="figure 2" aria-describedby="rc-Fig2" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig2_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig2">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30/figures/2" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig3">
               Fig. 3.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig3_HTML.png?"/>
              <img alt="figure 3" aria-describedby="rc-Fig3" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig3_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig3">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30/figures/3" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig4">
               Fig. 4.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig4_HTML.png?"/>
              <img alt="figure 4" aria-describedby="rc-Fig4" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig4_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig4">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30/figures/4" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig5">
               Fig. 5.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig5_HTML.png?"/>
              <img alt="figure 5" aria-describedby="rc-Fig5" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig5_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig5">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30/figures/5" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig6">
               Fig. 6.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig6_HTML.png?"/>
              <img alt="figure 6" aria-describedby="rc-Fig6" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig6_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig6">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30/figures/6" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
           <li class="c-reading-companion__figure-item">
            <figure>
             <figcaption>
              <b class="c-reading-companion__figure-title u-h4" id="rc-Fig7">
               Fig. 7.
              </b>
             </figcaption>
             <picture>
              <source data-srcset="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig7_HTML.png?"/>
              <img alt="figure 7" aria-describedby="rc-Fig7" data-src="https://media-springernature-com.proxy.lib.ohio-state.edu/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-19790-1_30/MediaObjects/539949_1_En_30_Fig7_HTML.png"/>
             </picture>
             <p class="c-reading-companion__figure-links">
              <a data-track="click" data-track-action="figure anchor" data-track-label="link" href="#Fig7">
               View in article
              </a>
              <a class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/chapter/10.1007/978-3-031-19790-1_30/figures/7" rel="nofollow">
               Full size image
               <svg class="u-icon" height="16" width="16">
                <use href="#icon-chevron-right">
                </use>
               </svg>
              </a>
             </p>
            </figure>
           </li>
          </ul>
         </div>
        </div>
        <div aria-labelledby="tab-references" class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references" role="tabpanel">
         <div class="c-reading-companion__scroll-pane">
          <ol class="c-reading-companion__references-list c-reading-companion__references-list--numeric">
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR1">
             Aberman, K., Weng, Y., Lischinski, D., Cohen-Or, D., Chen, B.: Unpaired motion style transfer from video to animation. ACM Trans. Graph. (TOG)
             <b>
              39
             </b>
             (4), 1–64 (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/3386569.3392469" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3386569.3392469">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Unpaired%20motion%20style%20transfer%20from%20video%20to%20animation&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=39&amp;issue=4&amp;pages=1-64&amp;publication_year=2020&amp;author=Aberman%2CK&amp;author=Weng%2CY&amp;author=Lischinski%2CD&amp;author=Cohen-Or%2CD&amp;author=Chen%2CB">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR2">
             Aigner, S., Körner, M.: Futuregan: anticipating the future frames of video sequences using spatio-temporal 3d convolutions in progressively growing gans. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1810.01325" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1810.01325">
              arXiv:1810.01325
             </a>
             (2018)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR3">
             Aksan, E., Hilliges, O.: Stcn: stochastic temporal convolutional networks. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1902.06568" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1902.06568">
              arXiv:1902.06568
             </a>
             (2019)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR4">
             Apostolidis, E., Adamantidou, E., Metsai, A.I., Mezaris, V., Patras, I.: Video summarization using deep neural networks: a survey. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2101.06072" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2101.06072">
              arXiv:2101.06072
             </a>
             (2021)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR5">
             Arora, R., Lee, Y.J.: Singan-gif: learning a generative video model from a single gif. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1310–1319 (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Arora%2C%20R.%2C%20Lee%2C%20Y.J.%3A%20Singan-gif%3A%20learning%20a%20generative%20video%20model%20from%20a%20single%20gif.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Winter%20Conference%20on%20Applications%20of%20Computer%20Vision%2C%20pp.%201310%E2%80%931319%20%282021%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR6">
             Babaeizadeh, M., Finn, C., Erhan, D., Campbell, R.H., Levine, S.: Stochastic variational video prediction. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.11252" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.11252">
              arXiv:1710.11252
             </a>
             (2017)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR7">
             Ballas, N., Yao, L., Pal, C., Courville, A.: Delving deeper into convolutional networks for learning video representations. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1511.06432" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1511.06432">
              arXiv:1511.06432
             </a>
             (2015)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR8">
             Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: unsupervised video retargeting. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 119–135 (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bansal%2C%20A.%2C%20Ma%2C%20S.%2C%20Ramanan%2C%20D.%2C%20Sheikh%2C%20Y.%3A%20Recycle-gan%3A%20unsupervised%20video%20retargeting.%20In%3A%20Proceedings%20of%20the%20European%20Conference%20on%20Computer%20Vision%20%28ECCV%29%2C%20pp.%20119%E2%80%93135%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR9">
             Barnes, C., Zhang, F.-L.: A survey of the state-of-the-art in patch-based synthesis. Comput. Vis. Media
             <b>
              3
             </b>
             (1), 3–20 (2017).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/s41095-016-0064-2" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/s41095-016-0064-2">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/s41095-016-0064-2
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/s41095-016-0064-2" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2Fs41095-016-0064-2">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20the%20state-of-the-art%20in%20patch-based%20synthesis&amp;journal=Comput.%20Vis.%20Media&amp;doi=10.1007%2Fs41095-016-0064-2&amp;volume=3&amp;issue=1&amp;pages=3-20&amp;publication_year=2017&amp;author=Barnes%2CC&amp;author=Zhang%2CF-L">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR10">
             Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. Graph.
             <b>
              28
             </b>
             (3), 24 (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/1531326.1531330" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F1531326.1531330">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Patchmatch%3A%20a%20randomized%20correspondence%20algorithm%20for%20structural%20image%20editing&amp;journal=ACM%20Trans.%20Graph.&amp;volume=28&amp;issue=3&amp;publication_year=2009&amp;author=Barnes%2CC&amp;author=Shechtman%2CE&amp;author=Finkelstein%2CA&amp;author=Goldman%2CDB">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR11">
             Barnes, C., Shechtman, E., Goldman, D.B., Finkelstein, A.: The generalized patchmatch correspondence algorithm. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6313, pp. 29–43. Springer, Heidelberg (2010).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-642-15558-1_3" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-15558-1_3">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-642-15558-1_3
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-642-15558-1_3" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-642-15558-1_3">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=The%20generalized%20patchmatch%20correspondence%20algorithm&amp;pages=29-43&amp;publication_year=2010%202010%202010&amp;author=Barnes%2CC&amp;author=Shechtman%2CE&amp;author=Goldman%2CDB&amp;author=Finkelstein%2CA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR12">
             Barnes, C., Zhang, F.-L., Lou, L., Xian, W., Shi-Min, H.: Patchtable: efficient patch queries for large datasets and applications. ACM Trans. Graph. (ToG)
             <b>
              34
             </b>
             (4), 1–10 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/2766934" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2766934">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Patchtable%3A%20efficient%20patch%20queries%20for%20large%20datasets%20and%20applications&amp;journal=ACM%20Trans.%20Graph.%20%28ToG%29&amp;volume=34&amp;issue=4&amp;pages=1-10&amp;publication_year=2015&amp;author=Barnes%2CC&amp;author=Zhang%2CF-L&amp;author=Lou%2CL&amp;author=Xian%2CW&amp;author=Shi-Min%2CH">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR13">
             Benaim, S., Mokady, R., Bermano, A., Wolf, L.: Structural analogy from a single image pair. In: Computer Graphics Forum, vol. 40, pp. 249–265. Wiley Online Library (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Benaim%2C%20S.%2C%20Mokady%2C%20R.%2C%20Bermano%2C%20A.%2C%20Wolf%2C%20L.%3A%20Structural%20analogy%20from%20a%20single%20image%20pair.%20In%3A%20Computer%20Graphics%20Forum%2C%20vol.%2040%2C%20pp.%20249%E2%80%93265.%20Wiley%20Online%20Library%20%282021%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR14">
             Bhat, K.S., Seitz, S.M., Hodgins, J.K., Khosla, P.K.: Flow-based video synthesis and editing. In: ACM SIGGRAPH 2004 Papers, pp. 360–363 (2004)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Bhat%2C%20K.S.%2C%20Seitz%2C%20S.M.%2C%20Hodgins%2C%20J.K.%2C%20Khosla%2C%20P.K.%3A%20Flow-based%20video%20synthesis%20and%20editing.%20In%3A%20ACM%20SIGGRAPH%202004%20Papers%2C%20pp.%20360%E2%80%93363%20%282004%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR15">
             Blattmann, A., Milbich, T., Dorkenwald, M., Ommer, B.: ipoke: poking a still image for controlled stochastic video synthesis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14707–14717 (2021)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Blattmann%2C%20A.%2C%20Milbich%2C%20T.%2C%20Dorkenwald%2C%20M.%2C%20Ommer%2C%20B.%3A%20ipoke%3A%20poking%20a%20still%20image%20for%20controlled%20stochastic%20video%20synthesis.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%2014707%E2%80%9314717%20%282021%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR16">
             Chan, C., Ginosar, S., Zhou, T., Efros, A.A.: Everybody dance now. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5933–5942 (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Chan%2C%20C.%2C%20Ginosar%2C%20S.%2C%20Zhou%2C%20T.%2C%20Efros%2C%20A.A.%3A%20Everybody%20dance%20now.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%205933%E2%80%935942%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR17">
             Denton, E., Fergus, R.: Stochastic video generation with a learned prior. In: International Conference on Machine Learning, pp. 1174–1183. PMLR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Denton%2C%20E.%2C%20Fergus%2C%20R.%3A%20Stochastic%20video%20generation%20with%20a%20learned%20prior.%20In%3A%20International%20Conference%20on%20Machine%20Learning%2C%20pp.%201174%E2%80%931183.%20PMLR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR18">
             Duggal, S., Wang, S., Ma, W. C., Hu, R., Urtasun, R.: Deeppruner: learning efficient stereo matching via differentiable patchmatch. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4384–4393 (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Duggal%2C%20S.%2C%20Wang%2C%20S.%2C%20Ma%2C%20W.%20C.%2C%20Hu%2C%20R.%2C%20Urtasun%2C%20R.%3A%20Deeppruner%3A%20learning%20efficient%20stereo%20matching%20via%20differentiable%20patchmatch.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%204384%E2%80%934393%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR19">
             Efros, A.A., Freeman, W.T.: Image quilting for texture synthesis and transfer. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 341–346 (2001)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Efros%2C%20A.A.%2C%20Freeman%2C%20W.T.%3A%20Image%20quilting%20for%20texture%20synthesis%20and%20transfer.%20In%3A%20Proceedings%20of%20the%2028th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20341%E2%80%93346%20%282001%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR20">
             Efros, A.A., Leung, T.K.: Texture synthesis by non-parametric sampling. In: Proceedings of the Seventh IEEE International Conference on Computer Vision, vol. 2, pp. 1033–1038. IEEE (1999)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Efros%2C%20A.A.%2C%20Leung%2C%20T.K.%3A%20Texture%20synthesis%20by%20non-parametric%20sampling.%20In%3A%20Proceedings%20of%20the%20Seventh%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20vol.%202%2C%20pp.%201033%E2%80%931038.%20IEEE%20%281999%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR21">
             Fišer, J., et al.: Stylit: illumination-guided example-based stylization of 3d renderings. ACM Trans. Graph. (TOG)
             <b>
              35
             </b>
             (4), 1–11 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/2897824.2925948" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2897824.2925948">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Stylit%3A%20illumination-guided%20example-based%20stylization%20of%203d%20renderings&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=35&amp;issue=4&amp;pages=1-11&amp;publication_year=2016&amp;author=Fi%C5%A1er%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR22">
             Franceschi, J.-Y., Delasalles, E., Chen, M., Lamprier, S., Gallinari, P.: Stochastic latent residual video prediction. In: International Conference on Machine Learning, pp. 3233–3246. PMLR (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Franceschi%2C%20J.-Y.%2C%20Delasalles%2C%20E.%2C%20Chen%2C%20M.%2C%20Lamprier%2C%20S.%2C%20Gallinari%2C%20P.%3A%20Stochastic%20latent%20residual%20video%20prediction.%20In%3A%20International%20Conference%20on%20Machine%20Learning%2C%20pp.%203233%E2%80%933246.%20PMLR%20%282020%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR23">
             Goodfellow, I.J., et al.: Generative adversarial networks. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1406.2661" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1406.2661">
              arXiv:1406.2661
             </a>
             (2014)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR24">
             Granot, N., Feinstein, B., Shocher, A., Bagon, S., Irani, M.: Drop the gan: in defense of patches nearest neighbors as single image generative models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13460–13469 (2022)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Granot%2C%20N.%2C%20Feinstein%2C%20B.%2C%20Shocher%2C%20A.%2C%20Bagon%2C%20S.%2C%20Irani%2C%20M.%3A%20Drop%20the%20gan%3A%20in%20defense%20of%20patches%20nearest%20neighbors%20as%20single%20image%20generative%20models.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%2013460%E2%80%9313469%20%282022%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR25">
             Gur, S., Benaim, S., Wolf, L.: Hierarchical patch vae-gan: generating diverse videos from a single sample. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2006.12226" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2006.12226">
              arXiv:2006.12226
             </a>
             (2020)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR26">
             Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analogies. In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, pp. 327–340 (2001)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Hertzmann%2C%20A.%2C%20Jacobs%2C%20C.E.%2C%20Oliver%2C%20N.%2C%20Curless%2C%20B.%2C%20Salesin%2C%20D.H.%3A%20Image%20analogies.%20In%3A%20Proceedings%20of%20the%2028th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20327%E2%80%93340%20%282001%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR27">
             Huang, J.-B., Kang, S.B., Ahuja, N., Kopf, J.: Temporally coherent completion of dynamic video. ACM Trans. Graph. (TOG)
             <b>
              35
             </b>
             (6), 1–11 (2016)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Temporally%20coherent%20completion%20of%20dynamic%20video&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=35&amp;issue=6&amp;pages=1-11&amp;publication_year=2016&amp;author=Huang%2CJ-B&amp;author=Kang%2CSB&amp;author=Ahuja%2CN&amp;author=Kopf%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR28">
             Jamriška, O., Fišer, J., Asente, P., Jingwan, L., Shechtman, E., Sỳkora, D.: Lazyfluids: appearance transfer for fluid animations. ACM Trans. Graph. (TOG)
             <b>
              34
             </b>
             (4), 1–10 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/2766983" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F2766983">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Lazyfluids%3A%20appearance%20transfer%20for%20fluid%20animations&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=34&amp;issue=4&amp;pages=1-10&amp;publication_year=2015&amp;author=Jamri%C5%A1ka%2CO&amp;author=Fi%C5%A1er%2CJ&amp;author=Asente%2CP&amp;author=Jingwan%2CL&amp;author=Shechtman%2CE&amp;author=S%E1%BB%B3kora%2CD">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR29">
             Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for improved quality, stability, and variation. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.10196" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1710.10196">
              arXiv:1710.10196
             </a>
             , 2017
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR30">
             Krähenbühl, P., Lang, M., Hornung, A., Gross, M.: A system for retargeting of streaming video. In: ACM SIGGRAPH Asia 2009 papers (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kr%C3%A4henb%C3%BChl%2C%20P.%2C%20Lang%2C%20M.%2C%20Hornung%2C%20A.%2C%20Gross%2C%20M.%3A%20A%20system%20for%20retargeting%20of%20streaming%20video.%20In%3A%20ACM%20SIGGRAPH%20Asia%202009%20papers%20%282009%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR31">
             Kwatra, V., Schödl, A., Essa, I., Turk, G., Bobick, A.: Graphcut textures: Image and video synthesis using graph cuts. Acm Trans. Graph. (ToG)
             <b>
              22
             </b>
             (3), 277–286 (2003)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/882262.882264" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F882262.882264">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Graphcut%20textures%3A%20Image%20and%20video%20synthesis%20using%20graph%20cuts&amp;journal=Acm%20Trans.%20Graph.%20%28ToG%29&amp;volume=22&amp;issue=3&amp;pages=277-286&amp;publication_year=2003&amp;author=Kwatra%2CV&amp;author=Sch%C3%B6dl%2CA&amp;author=Essa%2CI&amp;author=Turk%2CG&amp;author=Bobick%2CA">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR32">
             Kwatra, V., Essa, I., Bobick, A., Kwatra, N.: Texture optimization for example-based synthesis. In: ACM SIGGRAPH 2005 Papers, pp. 795–802 (2005)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Kwatra%2C%20V.%2C%20Essa%2C%20I.%2C%20Bobick%2C%20A.%2C%20Kwatra%2C%20N.%3A%20Texture%20optimization%20for%20example-based%20synthesis.%20In%3A%20ACM%20SIGGRAPH%202005%20Papers%2C%20pp.%20795%E2%80%93802%20%282005%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR33">
             Kwatra, V., Adalsteinsson, D., Kim, T., Kwatra, N., Carlson, M., Lin, M.: Texturing fluids. IEEE Trans. Visual Comput. Graph.
             <b>
              13
             </b>
             (5), 939–952 (2007)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1109/TVCG.2007.1044" href="https://doi-org.proxy.lib.ohio-state.edu/10.1109%2FTVCG.2007.1044">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Texturing%20fluids&amp;journal=IEEE%20Trans.%20Visual%20Comput.%20Graph.&amp;volume=13&amp;issue=5&amp;pages=939-952&amp;publication_year=2007&amp;author=Kwatra%2CV&amp;author=Adalsteinsson%2CD&amp;author=Kim%2CT&amp;author=Kwatra%2CN&amp;author=Carlson%2CM&amp;author=Lin%2CM">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR34">
             Le, T.T., Almansa, A., Gousseau, Y., Masnou, S.: Motion-consistent video inpainting. In: 2017 IEEE International Conference on Image Processing (ICIP), pp. 2094–2098. IEEE (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Le%2C%20T.T.%2C%20Almansa%2C%20A.%2C%20Gousseau%2C%20Y.%2C%20Masnou%2C%20S.%3A%20Motion-consistent%20video%20inpainting.%20In%3A%202017%20IEEE%20International%20Conference%20on%20Image%20Processing%20%28ICIP%29%2C%20pp.%202094%E2%80%932098.%20IEEE%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR35">
             LeCun, Y.: The mnist database of handwritten digits.
             <a data-track="click" data-track-action="external reference" data-track-label="http://yann.lecun.com/exdb/mnist/" href="http://yann.lecun.com/exdb/mnist/">
              http://yann.lecun.com/exdb/mnist/
             </a>
             (1998)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR36">
             Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., Levine, S.: Stochastic adversarial video prediction. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1804.01523" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1804.01523">
              arXiv:1804.01523
             </a>
             (2018)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR37">
             Lee, J., Ramanan, D., Girdhar, R.: Metapix: few-shot video retargeting. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1910.04742" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1910.04742">
              arXiv:1910.04742
             </a>
             (2019)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR38">
             Liao, J., Yao, Y., Yuan, L., Hua, G., Kang, S.B.: Visual attribute transfer through deep image analogy. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.01088" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1705.01088">
              arXiv:1705.01088
             </a>
             (2017)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR39">
             Liu, M., Chen, S., Liu, J., Tang, X.: Video completion via motion guided spatial-temporal global optimization. In: Proceedings of the 17th ACM International Conference on Multimedia, pp. 537–540 (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Liu%2C%20M.%2C%20Chen%2C%20S.%2C%20Liu%2C%20J.%2C%20Tang%2C%20X.%3A%20Video%20completion%20via%20motion%20guided%20spatial-temporal%20global%20optimization.%20In%3A%20Proceedings%20of%20the%2017th%20ACM%20International%20Conference%20on%20Multimedia%2C%20pp.%20537%E2%80%93540%20%282009%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR40">
             Mallya, A., Wang, T.-C., Sapra, K., Liu, M.-Y.: World-consistent video-to-video synthesis. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2007.08509" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2007.08509">
              arXiv:2007.08509
             </a>
             (2020)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR41">
             Newson, A., Almansa, A., Fradet, M., Gousseau, Y., Pérez, P.: Towards fast, generic video inpainting. In: Proceedings of the 10th European Conference on Visual Media Production, pp. 1–8 (2013)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Newson%2C%20A.%2C%20Almansa%2C%20A.%2C%20Fradet%2C%20M.%2C%20Gousseau%2C%20Y.%2C%20P%C3%A9rez%2C%20P.%3A%20Towards%20fast%2C%20generic%20video%20inpainting.%20In%3A%20Proceedings%20of%20the%2010th%20European%20Conference%20on%20Visual%20Media%20Production%2C%20pp.%201%E2%80%938%20%282013%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR42">
             Newson, A., Almansa, A., Fradet, M., Gousseau, Y., Pérez, P.: Video inpainting of complex scenes. SIAM J. Imag. Sci.
             <b>
              7
             </b>
             (4), 1993–2019 (2014)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1137/140954933" href="https://doi-org.proxy.lib.ohio-state.edu/10.1137%2F140954933">
              CrossRef
             </a>
             <a data-track="click" data-track-action="mathscinet reference" data-track-label="link" href="http://www-ams-org.proxy.lib.ohio-state.edu/mathscinet-getitem?mr=3268608">
              MathSciNet
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Video%20inpainting%20of%20complex%20scenes&amp;journal=SIAM%20J.%20Imag.%20Sci.&amp;volume=7&amp;issue=4&amp;pages=1993-2019&amp;publication_year=2014&amp;author=Newson%2CA&amp;author=Almansa%2CA&amp;author=Fradet%2CM&amp;author=Gousseau%2CY&amp;author=P%C3%A9rez%2CP">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR43">
             Okabe, M., Anjyo, K., Igarashi, T., Seidel, H.P.: Animating pictures of fluid using video examples. In: Computer Graphics Forum, vol. 28, pp. 677–686. Wiley Online Library (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Okabe%2C%20M.%2C%20Anjyo%2C%20K.%2C%20Igarashi%2C%20T.%2C%20Seidel%2C%20H.P.%3A%20Animating%20pictures%20of%20fluid%20using%20video%20examples.%20In%3A%20Computer%20Graphics%20Forum%2C%20vol.%2028%2C%20pp.%20677%E2%80%93686.%20Wiley%20Online%20Library%20%282009%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR44">
             Paszke, A., et al.: Pytorch: an imperative style, high-performance deep learning library. In: Wallach, H., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates Inc (2019).
             <a data-track="click" data-track-action="external reference" data-track-label="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">
              http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR45">
             Rav-Acha, A., Pritch, Y., Peleg, S.: Making a long video short: dynamic video synopsis. In: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), vol. 1, pp. 435–441. IEEE (2006)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Rav-Acha%2C%20A.%2C%20Pritch%2C%20Y.%2C%20Peleg%2C%20S.%3A%20Making%20a%20long%20video%20short%3A%20dynamic%20video%20synopsis.%20In%3A%202006%20IEEE%20Computer%20Society%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%E2%80%9906%29%2C%20vol.%201%2C%20pp.%20435%E2%80%93441.%20IEEE%20%282006%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR46">
             Ren, J., Chai, M., Tulyakov, S., Fang, C., Shen, X., Yang, J.: Human motion transfer from poses in the wild. In: Bartoli, A., Fusiello, A. (eds.) ECCV 2020. LNCS, vol. 12537, pp. 262–279. Springer, Cham (2020).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-030-67070-2_16" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-67070-2_16">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-67070-2_16
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-030-67070-2_16" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-030-67070-2_16">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Human%20motion%20transfer%20from%20poses%20in%20the%20wild&amp;pages=262-279&amp;publication_year=2020%202020%202020&amp;author=Ren%2CJ&amp;author=Chai%2CM&amp;author=Tulyakov%2CS&amp;author=Fang%2CC&amp;author=Shen%2CX&amp;author=Yang%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR47">
             Rubinstein, M., Shamir, A., Avidan, S.: Improved seam carving for video retargeting. ACM Trans. Graph. (TOG)
             <b>
              27
             </b>
             (3), 1–9 (2008)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/1360612.1360615" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F1360612.1360615">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Improved%20seam%20carving%20for%20video%20retargeting&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=27&amp;issue=3&amp;pages=1-9&amp;publication_year=2008&amp;author=Rubinstein%2CM&amp;author=Shamir%2CA&amp;author=Avidan%2CS">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR48">
             Saito, M., Matsumoto, E., Saito, S.: Temporal generative adversarial nets with singular value clipping. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2830–2839 (2017)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Saito%2C%20M.%2C%20Matsumoto%2C%20E.%2C%20Saito%2C%20S.%3A%20Temporal%20generative%20adversarial%20nets%20with%20singular%20value%20clipping.%20In%3A%20Proceedings%20of%20the%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%202830%E2%80%932839%20%282017%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR49">
             Sato, S., Dobashi, Y., Kim, T., Nishita, T.: Example-based turbulence style transfer. ACM Trans. Graph. (TOG)
             <b>
              37
             </b>
             (4), 1–9 (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/3197517.3201398" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3197517.3201398">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Example-based%20turbulence%20style%20transfer&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=37&amp;issue=4&amp;pages=1-9&amp;publication_year=2018&amp;author=Sato%2CS&amp;author=Dobashi%2CY&amp;author=Kim%2CT&amp;author=Nishita%2CT">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR50">
             Sato, S., Dobashi, Y., Nishita, T.: Editing fluid animation using flow interpolation. ACM Trans. Graph. (TOG)
             <b>
              37
             </b>
             (5), 1–12 (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1145/3213771" href="https://doi-org.proxy.lib.ohio-state.edu/10.1145%2F3213771">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=Editing%20fluid%20animation%20using%20flow%20interpolation&amp;journal=ACM%20Trans.%20Graph.%20%28TOG%29&amp;volume=37&amp;issue=5&amp;pages=1-12&amp;publication_year=2018&amp;author=Sato%2CS&amp;author=Dobashi%2CY&amp;author=Nishita%2CT">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR51">
             Schödl, A., Szeliski, R., Salesin, D.H., Essa, I.: Video textures. In: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, pp. 489–498 (2000)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Sch%C3%B6dl%2C%20A.%2C%20Szeliski%2C%20R.%2C%20Salesin%2C%20D.H.%2C%20Essa%2C%20I.%3A%20Video%20textures.%20In%3A%20Proceedings%20of%20the%2027th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20489%E2%80%93498%20%282000%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR52">
             Shaham, T.R., Dekel, T., Michaeli, T.: Singan: learning a generative model from a single natural image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4570–4580 (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Shaham%2C%20T.R.%2C%20Dekel%2C%20T.%2C%20Michaeli%2C%20T.%3A%20Singan%3A%20learning%20a%20generative%20model%20from%20a%20single%20natural%20image.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%204570%E2%80%934580%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR53">
             Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., Sebe, N.: Animating arbitrary objects via deep motion transfer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2377–2386 (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Siarohin%2C%20A.%2C%20Lathuili%C3%A8re%2C%20S.%2C%20Tulyakov%2C%20S.%2C%20Ricci%2C%20E.%2C%20Sebe%2C%20N.%3A%20Animating%20arbitrary%20objects%20via%20deep%20motion%20transfer.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%202377%E2%80%932386%20%282019%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR54">
             Siarohin, A., Lathuilière, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion model for image animation. Adv. Neural. Inf. Process. Syst.
             <b>
              32
             </b>
             , 7137–7147 (2019)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=First%20order%20motion%20model%20for%20image%20animation&amp;journal=Adv.%20Neural.%20Inf.%20Process.%20Syst.&amp;volume=32&amp;pages=7137-7147&amp;publication_year=2019&amp;author=Siarohin%2CA&amp;author=Lathuili%C3%A8re%2CS&amp;author=Tulyakov%2CS&amp;author=Ricci%2CE&amp;author=Sebe%2CN">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR55">
             Simakov, D., Caspi, Y., Shechtman, E., Irani, M.: Summarizing visual data using bidirectional similarity. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8. IEEE (2008)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Simakov%2C%20D.%2C%20Caspi%2C%20Y.%2C%20Shechtman%2C%20E.%2C%20Irani%2C%20M.%3A%20Summarizing%20visual%20data%20using%20bidirectional%20similarity.%20In%3A%202008%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%201%E2%80%938.%20IEEE%20%282008%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR56">
             Teed, Z., Deng, J.: RAFT: recurrent all-pairs field transforms for optical flow. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12347, pp. 402–419. Springer, Cham (2020).
             <a data-track="click" data-track-action="external reference" data-track-label="10.1007/978-3-030-58536-5_24" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58536-5_24">
              https://doi-org.proxy.lib.ohio-state.edu/10.1007/978-3-030-58536-5_24
             </a>
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="crossref reference" data-track-label="10.1007/978-3-030-58536-5_24" href="https://doi-org.proxy.lib.ohio-state.edu/10.1007%2F978-3-030-58536-5_24">
              CrossRef
             </a>
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar_lookup?&amp;title=RAFT%3A%20recurrent%20all-pairs%20field%20transforms%20for%20optical%20flow&amp;pages=402-419&amp;publication_year=2020%202020%202020&amp;author=Teed%2CZ&amp;author=Deng%2CJ">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR57">
             Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotemporal features with 3d convolutional networks. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 4489–4497 (2015)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tran%2C%20D.%2C%20Bourdev%2C%20L.%2C%20Fergus%2C%20R.%2C%20Torresani%2C%20L.%2C%20Paluri%2C%20M.%3A%20Learning%20spatiotemporal%20features%20with%203d%20convolutional%20networks.%20In%3A%20Proceedings%20of%20the%20IEEE%20International%20Conference%20on%20Computer%20Vision%2C%20pp.%204489%E2%80%934497%20%282015%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR58">
             Tulyakov, S., Liu, M. Y., Yang, X., Kautz, J.: Mocogan: decomposing motion and content for video generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1526–1535 (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Tulyakov%2C%20S.%2C%20Liu%2C%20M.%20Y.%2C%20Yang%2C%20X.%2C%20Kautz%2C%20J.%3A%20Mocogan%3A%20decomposing%20motion%20and%20content%20for%20video%20generation.%20In%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%201526%E2%80%931535%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR59">
             Villegas, R., Yang, J., Hong, S., Lin, X., Lee, H.: Decomposing motion and content for natural video sequence prediction. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1706.08033" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1706.08033">
              arXiv:1706.08033
             </a>
             (2017)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR60">
             Villegas, R., Erhan, D., Lee, H., et al.: Hierarchical long-term video prediction without supervision. In: International Conference on Machine Learning, pp. 6038–6046. PMLR (2018)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Villegas%2C%20R.%2C%20Erhan%2C%20D.%2C%20Lee%2C%20H.%2C%20et%20al.%3A%20Hierarchical%20long-term%20video%20prediction%20without%20supervision.%20In%3A%20International%20Conference%20on%20Machine%20Learning%2C%20pp.%206038%E2%80%936046.%20PMLR%20%282018%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR61">
             Villegas, R., Pathak, A., Kannan, H., Erhan, D., Le, Q.V., Lee, H.: High fidelity video prediction with large stochastic recurrent neural networks. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1911.01655" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1911.01655">
              arXiv:1911.01655
             </a>
             (2019)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR62">
             Vondrick, C., Pirsiavash, H., Torralba, A.: Generating videos with scene dynamics. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1609.02612" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1609.02612">
              arXiv:1609.02612
             </a>
             (2016)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR63">
             Wang, T.-C., et al.: Video-to-video synthesis. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1808.06601" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1808.06601">
              arXiv:1808.06601
             </a>
             (2018)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR64">
             Wang, T.-C., Liu, M.-Y., Tao, A., Liu, G., Kautz, J., Catanzaro, B.: Few-shot video-to-video synthesis. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1910.12713" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/1910.12713">
              arXiv:1910.12713
             </a>
             (2019)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR65">
             Wang, Y., Bilinski, P., Bremond, F., Dantcheva, A: Imaginator: conditional spatio-temporal gan for video generation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1160–1169 (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wang%2C%20Y.%2C%20Bilinski%2C%20P.%2C%20Bremond%2C%20F.%2C%20Dantcheva%2C%20A%3A%20Imaginator%3A%20conditional%20spatio-temporal%20gan%20for%20video%20generation.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Winter%20Conference%20on%20Applications%20of%20Computer%20Vision%2C%20pp.%201160%E2%80%931169%20%282020%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR66">
             Wang, Y., Bremond, F., Dantcheva, A.: Inmodegan: interpretable motion decomposition generative adversarial network for video generation. arXiv preprint
             <a data-track="click" data-track-action="external reference" data-track-label="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2101.03049" href="http://arxiv-org.proxy.lib.ohio-state.edu/abs/2101.03049">
              arXiv:2101.03049
             </a>
             (2021)
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR67">
             Wei, L.-Y., Levoy, M.: Fast texture synthesis using tree-structured vector quantization. In: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, pp. 479–488 (2000)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wei%2C%20L.-Y.%2C%20Levoy%2C%20M.%3A%20Fast%20texture%20synthesis%20using%20tree-structured%20vector%20quantization.%20In%3A%20Proceedings%20of%20the%2027th%20Annual%20Conference%20on%20Computer%20Graphics%20and%20Interactive%20Techniques%2C%20pp.%20479%E2%80%93488%20%282000%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR68">
             Wei, L. Y., Lefebvre, S., Kwatra, V., Turk, G.: State of the art in example-based texture synthesis. In: Eurographics 2009, State of the Art Report, EG-STAR, pp. 93–117. Eurographics Association (2009)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wei%2C%20L.%20Y.%2C%20Lefebvre%2C%20S.%2C%20Kwatra%2C%20V.%2C%20Turk%2C%20G.%3A%20State%20of%20the%20art%20in%20example-based%20texture%20synthesis.%20In%3A%20Eurographics%202009%2C%20State%20of%20the%20Art%20Report%2C%20EG-STAR%2C%20pp.%2093%E2%80%93117.%20Eurographics%20Association%20%282009%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR69">
             Wexler, Y., Shechtman, E., Irani, M.: Space-time video completion. In: Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004, vol. 1, pp. I-I. IEEE (2004)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wexler%2C%20Y.%2C%20Shechtman%2C%20E.%2C%20Irani%2C%20M.%3A%20Space-time%20video%20completion.%20In%3A%20Proceedings%20of%20the%202004%20IEEE%20Computer%20Society%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%202004.%20CVPR%202004%2C%20vol.%201%2C%20pp.%20I-I.%20IEEE%20%282004%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR70">
             Wolf, L., Guttmann, M., Cohen-Or, D.: Non-homogeneous content-driven video-retargeting. In: Proceedings of the Eleventh IEEE International Conference on Computer Vision (ICCV) (2007)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Wolf%2C%20L.%2C%20Guttmann%2C%20M.%2C%20Cohen-Or%2C%20D.%3A%20Non-homogeneous%20content-driven%20video-retargeting.%20In%3A%20Proceedings%20of%20the%20Eleventh%20IEEE%20International%20Conference%20on%20Computer%20Vision%20%28ICCV%29%20%282007%29">
              Google Scholar
             </a>
            </p>
           </li>
           <li class="c-reading-companion__reference-item">
            <p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR71">
             Yang, Z., et al.: Transmomo: invariance-driven unsupervised video motion retargeting. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5306–5315 (2020)
            </p>
            <p class="c-reading-companion__reference-links">
             <a data-track="click" data-track-action="google scholar reference" data-track-label="link" href="https://scholar.google.com/scholar?&amp;q=Yang%2C%20Z.%2C%20et%20al.%3A%20Transmomo%3A%20invariance-driven%20unsupervised%20video%20motion%20retargeting.%20In%3A%20Proceedings%20of%20the%20IEEE%2FCVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%20pp.%205306%E2%80%935315%20%282020%29">
              Google Scholar
             </a>
            </p>
           </li>
          </ol>
         </div>
        </div>
       </div>
      </div>
     </aside>
    </div>
   </div>
   <div class="app-elements">
    <footer data-test="universal-footer">
     <div class="c-footer" data-track-component="unified-footer">
      <div class="c-footer__container">
       <div class="c-footer__grid c-footer__group--separator">
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Discover content
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="journals a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/journals/a/1">
            Journals A-Z
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="books a-z" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/books/a/1">
            Books A-Z
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Publish with us
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="publish your research" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/authors">
            Publish your research
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="open access publishing" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/open-research/about/the-fundamentals-of-open-access-and-open-research">
            Open access publishing
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Products and services
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="our products" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/products">
            Our products
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="librarians" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/librarians">
            Librarians
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="societies" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/societies">
            Societies
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="partners and advertisers" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/partners">
            Partners and advertisers
           </a>
          </li>
         </ul>
        </div>
        <div class="c-footer__group">
         <h3 class="c-footer__heading">
          Our imprints
         </h3>
         <ul class="c-footer__list">
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Springer" data-track-label="link" href="https://www-springer-com.proxy.lib.ohio-state.edu/">
            Springer
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Nature Portfolio" data-track-label="link" href="https://www-nature-com.proxy.lib.ohio-state.edu/">
            Nature Portfolio
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="BMC" data-track-label="link" href="https://www-biomedcentral-com.proxy.lib.ohio-state.edu/">
            BMC
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Palgrave Macmillan" data-track-label="link" href="https://www.palgrave.com/">
            Palgrave Macmillan
           </a>
          </li>
          <li class="c-footer__item">
           <a class="c-footer__link" data-track="click" data-track-action="Apress" data-track-label="link" href="https://www.apress.com/">
            Apress
           </a>
          </li>
         </ul>
        </div>
       </div>
      </div>
      <div class="c-footer__container">
       <nav aria-label="footer navigation">
        <ul class="c-footer__links">
         <li class="c-footer__item">
          <button class="c-footer__link" data-cc-action="preferences" data-track="click" data-track-action="Manage cookies" data-track-label="link">
           <span class="c-footer__button-text">
            Your privacy choices/Manage cookies
           </span>
          </button>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="california privacy statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/legal/ccpa">
           Your US state privacy rights
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="accessibility statement" data-track-label="link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/gp/info/accessibility">
           Accessibility statement
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="terms and conditions" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/termsandconditions">
           Terms and conditions
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="privacy policy" data-track-label="link" href="https://link-springer-com.proxy.lib.ohio-state.edu/privacystatement">
           Privacy policy
          </a>
         </li>
         <li class="c-footer__item">
          <a class="c-footer__link" data-track="click" data-track-action="help and support" data-track-label="link" href="https://support-springernature-com.proxy.lib.ohio-state.edu/en/support/home">
           Help and support
          </a>
         </li>
        </ul>
       </nav>
       <div class="c-footer__user">
        <p class="c-footer__user-info">
         <span data-test="footer-user-ip">
          3.128.143.42
         </span>
        </p>
        <p class="c-footer__user-info" data-test="footer-business-partners">
         OhioLINK Consortium (3000266689)  - Ohio State University Libraries (8200724141)
        </p>
       </div>
       <a class="c-footer__link" href="https://www-springernature-com.proxy.lib.ohio-state.edu/">
        <img alt="Springer Nature" height="20" loading="lazy" src="/oscar-static/images/darwin/footer/img/logo-springernature_white-64dbfad7d8.svg" width="200"/>
       </a>
       <p class="c-footer__legal" data-test="copyright">
        © 2023 Springer Nature
       </p>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <div aria-hidden="true" class="u-visually-hidden">
   <!--?xml version="1.0" encoding="UTF-8"?-->
   <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <defs>
     <path d="M0 .74h56.72v55.24H0z" id="a">
     </path>
    </defs>
    <symbol id="icon-access" viewbox="0 0 18 18">
     <path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-account" viewbox="0 0 18 18">
     <path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-alert" viewbox="0 0 18 18">
     <path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-broad" viewbox="0 0 16 16">
     <path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)">
     </path>
    </symbol>
    <symbol id="icon-arrow-down" viewbox="0 0 16 16">
     <path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-left" viewbox="0 0 16 16">
     <path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-right" viewbox="0 0 16 16">
     <path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-sub" viewbox="0 0 16 16">
     <path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-arrow-up" viewbox="0 0 16 16">
     <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-article" viewbox="0 0 18 18">
     <path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-audio" viewbox="0 0 18 18">
     <path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-block" viewbox="0 0 24 24">
     <path d="m0 0h24v24h-24z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-book" viewbox="0 0 18 18">
     <path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-broad" viewbox="0 0 24 24">
     <path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)">
     </path>
    </symbol>
    <symbol id="icon-calendar" viewbox="0 0 18 18">
     <path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-cart" viewbox="0 0 18 18">
     <path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z">
     </path>
    </symbol>
    <symbol id="icon-chevron-less" viewbox="0 0 10 10">
     <path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)">
     </path>
    </symbol>
    <symbol id="icon-chevron-more" viewbox="0 0 10 10">
     <path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-chevron-right" viewbox="0 0 10 10">
     <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)">
     </path>
    </symbol>
    <symbol id="icon-circle-fill" viewbox="0 0 16 16">
     <path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-circle" viewbox="0 0 16 16">
     <path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-citation" viewbox="0 0 18 18">
     <path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-close" viewbox="0 0 16 16">
     <path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-collections" viewbox="0 0 18 18">
     <path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-compare" viewbox="0 0 18 18">
     <path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download-file" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-download" viewbox="0 0 16 16">
     <path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-editors" viewbox="0 0 18 18">
     <path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-email" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-error" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-ethics" viewbox="0 0 18 18">
     <path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-expand">
     <path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-explore" viewbox="0 0 18 18">
     <path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-filter" viewbox="0 0 16 16">
     <path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z">
     </path>
    </symbol>
    <symbol id="icon-home" viewbox="0 0 18 18">
     <path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-image" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-info" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-institution" viewbox="0 0 18 18">
     <path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-location" viewbox="0 0 18 18">
     <path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-minus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-newsletter" viewbox="0 0 18 18">
     <path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-orcid" viewbox="0 0 18 18">
     <path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-plus" viewbox="0 0 16 16">
     <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-print" viewbox="0 0 18 18">
     <path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-search" viewbox="0 0 22 22">
     <path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-facebook" viewbox="0 0 24 24">
     <path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-twitter" viewbox="0 0 24 24">
     <path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-social-youtube" viewbox="0 0 24 24">
     <path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-subject-medicine" viewbox="0 0 18 18">
     <path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-success" viewbox="0 0 18 18">
     <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-table" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick-circle" viewbox="0 0 24 24">
     <path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-tick" viewbox="0 0 16 16">
     <path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-update" viewbox="0 0 18 18">
     <path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-upload" viewbox="0 0 18 18">
     <path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-video" viewbox="0 0 18 18">
     <path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-warning" viewbox="0 0 18 18">
     <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-altmetric">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm-1.886 9.684-1.101 1.845a1 1 0 0 1-.728.479l-.13.008H3.056a9.001 9.001 0 0 0 17.886 0l-4.564-.001-2.779 4.156c-.454.68-1.467.55-1.758-.179l-.038-.113-1.69-6.195ZM12 3a9.001 9.001 0 0 0-8.947 8.016h4.533l2.017-3.375c.452-.757 1.592-.6 1.824.25l1.73 6.345 1.858-2.777a1 1 0 0 1 .707-.436l.124-.008h5.1A9.001 9.001 0 0 0 12 3Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-checklist-banner" viewbox="0 0 56.69 56.69">
     <path d="M0 0h56.69v56.69H0z" style="fill:none">
     </path>
     <clippath id="b">
      <use style="overflow:visible" xlink:href="#a">
      </use>
     </clippath>
     <path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round">
     </path>
     <path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round">
     </path>
    </symbol>
    <symbol id="icon-chevron-down" viewbox="0 0 16 16">
     <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)">
     </path>
    </symbol>
    <symbol id="icon-citations">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-eds-checklist" viewbox="0 0 32 32">
     <path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-external-link-medium" viewbox="0 0 24 24">
     <path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z">
     </path>
    </symbol>
    <symbol id="icon-eds-i-info-filled-medium" viewbox="0 0 24 24">
     <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z">
     </path>
    </symbol>
    <symbol id="icon-eds-menu" viewbox="0 0 24 24">
     <path d="M21.09 5c.503 0 .91.448.91 1s-.407 1-.91 1H2.91C2.406 7 2 6.552 2 6s.407-1 .91-1h18.18Zm-3.817 6c.401 0 .727.448.727 1s-.326 1-.727 1H2.727C2.326 13 2 12.552 2 12s.326-1 .727-1h14.546Zm3.818 6c.502 0 .909.448.909 1s-.407 1-.91 1H2.91c-.503 0-.91-.448-.91-1s.407-1 .91-1h18.18Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-search" viewbox="0 0 24 24">
     <path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-eds-small-arrow-right" viewbox="0 0 16 16">
     <g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035">
      </path>
     </g>
    </symbol>
    <symbol id="icon-eds-user-single" viewbox="0 0 24 24">
     <path d="M12 12c5.498 0 10 4.001 10 9a1 1 0 0 1-2 0c0-3.838-3.557-7-8-7s-8 3.162-8 7a1 1 0 0 1-2 0c0-4.999 4.502-9 10-9Zm0-11a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm0 2a3 3 0 1 1 0 6 3 3 0 0 1 0-6Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-email-new" viewbox="0 0 24 24">
     <path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z">
     </path>
    </symbol>
    <symbol id="icon-expand-image" viewbox="0 0 18 18">
     <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-github" viewbox="0 0 100 100">
     <path clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill-rule="evenodd">
     </path>
    </symbol>
    <symbol id="icon-mentions">
     <g fill-rule="evenodd" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <path d="M22 15.255A9.373 9.373 0 0 1 8.745 2L22 15.255ZM15.477 8.523l4.215-4.215">
      </path>
      <path d="m7 13-5 9h10l-1-5">
      </path>
     </g>
    </symbol>
    <symbol id="icon-metrics-accesses">
     <path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM7.708 13.308c2.004 0 3.969 1.198 5.802 2.995l.23.23a2.285 2.285 0 0 1 .009 3.233C11.853 21.693 9.799 23 7.707 23c-2.091 0-4.14-1.305-6.033-3.226a2.285 2.285 0 0 1-.007-3.233c1.9-1.93 3.949-3.233 6.04-3.233Zm0 2c-1.396 0-3.064 1.062-4.623 2.644a.285.285 0 0 0 .007.41C4.642 19.938 6.311 21 7.707 21c1.397 0 3.069-1.065 4.623-2.644a.285.285 0 0 0 0-.404l-.23-.229c-1.487-1.451-3.064-2.415-4.393-2.415Zm-.036 1.077a1.77 1.77 0 1 1 .126 3.537 1.77 1.77 0 0 1-.126-3.537Zm.072 1.538a.23.23 0 1 0-.017.461.23.23 0 0 0 .017-.46Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-metrics">
     <path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z" fill-rule="nonzero">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-left">
     <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z">
     </path>
    </symbol>
    <symbol id="icon-springer-arrow-right">
     <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z">
     </path>
    </symbol>
    <symbol id="icon-submit-open" viewbox="0 0 16 17">
     <path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero">
     </path>
    </symbol>
   </svg>
  </div>
  <script nomodule="true" src="/oscar-static/js/app-es5-bundle-774ca0a0f5.js">
  </script>
  <script src="/oscar-static/js/app-es6-bundle-047cc3c848.js" type="module">
  </script>
  <script nomodule="true" src="/oscar-static/js/global-article-es5-bundle-e58c6b68c9.js">
  </script>
  <script src="/oscar-static/js/global-article-es6-bundle-c14b406246.js" type="module">
  </script>
  <div class="c-cookie-banner">
   <div class="c-cookie-banner__container">
    <p>
     This website sets only cookies which are necessary for it to function. They are used to enable core functionality such as security, network management and accessibility. These cookies cannot be switched off in our systems. You may disable these by changing your browser settings, but this may affect how the website functions. Please view our privacy policy for further details on how we process your information.
     <button class="c-cookie-banner__dismiss">
      Dismiss
     </button>
    </p>
   </div>
  </div>
 </body>
</html>
