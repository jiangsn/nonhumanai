VGPNN Architecture Left: given a single input video , a spatio-temporal pyramid is constructed and an output video  is generated coarse-to-fine. At each scale, VPNN module (right) is applied to transfer an initial guess  to the output  which shares the same space-time patch distribution as the input . At the coarsest scale, noise is injected to induce spatial and temporal randomness. Right: VPNN module gets as input query, key and value RGB videos (QKV respectively) and outputs an RGB video. Q and K can be concatenated to additional auxiliary channels. (a) Inputs are unfolded to patches (each position holds a concatenation of neighboring positions); (b) Each patch in Q finds its nearest neighbor patch in K. This is achieved by solving the NNF using PatchMatch [10]; (c) Each patch in Q is replaced with a patch from V, according to the correspondences found in stage (b); (d) Resulting patches are “folded” back to an RGB video output (using the median of all suggested votes).