1 Introduction:
We present the task of model rewriting, which aims to add, remove, and alter the semantic and physical rules of a pretrained deep network. While modern image editing tools achieve a user-specified goal by manipulating individual input images, we enable a user to synthesize an unbounded number of new images by editing a generative model to carry out modified rules. For example in Fig. 1, we apply a succession of rule changes to edit a StyleGANv2 model 
[40] pretrained on LSUN church scenes 
[77]. The first change removes watermark text patterns (a); the second adds crowds of people in front of buildings (b); the third replaces the rule for drawing tower tops with a rule that draws treetops (c), creating a fantastical effect of trees growing from towers. Because each of these modifications changes the generative model, every single change affects a whole category of images, removing all watermarks synthesized by the model, arranging people in front of many kinds of buildings, and creating tree-towers everywhere. The images shown are samples from an endless distribution. Fig. 1.Rewriting the weights of a generator to change generative rules. Rules can be changed to (a) remove patterns such as watermarks; (b) add objects such as people; or (c) replace definitions such as making trees grow out of towers. Instead of editing individual images, our method edits the generator, so an infinite set of images can be potentially synthesized and manipulated using the altered rules. 
But why is rewriting a deep generative model useful? A generative model enforces many rules and relationships within the generated images. From a purely scientific perspective, the ability to edit such a model provides insights about what the model has captured and how the model can generalize to unseen scenarios. At a practical level, deep generative models are increasingly useful for image and video synthesis 
[12, 34, 54, 83]. In the future, entire image collections, videos, or virtual worlds could potentially be produced by deep networks, and editing individual images or frames will be needlessly tedious. Instead, we would like to provide authoring tools for modifying the models themselves. With this capacity, a set of similar edits could be effortlessly transferred to many images at once. A key question is how to edit a deep generative model. The computer vision community has become accustomed to training models using large data sets and expensive human annotations, but we wish to enable novice users to easily modify and customize a deep generative model without the training time, domain expertise, and computational cost of large-scale machine learning. In this paper, we present a new method that can locate and change a specific semantic relationship within a model. In particular, we show how to generalize the idea of a linear associative memory 
[45] to a nonlinear convolutional layer of a deep generator. Each layer stores latent rules as a set of key-value relationships over hidden features. Our constrained optimization aims to add or edit one specific rule within the associative memory while preserving the existing semantic relationships in the model as much as possible. We achieve it by directly measuring and manipulating the model’s internal structure, without requiring any new training data. We use our method to create several visual editing effects, including the addition of new arrangements of objects in a scene, systematic removal of undesired output patterns, and global changes in the modeling of physical light. Our method is simple and fast, and it does not require a large set of annotations: a user can alter a learned rule by providing a single example of the new rule or a small handful of examples. We demonstrate a user interface for novice users to modify specific rules encoded in the layers of a GAN interactively. Finally, our quantitative experiments on several datasets demonstrate that our method outperforms several fine-tuning baselines as well as image-based edit transfer methods, regarding both photorealism and desirable effects. Our code, data, and user interface are available at our website. 

2 Related Work:
Deep Image Manipulation. Image manipulation is a classic problem in computer vision, image processing, and computer graphics. Common operations include color transfer 
[50, 62], image deformation 
[64, 72], object cloning 
[11, 59], and patch-based image synthesis 
[5, 19, 28]. Recently, thanks to rapid advances of deep generative models 
[25, 30, 42], learning-based image synthesis and editing methods have become widely-used tools in the community, enabling applications such as manipulating the semantics of an input scene 
[6, 57, 69], image colorization 
[33, 49, 80, 82], photo stylization 
[24, 36, 51, 53], image-to-image translation 
[9, 31, 34, 52, 70, 83], and face editing and synthesis 
[23, 55, 60]. While our user interface is inspired by previous interactive systems, our goal is not to manipulate and synthesize a single image using deep models. Instead, our work aims to manipulate the structural rules of the model itself, creating an altered deep network that can produce countless new images following the modified rules. Edit Transfer and Propagation. Edit transfer methods propagate pixel edits to corresponding locations in other images of the same object or adjacent frames in the same video 
[2, 13, 14, 20, 27, 74, 78]. These methods achieve impressive results but are limited in two ways. First, they can only transfer edits to images of the same instance, as image alignment between different instances is challenging. Second, the edits are often restricted to color transfer or object cloning. In contrast, our method can change context-sensitive rules that go beyond pixel correspondences (Sect. 5.3). In Sect. 5.1, we compare to an edit propagation method based on state-of-the-art alignment algorithm, Neural Best-Buddies 
[1]. Interactive Machine Learning. systems aim to improve training through human interaction in labeling 
[15, 21, 65], or by allowing a user to to aid in the model optimization process via interactive feature selection 
[18, 26, 47, 61] or model and hyperparameter selection 
[35, 58]. Our work differs from these previous approaches because rather than asking for human help to attain a fixed objective, we enable a user to solve novel creative modeling tasks, given a pre-trained model. Model rewriting allows a user to create a network with new rules that go beyond the patterns present in the training data. Transfer Learning and Model Fine-Tuning. Transfer learning adapts a learned model to unseen learning tasks, domains, and settings. Examples include domain adaptation 
[63], zero-shot or few-shot learning 
[48, 68], model pre-training and feature learning 
[16, 75, 79], and meta-learning 
[4, 8, 22]. Our work differs because instead of extending the training process with more data or annotations, we enable the user to directly change the behavior of the existing model through a visual interface. Recently, several methods 
[6, 67, 71] propose to train or fine-tune an image generation model to a particular image for editing and enhancement applications. Our goal is different, as we aim to identify and change rules that can generalize to many different images instead of one. 

3 Method:
To rewrite the rules of a trained generative model, we allow users to specify a handful of model outputs that they wish to behave differently. Based on this objective, we optimize an update in model weights that generalizes the requested change. In Sect. 3, we derive and discuss this optimization. In Sect. 4, we present the user interface that allows the user to interactively define the objective and edit the model. Section 3.1 formulates our objective on how to add or modify a specific rule while preserving existing rules. We then consider this objective for linear systems and connect it to a classic technique—associative memory 
[3, 43, 44] (Sect. 3.2); this perspective allows us to derive a simple update rule (Sect. 3.3). Finally, we apply the solution to the nonlinear case and derive our full algorithm (Sect. 3.4). 

3.1 Objective: Changing a Rule with Minimal Collateral Damage:
Given a pre-trained generator \(G({z}; \theta _0)\) with weights \(\theta _0\), we can synthesize multiple images \({{x}_i}= G({{z}_i}; \theta _0)\), where each image is produced by a latent code \({{z}_i}\). Suppose we have manually created desired changes \({{{x}_*}_i}\) for those cases. We would like to find updated weights \(\theta _1\) that change a computational rule to match our target examples \({{{x}_*}_i}\approx G({{z}_i}; {\theta _1})\), while minimizing interference with other behavior: $$\begin{aligned} \theta _1 = \arg \min _{\theta } { \mathcal {L}_{\textsf {smooth}}}(\theta ) + \lambda { \mathcal {L}_{\textsf {constraint}}}(\theta ), \end{aligned}$$
(1)

$$\begin{aligned} { \mathcal {L}_{\textsf {smooth}}}(\theta ) \triangleq \mathbb {E}_{{z}}\left[ \ell ( G({z}; \theta _0) , G({z}; \theta ) ) \right] , \end{aligned}$$
(2)

$$\begin{aligned} { \mathcal {L}_{\textsf {constraint}}}(\theta ) \triangleq \sum _i \ell ( {{{x}_*}_i}, G({{z}_i}; \theta ) ). \end{aligned}$$
(3)
A traditional solution to the above problem is to jointly optimize the weighted sum of \({ \mathcal {L}_{\textsf {smooth}}}\) and \({ \mathcal {L}_{\textsf {constraint}}}\) over \(\theta \), where \(\ell ( \cdot )\) is a distance metric that measures the perceptual distance between images 
[17, 36, 81]. Unfortunately, this standard approach does not produce a generalized rule within G, because the large number of parameters \(\theta \) allow the generator to quickly overfit the appearance of the new examples without good generalization; we evaluate this approach in Sect. 5. However, the idea becomes effective with two modifications: (1) instead of modifying all of \(\theta \), we reduce the degrees of freedom by modifying weights W at only one layer, and (2) for the objective function, we directly minimize distance in the output feature space of that same layer. Given a layer L, we use \({k}\) to denote the features computed by the first \(L-1\) fixed layers of G, and then write \({v}=f({k};W_0)\) to denote the computation of layer L itself, with pretrained weights \(W_0\). For each exemplar latent \({{z}_i}\), these layers produce features \({{{k}_*}_i}\) and \({{{v}_*}_i}= f({{{k}_*}_i}; W_0)\). Now suppose, for each target example \({{{x}_*}_i}\), the user has manually created a feature change \({{{v}_*}_i}\). (A user interface to create target feature goals is discussed in Sect. 4.) Our objective becomes: $$\begin{aligned} W_1 = \arg \min _{W} { \mathcal {L}_{\textsf {smooth}}}(W) + \lambda { \mathcal {L}_{\textsf {constraint}}}(W), \end{aligned}$$
(4)

$$\begin{aligned} { \mathcal {L}_{\textsf {smooth}}}(W) \triangleq \mathbb {E}_{{k}}\left[ \; || f({k}; W_0) - f({k}; W) ||^2 \; \right] , \end{aligned}$$
(5)

$$\begin{aligned} { \mathcal {L}_{\textsf {constraint}}}(W) \triangleq \sum _i || {{{v}_*}_i}- f({{{k}_*}_i}; W)||^2, \end{aligned}$$
(6)
where \(||\cdot ||^2\) denotes the L2 loss. Even within one layer, the weights W contain many parameters. But the degrees of freedom can be further reduced to constrain the change to a specific direction that we will derive; this additional directional constraint will allow us to create a generalized change from a single \((k_*, v_*)\) example. To understand the constraint, it is helpful to interpret a single convolutional layer as an associative memory, a classic idea that we briefly review next. 

3.2 Viewing a Convolutional Layer as an Associative Memory:
Any matrix W can be used as an associative memory 
[44] that stores a set of key-value pairs \(\{({{k}_i}, {{v}_i})\}\) that can be retrieved by matrix multiplication: $$\begin{aligned} {{v}_i}\approx W {{k}_i}. \end{aligned}$$
(7)
The use of a matrix as a linear associative memory is a foundational idea in neural networks 
[3, 43, 44]. For example, if the keys \(\{{{k}_i}\}\) form a set of mutually orthogonal unit-norm vectors, then an error-free memory can be created as $$\begin{aligned} W_{\text {orth}} \triangleq \sum _{i} {{v}_i}{{k}_i}^T. \end{aligned}$$
(8)
Since \({{k}_i}^T {k}_j = 0\) whenever \(i \ne j\), all the irrelevant terms cancel when multiplying by \({k}_j\), and we have \(W_{\text {orth}} \; {k}_j = {v}_j\). A new value can be stored by adding \({v}_* {{k}_*}^T\) to the matrix as long as \({k}_*\) is chosen to be orthogonal to all the previous keys. This process can be used to store up to N associations in an \(M\times N\) matrix. Fig. 2.(a) A generator consists of a sequence of layers; we focus on one particular layer L. (b) The convolutional weights W serve an associative memory, mapping keys k to values v. The keys are single-location input features, and the values are patterns of output features. (c) A key will tend to match semantically similar contexts in different images. Shown are locations of generated images that have features that match a specific k closely. (d) A value renders shapes in a small region. Here the effect of a value v is visualized by rendering features at one location alone, with features at other locations set to zero. Image examples are taken from a StyleGANv2 model trained on LSUN outdoor church scenes. 
Figure 2 views the weights of one convolutional layer in a generator as an associative memory. Instead of thinking of the layer as a collection of convolutional filtering operations, we can think of the layer as a memory that associates keys to values. Here each key \({k}\) is a single-location feature vector. The key is useful because, in our trained generator, the same key will match many semantically similar locations across different images, as shown in Fig. 2c. Associated with each key, the map stores an output value \({v}\) that will render an arrangement of output shapes. This output can be visualized directly by rendering the features in isolation from neighboring locations, as shown in Fig. 2d. For example, consider a layer that transforms a 512-channel featuremap into a 256-channel featuremap using a \(3\times 3\) convolutional kernel; the weights form a \(256\times 512\times 3\times 3\) tensor. For each key \(k\in \mathbb {R}^{512}\), our layer will recall a value \(v \in \mathbb {R}^{256\times 3\times 3} = \mathbb {R}^{2304}\) representing a \(3\times 3\) output pattern of 256-channel features, flattened to a vector, as \(v=Wk\). Our interpretation of the layer as an associative memory does not change the computation: the tensor is simply reshaped and treated as a dense rectangular matrix \(W \in \mathbb {R}^{(256 \times 3 \times 3) \times 512}\), whose job is to map keys \(k\in \mathbb {R}^{512}\) to values \(v\in \mathbb {R}^{2304}\), via Eq. 7. Arbitrary Nonorthogonal Keys. In classic work, Kohonen 
[45] observed that an associative memory can support more than N nonorthogonal keys \(\{{{k}_i}\}\) if instead of requiring exact equality \({{v}_i}= W {{k}_i}\), we choose \(W_0\) to minimize error: $$\begin{aligned} W_0 \triangleq \mathop {\text {arg min}}\limits _{W} \sum _i \, || {{v}_i}- W {{k}_i}||^2. \end{aligned}$$
(9)
To simplify notation, let us assume a finite set of pairs \(\{({{k}_i}, {{v}_i})\}\) and collect keys and values into matrices K and V whose i-th column is the i-th key or value: $$\begin{aligned} K&\triangleq \left[ k_1 | k_2 | \cdots | k_i | \cdots \right] , \end{aligned}$$
(10)

$$\begin{aligned} V&\triangleq \left[ v_1 | v_2 | \cdots | v_i | \cdots \right] . \end{aligned}$$
(11)
The minimization (Eq. 9) is the standard linear least-squares problem. A unique minimal solution can be found by solving for \(W_0\) using the normal equation \(W_0 K K^T = V K^T\), or equivalently by using the pseudoinverse \(W_0 = V K^+\). 

3.3 Updating W to Insert a New Value:
Now, departing from Kohonen 
[45], we ask how to modify \(W_0\). Suppose we wish to overwrite a single key to assign a new value \(k_* \rightarrow v_*\) provided by the user. After this modification, our new matrix \(W_1\) should satisfy two conditions: $$\begin{aligned} W_1 = \mathop {\text {arg min}}\limits _W || V - W K ||^2, \end{aligned}$$
(12)

$$\begin{aligned} \text {subject to} \; v_* = W_1 k_*. \end{aligned}$$
(13)
That is, it should store the new value; and it should continue to minimize error in all the previously stored values. This forms a constrained linear least-squares (CLS) problem which can be solved exactly as \(W_1 K K^T = V K^T + \varLambda \,{k_*}^T\), where the vector \(\varLambda \in \mathbb {R}^m\) is determined by solving the linear system with the constraint in Eq. 13 (see Appendix B). Because \(W_0\) satisfies the normal equations, we can expand \(V K^T\) in the CLS solution and simplify: $$\begin{aligned} W_1 K K^T&= W_0 K K^T + \varLambda \,{k_*}^T \end{aligned}$$
(14)

$$\begin{aligned} W_1&= W_0 + \varLambda (C^{-1}k_*)^{T} \end{aligned}$$
(15)
Above, we have written \(C \triangleq K K^T\) as the second moment statistics. (C is symmetric; if K has zero mean, C is the covariance.) Now Eq. 15 has a simple form. Since \(\varLambda \in \mathbb {R}^m\) and \((C^{-1}k_*)^{T} \in \mathbb {R}^n\) are simple vectors, the update \(\varLambda (C^{-1}k_*)^{T}\) is a rank-one matrix with rows all multiples of the vector \((C^{-1}k_*)^T\). Equation 15 is interesting for two reasons. First, it shows that enforcing the user’s requested mapping \(k_* \rightarrow v_*\) transforms the soft error minimization objective (12) into the hard constraint that the weights be updated in a particular straight-line direction \(C^{-1}k_*\). Second, it reveals that the update direction is determined only by the overall key statistics and the specific targeted key \(k_*\). The covariance C is a model constant that can be pre-computed and cached, and the update direction is determined by the key regardless of any stored value. Only \(\varLambda \), which specifies the magnitude of each row change, depends on the target value \(v_*\). 

3.4 Generalize to a Nonlinear Neural Layer:
In practice, even a single network block contains several non-linear components such as a biases, ReLU, normalization, and style modulation. Below, we generalize our procedure to the nonlinear case where the solution to \(W_1\) cannot be calculated in a closed form. We first define our update direction: $$\begin{aligned} d \triangleq C^{-1}k_*. \end{aligned}$$
(16)
Then suppose we have a non-linear neural layer f(k; W) which follows the linear operation W with additional nonlinear steps. Since the form of Eq. 15 is sensitive to the rowspace of W and insensitive to the column space, we can use the same rank-one update form to constrain the optimization of \(f(k_*; W) \approx v_*\). Therefore, in our experiments, when we update a layer to insert a new key \(k_* \rightarrow v_*\), we begin with the existing \(W_0\), and we perform an optimization over the rank-one subspace defined by the row vector \(d^T\) from Eq. 16. That is, in the nonlinear case, we update \(W_1\) by solving the following optimization: $$\begin{aligned} \varLambda _1 = \mathop {\text {arg min}}\limits _{\varLambda \in \mathbb {R}^M} || v_* - f(k_*; W_0 + \varLambda \,d^T) ||. \end{aligned}$$
(17)
Once \(\varLambda _1\) is computed, we update the weight as \(W_1 = W_0 + \varLambda _1 d^T\). Our desired insertion may correspond to a change of more than one key at once, particularly if our desired target output forms a feature map patch \(V_*\) larger than a single convolutional kernel, i.e., if we wish to have \(V_* = f(K_*; W_1)\) where \(K_*\) and \(V_*\) cover many pixels. To alter S keys at once, we can define the allowable deltas as lying within the low-rank space spanned by the \(N\times S\) matrix \(D_S\) containing multiple update directions \(d_i = C^{-1}K_{*i}\), indicating which entries of the associative map we wish to change. $$\begin{aligned} \varLambda _S = \mathop {\text {arg min}}\limits _{\varLambda \in \mathbb {R}^{M\times S}} || V_* - f(K_*; W_0 + \varLambda \,{D_S}^T) ||, \end{aligned}$$
(18)

$$\begin{aligned} \text {where}\;D_S \triangleq \left[ d_{1} | d_{2} | \cdots | d_{i} | \cdots | d_{S} \right] . \end{aligned}$$
(19)
We can then update the layer weights using \(W_S = W_0 + \varLambda _S {D_S}^T\). The change can be made more specific by reducing the rank of \(D_S\); details are discussed Appendix D. To directly connect this solution to our original objective (Eq. 6), we note that the constrained optimization can be solved using projected gradient descent. That is, we relax Eq. 18 and use optimization to minimize \(\mathop {\text {arg min}}\limits \nolimits _W ||V_* - f(K_*; W)||\); then, to impose the constraint, after each optimization step, project W into into the subspace \(W_0 + \varLambda _S {D_S}^T\). Fig. 3.The Copy-Paste-Context interface for rewriting a model. (a) Copy: the user uses a brush to select a region containing an interesting object or shape, defining the target value \(V_*\). (b) Paste: The user positions and pastes the copied object into a single target image. This specifies the \(K_* \rightarrow V_*\) pair constraint. (c) Context: To control generalization, the user selects target regions in several images. This establishes the updated direction d for the associative memory. (d) The edit is applied to the model, not a specific image, so newly generated images will always have hats on top of horse heads. (e) The change has generalized to a variety of different types of horses and poses (see more in Appendix A). 


4 User Interface:
To make model rewriting intuitive for a novice user, we build a user interface that provides a three-step rewriting process: Copy, Paste, and Context. Copy and Paste allow the user to copy an object from one generated image to another. The user browses through a collection of generated images and highlights an area of interest to copy; then selects a generated target image and location for pasting the object. For example, in Fig. 3a, the user selects a helmet worn by a rider and then pastes it in Fig. 3b on a horse’s head. Our method downsamples the user’s copied region to the resolution of layer L and gathers the copied features as the target value \(V_*\). Because we wish to change not just one image, but the model rules themselves, we treat the pasted image as a new rule \(K_* \rightarrow V_*\) associating the layer \(L-1\) features \(K_*\) of the target image with the newly copied layer L values \(V_*\) that will govern the new appearance. Context Selection allows a user to specify how this change will be generalized, by pointing out a handful of similar regions that should be changed. For example, in Fig. 3b, the user has selected heads of different horses. We collect the layer \(L-1\) features at the location of the context selections as a set of relevant K that are used to determine the weight update direction d via Eq. 16. Generalization improves when we allow the user to select several context regions to specify the update direction (see Table 1); in Fig. 3, the four examples are used to create a single d. Appendix D discusses this rank reduction. Applying one rule change on a StyleGANv2 model requires about eight seconds on a single Titan GTX GPU. Please check out the demo video of our interface. 

5 Results:
We test model rewriting with three editing effects. First, we add new objects into the model, comparing results to several baseline methods. Then, we use our technique to erase objects using a low-rank change; we test this method on the challenging watermark removal task. Finally, we invert a rule for a physical relationship between bright windows and reflections in a model. Fig. 4.Adding and replacing objects in three different settings. (a) Replacing domes with an angular peaked spire causes peaked spires to be used throughout the model. (b) Replacing domes with trees can generate images unlike any seen in a training set. (c) Replacing closed lips with an open-mouth smile produces realistic open-mouth smiles. For each case, we show the images generated by an unchanged model, then the edit propagation results, with and without blending. Our method is shown in the last row. 


5.1 Putting Objects into a New Context:
Here we test our method on several specific model modifications. In a church generator, the model edits change the shape of domes to spires, and change the domes to trees, and in a face generator, we add open-mouth smiles. Examples of all the edits are shown in Fig. 4. Table 1. Editing a StyleGANv2 
[40] FFHQ 
[39] model to produce smiling faces in \(n=10,000\) images. To quantify the efficacy of the change, we show the percentage of smiling faces among the modified images, and we report the LPIPS distance on masked images to quantify undesired changes. For realism, workers make \(n=1,000\) pairwise judgements comparing images from other methods to ours.
Table 2. We edit a StyleGANv2 
[40] LSUN church 
[77] model to replace domes with spires/trees in \(n=10,000\) images. To quantify efficacy, we show the percentage of dome category pixels changed to the target category, determined by a segmenter 
[73]. To quantify undesired changes, we report LPIPS distance between edited and unchanged images, in non-dome regions. For realism, workers make \(n=1,000\) pairwise judgements comparing images from other methods to ours.
Quantitative Evaluation. In Tables 1 and 2, we compare the results to several baselines. We compare our method to the naive approach of fine-tuning all weights according to Eq. 3, as well as the method of optimizing all the weights of a layer without constraining the direction of the change, as in Eq. 6, and to a state-of-the-art image alignment algorithm, Neural Best-Buddies (NBB 
[1]), which is used to propagate an edit across a set of similar images by compositing pixels according to identified sparse correspondences. To transfer an edit from a target image, we use NBB and Moving Least Squares 
[64] to compute a dense correspondence between the source image we would like to edit and the original target image. We use this dense correspondence field to warp the masked target into the source image. We test both direct copying and Laplace blending. For each setting, we measure the efficacy of the edits on a sample of 10, 000 generated images, and we also quantify the undesired changes made by each method. For the smiling edit, we measure efficacy by counting images classified as smiling by an attribute classifier 
[66], and we also quantify changes made in the images outside the mouth region by masking lips using a face segmentation model 
[84] and using LPIPS 
[81] to quantify changes. For the dome edits, we measure how many dome pixels are judged to be changed to non-domes by a segmentation model 
[73], and we measure undesired changes outside dome areas using LPIPS. We also conduct a user study where users are asked to compare the realism of our edited output to the same image edited using baseline methods. We find that our method produces more realistic outputs that are more narrowly targeted than the baseline methods. For the smile edit, our method is not as aggressive as baseline methods at introducing smiles, but for the dome edits, our method is more effective than baseline methods at executing the change. Our metrics are further discussed in Appendix C. 

5.2 Removing Undesired Features:
Here we test our method on the removal of undesired features. Figure 5a shows several examples of images output by a pre-trained StyleGANv2 church model. This model occasionally synthesizes images with text overlaid in the middle and the bottom resembling stock-photo watermarks in the training set. The GAN Dissection study 
[7] has shown that some objects can be removed from a generator by zeroing the units that best match those objects. To find these units, we annotated the middle and bottom text regions in ten generated images, and we identified a set of 60 units that are most highly correlated with features in these regions. Zeroing the most correlated 30 units removes some of the text, but leaves much bottom text unremoved, as shown in Fig. 5b. Zeroing all 60 units reduces more of the bottom text but begins to alter the main content of the images, as shown in Fig. 5c. Fig. 5.Removing watermarks from StyleGANv2 
[40] LSUN church 
[77] model. (a) Many images generated by this model include transparent watermarks in the center or text on the bottom. (b) Using GAN Dissection 
[7] to zero 30 text-specific units removes middle but not bottom text cleanly. (c) Removing 60 units does not fully remove text, and distorts other aspects of the image. (b) Applying our method to create a rank-1 change erases both middle and bottom text cleanly. 
Table 3. Visible watermark text produced by StyleGANv2 church model in \(n=1000\) images, without modification, with sets of units zeroed (using the method of GAN Dissection), and using our method to apply a rank-one update.
For our method, we use the ten user-annotated images as a context to create a rank-one constraint direction d for updating the model, and as an optimization target \(K_* \rightarrow V_*\), we use one successfully removed watermark from the setting shown in Fig. 5b. Since our method applies a narrow rank-1 change constraint, it would be expected to produce a loose approximation of the rank-30 change in the training example. Yet we find that it has instead improved specificity and generalization of watermark removal, removing both middle and bottom text cleanly while introducing few changes in the main content of the image. We repeat the process for 1000 images and tabulate the results in Table 3. Fig. 6.Inverting a single semantic rule within a model. At the top row, a Progressive GAN 
[38] trained on LSUN kitchens 
[77] links windows to reflections: when windows are added by manipulating intermediate features identified by GAN Dissection 
[7], reflections appear on the table. In the bottom row, one rule has been changed within the model to invert the relationship between windows and reflections. Now adding windows decreases reflections and vice-versa. 


5.3 Changing Contextual Rules:
In this experiment, we find and alter a rule that determines the illumination interactions between two objects at different locations in an image. State-of-the-art generative models learn to enforce many relationships between distant objects. For example, it has been observed 
[6] that a kitchen-scene Progressive GAN model 
[38] enforces a relationship between windows on walls and specular reflections on tables. When windows are added to a wall, reflections will be added to shiny tabletops, and vice-versa, as illustrated in the first row of Fig. 6. Thus the model contains a rule that approximates the physical propagation of light in a scene. In the following experiment, we identified an update direction that allows us to change this model of light reflections. Instead of specifying an objective that copies an object from one context to another, we used a similar tool to specify a \(K_*\rightarrow V_*\) objective that swaps bright tabletop reflections with dim reflections on a set of 15 pairs of scenes that are identical other than the presence or absence of bright windows. To identify a rank-one change direction d, we used projected gradient descent, as described in Sect. 3.4, using SVD to limit the change to rank one during optimization. The results are shown in the second row of Fig. 6. The modified model differs from the original only in a single update direction of a single layer, but it inverts the relationship between windows and reflections: when windows are added, reflections are reduced, and vice-versa. 

6 Discussion:
Machine learning requires data, so how can we create effective models for data that do not yet exist? Thanks to the rich internal structure of recent GANs, in this paper, we have found it feasible to create such models by rewriting the rules within existing networks. Although we may never have seen a tree sprouting from a tower, our network contains rules for both trees and towers, and we can easily create a model that connects those compositional rules to synthesize an endless distribution of images containing the new combination. The development of sophisticated generative models beyond the image domain, such as the GPT-3 language model 
[10] and WaveNet for audio synthesis 
[56], means that it will be increasingly attractive to rewrite rules within other types of models as well. After training on vast datasets, large-scale deep networks have proven to be capable of representing an extensive range of different styles, sentiments, and topics. Model rewriting provides an avenue for using this structure as a rich medium for creating novel kinds of content, behavior, and interaction.