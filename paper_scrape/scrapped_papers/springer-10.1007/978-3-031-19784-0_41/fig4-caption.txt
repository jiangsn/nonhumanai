Video pipeline. (a) a pretrained and fixed neural layered atlas model [16] is used as a “video renderer”, which consists of: a set of 2D atlases, mapping functions from pixels to the atlases (and per-pixel fg/bg opacity values). Our framework takes in an atlas \(I_A\) and a target text prompt (e.g., “rusty car”), and trains a generator (b) to output an atlas edit layer \(\mathcal {E}_A\) (c). The edited atlas (d) is rendered to frames using the pre-trained mapping network \(\mathbb {M}\), and then (e) composited over the original video.