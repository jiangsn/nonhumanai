Image pipeline. Our method consists of a generator trained on a single input image and target text prompts. Left: an internal image-text dataset of diverse training examples is created by augmenting both image and text (see Sect. 3.1). Right: Our generator takes as input an image and outputs an edit RGBA layer (color+opacity), which is composited over the input to form the final edited image. The generator is trained by minimizing several loss terms that are defined in CLIP space, and include: \(\mathcal {L}_{\textsf{comp}}\), applied to the composite, and \(\mathcal {L}_\textsf{screen}\), applied to the edit layer (when composited over a green background). We apply additional augmentations before CLIP (Sect. 3.1) (Color figure online)