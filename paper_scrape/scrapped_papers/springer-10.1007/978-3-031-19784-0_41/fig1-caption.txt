Text2LIVE performs semantic, localized edits to real-world images (b), or videos (c). Our key idea is to generate an edit layer–RGBA image representing the target edit when composited over the input (a). This allows us to use text to guide not only the final composite, but also the edit layer itself (target text prompts are shown above each image). Our edit layers are synthesized by training a generator on a single input, without relying on user-provided masks or a pretrained generator.